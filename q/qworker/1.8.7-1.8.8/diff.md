# Comparing `tmp/qworker-1.8.7-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.zip` & `tmp/qworker-1.8.8-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.zip`

## zipinfo {}

```diff
@@ -1,37 +1,37 @@
-Zip file size: 318203 bytes, number of entries: 35
-drwxr-xr-x  2.0 unx        0 b- stor 23-Jun-09 12:04 qw/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Jun-09 12:04 qworker.libs/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Jun-09 12:04 qworker-1.8.7.dist-info/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Jun-09 12:04 qw/wrappers/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Jun-09 12:04 qw/utils/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Jun-09 12:04 qw/queues/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Jun-09 12:04 qw/executor/
--rw-r--r--  2.0 unx      137 b- defN 23-Jun-09 12:04 qw/__init__.py
--rw-r--r--  2.0 unx      622 b- defN 23-Jun-09 12:04 qw/version.py
--rw-r--r--  2.0 unx     7462 b- defN 23-Jun-09 12:04 qw/process.py
--rw-r--r--  2.0 unx     2800 b- defN 23-Jun-09 12:04 qw/conf.py
--rw-r--r--  2.0 unx     4129 b- defN 23-Jun-09 12:04 qw/protocols.py
--rw-r--r--  2.0 unx     2392 b- defN 23-Jun-09 12:04 qw/__main__.py
--rwxr-xr-x  2.0 unx   568544 b- defN 23-Jun-09 12:04 qw/exceptions.cpython-39-x86_64-linux-gnu.so
--rw-r--r--  2.0 unx    16261 b- defN 23-Jun-09 12:04 qw/client.py
--rw-r--r--  2.0 unx    15911 b- defN 23-Jun-09 12:04 qw/server.py
--rw-r--r--  2.0 unx     2160 b- defN 23-Jun-09 12:04 qw/discovery.py
--rw-r--r--  2.0 unx      380 b- defN 23-Jun-09 12:04 qw/decorators.py
--rw-r--r--  2.0 unx      320 b- defN 23-Jun-09 12:04 qw/wrappers/__init__.py
--rw-r--r--  2.0 unx     4577 b- defN 23-Jun-09 12:04 qw/wrappers/di_task.py
--rw-r--r--  2.0 unx     1447 b- defN 23-Jun-09 12:04 qw/wrappers/base.py
--rw-r--r--  2.0 unx     1246 b- defN 23-Jun-09 12:04 qw/wrappers/func.py
--rw-r--r--  2.0 unx      597 b- defN 23-Jun-09 12:04 qw/utils/versions.py
--rw-r--r--  2.0 unx       46 b- defN 23-Jun-09 12:04 qw/utils/__init__.py
--rw-r--r--  2.0 unx      512 b- defN 23-Jun-09 12:04 qw/utils/functions.py
--rwxr-xr-x  2.0 unx   434800 b- defN 23-Jun-09 12:04 qw/utils/json.cpython-39-x86_64-linux-gnu.so
--rw-r--r--  2.0 unx       62 b- defN 23-Jun-09 12:04 qw/queues/__init__.py
--rw-r--r--  2.0 unx     6013 b- defN 23-Jun-09 12:04 qw/queues/manager.py
--rw-r--r--  2.0 unx     4380 b- defN 23-Jun-09 12:04 qw/executor/__init__.py
--rw-r--r--  2.0 unx     3170 b- defN 23-Jun-09 12:04 qworker-1.8.7.dist-info/METADATA
--rw-r--r--  2.0 unx     1070 b- defN 23-Jun-09 12:04 qworker-1.8.7.dist-info/LICENSE
--rw-rw-r--  2.0 unx     2189 b- defN 23-Jun-09 12:04 qworker-1.8.7.dist-info/RECORD
--rw-r--r--  2.0 unx       40 b- defN 23-Jun-09 12:04 qworker-1.8.7.dist-info/entry_points.txt
--rw-r--r--  2.0 unx        3 b- defN 23-Jun-09 12:04 qworker-1.8.7.dist-info/top_level.txt
--rw-r--r--  2.0 unx      217 b- defN 23-Jun-09 12:04 qworker-1.8.7.dist-info/WHEEL
-35 files, 1081487 bytes uncompressed, 314097 bytes compressed:  71.0%
+Zip file size: 319192 bytes, number of entries: 35
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jun-29 23:41 qworker.libs/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jun-29 23:41 qworker-1.8.8.dist-info/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jun-29 23:41 qw/
+-rw-r--r--  2.0 unx     3170 b- defN 23-Jun-29 23:41 qworker-1.8.8.dist-info/METADATA
+-rw-r--r--  2.0 unx       40 b- defN 23-Jun-29 23:41 qworker-1.8.8.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx     1070 b- defN 23-Jun-29 23:41 qworker-1.8.8.dist-info/LICENSE
+-rw-r--r--  2.0 unx        3 b- defN 23-Jun-29 23:41 qworker-1.8.8.dist-info/top_level.txt
+-rw-r--r--  2.0 unx      217 b- defN 23-Jun-29 23:41 qworker-1.8.8.dist-info/WHEEL
+-rw-rw-r--  2.0 unx     2189 b- defN 23-Jun-29 23:41 qworker-1.8.8.dist-info/RECORD
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jun-29 23:41 qw/executor/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jun-29 23:41 qw/utils/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jun-29 23:41 qw/queues/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jun-29 23:41 qw/wrappers/
+-rw-r--r--  2.0 unx     2946 b- defN 23-Jun-29 23:41 qw/conf.py
+-rw-r--r--  2.0 unx      622 b- defN 23-Jun-29 23:41 qw/version.py
+-rw-r--r--  2.0 unx      137 b- defN 23-Jun-29 23:41 qw/__init__.py
+-rw-r--r--  2.0 unx     4129 b- defN 23-Jun-29 23:41 qw/protocols.py
+-rw-r--r--  2.0 unx    16261 b- defN 23-Jun-29 23:41 qw/client.py
+-rw-r--r--  2.0 unx     2160 b- defN 23-Jun-29 23:41 qw/discovery.py
+-rw-r--r--  2.0 unx     8058 b- defN 23-Jun-29 23:41 qw/process.py
+-rw-r--r--  2.0 unx    19258 b- defN 23-Jun-29 23:41 qw/server.py
+-rw-r--r--  2.0 unx      380 b- defN 23-Jun-29 23:41 qw/decorators.py
+-rwxr-xr-x  2.0 unx   568544 b- defN 23-Jun-29 23:41 qw/exceptions.cpython-39-x86_64-linux-gnu.so
+-rw-r--r--  2.0 unx     2472 b- defN 23-Jun-29 23:41 qw/__main__.py
+-rw-r--r--  2.0 unx     4265 b- defN 23-Jun-29 23:41 qw/executor/__init__.py
+-rw-r--r--  2.0 unx       46 b- defN 23-Jun-29 23:41 qw/utils/__init__.py
+-rw-r--r--  2.0 unx      512 b- defN 23-Jun-29 23:41 qw/utils/functions.py
+-rwxr-xr-x  2.0 unx   434800 b- defN 23-Jun-29 23:41 qw/utils/json.cpython-39-x86_64-linux-gnu.so
+-rw-r--r--  2.0 unx      597 b- defN 23-Jun-29 23:41 qw/utils/versions.py
+-rw-r--r--  2.0 unx       62 b- defN 23-Jun-29 23:41 qw/queues/__init__.py
+-rw-r--r--  2.0 unx     6008 b- defN 23-Jun-29 23:41 qw/queues/manager.py
+-rw-r--r--  2.0 unx     1246 b- defN 23-Jun-29 23:41 qw/wrappers/func.py
+-rw-r--r--  2.0 unx     4658 b- defN 23-Jun-29 23:41 qw/wrappers/di_task.py
+-rw-r--r--  2.0 unx      320 b- defN 23-Jun-29 23:41 qw/wrappers/__init__.py
+-rw-r--r--  2.0 unx     1483 b- defN 23-Jun-29 23:41 qw/wrappers/base.py
+35 files, 1085653 bytes uncompressed, 315086 bytes compressed:  71.0%
```

## zipnote {}

```diff
@@ -1,106 +1,106 @@
+Filename: qworker.libs/
+Comment: 
+
+Filename: qworker-1.8.8.dist-info/
+Comment: 
+
 Filename: qw/
 Comment: 
 
-Filename: qworker.libs/
+Filename: qworker-1.8.8.dist-info/METADATA
 Comment: 
 
-Filename: qworker-1.8.7.dist-info/
+Filename: qworker-1.8.8.dist-info/entry_points.txt
 Comment: 
 
-Filename: qw/wrappers/
+Filename: qworker-1.8.8.dist-info/LICENSE
 Comment: 
 
-Filename: qw/utils/
+Filename: qworker-1.8.8.dist-info/top_level.txt
 Comment: 
 
-Filename: qw/queues/
+Filename: qworker-1.8.8.dist-info/WHEEL
+Comment: 
+
+Filename: qworker-1.8.8.dist-info/RECORD
 Comment: 
 
 Filename: qw/executor/
 Comment: 
 
-Filename: qw/__init__.py
+Filename: qw/utils/
 Comment: 
 
-Filename: qw/version.py
+Filename: qw/queues/
 Comment: 
 
-Filename: qw/process.py
+Filename: qw/wrappers/
 Comment: 
 
 Filename: qw/conf.py
 Comment: 
 
-Filename: qw/protocols.py
+Filename: qw/version.py
 Comment: 
 
-Filename: qw/__main__.py
+Filename: qw/__init__.py
 Comment: 
 
-Filename: qw/exceptions.cpython-39-x86_64-linux-gnu.so
+Filename: qw/protocols.py
 Comment: 
 
 Filename: qw/client.py
 Comment: 
 
-Filename: qw/server.py
-Comment: 
-
 Filename: qw/discovery.py
 Comment: 
 
-Filename: qw/decorators.py
+Filename: qw/process.py
 Comment: 
 
-Filename: qw/wrappers/__init__.py
+Filename: qw/server.py
 Comment: 
 
-Filename: qw/wrappers/di_task.py
+Filename: qw/decorators.py
 Comment: 
 
-Filename: qw/wrappers/base.py
+Filename: qw/exceptions.cpython-39-x86_64-linux-gnu.so
 Comment: 
 
-Filename: qw/wrappers/func.py
+Filename: qw/__main__.py
 Comment: 
 
-Filename: qw/utils/versions.py
+Filename: qw/executor/__init__.py
 Comment: 
 
 Filename: qw/utils/__init__.py
 Comment: 
 
 Filename: qw/utils/functions.py
 Comment: 
 
 Filename: qw/utils/json.cpython-39-x86_64-linux-gnu.so
 Comment: 
 
-Filename: qw/queues/__init__.py
-Comment: 
-
-Filename: qw/queues/manager.py
-Comment: 
-
-Filename: qw/executor/__init__.py
+Filename: qw/utils/versions.py
 Comment: 
 
-Filename: qworker-1.8.7.dist-info/METADATA
+Filename: qw/queues/__init__.py
 Comment: 
 
-Filename: qworker-1.8.7.dist-info/LICENSE
+Filename: qw/queues/manager.py
 Comment: 
 
-Filename: qworker-1.8.7.dist-info/RECORD
+Filename: qw/wrappers/func.py
 Comment: 
 
-Filename: qworker-1.8.7.dist-info/entry_points.txt
+Filename: qw/wrappers/di_task.py
 Comment: 
 
-Filename: qworker-1.8.7.dist-info/top_level.txt
+Filename: qw/wrappers/__init__.py
 Comment: 
 
-Filename: qworker-1.8.7.dist-info/WHEEL
+Filename: qw/wrappers/base.py
 Comment: 
 
 Zip file comment:
```

## qw/version.py

```diff
@@ -2,15 +2,15 @@
    QueueWorker is a asyncio-based Worker for distributed functions.
 """
 
 __title__ = 'qworker'
 __description__ = ('QueueWorker is asynchronous Task Queue implementation '
                    'built on top of Asyncio.'
                    'Can you spawn distributed workers to run functions inside workers.')
-__version__ = '1.8.7'
+__version__ = '1.8.8'
 __author__ = 'Jesus Lara'
 __author_email__ = 'jesuslarag@gmail.com'
 __license__ = 'MIT'
 
 def get_version() -> tuple:  # pragma: no cover
     """ Get Queue Worker version as tuple.
     """
```

## qw/process.py

```diff
@@ -3,23 +3,24 @@
 import multiprocessing as mp
 import resource as res
 import subprocess
 from collections.abc import Callable
 import socket
 import aioredis
 from navconfig.logging import logging
-from qw.exceptions import QWException
+from qw.exceptions import QWException, ConfigError
 from qw.discovery import get_server_discovery
 from .utils import cPrint
 from .utils.json import json_encoder
 from .conf import (
     NOFILES,
     WORKER_REDIS,
     QW_WORKER_LIST,
-    WORKER_DISCOVERY_PORT
+    WORKER_DISCOVERY_PORT,
+    QW_MAX_WORKERS
 )
 
 from .server import start_server
 
 JOB_LIST = []
 
 def raise_nofile(value: int = 4096) -> tuple[str, int]:
@@ -43,14 +44,24 @@
             subprocess.Popen(ulimit.format(type='n', value=hard), shell=True)
         except Exception as e:  # pylint: disable=W0703
             print('Failed to set ulimit, giving up')
             logging.exception(e, stack_info=False)
     return 'nofile', (soft, hard)
 
 
+def is_port_available(host, port):
+    try:
+        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
+        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
+        sock.bind((host, port))
+        sock.close()
+        return True
+    except OSError:
+        return False
+
 class SpawnProcess(object):
     def __init__(self, args):
         try:
             self.loop: asyncio.AbstractEventLoop = asyncio.get_event_loop()
         except RuntimeError:
             raise
         self.host: str = args.host
@@ -67,14 +78,22 @@
         self.transport: asyncio.Transport = None
         # increase the ulimit of server
         raise_nofile(value=NOFILES)
         ## Logger:
         self.logger = logging.getLogger(
             name='QW.WorkerProcess'
         )
+        if args.workers > QW_MAX_WORKERS:
+            raise ConfigError(
+                f"Max Number of Workers exceeded: {args.workers}, exiting."
+            )
+        if not is_port_available(args.host, args.port):
+            raise RuntimeError(
+                "QW Error: Port is already in use"
+            )
         for i in range(args.workers):
             try:
                 p = mp.Process(
                     target=start_server,
                     name=f'{self.worker}_{i}',
                     args=(i, args.host, args.port, args.debug, )
                 )
@@ -198,30 +217,30 @@
         except Exception as err:
             self.logger.error(
                 f"Unexpected error when registering worker: {err}"
             )
             raise
 
     def terminate(self):
+        for j in JOB_LIST:
+            try:
+                j.terminate()
+            except (OSError, AssertionError) as ex:
+                self.logger.error(ex)
+            try:
+                j.join()
+            except TypeError as ex:
+                self.logger.error(ex)
         try:
             self.loop.run_until_complete(
                 self.remove_worker()
             )
             self.loop.run_until_complete(
                 self.stop_redis()
             )
-            for j in JOB_LIST:
-                try:
-                    j.terminate()
-                except (OSError, AssertionError) as ex:
-                    self.logger.exception(ex)
-                try:
-                    j.join()
-                except TypeError as ex:
-                    self.logger.exception(ex)
         except asyncio.TimeoutError as ex:
             self.logger.warning(
                 f"Timeout error: {ex}"
             )
         except asyncio.CancelledError as exc:
             self.logger.warning(str(exc))
         except Exception as err:  # pylint: disable=W0703
```

## qw/conf.py

```diff
@@ -5,14 +5,15 @@
     wl = []
     for worker in workers:
         w, p = worker.split(':')
         wl.append((w, p))
     return wl
 
 ### Worker Configuration
+QW_MAX_WORKERS = config.getint('QW_MAX_WORKERS', fallback=10)
 WORKER_DEFAULT_HOST = config.get('WORKER_DEFAULT_HOST', fallback='0.0.0.0')
 WORKER_DEFAULT_PORT = config.getint('WORKER_DEFAULT_PORT', fallback=8888)
 WORKER_DEFAULT_QTY = config.getint('WORKER_DEFAULT_QTY', fallback=4)
 WORKER_QUEUE_SIZE = config.getint('WORKER_QUEUE_SIZE', fallback=8)
 RESOURCE_THRESHOLD = config.getint('RESOURCE_THRESHOLD', fallback=90)
 CHECK_RESOURCE_USAGE = config.getboolean('CHECK_RESOURCE_USAGE', fallback=True)
 WORKER_RETRY_INTERVAL = config.getint('WORKER_RETRY_INTERVAL', fallback=10)
@@ -40,15 +41,16 @@
 expected_message = config.get('WORKER_DISCOVERY_MESSAGE')
 WORKER_SECRET_KEY = config.get('WORKER_SECRET_KEY')
 
 
 ### Redis Transport
 REDIS_HOST = config.get('REDIS_HOST', fallback='localhost')
 REDIS_PORT = config.getint('REDIS_PORT', fallback=6379)
-REDIS_WORKER_DB = config.getint('REDIS_WORKER_DB', fallback=2)
+REDIS_WORKER_DB = config.getint('REDIS_WORKER_DB', fallback=4)
+REDIS_WORKER_CHANNEL = config.get('REDIS_WORKER_CHANNEL', fallback='WorkerChannel')
 
 WORKER_REDIS = f"redis://{REDIS_HOST}:{REDIS_PORT}/{REDIS_WORKER_DB}"
 
 WORKERS = [e.strip() for e in list(config.get(
     'WORKER_LIST', fallback='127.0.0.1:8181').split(","))]
 WORKER_LIST = get_worker_list(WORKERS)
```

## qw/__main__.py

```diff
@@ -65,26 +65,29 @@
     )
     parser.add_argument(
         '--debug', action="store_true",
         default=False,
         help="Start workers in Debug Mode"
     )
     args = parser.parse_args()
+    process = None
     try:
-        loop = asyncio.get_event_loop()
+        loop = asyncio.new_event_loop()
+        asyncio.set_event_loop(loop)
         cPrint('::: Starting Workers ::: ')
         process = SpawnProcess(args)
         process.start()
         loop.run_forever()
     except KeyboardInterrupt:
         process.terminate()
     except Exception as ex:
         # log the unexpected error
         print(f"Unexpected error: {ex}")
-        process.terminate()
+        if process:
+            process.terminate()
     finally:
         cPrint('Shutdown all workers ...', level='WARN')
         loop.close()  # close the event loop
 
 
 if __name__ == '__main__':
     main()
```

## qw/server.py

```diff
@@ -1,32 +1,34 @@
 """QueueWorker Server Implementation"""
 import os
 import time
 import socket
 import uuid
 import asyncio
 import inspect
+import random
 from typing import Any
 from collections.abc import Callable
 import multiprocessing as mp
 import cloudpickle
 from navconfig.logging import logging
 from qw.exceptions import (
     QWException,
     ParserError,
     DiscardedTask
 )
 from qw.utils import make_signature
+from redis import asyncio as aioredis
 from .conf import (
     WORKER_DEFAULT_HOST,
     WORKER_DEFAULT_PORT,
-    WORKER_DEFAULT_QTY,
     expected_message,
     WORKER_SECRET_KEY,
-    WORKER_QUEUE_CALLBACK
+    REDIS_WORKER_CHANNEL,
+    WORKER_REDIS
 )
 from .utils.json import json_encoder
 from .utils.versions import get_versions
 from .utils import cPrint
 from .queues import QueueManager
 from .wrappers import (
     QueueWrapper
@@ -34,18 +36,14 @@
 from .executor import TaskExecutor
 
 DEFAULT_HOST = WORKER_DEFAULT_HOST
 if not DEFAULT_HOST:
     DEFAULT_HOST = socket.gethostbyname(socket.gethostname())
 
 
-# Initialize a semaphore with Worker Limit
-semaphore = asyncio.Semaphore(WORKER_DEFAULT_QTY)
-
-
 class QWorker:
     """Queue Task Worker server.
 
     Attributes:
         host: Hostname of the server.
         port: Port number of the server.
         loop: Event loop to run in.
@@ -62,14 +60,15 @@
             protocol: Any = None
     ):
         self.host = host
         self.port = port
         self.debug = debug
         self.queue = None
         self._id = worker_id
+        self._running: bool = True
         if name:
             self._name = name
         else:
             self._name = mp.current_process().name
         self._loop = event_loop if event_loop else asyncio.new_event_loop()
         self._server: Callable = None
         self._pid = os.getpid()
@@ -79,17 +78,89 @@
             f'QW.Server:{self._name}.{self._id}'
         )
 
     @property
     def name(self):
         return self._name
 
+    def start_redis(self):
+        self.pool = aioredis.ConnectionPool.from_url(
+            WORKER_REDIS,
+            encoding='utf8',
+            decode_responses=True,
+            max_connections=5000
+        )
+        self.redis = aioredis.Redis(connection_pool=self.pool)
+
+    async def start_subscription(self):
+        """Starts PUB/SUB system based on Redis."""
+        try:
+            self.pubsub = self.redis.pubsub()
+            await self.pubsub.subscribe(REDIS_WORKER_CHANNEL)
+
+            while self._running:
+                try:
+                    msg = await self.pubsub.get_message()
+                    if msg and msg['type'] == 'message':
+                        self.logger.debug(f'Received Task: {msg}')
+                    await asyncio.sleep(0.001)  # sleep a bit to prevent high CPU usage
+                except ConnectionResetError:
+                    self.logger.error(
+                        "Connection was closed, trying to reconnect."
+                    )
+                    await asyncio.sleep(1)  # Wait for a bit before trying to reconnect
+                    await self.start_subscription()  # Try to restart the subscription
+                except asyncio.CancelledError:
+                    await self.pubsub.unsubscribe(REDIS_WORKER_CHANNEL)
+                    break
+                except KeyboardInterrupt:
+                    break
+                except Exception as exc:
+                    # Handle other exceptions as necessary
+                    self.logger.error(
+                        f"Error in start_subscription: {exc}"
+                    )
+                    break
+        except Exception as exc:
+            self.logger.error(
+                f"Could not establish initial connection: {exc}"
+            )
+
+    async def close_redis(self):
+        try:
+            # Get a new pubsub object and unsubscribe from 'channel'
+            try:
+                await self.pubsub.unsubscribe(REDIS_WORKER_CHANNEL)
+                await asyncio.wait_for(self.redis.close(), timeout=2.0)
+            except asyncio.TimeoutError:
+                self.logger.error(
+                    "Redis took too long to close."
+                )
+            await self.pool.disconnect(
+                inuse_connections=True
+            )
+        except RuntimeError as err:
+            self.logger.exception(
+                err, stack_info=True
+            )
+        try:
+            self.subscription_task.cancel()
+            await self.subscription_task
+        except asyncio.CancelledError:
+            pass
+
     async def start(self):
+        # Redis Service:
+        self.start_redis()
         """Starts Queue Manager."""
         self.queue = QueueManager(worker_name=self._name)
+        # Subscription Manager:
+        self.subscription_task = self._loop.create_task(
+            self.start_subscription()
+        )
         try:
             if self._protocol:
                 self._server = await self._loop.create_server(
                     self._protocol,
                     host=self.host,
                     port=self.port,
                     family=socket.AF_INET,
@@ -122,24 +193,30 @@
             await self.queue.fire_consumers()
             async with self._server:
                 await self._server.serve_forever()
         except (RuntimeError, KeyboardInterrupt) as err:
             self.logger.exception(err, stack_info=True)
 
     async def shutdown(self):
+        self._running = False
         if self.debug is True:
             cPrint(
                 f'Shutting down worker {self.name!s}'
             )
         try:
             # forcing close the queue
             await self.queue.empty_queue()
         except KeyboardInterrupt:
             pass
         try:
+            # closing redis:
+            await self.close_redis()
+        except KeyboardInterrupt:
+            pass
+        try:
             self._server.close()
             await self._server.wait_closed()
         except RuntimeError as err:
             self.logger.exception(
                 err, stack_info=True
             )
         except Exception as exc:
@@ -217,14 +294,15 @@
             writer=writer
         )
 
     async def discard_task(self, message: str, writer: asyncio.StreamWriter):
         exc = DiscardedTask(
             message
         )
+        self.logger.critical(message, stack_info=True)
         result = cloudpickle.dumps(exc)
         await self.closing_writer(
             writer,
             result
         )
         return False
 
@@ -293,15 +371,15 @@
             if reader.at_eof():
                 break
         return serialized_task
 
     async def deserialize_task(self, serialized_task, writer: asyncio.StreamWriter):
         try:
             task = cloudpickle.loads(serialized_task)
-            self.logger.debug(
+            self.logger.info(
                 f'TASK RECEIVED: {task} at {int(time.time())}'
             )
             return task
         except (EOFError, RuntimeError) as ex:
             ### Empty Task:
             ex = ParserError(
                 f"Error Decoding Serialized Task: {ex}"
@@ -328,29 +406,32 @@
                 )
                 return f'Task {task!s} with id {uid} was queued.'.encode('utf-8')
             except asyncio.QueueFull:
                 return await self.discard_task(
                     f"Worker {self.name!s} Queue is Full, discarding Task {task!r}"
                 )
         else:
+            result = None
             try:
                 # executed and send result to client
                 executor = TaskExecutor(task)
-                return await executor.run()
+                result = await executor.run()
             except Exception as err:  # pylint: disable=W0703
                 try:
                     result = cloudpickle.dumps(err)
                 except Exception as ex:  # pylint: disable=W0703
                     result = cloudpickle.dumps(
                         QWException(
                             f'Error on Deal with Exception: {ex!s}'
                         )
                     )
                 await self.closing_writer(writer, result)
                 return False
+            finally:
+                return result
 
     async def connection_handler(
             self,
             reader: asyncio.StreamReader,
             writer: asyncio.StreamWriter
     ):
         """ Handler for Function/Task Execution.
@@ -374,33 +455,39 @@
         # after: deserialize Task:
         serialized_task = await self._read_task(reader)
         task = None
         result = None
         task = await self.deserialize_task(
             serialized_task, writer
         )
-        task_uuid = uuid.uuid4()
         if not task:
+            self.logger.error(f'No Task was received, received: {serialized_task}')
+            await self.closing_writer(writer, result)
             return False
-        elif isinstance(task, QueueWrapper):
+        task_uuid = task.id if task.id else uuid.uuid1(
+            node=random.getrandbits(48) | 0x010000000000
+        )
+        if isinstance(task, QueueWrapper):
             if not (result := await self.handle_queue_wrapper(task, task_uuid, writer)):
+                await self.closing_writer(writer, result)
                 return False
         elif callable(task):
             executor = TaskExecutor(task)
             result = await executor.run()
         else:
             # put work in Queue:
             try:
                 await self.queue.put(task, id=task_uuid)
                 result = f'Task {task!s} was Queued.'.encode('utf-8')
             except asyncio.QueueFull:
                 return await self.discard_task(
                     message=f'Task {task!s} was discarded, queue full',
                     writer=writer
                 )
+        print('RESULT > ', result)
         if result is None:
             # Not always a Task returns Value, sometimes returns None.
             result = [
                 {
                     "task": task,
                     "uuid": task_uuid,
                     "worker": self.name
```

## qw/wrappers/di_task.py

```diff
@@ -39,15 +39,18 @@
             del kwargs['debug']
         except KeyError:
             self._debug = False
         self.program = program
         self.task = task
         self._task = None
         self.args, self.kwargs = args, kwargs
-        self.id = task_id
+        if task_id is not None:
+            self.id = task_id
+        else:
+            self.id = self._id
 
     def task_id(self):
         return f'{self.id!s}'
 
     def task_obj(self):
         return self._task
```

## qw/wrappers/base.py

```diff
@@ -1,26 +1,28 @@
 """
 Abstract Wrapper Base.
 
 Any other wrapper extends this.
 """
-from typing import Callable, Coroutine, Any
+import random
 import uuid
 
 # coro = Callable[[int], Coroutine[Any, Any, str]]
 
 class QueueWrapper:
     _queued: bool = True
     _debug: bool = False
 
     def __init__(self, coro=None, *args, **kwargs):
         if 'queued' in kwargs:
             self._queued = kwargs['queued']
             del kwargs['queued']
-        self._id = uuid.uuid4()
+        self._id = uuid.uuid1(
+            node=random.getrandbits(48) | 0x010000000000
+        )
         self.args = args
         self.kwargs = kwargs
         self.loop = None
         ## retry functionality
         self.retries = 0
         # function to be handled:
         self.coro = coro
```

## qw/queues/manager.py

```diff
@@ -38,16 +38,16 @@
             WORKER_QUEUE_CALLBACK
         )
         self.logger.notice(
             f'Callback Queue: {self._callback!r}'
         )
 
     async def task_callback(self, task, **kwargs):
-        self.logger.notice(
-            f'Task Consumed >>> {task!r} with ID {task.id}'
+        self.logger.info(
+            f'Task Consumed: {task!r} with ID {task.id}'
         )
 
     def get_callback(self, done_callback: str) -> Union[Callable, Awaitable]:
         if not done_callback:
             ## returns a simple logger:
             return self.task_callback
         try:
```

## qw/executor/__init__.py

```diff
@@ -10,51 +10,49 @@
     ENVIRONMENT
 )
 from ..wrappers import QueueWrapper, FuncWrapper, TaskWrapper
 from ..conf import WORKER_CONCURRENCY_NUMBER, WORKER_TASK_TIMEOUT
 
 class TaskExecutor:
     def __init__(self, task, *args, **kwargs):
-        self.logger = logging.getLogger('QW.Executor')
+        self.logger = logging.getLogger(
+            'QW.Executor'
+        )
         self.task = task
         self.semaphore = asyncio.Semaphore(WORKER_CONCURRENCY_NUMBER)
 
     async def run_task(self):
         result = None
         self.logger.info(
             f"Running Task: {self.task!s}"
         )
         try:
-            await self.task.create()
-            task = asyncio.create_task(self.task.run())
+            task = asyncio.create_task(self.task())
             task.add_done_callback(self.task_done)
-            _, pending = await asyncio.wait(
-                [task], timeout=WORKER_TASK_TIMEOUT * 60
+            result = await asyncio.wait_for(
+                task, timeout=WORKER_TASK_TIMEOUT * 60
+            )
+        except asyncio.TimeoutError:
+            raise asyncio.TimeoutError(
+                f"Task {self.task} with id {self.task.id} was cancelled."
             )
-            if task in pending:
-                await self.task_pending(task)
-                task.cancel()
-                raise asyncio.TimeoutError(
-                    f"Task {self.task} was cancelled."
-                )
-            # get the result:
-            result = task.result()
         except Exception as err:  # pylint: disable=W0703
             self.logger.error(
-                f"An Error occurred while running Task {self.task}: {err}"
+                f"An Error occurred while running Task {self.task}.{self.task.id}: {err}"
             )
             result = err
         finally:
             await self.task.close()
             return result
 
     def task_done(self, task, *args, **kwargs):
-        self.logger.notice(
+        self.logger.info(
             f"Finalized Task {task}"
         )
+        return True
 
     def get_notify(self):
         # TODO: implement other notify connectors:
         # defining the Default chat object:
         recipient = Chat(
             **{"chat_id": EVENT_CHAT_ID, "chat_name": "Navigator"}
         )
@@ -97,15 +95,15 @@
             if type(self.task) in (FuncWrapper, QueueWrapper):
                 self.logger.notice(
                     f"Running Function: {self.task}"
                 )
                 self.task.set_loop(loop)
                 result = await self.task()
             elif isinstance(self.task, TaskWrapper):
-                self.logger.notice(
+                self.logger.info(
                     f"Running Task: {self.task}"
                 )
                 self.task.set_loop(loop)
                 async with self.semaphore:
                     result = await self.run_task()
             elif (
                 inspect.isawaitable(self.task) or asyncio.iscoroutinefunction(self.task)
```

## Comparing `qworker-1.8.7.dist-info/METADATA` & `qworker-1.8.8.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: qworker
-Version: 1.8.7
+Version: 1.8.8
 Summary: QueueWorker is asynchronous Task Queue implementation built on top of Asyncio.Can you spawn distributed workers to run functions inside workers.
 Home-page: https://github.com/phenobarbital/qworker
 Author: Jesus Lara
 Author-email: jesuslara@phenobarbital.info
 License: MIT
 Project-URL: Source, https://github.com/phenobarbital/qworker
 Project-URL: Funding, https://paypal.me/phenobarbital
```

## Comparing `qworker-1.8.7.dist-info/LICENSE` & `qworker-1.8.8.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `qworker-1.8.7.dist-info/RECORD` & `qworker-1.8.8.dist-info/RECORD`

 * *Files 15% similar despite different names*

```diff
@@ -1,28 +1,28 @@
+qworker-1.8.8.dist-info/METADATA,sha256=7MhusvmcusYgaNrYdRvMQG3PeDtMuC66RjonMcFpRQ0,3170
+qworker-1.8.8.dist-info/entry_points.txt,sha256=ooHTYYyEjHI9Rj79mHQmU_s4HP-PTvD3g3xSa26vIGc,40
+qworker-1.8.8.dist-info/LICENSE,sha256=EW8vB8vWRFvBxGC3soG2HCxkuNrQpOGUgNFd81h7u54,1070
+qworker-1.8.8.dist-info/top_level.txt,sha256=2NxbFCeI_G1kzzf628VnbDchanCX6rcTJORdENY-rHI,3
+qworker-1.8.8.dist-info/WHEEL,sha256=G1-Tt_WO10x9oXjmzv4wXxMLbyVnQaK1ig1QydJbPuA,217
+qworker-1.8.8.dist-info/RECORD,,
+qw/conf.py,sha256=K7JRS16jsVinw0cJM4MbTUtCWNg2a5tHTJMnnjpwSkw,2946
+qw/version.py,sha256=1nYRvnaS5sV75ByMOt8G_oBfZMkNDc8vwy_oyyQ5Ngc,622
 qw/__init__.py,sha256=awMNjg7WGznbrNuJLoEJV6BbnREk4BPD8k9My8BD7Uo,137
-qw/version.py,sha256=Caf0ezdjg_XROtZnSh3wmeqBA2Qt7tU5NOGWUAUrnzs,622
-qw/process.py,sha256=Mbd-zlYS8oefzxpiMnWw-LziBj6hiZXu2uQT6yySmuw,7462
-qw/conf.py,sha256=BEvUUasYye3axv9ohkf4pIokDbO3bwUWZQFgLeM5T50,2800
 qw/protocols.py,sha256=I57MsY72OlVlGFT1-ggZCm4nWAd89m4m5YEwGUWGAmg,4129
-qw/__main__.py,sha256=oUCTmeloFK83is_ipZ3yoRreu23TIcAELKhpOiIt_Xc,2392
-qw/exceptions.cpython-39-x86_64-linux-gnu.so,sha256=IYBQ4BGCKA3duo8PEQylQhSS4BTYgAJ_rNjMOWU2nf0,568544
 qw/client.py,sha256=-JRd9E6v0bUeUuLH-2a2WLfLujkQW6AWnEEbYoTZWPc,16261
-qw/server.py,sha256=86NAAw92gSuGfpDH71YpYMhFxbhf-FKUd-LC8NApjP0,15911
 qw/discovery.py,sha256=l_Lb3Bmni6WTTu5fxzj4-9KquiRak1rq8k9I70f_hSI,2160
+qw/process.py,sha256=TnA7JKqMmWaLQCKQEFY3A8Q1poC0cMtpsHuB412OJgw,8058
+qw/server.py,sha256=dcvINYJZM6eyfWlxpQNhSI2iFMXZEWx7quodtdBfsg0,19258
 qw/decorators.py,sha256=lL5CN9a7gUi8iDEfOI7kSpABkScu5cE99yj9eYGn69w,380
-qw/wrappers/__init__.py,sha256=Ot_f0GTaDB50Za4Hxsz2FZSkZDn4zZoDHjXOvqj9T9k,320
-qw/wrappers/di_task.py,sha256=RLf40bnt10W_YVkCO-r0CUyGMN86P-qSaf8zD6IpR7Y,4577
-qw/wrappers/base.py,sha256=FeeaDDGLZH1Q6PN4dxlshBHSB5fMGEoVXRrhQB3xtBA,1447
-qw/wrappers/func.py,sha256=sL43tB6BWPbz-iOpkySTaOIpGtd6J1F37R8V7WfwcE8,1246
-qw/utils/versions.py,sha256=d8AdLmhM1bPc82vTAshCJBljGr-Ur3_hiZ_GtTbbORA,597
+qw/exceptions.cpython-39-x86_64-linux-gnu.so,sha256=IYBQ4BGCKA3duo8PEQylQhSS4BTYgAJ_rNjMOWU2nf0,568544
+qw/__main__.py,sha256=H52Hv_ll_8Nwnbj4UAhGfe3bfCbAhYAdH6f5Y3kbpFo,2472
+qw/executor/__init__.py,sha256=3ggAsCVXZnimnhOcs5aLUHJZRV-A7vqNd46ScvmM-mY,4265
 qw/utils/__init__.py,sha256=bYf_I4ymTf8vsqMjK00NJi2-A-lVijPTwN6HDOVjcjQ,46
 qw/utils/functions.py,sha256=9iXVvYLtQzOK0tRecOV2Oqrl-2raxAHwBCNilLui1xQ,512
 qw/utils/json.cpython-39-x86_64-linux-gnu.so,sha256=jHsLujozPguK1eL9t-Grw2KYTVlZvo4k7rLtF8a6YuU,434800
+qw/utils/versions.py,sha256=d8AdLmhM1bPc82vTAshCJBljGr-Ur3_hiZ_GtTbbORA,597
 qw/queues/__init__.py,sha256=itGqt8q7feqZePbUWJTeA82Q2AwU0B2agUFXRmpLuDc,62
-qw/queues/manager.py,sha256=q7ao07sWu1kYTej0mh3dcaWNvTBB1GpFxnsxaiKQm5c,6013
-qw/executor/__init__.py,sha256=0p8VVBy8_JCCpq3_7AsEZzDEabfezlw5n1qQrAZaTRo,4380
-qworker-1.8.7.dist-info/METADATA,sha256=bjvAjHLTv3MMtqn7JaHmTASSCXWTUr3ekEM6cDIHH_M,3170
-qworker-1.8.7.dist-info/LICENSE,sha256=EW8vB8vWRFvBxGC3soG2HCxkuNrQpOGUgNFd81h7u54,1070
-qworker-1.8.7.dist-info/RECORD,,
-qworker-1.8.7.dist-info/entry_points.txt,sha256=ooHTYYyEjHI9Rj79mHQmU_s4HP-PTvD3g3xSa26vIGc,40
-qworker-1.8.7.dist-info/top_level.txt,sha256=2NxbFCeI_G1kzzf628VnbDchanCX6rcTJORdENY-rHI,3
-qworker-1.8.7.dist-info/WHEEL,sha256=G1-Tt_WO10x9oXjmzv4wXxMLbyVnQaK1ig1QydJbPuA,217
+qw/queues/manager.py,sha256=NihB5HypjYkMoUaA2nRR7THBKpywC_QUpcYXjJqZdEw,6008
+qw/wrappers/func.py,sha256=sL43tB6BWPbz-iOpkySTaOIpGtd6J1F37R8V7WfwcE8,1246
+qw/wrappers/di_task.py,sha256=p1Yfa_rqDJ5w6VR8XtP9fKp8PvvndzJL08Izc1W60OE,4658
+qw/wrappers/__init__.py,sha256=Ot_f0GTaDB50Za4Hxsz2FZSkZDn4zZoDHjXOvqj9T9k,320
+qw/wrappers/base.py,sha256=2gazsxzyqwJOjopfJZsRX9Lif6o4upusAKkvrUIQoBM,1483
```

