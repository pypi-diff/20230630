# Comparing `tmp/mllibs-0.1.4-py3-none-any.whl.zip` & `tmp/mllibs-0.1.5-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,8 +1,8 @@
-Zip file size: 158106 bytes, number of entries: 35
+Zip file size: 159489 bytes, number of entries: 35
 -rw-rw-rw-  2.0 fat        0 b- defN 23-Jun-23 14:43 mllibs/__init__.py
 -rw-rw-rw-  2.0 fat    15722 b- defN 23-Jun-28 18:43 mllibs/common_eval.py
 -rw-rw-rw-  2.0 fat     4821 b- defN 23-Jun-28 18:53 mllibs/interface.py
 -rw-rw-rw-  2.0 fat    10770 b- defN 23-Jun-27 14:31 mllibs/mdsplit.py
 -rw-rw-rw-  2.0 fat     9912 b- defN 23-Jun-24 06:11 mllibs/meda_scplot.py
 -rw-rw-rw-  2.0 fat    22347 b- defN 23-Jun-25 14:45 mllibs/meda_splot.py
 -rw-rw-rw-  2.0 fat    31706 b- defN 23-Jun-25 21:30 mllibs/membedding.py
@@ -10,28 +10,28 @@
 -rw-rw-rw-  2.0 fat     5312 b- defN 23-Jun-25 11:40 mllibs/mloader.py
 -rw-rw-rw-  2.0 fat    12911 b- defN 23-Jun-25 14:33 mllibs/moutliers.py
 -rw-rw-rw-  2.0 fat     6457 b- defN 23-Jun-24 04:53 mllibs/mpd_df.py
 -rw-rw-rw-  2.0 fat     6940 b- defN 23-Jun-25 10:54 mllibs/mseda.py
 -rw-rw-rw-  2.0 fat    10512 b- defN 23-Jun-28 19:16 mllibs/mslcatboost.py
 -rw-rw-rw-  2.0 fat    28139 b- defN 23-Jun-28 12:54 mllibs/mslensemble.py
 -rw-rw-rw-  2.0 fat    19284 b- defN 23-Jun-28 11:05 mllibs/msllinear.py
--rw-rw-rw-  2.0 fat    11831 b- defN 23-Jun-28 12:34 mllibs/msltree.py
+-rw-rw-rw-  2.0 fat    11831 b- defN 23-Jun-28 20:25 mllibs/msltree.py
 -rw-rw-rw-  2.0 fat    16763 b- defN 23-Jun-23 14:44 mllibs/mtextnorm.py
 -rw-rw-rw-  2.0 fat    19605 b- defN 23-Jun-24 17:56 mllibs/musldimred.py
--rw-rw-rw-  2.0 fat    32146 b- defN 23-Jun-28 18:45 mllibs/nlpi.py
--rw-rw-rw-  2.0 fat    17085 b- defN 23-Jun-27 15:35 mllibs/nlpm.py
+-rw-rw-rw-  2.0 fat    39882 b- defN 23-Jun-30 12:52 mllibs/nlpi.py
+-rw-rw-rw-  2.0 fat    17773 b- defN 23-Jun-29 16:52 mllibs/nlpm.py
 -rw-rw-rw-  2.0 fat    75880 b- defN 23-Jun-23 08:27 mllibs/corpus/wordlist.10000.txt
 -rw-rw-rw-  2.0 fat   126077 b- defN 23-Jun-24 17:03 mllibs/models/cv_ner_tagger.pickle
 -rw-rw-rw-  2.0 fat     2745 b- defN 23-Jun-24 17:03 mllibs/models/dtc_ner_tagger.pickle
 -rw-rw-rw-  2.0 fat        0 b- defN 23-Jun-23 08:27 mlmodels/__init__.py
 -rw-rw-rw-  2.0 fat     2994 b- defN 23-Jun-23 08:27 mlmodels/bl_regressor.py
 -rw-rw-rw-  2.0 fat     4402 b- defN 23-Jun-23 08:27 mlmodels/gmm.py
 -rw-rw-rw-  2.0 fat     5761 b- defN 23-Jun-23 08:27 mlmodels/gp_bclassifier.py
 -rw-rw-rw-  2.0 fat     4627 b- defN 23-Jun-23 08:27 mlmodels/gp_regressor.py
 -rw-rw-rw-  2.0 fat     6350 b- defN 23-Jun-23 08:27 mlmodels/gpr_bclassifier.py
 -rw-rw-rw-  2.0 fat     5170 b- defN 23-Jun-23 08:27 mlmodels/kriging_regressor.py
--rw-rw-rw-  2.0 fat     1091 b- defN 23-Jun-28 19:17 mllibs-0.1.4.dist-info/LICENSE
--rw-rw-rw-  2.0 fat     8344 b- defN 23-Jun-28 19:17 mllibs-0.1.4.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 23-Jun-28 19:17 mllibs-0.1.4.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       16 b- defN 23-Jun-28 19:17 mllibs-0.1.4.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     2734 b- defN 23-Jun-28 19:17 mllibs-0.1.4.dist-info/RECORD
-35 files, 543899 bytes uncompressed, 153852 bytes compressed:  71.7%
+-rw-rw-rw-  2.0 fat     1091 b- defN 23-Jun-30 12:53 mllibs-0.1.5.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat     8344 b- defN 23-Jun-30 12:53 mllibs-0.1.5.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 23-Jun-30 12:53 mllibs-0.1.5.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       16 b- defN 23-Jun-30 12:53 mllibs-0.1.5.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat     2734 b- defN 23-Jun-30 12:53 mllibs-0.1.5.dist-info/RECORD
+35 files, 552323 bytes uncompressed, 155235 bytes compressed:  71.9%
```

## zipnote {}

```diff
@@ -84,23 +84,23 @@
 
 Filename: mlmodels/gpr_bclassifier.py
 Comment: 
 
 Filename: mlmodels/kriging_regressor.py
 Comment: 
 
-Filename: mllibs-0.1.4.dist-info/LICENSE
+Filename: mllibs-0.1.5.dist-info/LICENSE
 Comment: 
 
-Filename: mllibs-0.1.4.dist-info/METADATA
+Filename: mllibs-0.1.5.dist-info/METADATA
 Comment: 
 
-Filename: mllibs-0.1.4.dist-info/WHEEL
+Filename: mllibs-0.1.5.dist-info/WHEEL
 Comment: 
 
-Filename: mllibs-0.1.4.dist-info/top_level.txt
+Filename: mllibs-0.1.5.dist-info/top_level.txt
 Comment: 
 
-Filename: mllibs-0.1.4.dist-info/RECORD
+Filename: mllibs-0.1.5.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## mllibs/msltree.py

```diff
@@ -147,15 +147,15 @@
                               'subtopic':'model prediction',
                               'input_format':'pd.DataFrame',
                               'description':"A decision tree regression model is a type of machine learning model that uses a decision tree algorithm to make predictions on continuous numerical values. It works by recursively partitioning the input space into smaller regions based on the values of the input features, and then assigning a constant value (usually the mean or median) to each region as the predicted output. The decision tree algorithm determines the optimal splits and thresholds for partitioning the data based on certain criteria, such as minimizing the variance of the predicted values within each region. This allows the model to capture non-linear relationships between the input features and the target variable, making it suitable for regression tasks.",
                             'token_compat':'data features target',
                             'arg_compat':'criterion splitter max_depth min_samples_leaf min_samples_split max_features rs'},
 
 
-                  'fit_rdc':{'module':'sltree',
+                  'fit_cdc':{'module':'sltree',
                             'action':'train model',
                             'topic':'decision tree classification',
                             'subtopic':'model training',
                             'input_format':'pd.DataFrame',
                             'description':"A decision tree classification model is a type of machine learning model that uses a decision tree algorithm to make predictions on categorical or discrete values. Instead of predicting continuous numerical values, the decision tree classification model predicts the class or category that an input data point belongs to. It works by recursively partitioning the input space into smaller regions based on the values of the input features, and then assigning a class label to each region as the predicted output. The decision tree algorithm determines the optimal splits and thresholds for partitioning the data based on certain criteria, such as maximizing the purity or homogeneity of the predicted classes within each region. This allows the model to capture complex decision boundaries and classify new data points based on their feature values. ",
                             'arg_compat':'criterion splitter max_depth min_samples_leaf min_samples_split max_features rs'},
```

## mllibs/nlpi.py

```diff
@@ -1,48 +1,56 @@
 
 from mllibs.nlpm import nlpm
+from mllibs.common_corpus import corpus_model
 import numpy as np
 import pandas as pd
 import random
 import panel as pn
 from nltk.tokenize import word_tokenize, WhitespaceTokenizer 
 from inspect import isfunction
 from seaborn import load_dataset
 
+# models
+from sklearn.ensemble import AdaBoostRegressor,AdaBoostClassifier,RandomForestRegressor,RandomForestClassifier
+from sklearn.ensemble import HistGradientBoostingRegressor,HistGradientBoostingClassifier
+from catboost import CatBoostClassifier,CatBoostRegressor
+from sklearn.linear_model import LinearRegression, LogisticRegression,Ridge, RidgeClassifier, Lasso, ElasticNet, BayesianRidge
+
 # default plot palette
 
 def hex_to_rgb(h):
     h = h.lstrip('#')
     return tuple(int(h[i:i+2], 16)/255 for i in (0, 2, 4))
 
 palette = ['#b4d2b1', '#568f8b', '#1d4a60', '#cd7e59', '#ddb247', '#d15252']
 palette_rgb = [hex_to_rgb(x) for x in palette]
 
 ########################################################################
 
-
 # interaction & tect interpreter class
  
 class nlpi(nlpm):
 
     data = {}    # dictionary for storing data
     iter = -1    # keep track of all user requests
     memory_name = []                 # store order of executed tasks
     memory_stack = []                # memory stack of task information
-    memory_output = []
+    memory_output = []               # memory output
+    model = {}                       # store models
     
     # instantiation requires module
     def __init__(self,module=None,verbose=0):
         
         self.module = module                  # collection of modules
         self._make_task_info()                # create self.task_info
         self.dsources = {}                    # store all data source keys
         self.token_data = []                  # store all token data
         self.verbose = verbose                # print output text flag
-        nlpi.silent = False                    
+        nlpi.silent = False     
+                    
 
         # class plot parameters
         nlpi.pp = {'alpha':1,'mew':0,'mec':'k','fill':True,'stheme':palette_rgb,'s':30}
         
     # set plotting parameter
         
     def setpp(self,params:dict):
@@ -65,43 +73,39 @@
         lst_data = list(nlpi.data.keys())            # data has been loaded
         self.dsources = {'inputs':lst_data}
                
         if(nlpi.silent is False): 
             print('inputs:')
             print(lst_data,'\n')
         
+        
     ''' 
     
-    store data 
+    STORE INPUT DATA
     
     '''
     
     # split dataframe columns into numeric and categorical
     
     @staticmethod
     def split_types(df):
         numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']  
         numeric = df.select_dtypes(include=numerics)
         categorical = df.select_dtypes(exclude=numerics)
         return list(numeric.columns),list(categorical.columns)
 
-        
-    ''' 
-    
-    STORE INPUT DATA
-    
-    '''
-
     # Load Dataset from Seaborn Repository
     
     def load_dataset(self,name:str,info:str=None):
         
         # load data from seaborn repository             
         data = load_dataset(name)
         self.store(data,name,info)
+
+    # Store Data Function
         
     def store(self,data,name:str,info:str=None):
         
 		# dictionary to store data information
         datainfo = {'data':None,'subset':None,'splits':None,
                     'features':None,'target':None,
                     'cat':None,'num':None,
@@ -208,14 +212,221 @@
         if(nlpi.silent is False):
             print(f'\ndata information for {name}')
             print('=========================================')
             print(datainfo)
                  
         datainfo['data'] = data
         nlpi.data[name] = datainfo
+
+        
+        '''
+
+        Function to parase model information
+
+        '''
+        # required for storing model information
+
+        def store_model(self,model:str,p:str=None,name:str=None):
+
+            if(name is None):
+                name = 'model'
+
+            # imported models
+            available_models = list(corpus_model.keys())
+
+            # ///////////////////////////////////////////////////////////
+
+            # [A] if p is specified
+
+            # ///////////////////////////////////////////////////////////
+
+            params = {}
+            if(p is not None):
+
+                # interpret model parameters p
+
+                # given in " " format
+
+                if('\n' in p):
+
+                    splits = p.split('\n')
+                    splits_clean = list(filter(lambda a: a != "", splits))
+
+                    params = {}               # <--- target dictionary
+                    for opt in splits_clean:
+
+                        key = opt.split('=')[0].strip()
+                        value = opt.split('=')[1].strip()
+
+                        try:
+                            if('.' in value):
+                                value = float(value)
+                            else:
+                                value = int(value)
+                        except:
+                            pass
+                        
+                        params[key] = value
+
+                # given as a list with ,
+
+                else:
+
+                    splits = p.split(',')
+                    print(splits)
+                    
+                    params = {}
+                    for parameter in splits:
+                        
+                        key = parameter.split('=')[0]
+                        value = parameter.split('=')[1]
+
+                        try:
+                            if('.' in value):
+                                value = float(value)
+                            else:
+                                value = int(value)
+                        except:
+                            pass
+                            
+                        params[key] = value
+
+                '''
+                
+                Interpret Model Input
+                
+                '''
+
+                # not yet defined parameters, just model name w/o ()
+
+                if("()" not in model or ("(" not in model or ")" not in model)):
+
+                    # If model has been written in correct format
+
+                    if(model in available_models):
+                        if(len(params) != 0):
+                            output = model + f"(**{params})"
+                        else:
+                            output = model + "()"
+
+                    # else use model to predict model
+
+                    else:
+                        print('model not available, predicting model')
+                        model = self.module.model['select_model']
+                        vectoriser = self.module.vectoriser['select_model']
+                        X2 = vectoriser.transform([model]).toarray()
+
+                        if(len(params) != 0):
+                            output = model.predict(X) + f"(**{params})"
+                        else:
+                            output = model.predict(X) + "()"
+
+                    
+                # if we have specified parametes p and model mentioned in form ()
+
+                elif("()" in model and p is not None):
+
+                    if(model in available_models):
+                        if(len(params) != 0):
+                            output = model.split('(')[0] + f"(**{params})"
+                        else:
+                            output = model.predict(X) + "()"
+
+                    else:
+
+                        print('model not available, predicting model')
+                        model = self.module.model['select_model']
+                        vectoriser = self.module.vectoriser['select_model']
+                        X2 = vectoriser.transform([model]).toarray()
+
+                        if(len(params) != 0):
+                            output = model.predict(X) + f"(**{params})"
+                        else:
+                            output = model.predict(X) + "()"
+
+            # [B] if no parameter is given; all data in model string
+
+            elif(p is None):
+
+                # if model is written with (), which may contain parameters
+
+                if('(' in model):
+
+                    # remove the brackets
+                    model_name = model.split('(')[0]
+
+                    # model name must be available 
+
+                    if(model_name in available_models):
+
+                        # find text inside ()
+                        parameters = re.findall(r'\((.*?)\)',model)
+                        lst_parameters = parameters[0].split(',')
+                        
+                        # try to find parameters inside brackets
+
+                        params = {}
+                        for parameter in lst_parameters:
+                            
+                            key = parameter.split('=')[0]
+                            value = parameter.split('=')[1]
+
+                            try:
+                                if('.' in value):
+                                    value = float(value)
+                                else:
+                                    value = int(value)
+                            except:
+                                pass
+                                
+                            params[key] = value
+
+                        if(len(params) == 0):
+                            output = model_name + "()"
+                        else:
+                            output = model_name + f"(**{params})"
+
+                        # save model
+                        return nlpi.model[name] = eval(output)
+
+                    else:
+
+                        print('model not available, using prediction model')
+                        model = self.module.model['select_model']
+                        vectoriser = self.module.vectoriser['select_model']
+                        X2 = vectoriser.transform([model_name]).toarray()
+
+                        if(len(params) != 0):
+                            output = model.predict(X) + f"(**{params})"
+                        else:
+                            output = model.predict(X) + "()"
+
+                # if model is wrtten in plain form without ()
+
+                else:
+
+                    # check if the base model name is in available models
+                    # no parameters are available
+
+                    # just add brackets
+                    if(model in available_models):
+                        output = model + "()"
+                    else:
+                        print('model not available, using prediction model')
+                        model = self.module.model['select_model']
+                        vectoriser = self.module.vectoriser['select_model']
+                        X2 = vectoriser.transform([model]).toarray()
+                        model_found = model.predict(X)
+                        output = model_found + "()"
+
+            nlpi.model[name] = eval(output)
+            print('model stored!')
+
+
         
     # activation function list
     
     def fl(self,show='all'):
                             
         # function information
         df_funct = self.task_info
```

## mllibs/nlpm.py

```diff
@@ -1,21 +1,23 @@
+from mllibs.common_corpus import corpus_model
 from sklearn.preprocessing import LabelEncoder
 from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
 from sklearn.linear_model import LogisticRegression
 from sklearn.tree import DecisionTreeClassifier
 from sklearn.metrics.pairwise import cosine_similarity
 from sklearn.metrics.pairwise import linear_kernel,sigmoid_kernel
 from sklearn.base import clone
 from collections import OrderedDict
 import pickle
 import numpy as np
 import pandas as pd
 # import zipfile
 import pkgutil
 
+
 import nltk
 # nltk.download('wordnet')
 
 # wordn = '/usr/share/nltk_data/corpora/wordnet.zip'
 # wordnt = '/usr/share/nltk_data/corpora/'
 
 # with zipfile.ZipFile(wordn,"r") as zip_ref:
@@ -312,24 +314,46 @@
     #         lvect = clone(vect)
     #         self.train_loop(self.corpus_top,'top',lvect)
     #         lvect = clone(vect)
     #         self.train_loop(self.corpus_sub,'sub',lvect)
     
             self.toksub_model()
             self.ner_tokentag_model()
+            self.store_model()
 
             print('models trained...')
         
     '''
     ///////////////////////////////////////////////////////////////
     
     ADDITIONAL MODELS
     
     ///////////////////////////////////////////////////////////////
+    '''
+
+    # store model model & vectoriser
+    def store_model(self):
+
+        lst_all = []; lst_tag = []
+        for ii,(key,value) in enumerate(corpus_model.items()):
+            lst_all.extend(value)
+            lst_tag.extend([key for i in range(0,len(value))])
+
+        data = {'corpus':lst_all,'tag':lst_tag}
+
+        vect = CountVectorizer(stop_words=['using','use'])
+        X = vect.fit_transform(list(data['corpus'])).toarray()
+        y = data['tag']
+
+        model = LogisticRegression().fit(X,y)
+        self.model['store_model'] = model 
+        self.vectoriser['store_model'] = vect
+
     
+    '''
     1. CREATE SUBSET DETERMINATION MODEL 
     
     create multiclass classification model which will determine 
     which approach to utilise for the selection of subset features    
     
     '''
```

## Comparing `mllibs-0.1.4.dist-info/LICENSE` & `mllibs-0.1.5.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `mllibs-0.1.4.dist-info/METADATA` & `mllibs-0.1.5.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: mllibs
-Version: 0.1.4
+Version: 0.1.5
 Summary: Simplifying Machine Learning
 Home-page: https://github.com/shtrausslearning/mllibs
 Author: Andrey Shtrauss
 Author-email: shtraussart@gmail.com
 Project-URL: Bug Tracker, https://github.com/shtrausslearning/mllibs/issues
 Classifier: Programming Language :: Python :: 3
 Classifier: License :: OSI Approved :: MIT License
```

## Comparing `mllibs-0.1.4.dist-info/RECORD` & `mllibs-0.1.5.dist-info/RECORD`

 * *Files 12% similar despite different names*

```diff
@@ -9,27 +9,27 @@
 mllibs/mloader.py,sha256=3D56Fkft0IktHdw1upQtwMFT4msvqacMQzF5YemRdMU,5312
 mllibs/moutliers.py,sha256=962EY_bA0aHRFkU_QhPvDas2BCrFS-1ObZb_-NrQenY,12911
 mllibs/mpd_df.py,sha256=bXrF7OwGRYMQKRHYe84jftiW5R6J6AEgfB5JfNpQOzQ,6457
 mllibs/mseda.py,sha256=Qv4G1iBdmjUtqiYsYwRZq3uLAEmyxMDd3FMYLpK2WAc,6940
 mllibs/mslcatboost.py,sha256=yyFFwdywxrh9_DAZE1k9S8lMMft5urQ_6gtVHU69VB4,10512
 mllibs/mslensemble.py,sha256=w6ZuXH1ycx_GR7qAIL1ipDxYgtHR5xByk3oPAt4b3c0,28139
 mllibs/msllinear.py,sha256=IP_CVg0Fthss_WOgwsvzBK0iaD-Kj_TT3cu0NXs9NTU,19284
-mllibs/msltree.py,sha256=RTMAf6Z4Y3PnMBwkCm67c8ceAv-xNPPZkhEnL-lwP1c,11831
+mllibs/msltree.py,sha256=85osfqJJt2SNVN17xqWVLfgAnoUYq9VD9PlHU3GTY_Q,11831
 mllibs/mtextnorm.py,sha256=UzULYUE0sC05YgYvLxdcbTglKFiQS_7R-5_KItGiiDk,16763
 mllibs/musldimred.py,sha256=zm1yfU4LClEiCZE7gku-Vd-MyeND10HWsbstm7VyBVc,19605
-mllibs/nlpi.py,sha256=z2qbJzL4s-54VhVfnW9HhQzwtNGLvpe91ZLysYiTB98,32146
-mllibs/nlpm.py,sha256=3nd39JqyLj9NxXhvnr-hgs5nDYeL_YGllX1H55L7x7s,17085
+mllibs/nlpi.py,sha256=Niry0LeDg88ol3KUaWdoZJHp3dS9HAE7PDapcYWnHro,39882
+mllibs/nlpm.py,sha256=rG6iU9hUSPc-1SDwpaxoQLMWgJULi32NegZUozl0HP8,17773
 mllibs/corpus/wordlist.10000.txt,sha256=r2Zt6NAnUYF3WgVpXwWfcb49f0YwOzMIcnHNChSaYbw,75880
 mllibs/models/cv_ner_tagger.pickle,sha256=AyQjmX6WjO1Ubw5K55KDaxQ7vnAS6oQ3v15l_QHlTxs,126077
 mllibs/models/dtc_ner_tagger.pickle,sha256=1QXY45gqUaoOzU4zYik6DQoD-DB2hwsOliESF8iVE-Q,2745
 mlmodels/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 mlmodels/bl_regressor.py,sha256=XxpL3yeQPe_46SnkoRxpTt_4AM6q4PWAT8GD9dorVY0,2994
 mlmodels/gmm.py,sha256=y_HtXZvFJJ3gPbCHn_AAM7SMsF-6hPJpW9NVjIY38_s,4402
 mlmodels/gp_bclassifier.py,sha256=rDgCJhSchYx45iE-e7WuWdzax4bIoJgGQW_ZFKb8_kQ,5761
 mlmodels/gp_regressor.py,sha256=l43eI4a_4URUr6iz1ZHWi4U5NEWb5kvWewS_Sag0nAM,4627
 mlmodels/gpr_bclassifier.py,sha256=1c3Fr5f98F1MfLIiuAzB3Owdhn92dRqtZGNMECId_IY,6350
 mlmodels/kriging_regressor.py,sha256=sEyYE4U53N6iT7W38RH6Cj-QCQ-sEEkQHo2udqZzhtk,5170
-mllibs-0.1.4.dist-info/LICENSE,sha256=6kbiFSfobTZ7beWiKnHpN902HgBx-Jzgcme0SvKqhKY,1091
-mllibs-0.1.4.dist-info/METADATA,sha256=CK5kvpDVsuxRDJhOsJ8utTBqUTvHCj1zyIgK7hEEn4g,8344
-mllibs-0.1.4.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-mllibs-0.1.4.dist-info/top_level.txt,sha256=pOn_KI6Jw7pep1KTFkqiQHucEYgV3Mxor9LucXj_LY4,16
-mllibs-0.1.4.dist-info/RECORD,,
+mllibs-0.1.5.dist-info/LICENSE,sha256=6kbiFSfobTZ7beWiKnHpN902HgBx-Jzgcme0SvKqhKY,1091
+mllibs-0.1.5.dist-info/METADATA,sha256=sJptvYt8O1EF4_R5qA6frwbhM5EB6rhzwBDayr9t_Go,8344
+mllibs-0.1.5.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
+mllibs-0.1.5.dist-info/top_level.txt,sha256=pOn_KI6Jw7pep1KTFkqiQHucEYgV3Mxor9LucXj_LY4,16
+mllibs-0.1.5.dist-info/RECORD,,
```

