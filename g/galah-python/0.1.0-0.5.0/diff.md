# Comparing `tmp/galah_python-0.1.0-py3-none-any.whl.zip` & `tmp/galah_python-0.5.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,27 +1,32 @@
-Zip file size: 39369 bytes, number of entries: 25
--rw-r--r--  2.0 unx     1333 b- defN 23-Mar-13 22:09 galah/__init__.py
--rw-r--r--  2.0 unx     1963 b- defN 23-Apr-11 02:14 galah/apply_data_profile.py
--rw-r--r--  2.0 unx     2503 b- defN 23-Feb-28 02:29 galah/atlas_citation.py
--rw-r--r--  2.0 unx    10381 b- defN 23-Apr-26 02:57 galah/atlas_counts.py
--rw-r--r--  2.0 unx    11072 b- defN 23-Mar-16 23:50 galah/atlas_media.py
--rw-r--r--  2.0 unx    16453 b- defN 23-Apr-26 04:09 galah/atlas_occurrences.py
--rw-r--r--  2.0 unx     7337 b- defN 23-Apr-25 23:52 galah/atlas_species.py
--rw-r--r--  2.0 unx     4408 b- defN 23-Feb-28 02:29 galah/atlas_taxonomy.py
--rw-r--r--  2.0 unx      119 b- defN 23-Apr-24 05:01 galah/config.ini
--rw-r--r--  2.0 unx     3852 b- defN 23-Mar-16 23:50 galah/galah_config.py
--rw-r--r--  2.0 unx     5009 b- defN 23-Mar-13 21:58 galah/galah_filter.py
--rw-r--r--  2.0 unx       53 b- defN 22-Aug-31 03:44 galah/galah_geolocate.py
--rw-r--r--  2.0 unx     6839 b- defN 23-Feb-12 22:48 galah/galah_group_by.py
--rw-r--r--  2.0 unx     1669 b- defN 23-Mar-09 23:00 galah/galah_select.py
--rw-r--r--  2.0 unx     4007 b- defN 23-Apr-11 02:05 galah/get_api_url.py
--rw-r--r--  2.0 unx     7358 b- defN 23-Apr-24 05:02 galah/node_config.csv
--rw-r--r--  2.0 unx    14916 b- defN 23-Mar-09 04:28 galah/search_all.py
--rw-r--r--  2.0 unx     4841 b- defN 23-Apr-26 03:59 galah/search_taxa.py
--rw-r--r--  2.0 unx     2161 b- defN 23-Mar-09 04:29 galah/search_values.py
--rw-r--r--  2.0 unx    19967 b- defN 23-Apr-24 05:06 galah/show_all.py
--rw-r--r--  2.0 unx     3955 b- defN 23-Mar-09 21:43 galah/show_values.py
--rw-r--r--  2.0 unx      450 b- defN 23-May-01 03:58 galah_python-0.1.0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-May-01 03:58 galah_python-0.1.0.dist-info/WHEEL
--rw-r--r--  2.0 unx        6 b- defN 23-May-01 03:58 galah_python-0.1.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     1959 b- defN 23-May-01 03:58 galah_python-0.1.0.dist-info/RECORD
-25 files, 132703 bytes uncompressed, 36263 bytes compressed:  72.7%
+Zip file size: 57737 bytes, number of entries: 30
+-rw-r--r--  2.0 unx     1378 b- defN 23-Jun-29 22:09 galah/__init__.py
+-rw-r--r--  2.0 unx     2024 b- defN 23-Jun-29 22:09 galah/apply_data_profile.py
+-rw-r--r--  2.0 unx     2503 b- defN 23-Jun-29 22:08 galah/atlas_citation.py
+-rw-r--r--  2.0 unx    12317 b- defN 23-Jun-29 22:09 galah/atlas_counts.py
+-rw-r--r--  2.0 unx    11183 b- defN 23-Jun-29 22:09 galah/atlas_media.py
+-rw-r--r--  2.0 unx    23774 b- defN 23-Jun-29 22:09 galah/atlas_occurrences.py
+-rw-r--r--  2.0 unx     6969 b- defN 23-Jun-29 22:09 galah/atlas_species.py
+-rw-r--r--  2.0 unx     4408 b- defN 23-Jun-29 22:08 galah/atlas_taxonomy.py
+-rw-r--r--  2.0 unx     8768 b- defN 23-Jun-29 22:09 galah/common_dictionaries.py
+-rw-r--r--  2.0 unx     1587 b- defN 23-Jun-29 22:09 galah/common_functions.py
+-rw-r--r--  2.0 unx      187 b- defN 23-Jun-29 22:09 galah/config.ini
+-rw-r--r--  2.0 unx     4314 b- defN 23-Jun-29 22:09 galah/galah_config.py
+-rw-r--r--  2.0 unx     8191 b- defN 23-Jun-29 22:09 galah/galah_filter.py
+-rw-r--r--  2.0 unx     2785 b- defN 23-Jun-29 22:09 galah/galah_gbif_filters.py
+-rw-r--r--  2.0 unx     2679 b- defN 23-Jun-29 22:09 galah/galah_geolocate.py
+-rw-r--r--  2.0 unx    13133 b- defN 23-Jun-29 22:09 galah/galah_group_by.py
+-rw-r--r--  2.0 unx     1700 b- defN 23-Jun-29 22:09 galah/galah_select.py
+-rw-r--r--  2.0 unx     6550 b- defN 23-Jun-29 22:09 galah/gbif_assertions.csv
+-rw-r--r--  2.0 unx    11155 b- defN 23-Jun-29 22:09 galah/gbif_fields.csv
+-rw-r--r--  2.0 unx     4542 b- defN 23-Jun-29 22:09 galah/get_api_url.py
+-rw-r--r--  2.0 unx    11622 b- defN 23-Jun-29 22:09 galah/node_config.csv
+-rw-r--r--  2.0 unx    17468 b- defN 23-Jun-29 22:09 galah/search_all.py
+-rw-r--r--  2.0 unx    12787 b- defN 23-Jun-29 22:09 galah/search_taxa.py
+-rw-r--r--  2.0 unx     2277 b- defN 23-Jun-29 22:09 galah/search_values.py
+-rw-r--r--  2.0 unx    24134 b- defN 23-Jun-29 22:09 galah/show_all.py
+-rw-r--r--  2.0 unx     5520 b- defN 23-Jun-29 22:09 galah/show_values.py
+-rw-r--r--  2.0 unx      699 b- defN 23-Jun-29 22:38 galah_python-0.5.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jun-29 22:38 galah_python-0.5.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx        6 b- defN 23-Jun-29 22:38 galah_python-0.5.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     2376 b- defN 23-Jun-29 22:38 galah_python-0.5.0.dist-info/RECORD
+30 files, 207128 bytes uncompressed, 53999 bytes compressed:  73.9%
```

## zipnote {}

```diff
@@ -18,32 +18,47 @@
 
 Filename: galah/atlas_species.py
 Comment: 
 
 Filename: galah/atlas_taxonomy.py
 Comment: 
 
+Filename: galah/common_dictionaries.py
+Comment: 
+
+Filename: galah/common_functions.py
+Comment: 
+
 Filename: galah/config.ini
 Comment: 
 
 Filename: galah/galah_config.py
 Comment: 
 
 Filename: galah/galah_filter.py
 Comment: 
 
+Filename: galah/galah_gbif_filters.py
+Comment: 
+
 Filename: galah/galah_geolocate.py
 Comment: 
 
 Filename: galah/galah_group_by.py
 Comment: 
 
 Filename: galah/galah_select.py
 Comment: 
 
+Filename: galah/gbif_assertions.csv
+Comment: 
+
+Filename: galah/gbif_fields.csv
+Comment: 
+
 Filename: galah/get_api_url.py
 Comment: 
 
 Filename: galah/node_config.csv
 Comment: 
 
 Filename: galah/search_all.py
@@ -57,20 +72,20 @@
 
 Filename: galah/show_all.py
 Comment: 
 
 Filename: galah/show_values.py
 Comment: 
 
-Filename: galah_python-0.1.0.dist-info/METADATA
+Filename: galah_python-0.5.0.dist-info/METADATA
 Comment: 
 
-Filename: galah_python-0.1.0.dist-info/WHEEL
+Filename: galah_python-0.5.0.dist-info/WHEEL
 Comment: 
 
-Filename: galah_python-0.1.0.dist-info/top_level.txt
+Filename: galah_python-0.5.0.dist-info/top_level.txt
 Comment: 
 
-Filename: galah_python-0.1.0.dist-info/RECORD
+Filename: galah_python-0.5.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## galah/__init__.py

```diff
@@ -9,26 +9,27 @@
 #-------------------------------------------------------
 # galah titled functions
 #-------------------------------------------------------
 from .galah_config import galah_config
 from .galah_filter import galah_filter
 from .galah_group_by import galah_group_by
 from .galah_select import galah_select
+from .galah_geolocate import galah_geolocate
 
 #-------------------------------------------------------
 # search and show titled functions
 #-------------------------------------------------------
 from .search_all import search_all
 from .search_taxa import search_taxa
 from .show_all import show_all
 from .search_all import search_all
 from .show_values import show_values
 from .search_values import search_values
 
 #-------------------------------------------------------
 # version
 #-------------------------------------------------------
-__version__ = "0.1.0"
+__version__ = "0.5.0"
 
 # get all functions to display
 __all__=["atlas_counts","atlas_media","atlas_occurrences","atlas_species","galah_config","search_all","search_taxa",
          "show_all","search_all","show_values","search_values"]
```

## galah/apply_data_profile.py

```diff
@@ -6,45 +6,38 @@
     A 'profile' is a group of filters that are pre-applied by the ALA. Using a data profile allows a query to be filtered 
     quickly to the most relevant or quality-assured data that is fit-for-purpose. For example, the "ALA" profile is designed 
     to exclude lower quality records, whereas other profiles apply filters specific to species distribution modelling (e.g. CDSM).
 
     Parameters
     ----------
         baseURL : string
-            TBD  
+            The base URL that   
 
     Returns
     -------
-        TBD
-
-    Examples
-    --------
-
-        .. prompt:: python
-
-            galah.apply_data_profile()
-
-        .. program-output:: python -c "import galah; print(galah.atlas_counts())"
-
+        a string with the URL containing the data quality profile a user wants.
     """
 
     # first, get configurations and check for configurations
     configs = readConfig()
 
-    # adding a few things to baseURL
+    # check if the user has specified no data quality profile
     if configs['galahSettings']['data_profile'].lower() == "none":
         baseURL += "disableAllQualityfilters=true"
+
+    # if they have specified, add their specified data quality profile or throw an error
     else:
         data_profile_list = list(show_all(profiles=True)['shortName'])
         if configs['galahSettings']['data_profile'] in data_profile_list:
             baseURL += "qualityProfile={}&".format(configs['galahSettings']['data_profile'])
         else:
             raise ValueError("The data quality profile not recognised. To see valid data quality profiles, run \n\n"
                              "profiles = galah.show_all(profiles=True)\n\n"
                              "then type\n\n"
                              "profiles['shortName']\n\n"
                              "  To set a data profile, type\n" 
                              "galah.galah_config(data_profile=\"NAME FROM SHORTNAME HERE\")"
                              "If you don't want to use a data quality profile, set it to None by typing the following:\n\n"
                              "galah.galah_config(data_profile=\"None\")")
 
+    # return URL with data quality filter
     return baseURL
```

## galah/atlas_counts.py

```diff
@@ -1,57 +1,47 @@
-import requests,urllib.parse,warnings
+import requests,urllib.parse
 import pandas as pd
-from .galah_filter import galah_filter
 from .galah_group_by import galah_group_by
 from .search_taxa import search_taxa
-from .get_api_url import get_api_url
-from .get_api_url import readConfig
+from .get_api_url import get_api_url,readConfig
 from .apply_data_profile import apply_data_profile
+#from .galah_geolocate import galah_geolocate
+from .common_functions import add_filters
+from .common_dictionaries import ATLAS_KEYWORDS,COUNTS_NAMES,atlases
 
-ATLAS_KEYWORDS = {
-    "Australia": "taxonConceptID",
-    "Austria": "guid",
-    "Brazil": "guid", 
-    "Canada": "usageKey",
-    "Estonia": "guid",
-    "France": "usageKey",
-    "Guatemala": "guid",
-    "Portugal": "usageKey",
-    "Spain": "taxonConceptID",
-    "Sweden": "guid",
-    "United Kingdom": "guid",
-}
-
-atlases = ["Australia","Austria","Brazil","Canada","Estonia","France","Guatemala","Portugal","Sweden","Spain","United Kingdom"]
-
+#  polygon=None,
+#  bbox=None,
 def atlas_counts(taxa=None,
                  filters=None,
                  group_by=None,
+                 total_group_by=False,
                  expand=True,
-                 verbose=False,
                  use_data_profile=False,
+                 verbose=False,
                  ):
     """
-    Prior to downloading data it is often valuable to have some estimate of how many records are available, both for deciding 
-    if the query is feasible, and for estimating how long it will take to download. Alternatively, for some kinds of reporting, 
-    the count of observations may be all that is required, for example for understanding how observations are growing or shrinking 
-    in particular locations, or for particular taxa. 
+    Prior to downloading data it is often valuable to have some estimate of how many records are available, both for deciding
+    if the query is feasible, and for estimating how long it will take to download. Alternatively, for some kinds of reporting,
+    the count of observations may be all that is required, for example for understanding how observations are growing or shrinking
+    in particular locations, or for particular taxa.
     
-    To this end, ``galah.atlas_counts()`` takes arguments in the same format as 
-    ``galah.atlas_occurrences()``, and provides either a total count of records matching the criteria, or a data.frame of counts matching 
+    To this end, ``galah.atlas_counts()`` takes arguments in the same format as
+    ``galah.atlas_occurrences()``, and provides either a total count of records matching the criteria, or a data.frame of counts matching
     the criteria supplied to the `group_by` argument.
 
     Parameters
     ----------
         taxa : string
-            one or more scientific names. Use ``galah.search_taxa()`` to search for valid scientific names.  
+            one or more scientific names. Use ``galah.search_taxa()`` to search for valid scientific names.
         filters : pandas.DataFrame
             filters, in the form ``field`` ``logical`` ``value`` (e.g. ``"year=2021"``)
         group_by : string
             zero or more individual column names (i.e. fields) to include. See ``galah.show_all()`` and ``galah.search_all()`` to see valid fields.
+        total_group_by : logical
+            If ``True``, galah gives total number of groups in data. Defaults to ``False``.
         expand : logical
             When using the ``group_by`` argument of ``galah.atlas_counts()``, controls whether counts for each row value are combined or calculated separately. Defaults to ``True``.
         verbose : logical
             If ``True``, galah gives more information like progress bars. Defaults to ``False``.
         use_data_profile : string
             A profile name. Should be a string - the name or abbreviation of a data quality profile to apply to the query. Valid values can be seen using ``galah.show_all(profiles=True)``
 
@@ -65,182 +55,230 @@
 
         .. prompt:: python
 
             galah.atlas_counts()
 
         .. program-output:: python -c "import galah; print(galah.atlas_counts())"
 
-        Return records from 2020 onwards, grouped by year    
+        Return records from 2020 onwards, grouped by year
 
         .. prompt:: python
 
             galah.atlas_counts(filters="year>2019",group_by="year")
 
         .. program-output:: python -c "import galah; print(galah.atlas_counts(filters=\\\"year>2019\\\",group_by=\\\"year\\\",expand=False))"
-
+        
     """
 
     # get configs
     configs = readConfig()
-    # get the URL needed for the query
-    if use_data_profile and configs['galahSettings']['atlas'] == "Australia":
+
+    # get atlas
+    atlas = configs['galahSettings']['atlas']
+
+    # check for data quality profile for Australian atlas
+    if use_data_profile and atlas == "Australia":
         baseURL = apply_data_profile("{}?".format(get_api_url(column1='called_by',column1value='atlas_counts',column2="api_name",
                                                               column2value="records_counts"))) + "&"
+    
+    # check for Brazilian atlas
+    elif not use_data_profile and group_by is not None and atlas in ["Brazil"]:
+        baseURL = "{}?".format(get_api_url(column1='called_by', column1value='atlas_counts',column2="api_name",
+                                           column2value="records_facets"))
+    
+    # use this if they don't want a data quality profile or none exists
     elif not use_data_profile:
         baseURL = "{}?".format(get_api_url(column1='called_by', column1value='atlas_counts',column2="api_name",
                                            column2value="records_counts"))
+        
+    # raise error if argument is wrong type and/or the atlas doesn't have a quality profile but the user has specified one
     else:
         raise ValueError("True and False are the only values accepted for data_profile, and the only atlas using a data \n"
                          "quality profile is Australia.  Your atlas and data profile is \n"
                          "set in your config file.  To set your default filter, find out what profiles are on offer:\n"
                          "profiles = galah.show_all(profiles=True)\n\n"
                          "and then type\n\n"
                          "profiles['shortName']\n\n"
                          "to get the names of the data quality profiles you can use.  To set a data profile, type\n" 
                          "galah.galah_config(data_profile=\"NAME FROM SHORTNAME HERE\")"
                          "If you don't want to use a data quality profile, set it to None by typing the following:\n\n"
                          "galah.galah_config(data_profile=\"None\")"
                          )
+    
+    # test for data quality profile
+    if atlas in ["Australia"]:
+        baseURL = apply_data_profile(baseURL=baseURL)
 
     # if there is no taxa, assume you will get the total number of records in the ALA
     if taxa is None:
 
         # check if group_by exist
         if group_by is None:
 
             # check if filters are specified
             if filters is not None:
 
                 # check the type of variable filters is
                 if type(filters) is list or type(filters) is str:
-
-                    # change to list for easier looping
-                    if type(filters) is str:
-                        filters = [filters]
-
-                    # start URL - might need to add + "&" later
-                    URL = baseURL + "fq=%28"
-
-                    # loop over filters
-                    for f in filters:
-                        URL += galah_filter(f, ifgroupBy=expand) + "%20AND%20"
-
-                    # add final part of URL
-                    URL = URL[:-len("%20AND%20")] + "%29"
+                   
+                    URL = add_filters(URL=baseURL,atlas=atlas,filters=filters)
 
                 # else, make sure that the filters is in the following format
                 else:
                     raise TypeError(
                         "filters should only be a list, and are in the following format:\n\nfilters=[\'year:2020\']")
 
-            # else, add the final bit of the URL
+            # testing for galah_geolocate - implemented in next version
+            #if polygon or bbox:
+            #    URL += "&" + galah_geolocate(polygon=polygon,bbox=bbox)
+
+            # else, speficy that the page size is 0 to get only data we need
             else:
                 if configs["galahSettings"]["atlas"] == "Australia":
-                    URL = baseURL + "flimit=10000&pageSize=0"
+                    URL = baseURL + "&pageSize=0"
                 else:
-                    URL = baseURL
+                    URL = baseURL + "&pageSize=0"
 
             # check to see if the user wants the querying URL
             if verbose:
                 print("URL for querying:\n\n{}\n".format(URL))
 
             # get the response and data
             response = requests.get(URL)
-            json = response.json()
-
+            response_json = response.json()
+            
             # return dataFrame with total number of records
-            return pd.DataFrame({'totalRecords': [json['totalRecords']]})
+            return pd.DataFrame({'totalRecords': [response_json[COUNTS_NAMES[atlas]]]})
 
         # else, the user wants a grouped dataFrame
         else:
 
-            # return a grouped dataFrame
-            URL = baseURL + "fq="
-            return galah_group_by(URL, group_by=group_by, filters=filters, expand=expand, verbose=verbose)
+            # check for GBIF first
+            if configs["galahSettings"]['atlas'] not in ["Global","GBIF"]:
+
+                # add fq= to beginning to ensure filters are parsed correctly
+                URL = baseURL + "fq="
+                
+                # return grouped data frame
+                return galah_group_by(URL, group_by=group_by, filters=filters, expand=expand, verbose=verbose, total_group_by=total_group_by)
+            
+            # else, if not GBIF, just run group_by
+            else:
+
+                # return grouped data frame
+                return galah_group_by(baseURL, group_by=group_by, filters=filters, expand=expand, verbose=verbose, total_group_by=total_group_by)
 
     # if taxa exist, do this
     elif type(taxa) is str or type(taxa) is list:
 
         # change taxa into list for easier looping
         if type(taxa) is str:
             taxa = [taxa]
 
         # get the number of records associated with each taxa
-        for i,name in enumerate(taxa):
+        for name in taxa:
 
             # create temporary dataframe for taxon id
             tempdf = search_taxa(name)
-
+            
             # check if dataframe is empty - if so, return None; else, continue
             if tempdf.empty:
                 print("No taxon matches were found for {} in the selected atlas ({})".format(name, configs[
                     'galahSettings']['atlas']))
                 if len(taxa) == 1:
                     return None
                 continue
 
-        # get the taxonConceptID for taxa - first check for extant atlas
-        if configs['galahSettings']['atlas'] in atlases:
-            taxonConceptID = list(search_taxa(taxa)[ATLAS_KEYWORDS[configs['galahSettings']['atlas']]])
+        # get the taxonConceptID for taxa while checking for extant atlas
+        if atlas in atlases:
+            taxonConceptID = list(search_taxa(taxa)[ATLAS_KEYWORDS[atlas]])
         else:
-            raise ValueError("Atlas {} is not taken into account".format(configs['galahSettings']['atlas']))
+            raise ValueError("Atlas {} is not taken into account".format(atlas))
 
-        # add this ID to the URL
-        URL = baseURL + "fq=%28lsid%3A" + "%20OR%20lsid%3A".join(
-            urllib.parse.quote(str(tid)) for tid in taxonConceptID) + "%29"
+        # add taxon IDs to URL, but first check for GBIF
+        if atlas in ["Global","GBIF"]:
+
+            # add using taxonKey
+            URL = baseURL + "".join(["taxonKey={}&".format(urllib.parse.quote(str(tid))) for tid in taxonConceptID])
+        
+        else:
+
+            # add using lsid
+            URL = baseURL + "fq=%28lsid%3A" + "%20OR%20lsid%3A".join(
+                urllib.parse.quote(str(tid)) for tid in taxonConceptID) + "%29"
+
+        # testing for galah_geolocate
+        #if polygon or bbox:
+        #    URL += "&" + galah_geolocate(polygon=polygon,bbox=bbox)
 
         # return a grouped dataFrame
         if group_by is not None:
 
+            # check for filters
             if filters is not None:
-                URL += "%20AND%20"
-                return galah_group_by(URL, group_by=group_by, filters=filters, expand=expand, verbose=verbose)
+
+                # check for GBIF filters
+                if configs["galahSettings"]['atlas'] not in ["Global","GBIF"]:
+                    URL += "AND" #"%20AND%20"
+                
+                # return grouped data frame
+                return galah_group_by(URL, group_by=group_by, filters=filters, expand=expand, verbose=verbose, total_group_by=total_group_by)
+            
+            # no filters
             else:
-                return galah_group_by(URL, group_by=group_by, filters=filters, expand=expand, verbose=verbose)
+
+                # return grouped data frame
+                return galah_group_by(URL, group_by=group_by, filters=filters, expand=expand, verbose=verbose, total_group_by=total_group_by)
 
         else:
 
             # check to see if filters exist
             if filters is not None:
 
                 # check the type of variable filters is
                 if type(filters) is list or type(filters) is str:
 
                     # change to list for easier looping
                     if type(filters) is str:
                         filters = [filters]
 
-                    URL += "%20AND%20%28"
-
-                    # loop over filters
-                    for f in filters:
-                        URL += galah_filter(f, ifgroupBy=expand) + "%20AND%20"
-
-                    # add final part of URL
-                    URL = URL[:-len("%20AND%20")] + "%29&flimit=10000&pageSize=0"
+                    # raise an exception until I can figure out how to do this
+                    if atlas in ["Global","GBIF"] and ("!=" in filters or "=!" in filters):
+                        raise ValueError("The current iteration of GBIF and galah does not support != as an option.")
+                    
+                    # add filters to URL
+                    print(URL)
+                    URL = add_filters(URL=URL+"AND",atlas=atlas,filters=filters) + "&pageSize=0"
+                    print(URL)
 
                 # else, make sure that the filters is in the following format
                 else:
                     raise TypeError(
                         "filters should only be a list, and are in the following format:\n\nfilters=[\'year:2020\']")
 
             # add the last bit of the URL
             else:
 
-                # check if it's separate one last time
-                URL += "&flimit=10000&pageSize=0"
+                # check if atlas in GBIF
+                if atlas not in ["Global","GBIF"]:
+
+                    # add argument to reduce number of data coming in
+                    URL += "&pageSize=0"
 
-        # check to see if the user wants the URL for querying
+        # if verbose argument, print URL you are querying
         if verbose:
             print("URL for querying:\n\n{}\n".format(URL))
 
-        # get results form the URL
+        # response from query
         response = requests.get(URL)
-        json = response.json()
 
-        return pd.DataFrame({'totalRecords': [int(json['totalRecords'])]})
+        # turn response into json 
+        response_json = response.json()
+        
+        # return data frame
+        return pd.DataFrame({'totalRecords': [response_json[COUNTS_NAMES[atlas]]]})
 
     # if the taxa variable isn't a string or a list, raise an exception
     else:
         raise TypeError("The taxa argument can only be a string or a list."
                         "\nExample: atlas.counts(\"Vulpes vulpes\")"
                         "\n         atlas.counts[\"Osphranter rufus\",\"Vulpes vulpes\",\"Macropus giganteus\",\"Phascolarctos cinereus\"])")
```

## galah/atlas_media.py

```diff
@@ -65,18 +65,18 @@
         An object of class ``pandas.DataFrame``. If ``collect=True``, available image & media files are downloaded to a user local directory.
 
     Examples
     --------
 
     .. prompt:: python
 
-        filters = ["year=2020","decimalLongitude>153.0"]
-        galah.atlas_media(taxa="Ornithorhynchus anatinus",filters=filters)
+        galah.galah_config(atlas="Australia",email="youremail@example.com")
+        galah.atlas_media(taxa="Ornithorhynchus anatinus",filters=["year=2020","decimalLongitude>153.0")
 
-    .. program-output:: python -c "import galah; filters = [\\\"year=2020\\\",\\\"decimalLongitude>153.0\\\"];print(galah.atlas_media(taxa=\\\"Ornithorhynchus anatinus\\\",filters=filters))"
+    .. program-output:: python -c "import galah; galah.galah_config(atlas=\\\"Australia\\\",email=\\\"amanda.buyan@csiro.au\\\");print(galah.atlas_media(taxa=\\\"Ornithorhynchus anatinus\\\",filters=[\\\"year=2020\\\",\\\"decimalLongitude>153.0\\\"]))"
     
     """
 
     # get configs
     configs = readConfig()
 
     # check for fields
```

## galah/atlas_occurrences.py

```diff
@@ -1,56 +1,28 @@
-import sys,requests,urllib.parse,time,zipfile,io
+import sys,requests,urllib.parse,time,zipfile,io,json
+from requests.auth import HTTPBasicAuth
 import pandas as pd
+from .atlas_counts import atlas_counts
 from .galah_filter import galah_filter
 from .galah_select import galah_select
 from .search_taxa import search_taxa
 from .get_api_url import get_api_url, readConfig
 from .apply_data_profile import apply_data_profile
-
-ATLAS_KEYWORDS = {
-    "Australia": "taxonConceptID",
-    "Austria": "guid",
-    "Brazil": "guid",
-    "Canada": "usageKey",
-    "Estonia": "guid",
-    "France": "usageKey",
-    "Guatemala": "guid",
-    "Portugal": "usageKey",
-    "Spain": "taxonConceptID",
-    "Sweden": "guid",
-    "United Kingdom": "guid",
-}
-
-ATLAS_SELECTIONS = {
-    "Australia": "basic",
-    "Austria": [],
-    "Brazil": ["latitude","longitude","occurrence_date","taxon_name","common_name",
-                "taxon_concept_lsid","occurrence_id","data_resource_uid","occurrence_status"],
-    "Canada": [],
-    "Estonia": [],
-    "France": [],
-    "Guatemala": [],
-    "Portugal": [],
-    "Spain": ["latitude","longitude","occurrence_date","taxon_name","common_name",
-                "taxon_concept_lsid","occurrence_id","data_resource_uid","occurrence_status"],
-    "Sweden": [],
-    "United Kingdom": [],
-}
-
-#atlases = ["Australia","Austria","Brazil","Canada","Estonia","France","Guatemala","Portugal","Sweden","Spain","United Kingdom"]
-
-atlases = ["Australia","Brazil","Spain"]
+from .common_dictionaries import ATLAS_KEYWORDS,ATLAS_SELECTIONS, atlases
+from .common_functions import add_filters,add_predicates
 
 def atlas_occurrences(taxa=None,
                       filters=None,
                       test=False,
                       verbose=False,
                       fields=None,
                       assertions=None,
                       use_data_profile=False,
+                      species_list=False,
+                      status_accepted=True
                       ):
     """
     The most common form of data stored by living atlases are observations of individual life forms, known as 'occurrences'. 
     This function allows the user to search for occurrence records that match their specific criteria, and return them as a 
     ``pandas.DataFrame`` for analysis. Optionally, the user can also request a DOI for a given download to facilitate 
     citation and re-use of specific data resources.
 
@@ -79,15 +51,19 @@
                 - multimedia, multimediaLicence, images, videos, sounds
 
             See ``galah.show_all()`` and ``galah.search_all()`` to see all valid fields.
         assertions : string / list
             Using "assertions" returns all quality assertion-related columns. These columns are data quality checks run by each living atlas. The list of assertions is shown by ``galah.show_all(assertions=True)``.
         use_data_profile : string
             A profile name. Should be a string - the name or abbreviation of a data quality profile to apply to the query. Valid values can be seen using ``galah.show_all(profiles=True)``
-
+        species_list : logical
+            Denotes whether or not you want a species list for GBIF.  Default to ``False``.  For species lists, refer to ``atlas_species``
+        status_accepted : logical
+            Denotes whether or not you want only accepted taxonomic ranks for GBIF.  Default to ``True``.  For species lists, refer to ``atlas_species``
+    
     Returns
     -------
         An object of class ``pandas.DataFrame``.
 
     Examples
     --------
     
@@ -112,222 +88,377 @@
     .. program-output:: python -c "import galah; galah.galah_config(atlas=\\\"Australia\\\",email=\\\"amanda.buyan@csiro.au\\\"); print(galah.atlas_occurrences(taxa=\\\"Vulpes vulpes\\\",filters=\\\"year=2023\\\",fields=\\\"eventDate\\\"))"
 
     """
 
     # get configs
     configs = readConfig()
 
+    # get atlas
+    atlas = configs['galahSettings']['atlas']
+
     if configs["galahSettings"]["email"] is None:
         raise ValueError("Please provide an email for querying")
 
     # test to check if ALA is working
     requestURL = "{}?pageSize=0".format(get_api_url(column1='called_by',column1value='atlas_counts',column2="api_name",
                                                     column2value="records_counts"))
 
     # check if the ALA is working - if not, let the user know
     response = requests.get(requestURL)
     try:
         response.raise_for_status()
         if test:
             return
     except requests.exceptions.HTTPError as e:
-        print("The Atlas might be down...")
+        print("The {} atlas might be down...")
         print("Error: " + str(e))
         sys.exit()
 
     # get base URL
-    if use_data_profile and configs['galahSettings']['atlas'] == "Australia":
+    if use_data_profile and atlas == "Australia":
         baseURL = apply_data_profile("{}".format(get_api_url(column1='called_by', column1value='atlas_occurrences',
                                                               column2='api_name',column2value='records_occurrences',
                                                               add_email=True)))
     elif not use_data_profile:
         # check for these atlases
-        if configs['galahSettings']['atlas'] in ["Australia","Austria","Brazil","Guatemala","Spain","Sweden","United Kingdom"]:
+        if atlas in ["Australia","Austria","Brazil","France","Guatemala","Spain","Sweden","United Kingdom"]:
             baseURL = "{}disableAllQualityfilters=true&".format(get_api_url(column1='called_by', column1value='atlas_occurrences',
                                                                  column2='api_name', column2value='records_occurrences',
                                                                  add_email=True))
-        elif configs['galahSettings']['atlas'] in ["Estonia"]:
-            baseURL = "{}disableAllQualityfilters=true&".format(get_api_url(column1='called_by',
-                                                                                column1value='atlas_occurrences',
-                                                                                column2='api_name',
-                                                                                column2value='records',
-                                                                                add_email=True))
-        elif configs['galahSettings']['atlas'] in ["France","Portugal"]:
+        elif atlas in ["Estonia"]:
+            baseURL = "{}&".format(get_api_url(column1='called_by',column1value='atlas_occurrences',
+                                               column2='api_name',column2value='records',add_email=False))
+        elif atlas in ["Global","GBIF"]:
+            URL = "{}".format(get_api_url(column1='called_by',column1value='atlas_occurrences',
+                                    column2='api_name',column2value='records',add_email=False))
+        elif atlas in ["Portugal"]:
             baseURL = "{}disableAllQualityfilters=true&".format(get_api_url(column1='called_by',
                                                                                 column1value='atlas_occurrences',
                                                                                 column2='api_name',
                                                                                 column2value='records_query',
                                                                                 add_email=True))
         else:
-            raise ValueError("Atlas {} not taken into account".format(configs['galahSettings']['atlas']))
+            raise ValueError("Atlas {} not taken into account".format(atlas))
     else:
         raise ValueError("True and False are the only values accepted for data_profile, and the only atlas using a data \n"
                          "quality profile is Australia.  Your atlas and data profile is \n"
                          "set in your config file.  To set your default filter, find out what profiles are on offer:\n"
                          "profiles = galah.show_all(profiles=True)\n\n"
                          "and then type\n\n"
                          "profiles['shortName']\n\n"
                          "to get the names of the data quality profiles you can use.  To set a data profile, type\n" 
                          "galah.galah_config(data_profile=\"NAME FROM SHORTNAME HERE\")"
                          "If you don't want to use a data quality profile, set it to None by typing the following:\n\n"
                          "galah.galah_config(data_profile=\"None\")"
                          )
 
     # goes to the 'fields' argument in occurrence download (csv list, commas between)
-    if fields is not None:
-        baseURL += galah_select(select=fields)[:-3] + "&"
-    elif configs['galahSettings']['atlas'] in ["Australia","Brazil","Spain"]:
-        baseURL += galah_select(select=ATLAS_SELECTIONS[configs['galahSettings']['atlas']])[:-3] + "&"
+    if fields is not None and atlas not in ["Global","GBIF"]:
+        if fields != "all":
+            baseURL += galah_select(select=fields)[:-3] + "&"
+    elif atlas in ["Australia","Austria","Brazil","France","Spain"]:
+        baseURL += galah_select(select=ATLAS_SELECTIONS[atlas])[:-3] + "&"
+    elif fields is not None and atlas in ["Global","GBIF"]:
+        print("GBIF, unfortunately, does not support choosing your desired data fields before download.  You will have to download them and then get categories you want.")
+    elif atlas in ["Global","GBIF"]:
+        pass
     else:
-        raise ValueError("We currently cannot get occurrences from the {} atlas.".format(configs['galahSettings']['atlas']))
+        raise ValueError("We currently cannot get occurrences from the {} atlas.".format(atlas))
+
+    # create headers for GBIF
+    # did have username and notification thing here
+    headers = {
+        "User-Agent": "galah-python v0.1.0",
+        "X-USER-AGENT": "galah-python v0.1.0",
+        "Content-type": "application/json",
+        "Accept": "application/json",
+    }
+
+    # try this
+    predicates = []
 
     # check if taxa is specified
     if taxa is not None:
 
         # check variable type
         if type(taxa) == list or type(taxa) is str:
 
             # make taxa a list for easier looping
             if type(taxa) is str:
                 taxa=[taxa]
 
             # get the taxonConceptID for taxa - first check for extant atlas
-            if configs['galahSettings']['atlas'] in atlases:
-                taxonConceptID = list(search_taxa(taxa)[ATLAS_KEYWORDS[configs['galahSettings']['atlas']]])
+            if atlas in atlases:
+                taxonConceptID = list(search_taxa(taxa)[ATLAS_KEYWORDS[atlas]])
             else:
-                raise ValueError("Atlas {} is not taken into account".format(configs['galahSettings']['atlas']))
+                raise ValueError("Atlas {} is not taken into account".format(atlas))
 
             # generate the desired URL and get a response from the API - add taxonConceptIDs to the URL
-            URL = baseURL + "&fq=%28lsid%3A" + "%20OR%20lsid%3A".join(
-                urllib.parse.quote(str(tid)) for tid in taxonConceptID) + "%29"
+            if atlas in ["Global","GBIF"]:
+                for tid in taxonConceptID:
+                    predicates.append({"type":"equals","key":"TAXON_KEY","value":str(tid)})
+            else:
+                URL = baseURL + "&fq=%28lsid%3A" + "%20OR%20lsid%3A".join(
+                    urllib.parse.quote(str(tid)) for tid in taxonConceptID) + "%29"
 
             # check what type of variable filters is; handle accordingly
             if filters is not None:
-                URL += "%20AND%20%28"
-                if type(filters) is str:
-                    URL += galah_filter(filters) + "%29"
-                elif type(filters) is list:
-                    for f in filters:
-                        URL += galah_filter(f) + "%20AND%20"
-                    URL = URL[:-len("%20AND%20")] + "%29"
-                else:
-                    raise ValueError("The filters argument needs to be either a string or a list")
+
+                if type(filters) is list or type(filters) is str:
                 
+                    # try this out
+                    if atlas in ["Global","GBIF"] and ("!=" in filters or "=!" in filters):
+                        raise ValueError("The current iteration of GBIF and galah does not support != as an option.")
+                    elif atlas in ["Global","GBIF"]:
+                        predicates = add_predicates(predicates=predicates,filters=filters)
+                    else:
+                        URL += "%20AND%20"
+                        URL = add_filters(URL=URL,atlas=atlas,filters=filters)
+
             # take care of assertions
             if assertions is not None:
 
                 # check type
                 if type(assertions) is list or type(assertions) is str:
-                    if type(assertions) is str:
-                        URL += galah_filter(assertions) + "%29"
-                    elif type(assertions) is list:
-                        for a in assertions:
-                            URL += galah_filter(a) + "%20AND%20"
-                        URL = URL[:-len("%20AND%20")] + "%29"
+
+                    # check for GBIF
+                    if atlas in ["Global","GBIF"]:
+                        predicates = add_predicates(predicates=predicates,filters=filters)
+                    else:
+                        URL = add_filters(URL=URL,atlas=atlas,filters=assertions)
+
                 else:
                     raise ValueError("Assertions needs to be a string or a list of strings, i.e. identificationIncorrect == TRUE")
             
             # add final part of URL
-            URL += "&qa=none&"
+            if atlas not in ["Global","GBIF"]:
+                URL += "&qa=none&"
 
-            # check to see if user wants the query URL
             if verbose:
                 print("URL for querying:\n\n{}\n".format(URL))
 
+            # authentication
+            if atlas in ["Global","GBIF"]:
+                # create authentication key
+                authentication = HTTPBasicAuth(configs['galahSettings']['usernameGBIF'],configs['galahSettings']['passwordGBIF'])
+                # create payload
+                if species_list:
+                    format="SPECIES_LIST"
+                    if status_accepted:
+                        predicates.append({"type": "equals","key":"TAXONOMIC_STATUS","value":"ACCEPTED"})
+                else:
+                    format="SIMPLE_CSV"
+                payload = json.dumps({
+                    "creator": configs['galahSettings']['usernameGBIF'], # username
+                    "notificationAddresses": [configs['galahSettings']['email']], # change from hard-coded
+                    "sendNotification": "false",
+                    "format": format,
+                    "predicate": {
+                        "type": "and",
+                        "predicates": predicates
+                    }
+                })
+
+                # check to see if user wants the query URL
+                if verbose:
+                    print("URL for querying:\n\n{}\n".format(URL))
+                    print("payload: \n\n{}\n".format(payload))
+                # check counts
+                counts = atlas_counts(taxa,filters=filters)
+                if not species_list:
+                    print("total records for occurrences: {}".format(counts['totalRecords'][0]))
+                    if int(counts['totalRecords'][0]) > 101000:
+                        raise ValueError("Your data request of {} is too large. \nThe maximum number of requests is 101,000.\nPlease filter your data and use atlas_counts() to get the counts to a reasonable number.".format(counts['totalRecords'][0]))
+                # get response
+                response = requests.post(URL,headers=headers,auth=authentication,data=payload)
+            else:
+                response = requests.get(URL)
+
+            # get job number
+            job_number = response.text
+
             # query the api
-            response = requests.get(URL)
             if response.status_code == 403:
-                if configs['galahSettings']['atlas'] == "Brazil":
+                if atlas == "Australia":
+                    raise ValueError("It appears that you are not registered as a user on the Australian atlas.  Please go to https://auth.ala.org.au/cas/login to register.")
+                if atlas == "Brazil":
                     raise ValueError("It appears that you are not registered as a user on the Brazilian atlas.  Please email atendimento_sibbr@rnp.br to find out more information.")
-                if configs['galahSettings']['atlas'] == "Spain":
+                if atlas == "France":
+                    raise ValueError("It appears that you are not registered as a user on the French atlas.  Please email ??? to find out more information.")
+                if atlas == "GBIF":
+                    raise ValueError("It appears that you are not registered as a user on the GBIF Global atlas.  Please go to https://www.gbif.org/user/profile to register.")
+                if atlas == "Spain":
                     raise ValueError("It appears that you are not registered as a user on the Spanish atlas.  Please go to https://auth.gbif.es/cas/login?lang=en to register.")
-            if response.json()['status'] == "skipped":
-                raise ValueError(response.json()["error"])
+            if atlas not in ["GBIF","Global"]:
+                if response.json()['status'] == "skipped":
+                    raise ValueError(response.json()["error"])
 
             # this may take a while - occasionally check if status has changed
-            statusURL = requests.get(response.json()['statusUrl'])
-            while statusURL.json()['status'] == 'inQueue':
-                time.sleep(5)
-                statusURL = requests.get(response.json()['statusUrl'])
-            while statusURL.json()['status'] == 'running':
-                time.sleep(5)
+            if atlas in ["Global","GBIF"]:
+                downloadURL = URL.replace("request",job_number)
+                # check to see if the user wants the zip URL
+                if verbose:
+                    print("URL for download:\n\n{}\n".format(downloadURL))
+                response_download = requests.get(downloadURL,headers=headers,auth=authentication)
+                while response_download.json()["status"] != "SUCCEEDED":
+                    time.sleep(5)
+                    response_download = requests.get(downloadURL,headers=headers,auth=authentication)
+                zipURL = response_download.json()["downloadLink"]
+
+                # check to see if the user wants the zip URL
+                if verbose:
+                    print("Data for download:\n\n{}\n".format(zipURL))
+
+                # return dataFrame
+                data = requests.get(zipURL)
+                return pd.read_csv(zipfile.ZipFile(io.BytesIO(data.content)).open('{}.csv'.format(job_number)),sep='\t',low_memory=False)    
+            
+            else:
                 statusURL = requests.get(response.json()['statusUrl'])
-            zipURL = requests.get(statusURL.json()['downloadUrl'])
-
-            # check to see if the user wants the zip URL
-            if verbose:
-                print("Data for download:\n\n{}\n".format(statusURL.json()['downloadUrl']))
+                while statusURL.json()['status'] == 'inQueue':
+                    time.sleep(5)
+                    statusURL = requests.get(response.json()['statusUrl'])
+                while statusURL.json()['status'] == 'running':
+                    time.sleep(5)
+                    statusURL = requests.get(response.json()['statusUrl'])
+                zipURL = statusURL.json()['downloadUrl']
+                data = requests.get(zipURL)
+
+                # check to see if the user wants the zip URL
+                if verbose:
+                    print("Data for download:\n\n{}\n".format(zipURL))
 
-            # return dataFrame
-            return pd.read_csv(zipfile.ZipFile(io.BytesIO(zipURL.content)).open('data.csv'),low_memory=False)
+                # return dataFrame
+                return pd.read_csv(zipfile.ZipFile(io.BytesIO(data.content)).open('data.csv'),low_memory=False)
 
         # else, the user needs to specify the taxa in the correct format
         else:
             raise TypeError("The taxa argument can only be a string or a list."
                         "\nExample: taxa.taxa(\"Vulpes vulpes\")"
                         "\n         taxa.taxa([\"Osphranter rufus\",\"Vulpes vulpes\",\"Macropus giganteus\",\"Phascolarctos cinereus\"])")
     
     elif filters is not None:
-    
-        # start URL
-        URL = baseURL + "&fq=%28"
 
-        if type(filters) is str:
-            URL += galah_filter(filters) + "%20AND%20"
-        elif type(filters) is list:
-            for f in filters:
-                URL += galah_filter(f) + "%20AND%20"
+        if type(filters) is str or type(filters) is list:
+            if atlas in ["Global","GBIF"] and ("!=" in filters or "=!" in filters):
+                raise ValueError("The current iteration of GBIF and galah does not support != as an option.")
+            elif atlas in ["Global","GBIF"]:
+                predicates = add_predicates(predicates=predicates,filters=filters)
+            else:
+                # start URL
+                URL += "&fq=%28"
+                URL = add_filters(URL=URL,atlas=atlas,filters=filters)
         else:
             raise ValueError("The filters argument needs to be either a string or a list")
 
         # take care of assertions
         if assertions is not None:
 
             # check type
             if type(assertions) is list or type(assertions) is str:
                 if type(assertions) is str:
                     assertions=[assertions]
-                for a in assertions:
-                    URL += galah_filter(a) + "%20AND%20"
-
+                if atlas in ["Global","GBIF"]:
+                    predicates = add_predicates(predicates=predicates,filters=filters)
+                else:
+                    for a in assertions:
+                        URL += galah_filter(a) + "%20AND%20"
+                        URL = URL[:-len("%20AND%20")] + "%29&qa=none&"
             else:
                 raise ValueError("Assertions needs to be a string or a list of strings, i.e. identificationIncorrect == TRUE")
 
-        # add final part of URL
-        URL = URL[:-len("%20AND%20")] + "%29&qa=none&"
-
         # check to see if user wants the query URL
         if verbose:
             print("URL for querying:\n\n{}\n".format(URL))
 
         # query the api
-        response = requests.get(URL)
-        if response.status_code == 403:
-            # TODO: write more exceptions to make sure contact details are ok
-            if configs['galahSettings']['atlas'] == "Brazil":
-                raise ValueError("It appears that you are not registered as a user on the Brazilian atlas.  Please email atendimento_sibbr@rnp.br to find out more information.")
-            if configs['galahSettings']['atlas'] == "Spain":
-                raise ValueError("It appears that you are not registered as a user on the Spanish atlas.  Please go to https://auth.gbif.es/cas/login?lang=en to register.")
-        if response.json()['status'] == "skipped":
-            raise ValueError(response.json()["error"])
-
-        # this may take a while - occasionally check if status has changed
-        statusURL = requests.get(response.json()['statusUrl'])
-        while statusURL.json()['status'] == 'inQueue':
-            time.sleep(5)
-            statusURL = requests.get(response.json()['statusUrl'])
-        while statusURL.json()['status'] == 'running':
-            time.sleep(5)
+        # authentication
+        if atlas in ["Global","GBIF"]:
+            # create authentication key
+            authentication = HTTPBasicAuth(configs['galahSettings']['usernameGBIF'],configs['galahSettings']['passwordGBIF'])
+                
+            # create payload
+            if species_list:
+                format="SPECIES_LIST"
+                if status_accepted:
+                    predicates.append({"type": "equals","key":"TAXONOMIC_STATUS","value":"ACCEPTED"})
+                else:
+                    format="SIMPLE_CSV"
+            payload = json.dumps({
+                "creator": configs['galahSettings']['usernameGBIF'], # username
+                "notificationAddresses": [configs['galahSettings']['email']], # change from hard-coded
+                "sendNotification": "false",
+                "format": format,
+                "predicate": {
+                    "type": "and",
+                    "predicates": predicates
+                }
+            })
+
+            # check to see if user wants the query URL
+            if verbose:
+                print("URL for querying:\n\n{}\n".format(URL))
+                print("payload: \n\n{}\n".format(payload))
+            
+            # check counts
+            counts = atlas_counts(taxa,filters=filters)
+            if not species_list:
+                print("total records for occurrences: {}".format(counts['totalRecords'][0]))
+                if int(counts['totalRecords'][0]) > 101000:
+                    raise ValueError("Your data request of {} is too large. \nThe maximum number of requests is 101,000.\nPlease filter your data and use atlas_counts() to get the counts to a reasonable number.".format(counts['totalRecords'][0]))
+            # get response
+            response = requests.post(URL,headers=headers,auth=authentication,data=payload)
+            
+            # get job number
+            job_number = response.text
+
+            # get download URL
+            downloadURL = URL.replace("request",job_number)
+            # check to see if the user wants the zip URL
+            if verbose:
+                print("URL for download:\n\n{}\n".format(downloadURL))
+            response_download = requests.get(downloadURL,headers=headers,auth=authentication)
+            while response_download.json()["status"] != "SUCCEEDED":
+                time.sleep(5)
+                response_download = requests.get(downloadURL,headers=headers,auth=authentication)
+            zipURL = response_download.json()["downloadLink"]
+
+            # check to see if the user wants the zip URL
+            if verbose:
+                print("Data for download:\n\n{}\n".format(zipURL))
+
+            # return dataFrame
+            data = requests.get(zipURL)
+            return pd.read_csv(zipfile.ZipFile(io.BytesIO(data.content)).open('{}.csv'.format(job_number)),sep='\t',low_memory=False)    
+            
+        else:    
+            response = requests.get(URL)
+
+            # if response.status_code == 403:
+            #     # TODO: write more exceptions to make sure contact details are ok
+            #     if atlas == "Brazil":
+            #         raise ValueError("It appears that you are not registered as a user on the Brazilian atlas.  Please email atendimento_sibbr@rnp.br to find out more information.")
+            #     if atlas == "Spain":
+            #         raise ValueError("It appears that you are not registered as a user on the Spanish atlas.  Please go to https://auth.gbif.es/cas/login?lang=en to register.")
+            # if response.json()['status'] == "skipped":
+            #     raise ValueError(response.json()["error"])
+
             statusURL = requests.get(response.json()['statusUrl'])
-        zipURL = requests.get(statusURL.json()['downloadUrl'])
+            while statusURL.json()['status'] == 'inQueue':
+                time.sleep(5)
+                statusURL = requests.get(response.json()['statusUrl'])
+            while statusURL.json()['status'] == 'running':
+                time.sleep(5)
+                statusURL = requests.get(response.json()['statusUrl'])
+            zipURL = requests.get(statusURL.json()['downloadUrl'])
 
-        # check to see if the user wants the zip URL
-        if verbose:
-            print("Data for download:\n\n{}\n".format(statusURL.json()['downloadUrl']))
+            # check to see if the user wants the zip URL
+            if verbose:
+                print("Data for download:\n\n{}\n".format(statusURL.json()['downloadUrl']))
 
-        # return dataFrame
-        return pd.read_csv(zipfile.ZipFile(io.BytesIO(zipURL.content)).open('data.csv'),low_memory=False)
+            # return dataFrame
+            return pd.read_csv(zipfile.ZipFile(io.BytesIO(zipURL.content)).open('data.csv'),low_memory=False)
 
     else:
         raise Exception('You cannot get all 10 million records for the ALA.  Please specify at least one taxa and/or '
                         'filters to get occurrence records associated with the taxa.')
```

## galah/atlas_species.py

```diff
@@ -1,87 +1,39 @@
-import requests,urllib.parse
+import requests,urllib.parse,io
 import pandas as pd
 
 from .search_taxa import search_taxa
 from .get_api_url import get_api_url,readConfig
-from .galah_filter import galah_filter
-
-import sys
-
-ATLAS_KEYWORDS = {
-    "Australia": "taxonConceptID",
-    "Austria": "guid",
-    "Brazil": "guid", 
-    "Canada": "usageKey",
-    "Estonia": "guid",
-    "France": "usageKey",
-    "Guatemala": "guid",
-    "Portugal": "usageKey",
-    "Spain": "taxonConceptID",
-    "Sweden": "guid",
-    "United Kingdom": "guid",
-}
-
-VERNACULAR_NAMES = {
-    "Australia": ["commonNames","nameString"],
-    "Austria": "",
-    "Brazil": ["commonNames","nameString"], 
-    "Canada": "",
-    "Estonia": "",
-    "France": "",
-    "Guatemala": "",
-    "Portugal": "",
-    "Spain": ["commonNames","nameString"],
-    "Sweden": "",
-    "United Kingdom": "",
-}
-
-TAXONCONCEPT_NAMES = {
-    "Australia": {"species_guid": "guid","species": "nameString","author": "author"},
-    "Austria": "",
-    "Brazil": {"species_guid": "guid","species": "nameString","author": "author"},
-    "Canada": "",
-    "Estonia": "",
-    "France": "",
-    "Guatemala": "",
-    "Portugal": "",
-    "Spain": {"species_guid": "guid","species": "nameString","author": "author"},
-    "Sweden": "",
-    "United Kingdom": "",
-}
-
-FACETS_STRINGS = {
-    "Australia": "speciesID",
-    "Austria": "",
-    "Brazil": "species", 
-    "Canada": "",
-    "Estonia": "",
-    "France": "",
-    "Guatemala": "",
-    "Portugal": "",
-    "Spain": "species",
-    "Sweden": "",
-    "United Kingdom": "",
-}
+from .atlas_occurrences import atlas_occurrences
+from .common_dictionaries import ATLAS_KEYWORDS,ATLAS_SPECIES_FIELDS,atlases
+from .common_functions import add_filters
 
 # this function looks for all species with the associated name
-def atlas_species(taxa=None,filters=None,verbose=False):
+def atlas_species(taxa=None,
+                  rank="species",
+                  filters=None,
+                  verbose=False,
+                  status_accepted=True):
     """
     While there are reasons why users may need to check every record meeting their search criteria (i.e. using ``galah.atlas_occurrences()``), 
     a common use case is to simply identify which species occur in a specified region, time period, or taxonomic group. 
     This function returns a ``pandas.DataFrame`` with one row per species, and columns giving associated taxonomic information.
 
     Parameters
     ----------
         taxa : string / list
             one or more scientific names. Use ``galah.search_taxa()`` to search for valid scientific names.  
         rank : string
+            the rank you ultimately want to get names for, i.e. "genus" or "species".  Default is ``species``.
+        filters : string
             filters, in the form ``field`` ``logical`` ``value`` (e.g. ``"year=2021"``)
-        verbose : 
+        verbose : logical
             If ``True``, galah gives you the URLs used to query all the data.  Default to ``False``.
+        status_accepted : logical
+            If ``True``, galah gives you only the accepted taxonomic ranks. Default is ``False``.  **FOR GBIF ONLY
 
     Returns
     -------
         An object of class ``pandas.DataFrame``.
 
     Examples
     --------
@@ -92,111 +44,125 @@
 
     .. program-output:: python -c "import galah; print(galah.atlas_species(taxa=\\\"Heleioporus\\\"))"
     """
 
     # get configs
     configs = readConfig()
 
-    # first, check if the user has specified a taxa
-    if taxa is None:
-        raise ValueError("You need to specify a name for this function to work, i.e. \"Heleioporus\"")
-    elif type(taxa) is not str and type(taxa) is not list:
+    # get atlas
+    atlas = configs['galahSettings']['atlas'] 
+
+    # first, check if the user has specified a taxa and if it is of the right variable type
+    if type(taxa) is not str and type(taxa) is not list and taxa is not None:
         raise ValueError("Only a string or list can be specified for taxa names")
 
-    # call galah_identify (or search_taxa for now?) to do something
-    baseURL = get_api_url(column1='api_name', column1value='records_species')
+    # check to see if rank is in possible ranks for atlas
+    if rank.lower() not in ATLAS_SPECIES_FIELDS[atlas]:
+        raise ValueError("{} is not a valid rank for the {} atlas.  Possible ranks are:\n\n{}\n".format(rank,atlas,", ".join(ATLAS_SPECIES_FIELDS[atlas])))
+
+    # get the ID of the rank to use to facet the data
+    rankID = ATLAS_SPECIES_FIELDS[atlas][rank]
+    
+    # get initial url
+    if atlas not in ["Global","GBIF"]:
+        baseURL = get_api_url(column1='api_name', column1value='records_species')
+    else:
+        baseURL = get_api_url(column1='api_name', column1value='records_occurrences')
 
     # raise warning - not sure how to fix it
-    if configs['galahSettings']['atlas'] in ["Spain"]:
-        print("There have been some issues getting all species when using a genus name.  Instead, either use a species name or anything of family or higher order.")
+    if atlas in ["Spain"]:
+        print("There have been some issues getting all species when using a genus name.  If genus doesn't work, either use a species name or anything of family or higher order.")
+    if atlas in ["Sweden"]:
+        print("There have been some issues getting taxonomy from the Swedish atlas, as they don't store names of taxon higher than species.")
 
-    # check if filters are specified
-    if filters is not None:
+    # get the taxonConceptID for taxa
+    if atlas in ["Australia","Austria","Brazil","France","Guatemala","Spain","Sweden"]:
 
-        # check the type of variable filters is
-        if type(filters) is list or type(filters) is str:
+        # if there is no taxa, just add question mark
+        if taxa is None:
+            
+            # remember to add question mark
+            URL = baseURL + "?"
 
-            # change to list for easier looping
-            if type(filters) is str:
-                filters = [filters]
+        # if there is taxa, add these as fields on the URL
+        else:
+            
+            # get the taxonConceptID for taxa - first check for extant atlas
+            if atlas in atlases:
+                taxonConceptID = list(search_taxa(taxa)[ATLAS_KEYWORDS[atlas]])
+            else:
+                raise ValueError("Atlas {} is not taken into account".format(atlas))
 
-            # start URL - might need to add + "&" later
-            URL = baseURL + "?&fq=%28"
+            # add taxon IDs to the URL
+            if atlas in ["Global","GBIF"]:
+                URL = baseURL + "".join(["taxonKey={}&".format(
+                    urllib.parse.quote(str(tid))) for tid in taxonConceptID])
+            else:
+                URL = baseURL + "?fq=%28lsid%3A" + "%20OR%20lsid%3A".join(
+                    urllib.parse.quote(str(tid)) for tid in taxonConceptID) + "%29"
+        
+        # check if filters are specified
+        if filters is not None:
 
-            # loop over filters
-            for f in filters:
-                URL += galah_filter(f) + "%20AND%20"
+            # check the type of variable filters is
+            if type(filters) is list or type(filters) is str:
 
-            # add final part of URL
-            URL = URL[:-len("%20AND%20")] + "%29"
+                # add filters to URL
+                if taxa is None:
+                    URL = add_filters(URL=URL+"fq=",atlas=atlas,filters=filters) + "&facets={}".format(rankID)
+                else:
+                    URL = add_filters(URL=URL+"AND",atlas=atlas,filters=filters) + "&facets={}".format(rankID) #%29
 
-        # else, make sure that the filters is in the following format
+            # else, make sure that the filters is in the following format
+            else:
+                raise TypeError("filters should only be a list, and are in the following format:\n\nfilters=[\'year:2020\']")
+        
         else:
-            raise TypeError("filters should only be a list, and are in the following format:\n\nfilters=[\'year:2020\']")
-    else:
-
-        URL = baseURL + "?"
-    
-     # get the taxonConceptID for taxa
-    if configs['galahSettings']['atlas'] in ["Australia","Brazil","Spain"]:
-        taxonConceptID = search_taxa(taxa)[ATLAS_KEYWORDS[configs['galahSettings']['atlas']]][0]
-        URL += "&fq=%28lsid%3A" + urllib.parse.quote(taxonConceptID) + "%29&facets={}".format(FACETS_STRINGS[configs['galahSettings']['atlas']])
-    else:
-        raise ValueError("Atlas {} is not taken into account".format(configs['galahSettings']['atlas']))
+             
+            # add facets into URL
+            URL += "&facets={}".format(rankID)
 
-    # check to see if user wants the query URL
-    if verbose:
-        print("URL for querying:\n\n{}\n".format(URL))
-
-    # get url and transform from a text string to a list
-    response = requests.get(URL)
-    all_ids = response.text[1:-2].split('"\n"')[1:]
-
-    # need to get species, author and
-    data_dict = {"species": [], "author": [], "species_guid": [], "kingdom": [],"phylum": [],
-                 "class": [],"order": [],"family": [], "vernacular_name": []}
-
-    # species_lookup for each individual species
-    baseURL = get_api_url(column1='api_name', column1value='species_lookup')
-
-    # get all the taxonomic information for every species ID
-    for species_guid in all_ids:
-        
-        # check for atlas
-        if configs['galahSettings']['atlas'] in ["Brazil","Spain"]:
-            URL = baseURL + "/" + urllib.parse.quote(species_guid)
-        elif configs['galahSettings']['atlas'] in ["Australia"]:
-            URL = baseURL.replace("{id}", species_guid)
-        else:
-            raise ValueError("Atlas {} is not taken into account".format(configs['galahSettings']['atlas']))
+        # set lookup=True to get all species data
+        URL += "&lookup=True"
 
         # check to see if user wants the query URL
         if verbose:
             print("URL for querying:\n\n{}\n".format(URL))
 
-        # get response from the API
+        # get url and transform from a text string to a list
         response = requests.get(URL)
-        json = response.json()
+        return pd.read_csv(io.StringIO(response.text))
         
-        # get taxonomic information
-        for depth in ['kingdom','phylum','class','order','family']:
-            if depth in json['classification']:
-                data_dict[depth].append(json['classification'][depth].lower().capitalize())
-            else:
-                data_dict[depth].append("")
-            
-        for others in ['species','author','species_guid']:
-            if json['taxonConcept'][TAXONCONCEPT_NAMES[configs["galahSettings"]["atlas"]][others]]:
-                data_dict[others].append(
-                    json['taxonConcept'][TAXONCONCEPT_NAMES[configs["galahSettings"]["atlas"]][others]].lower().capitalize())
-            else:
-                data_dict[others].append("")
+    # GBIF is treated differently, as it gives us a species list
+    elif atlas in ["Global","GBIF"]:
 
-        # get common names (vernacular names)
-        if json[VERNACULAR_NAMES[configs['galahSettings']['atlas']][0]]:
-            data_dict["vernacular_name"].append(
-                json[VERNACULAR_NAMES[configs['galahSettings']['atlas']][0]][0][VERNACULAR_NAMES[configs['galahSettings']['atlas']][1]])
-        else:
-            data_dict["vernacular_name"].append("")
+        # get the initial list
+        test_list = atlas_occurrences(taxa=taxa,filters=filters,species_list=True,verbose=verbose,status_accepted=status_accepted)
+        
+        # get the species fields for the data frame to check against
+        species_fields = list(ATLAS_SPECIES_FIELDS[atlas].keys())
 
-    # return data as a pandas dataframe
-    return pd.DataFrame.from_dict(data_dict)
+        # get the index of your rank, and the one below it
+        index = species_fields.index(rank)
+        
+        # if rank is not species, only select for rank user has specified
+        if rank != "species":
+
+            rank_below = species_fields[index+1]
+
+            # only select ranks user is interested in
+            curated_list = test_list[test_list[rank_below].map(type) == float]
+            
+            # remove unnecessary fields
+            for i in species_fields[index+1:]:
+                del curated_list[i]
+                del curated_list["{}Key".format(i)]
+
+            # return the curated list
+            return curated_list.reset_index(drop=True)
+        
+        # else, return everything
+        return test_list.reset_index(drop=True)
+    
+    # else, this atlas hasn't been integrated into atlas_species yet
+    else:
+        raise ValueError("Atlas {} is not taken into account".format(atlas))
```

## galah/config.ini

```diff
@@ -1,7 +1,10 @@
 [galahSettings]
 email = email@example.com
 email_notify = False
 atlas = Australia
 data_profile = ALA
 ranks=all
 reason=4
+# remove defaults when releasing
+usernameGBIF = ""
+passwordGBIF = ""
```

## galah/galah_config.py

```diff
@@ -9,15 +9,17 @@
 # https://www.codeproject.com/Articles/5319621/Configuration-Files-in-Python
 # run this first at installation
 def galah_config(email=None,
                  email_notify=None,
                  atlas=None,
                  data_profile = None,
                  ranks = None,
-                 reason = None):
+                 reason = None,
+                 usernameGBIF = None,
+                 passwordGBIF = None):
     """
     The galah package supports large data downloads, and also interfaces with the ALA which requires that users of some 
     services provide a registered email address and reason for downloading data. The ``galah_config()`` function provides a way 
     to manage these issues as simply as possible.
 
     Parameters
     ----------
@@ -29,14 +31,18 @@
             Living Atlas to point to, ``Australia`` by default. Can be an organisation name, acronym, or region (see ``show_all(atlases=True)`` for admissible values)
         data_profile : string
             A profile name. Should be a string - the name or abbreviation of a data quality profile to apply to the query. Valid values can be seen using ``galah.show_all(profiles=True)``
         ranks: string
             A string letting galah know what taxonomic ranks to show.  Use "all" to see all 69 possible ranks, and "gbif" to see the 9 most common ranks.
         reason: integer
             A number (integer) providing the reason you are downloading data.  Default is set to 4 (scientific research).  For a list of all possible reasons run ``galah.show_all_reasons()``
+        usernameGBIF: string
+            Your username for GBIF atlas.  Default is "".
+        passwordGBIF: string
+            Your password for GBIF atlas.  Default is "".
             
     Returns
     -------
         - No arguments: A ``pandas.DataFrame`` of all current configuration options.
         - >=1 arguments: None
 
     Examples
@@ -78,14 +84,15 @@
             configParser["galahSettings"]["atlas"] = atlas
         if data_profile is not None:
             configParser["galahSettings"]["data_profile"] = data_profile
         if ranks is not None:
             configParser["galahSettings"]["ranks"] = ranks
         if reason is not None:
             configParser["galahSettings"]["reason"] = reason
+        if usernameGBIF is not None:
+            configParser["galahSettings"]["usernameGBIF"] = usernameGBIF
+        if passwordGBIF is not None:
+            configParser["galahSettings"]["passwordGBIF"] = passwordGBIF
 
         # write to file
         with open(inifile,"w") as fileObject:
-            configParser.write(fileObject)
-
-    
-
+            configParser.write(fileObject)
```

## galah/galah_filter.py

```diff
@@ -1,121 +1,173 @@
-import re
-
-ATLAS_KEYWORDS = {
-    "Australia": "taxonConceptID",
-    "Austria": "guid",
-    "Brazil": "guid",
-    "Canada": "usageKey",
-    "Estonia": "guid",
-    "France": "usageKey",
-    "Guatemala": "guid",
-    "Portugal": "usageKey",
-    "Spain": "taxonConceptID",
-    "Sweden": "guid",
-    "United Kingdom": "guid",
-}
-
-def galah_filter(f, ifgroupBy=False):
+import re,urllib
+from .get_api_url import readConfig
+from .common_dictionaries import GBIF_PREDICATE_DEFINITIONS
+
+def galah_filter(f, 
+                 ifgroupBy=False,
+                 occurrencesGBIF=False):
     """
     "Filters" are arguments of the form field logical value that are used to narrow down the number of records returned by 
     a specific query. For example, it is common for users to request records from a particular year (``year=2020``), or 
     to return all records except for fossils (``basisOfRecord!=FossilSpecimen``).
 
     Filters are passed to ``atlas_occurrences()``, ``atlas_species()``, ``atlas_counts()`` or ``atlas_media()``.
-
-    
-    Examples
-    --------
-
-    To know how many total records are in your chosen atlas, type
-
-    .. prompt:: python
-
-        import galah
-        galah.galah_filter(filters="year=2020")
-
-    which returns
-
-    .. program-output:: python -c "import galah; print(galah.galah_filter(filters=\\\"year=2020\\\"))"
     """
 
     # first, check for special characters
     char_string='[!=<>]'
     specialChars = re.compile(char_string) #["=","!",">","<"] #/\|:
+    otherSpecialChars = re.compile("withingeoDistanceisNullisNotNull") # not sure about this
     returnString=""
 
-    # check to make sure the filter type is correct
+    # get configs
+    configs = readConfig()
+
+    # get atlas
+    atlas = configs["galahSettings"]['atlas']
+
+        # check to make sure the filter type is correct
     if type(f) is str:
 
         # need to check for special characters
         specialChar = specialChars.findall(f)
         if specialChar is None:
-            raise ValueError("Either your filters does not have the correct special characters {}".format(char_string) 
-                             + "or we need to include another special character we have forgotten about.")
+            if ["within","geoDistance","isNull","isNotNull"] not in specialChar:
+                raise ValueError("Either your filters does not have the correct special characters {}".format(char_string) 
+                                + "or we need to include another special character we have forgotten about.")
+            else:
+                specialChar = otherSpecialChars.findall(f)
         else:
             specialChar = "".join(specialChar)
 
         # split filter into parts
         parts = f.split(specialChar)
 
         # remove leading and trailing white spaces from each filter part
         for i, p in enumerate(parts):
             parts[i] = p.strip()
 
-        # start checking for different logical operators, starting with equals
-        if specialChar == '=' or specialChar == '==':
+        if atlas in ["Global","GBIF"] and occurrencesGBIF:
 
-            # check if the filter is a number or a string and if there is a group by
-            if parts[1].isdigit() and ifgroupBy:
-                # this one is square brackets
-                #returnString += "%5B{}:%22{}%22%5d".format(parts[0], parts[1])
-                returnString += "%28{}%3A%22{}%22%29".format(parts[0], parts[1].replace(" ", "%20"))
-            # if filter is querying a field that has no value
-            elif parts[1] == '':
-                returnString += "%28{}%3A%28*%29%29".format(parts[0])
-            elif parts[1] == "True":
-                returnString += "%28assertions%3A%22{}%22%29".format(parts[0])
-            elif parts[1] == "False":
-                returnString += "-%28assertions%3A%22{}%22%29".format(parts[0])
+            # how would within even be formatted?
+            if specialChar in ["within"]:
+                return {
+                    "type": specialChar,
+                    "geometry": parts[1]
+                }
+            # how would within even be formatted?
+            elif specialChar in ["geoDistance"]:
+                return {
+                    "type": specialChar,
+                    "latitude": parts[0],
+                    "longitude": parts[1],
+                    "distance": parts[2]
+                }
+            elif specialChar in ["isNull","isNotNull"]:
+                return {
+                    "type": specialChar,
+                    "parameter": parts[0]
+                }
+            elif specialChar in GBIF_PREDICATE_DEFINITIONS.keys() and type(GBIF_PREDICATE_DEFINITIONS[specialChar]) is dict:
+                parts[0] = "_".join([entry.upper() for entry in re.findall('.[^A-Z]*', parts[0])])
+                return {
+                    "type": GBIF_PREDICATE_DEFINITIONS[specialChar][0],
+                    "predicates": [{
+                            "type":GBIF_PREDICATE_DEFINITIONS[specialChar][1],
+                            "key":parts[0],
+                            "value":parts[1]
+                        }
+                    ]
+                }
+            elif specialChar in GBIF_PREDICATE_DEFINITIONS.keys():
+                parts[0] = "_".join([entry.upper() for entry in re.findall('.[^A-Z]*', parts[0])])
+                return {
+                    "type": GBIF_PREDICATE_DEFINITIONS[specialChar],
+                    "key":parts[0],
+                    "value":parts[1]
+                }
             else:
-                # check if this is array
-                arrayChars = re.compile('[\[\]]')
-                arrayChar = arrayChars.findall(parts[1])
-                if arrayChar:
-                    returnString += "%28"
-                    temp_array = parts[1][1:-1].split(",")
-                    for value in temp_array:
-                        returnString += "{}%3A22{}%22%20OR%20".format(parts[0], value.replace(" ","%20").replace('\'','').replace('"',''))
-                    returnString = returnString[:-8] + "%29"
-                # added quotes
-                else:
-                    returnString += "%28{}%3A%22{}%22%29".format(parts[0], parts[1].replace(" ", "%20"))
+                raise ValueError("{} is not a valid logical filter for GBIF.  For a list of those, go to https://www.gbif.org/developer/occurrence#download".format(f))
+
+        elif atlas in ["Global","GBIF"]:
 
-        elif specialChar == '>':
-            returnString+="%28{}:%5B{}%20TO%20*%5d%20AND%20-%28{}%3A%22{}%22%29%29".format(parts[0], parts[1], parts[0], parts[1])
+            # start checking for different logical operators, starting with equals
+            if specialChar == '=' or specialChar == '==':
+                returnString+="{}={}".format(parts[0],parts[1].replace(" ", "%20"))
+            elif specialChar == '>=' or specialChar == '=>':
+                returnString+="{}={}%2C%2A".format(parts[0],parts[1].replace(" ", "%20")) #,*
+            elif specialChar == '>':
+                returnString+="{}={}%2C%2A".format(parts[0],int(parts[1])+1) #,*
+            elif specialChar == '<':
+                returnString+="{}=%2A%2C{}".format(parts[0],int(parts[1])-1) #,*
+            elif specialChar == '!=' or specialChar == '=!': ### NOT SURE ABOUT THIS
+                returnString+="{}=%2A%2C{}".format(parts[0],parts[1].replace(" ", "%20")) #,*
+            else:
+                raise ValueError("The symbol {} is not currently available in galah for atlas {}.".format(specialChar,atlas))
+
+            return returnString
 
-        # less than
-        elif specialChar == '<':
-            returnString += "%28{}%3A%5B*%20TO%20{}%5d%20AND%20-%28{}%3A\"{}\"%29%29".format(parts[0], parts[1], parts[0], parts[1])
-
-        # greater than or equal to
-        elif specialChar == '=>' or specialChar == '>=':
-            returnString += "%28{}%3A%5B{}%20TO%20%2A%5d%29".format(parts[0], parts[1])
-
-        # less than or equal to
-        elif specialChar == '<=' or specialChar == '=<':
-            returnString += "%28{}%3A%5B*%20TO%20{}%5d%29".format(parts[0], parts[1])
-
-        # not equal to
-        elif specialChar == '!=' or specialChar == '=!':
-            returnString += "-%28{}%3A%22{}%22%29".format(parts[0], parts[1])
-        
-        # else, there is either an error in the filters or a missing case
+        # all other atlases are like this
         else:
-            raise ValueError("The special character {} is not included in the filters function.  Either it is not a logical operator, or it has not been included yet.".format(specialChar[0]))
+
+            # start checking for different logical operators, starting with equals
+            if specialChar == '=' or specialChar == '==':
+
+                # check if the filter is a number or a string and if there is a group by
+                if parts[1].isdigit() and ifgroupBy:
+                    # this one is square brackets
+                    #returnString += "%5B{}:%22{}%22%5d".format(parts[0], parts[1])
+                    returnString += "%28{}%3A%22{}%22%29".format(parts[0], parts[1].replace(" ", "%20"))
+                # if filter is querying a field that has no value
+                elif parts[1] == '':
+                    #returnString += "%28{}%3A%28%2A%29%29".format(parts[0])
+                    returnString += "%2A%3A%2A%20AND%20-{}%3A%2A".format(parts[0])
+                elif parts[1] == "True":
+                    returnString += "%28assertions%3A%22{}%22%29".format(parts[0])
+                elif parts[1] == "False":
+                    returnString += "-%28assertions%3A%22{}%22%29".format(parts[0])
+                else:
+                    # check if this is array
+                    arrayChars = re.compile('[\[\]]')
+                    arrayChar = arrayChars.findall(parts[1])
+                    if arrayChar:
+                        returnString += "%28"
+                        temp_array = parts[1][1:-1].split(",")
+                        for value in temp_array:
+                            returnString += "{}%3A22{}%22%20OR%20".format(parts[0], value.replace(" ","%20").replace('\'','').replace('"',''))
+                        returnString = returnString[:-8] + "%29"
+                    # added quotes
+                    else:
+                        returnString += "%28{}%3A%22{}%22%29".format(parts[0], parts[1].replace(" ", "%20"))
+
+            elif specialChar == '>':
+                returnString+="%28{}:%5B{}%20TO%20*%5d%20AND%20-%28{}%3A%22{}%22%29%29".format(parts[0], parts[1], parts[0], parts[1])
+
+            # less than
+            elif specialChar == '<':
+                returnString += "%28{}%3A%5B*%20TO%20{}%5d%20AND%20-%28{}%3A\"{}\"%29%29".format(parts[0], parts[1], parts[0], parts[1])
+
+            # greater than or equal to
+            elif specialChar == '=>' or specialChar == '>=':
+                returnString += "%28{}%3A%5B{}%20TO%20%2A%5d%29".format(parts[0], parts[1])
+
+            # less than or equal to
+            elif specialChar == '<=' or specialChar == '=<':
+                returnString += "%28{}%3A%5B*%20TO%20{}%5d%29".format(parts[0], parts[1])
+
+            # not equal to
+            elif specialChar == '!=' or specialChar == '=!':
+                returnString += "-%28{}%3A%22{}%22%29".format(parts[0], parts[1])
+            
+            # else, there is either an error in the filters or a missing case
+            else:
+                raise ValueError("The special character {} is not included in the filters function.  Either it is not a logical operator, or it has not been included yet.".format(specialChar[0]))
+
+            # return a string to be added to the URL
+            return returnString
 
     # let the user know that their variable is not of the correct type
     else:
         raise TypeError("Your filters need to either be a string (for one filters), or a list of strings.")
 
-    # return a string to be added to the URL
-    return returnString
+            
+
```

## galah/galah_geolocate.py

```diff
@@ -1,3 +1,72 @@
-def galah_geolocate():
-    # pseudocode here
-    n=1
+import urllib
+import shapely
+import shapely.wkt
+from shapely.geometry import box,shape
+from shapely.geometry.polygon import Polygon
+
+def galah_geolocate(polygon=None,
+                    bbox=None):
+    """
+    Restrict results to those from a specified area. Areas can be specified as 
+    either polygons or bounding boxes, depending on type.
+    
+    Parameters
+    ----------
+        polygon : string, polygon object
+            one polygon used to search (can be file name or polygon object).
+        bbox : list, string
+            list containing [xmin, ymin, xmax, ymax] or a polygon object.
+        verbose : 
+
+    Returns
+    -------
+        An object of class ``pandas.DataFrame``.
+
+    Examples
+    --------
+
+    .. prompt:: python
+
+        import galah
+        galah.search_taxa(taxa="Vulpes vulpes")
+
+    .. program-output:: python -c "import galah; print(galah.search_taxa(taxa=\\\"Vulpes vulpes\\\"))"
+    """
+
+    if polygon is not None:
+
+        print(polygon)
+        print(type(polygon))
+
+        if type(polygon) is list:
+            raise ValueError("Unsure how to deal with polygon lists")
+        elif type(polygon) is str:
+            if "POLYGON" not in polygon and "MULTIPOLYGON" not in polygon:
+                if "shp" not in polygon:
+                    raise ValueError("Only a shape file or wkt should be passed to polygon")
+            geometry = shapely.wkt.loads(polygon)
+            new_geom = str(geometry).replace("POLYGON ","MULTIPOLYGON (") + ")"
+            return "wkt={}".format(urllib.parse.quote(str(new_geom))) # geometry
+        elif type(polygon) is Polygon or MultiPolygon:
+            return "wkt={}".format(urllib.parse.quote(str(polygon)))
+        else:
+            raise ValueError("The only types of variables geolocate takes are list, str and polygons")
+
+    elif bbox is not None:
+
+        print(bbox)
+        print(type(bbox))
+
+        if type(bbox) is list:
+            new_bbox = Polygon.from_bounds(bbox[0],bbox[1],bbox[2],bbox[3])
+            return "wkt={}".format(urllib.parse.quote(str(new_bbox)))
+        elif type(bbox) is str:
+            geometry = shapely.wkt.loads(bbox)
+            new_geom = str(geometry).replace("POLYGON ","MULTIPOLYGON (") + ")"
+            return "wkt={}".format(urllib.parse.quote(new_geom))
+        elif type(bbox) is shapely.geometry.polygon.Polygon or shapely.geometry.polygon.MultiPolygon:
+            bounds=list(bbox.bounds)
+            new_bbox = box(bounds[0],bounds[1],bounds[2],bounds[3])
+            return "wkt={}".format(urllib.parse.quote(str(new_bbox)))
+        else:
+            raise ValueError("The only types of variables geolocate takes are list, str and polygons")
```

## galah/galah_group_by.py

```diff
@@ -1,52 +1,42 @@
 import requests
 import pandas as pd
-from .galah_filter import galah_filter
+from .get_api_url import readConfig
+from .common_functions import add_filters
 
-'''
-TODO
-----
-1. Generalise this to N group_by, where N>2
-
-
-'''
 def galah_group_by(URL,
                    group_by=None,
+                   total_group_by=False,
                    filters=None,
                    expand=True,
                    verbose=False
                    ):
     """
     Used for grouping counts by a specific query, i.e. "year" or "basisOfRecord".  It's mainly utilized in atlas_counts.
     """
 
+    # get configs
+    configs = readConfig()
+
+    # get atlas
+    atlas = configs['galahSettings']['atlas']
+
     # check if expand option works
     if expand:
         ifGroupBy = True
     else:
         ifGroupBy = False
 
     # first, check for filters
     if filters is not None:
 
         # check type of filter
         if type(filters) == str or type(filters) == list:
 
-            # change type of filters to list for easy looping
-            if type(filters) == str:
-                filters = [filters]
-
-            URL += "%28"
-
-            # loop over filters
-            for f in filters:
-                
-                URL += galah_filter(f,ifgroupBy=ifGroupBy) + "%20AND%20"
-                   
-            URL = URL[:-len("%20AND%20")] + "%29" 
+            URL = add_filters(URL=URL,atlas=atlas,filters=filters,ifGroupBy=ifGroupBy)
 
         # else, raise a TypeError because this variable needs to be either a string or a list
         else:
             raise TypeError("Your filters need to either be a string (for one filter), or a list of strings.")
 
     # check for group_by
     if group_by is None:
@@ -70,113 +60,253 @@
 
             # create a base URL
             startingURL = URL
 
             # loop over group_by
             for g in group_by:
 
-                startingURL += "&facets={}".format(g)
+                # ensure each group is given its own facet
+                if atlas in ["Global","GBIF"]:
+                    startingURL += "&facet={}".format(g)
+                else:
+                    startingURL += "&facets={}".format(g)
 
             # round out the URL
-            startingURL += "&flimit=10000&pageSize=0"
+            startingURL += "&&flimit=-1&pageSize=0"
 
             # check to see if the user wants the URL for querying
             if verbose:
                 print("URL for querying:\n\n{}\n".format(startingURL))
 
-            # tab this if this doesn't work
+            # get response from your query, which will include all available fields
             response = requests.get(startingURL)
-            json = response.json()
+            response_json = response.json()
             facets_array=[]
-            # try to make it generalised
-            # add a check to see if a single value is there for filters; otherwise, can do this?
+
+            # set some common variables
+            if atlas in ["Global","GBIF"]:
+                group_by = sorted(group_by)
+                response_json['facets'] = sorted(response_json['facets'],key = lambda d: d['field'])
+                length = len(response_json['facets'])
+                results_array = response_json['facets']
+                field_name = 'counts'
+                facet_name = 'name'
+            elif atlas in ["Brazil"]:
+                length = len(response_json)
+                results_array = response_json 
+                field_name = 'fieldResult' 
+                facet_name = 'fq'
+            else:
+                length = len(response_json['facetResults'])
+                results_array = response_json['facetResults']
+                field_name = 'fieldResult' #i18nCode
+                facet_name = 'fq'
+
+            # add a check to see if a single value is there for filters; otherwise, will do this
             for i in range(1,len(group_by)):
                 temp_array=[]
-                # how to ensure we catch the 
-                for entry in json['facetResults'][i]['fieldResult']:
-                    temp_array.append(entry['fq'])
+                for entry in results_array[i][field_name]:
+                    temp_array.append(entry[facet_name])
                 facets_array.append(temp_array)
 
             # get all counts for each value
             dict_values = {entry: [] for entry in [*group_by,'count']}
-            for f in facets_array:
-                for facet in f:
-                    name,value = facet.split(':')
-                    value = value.replace('"', '')
-                    if name in group_by:
-                        tempURL = URL + "%20AND%20%28{}%3A%22{}%22%29".format(name,value)
-                    else:
-                        continue
-                    for group in group_by:
-                        if (group != name) and ("facets={}".format(group) not in URL):
-                            tempURL += "&facets={}".format(group)
-                    tempURL += "&flimit=10000&pageSize=0"
-
-                    # check to see if the user wants the URL for querying
-                    if verbose:
-                        print("URL for querying:\n\n{}\n".format(tempURL))
-
-                    # get data
-                    response=requests.get(tempURL)
-                    json = response.json()
-
-                    # put data in table
-                    for entry in json['facetResults'][0]['fieldResult']:
-                        # generalise this for more than one thing
-                        if entry['fq'].split(":")[0] == group_by[0]:
-                            name2,value2 = entry['fq'].split(":")
-                            value2 = value2.replace('"', '')
-                            if value2.isdigit():
-                                value2 = int(value2)
-                            dict_values[name2].append(value2)
+
+            # loop over facets array
+            for i,f in enumerate(facets_array):
+
+                # check for GBIF atlas
+                if atlas in ["Global","GBIF"]:
+
+                    # loop over all specified facets
+                    for facet in f:
+
+                        # check if user is grouping by scientific name
+                        if group_by[i+1] == "scientificName":
+                            tempURL = URL + "&{}={}".format(group_by[i+1],"%20".join(facet.split(" ")[0:2])) + "&facet=" + group_by[i] + "&flimit=-1&pageSize=0"
+                        else:
+                            tempURL = URL + "&{}={}".format(group_by[i+1],"%20".join(facet.split(" "))) + "&facet=" + group_by[i] + "&flimit=-1&pageSize=0"
+
+                        # print the URL
+                        if verbose:
+                            print("URL for querying:\n\n{}\n".format(tempURL))
+
+                        # get the data
+                        response=requests.get(tempURL)
+                        response_json = response.json()
+
+                        # put data in dict
+                        for entry in response_json['facets'][0]['counts']:
+                            dict_values[group_by[i]].append(entry['name'])
                             dict_values['count'].append(int(entry['count']))
-                            dict_values[name].append(value)
+                            dict_values[group_by[i+1]].append(facet)
                             for key in dict_values:
-                                if (key != name2) and (key != name) and (key != 'count'):
+                                if (key != group_by[i+1]) and (key != group_by[i]) and (key != 'count'):
                                     dict_values[key].append("-")
+                
+                # do this loop for all other atlases 
+                else:
+
+                    # loop over each facet
+                    for facet in f:
+
+                        # split each facet to make it human readable
+                        name,value = facet.split(':')
+                        value = value.replace('"', '')
+                        if name in group_by:
+                            tempURL = URL + "%20AND%20%28{}%3A%22{}%22%29".format(name,value)
+                        else:
+                            continue
+                        for group in group_by:
+                            if (group != name) and ("facets={}".format(group) not in URL):
+                                tempURL += "&facets={}".format(group)
+
+                        # finalise the URL for querying
+                        tempURL += "&flimit=-1&pageSize=0"
+
+                        # check to see if the user wants the URL for querying
+                        if verbose:
+                            print("URL for querying:\n\n{}\n".format(tempURL))
+
+                        # get data
+                        response=requests.get(tempURL)
+                        response_json = response.json()
+
+                        # if there is no data available, move onto next variable
+                        if response_json is None:
+                            continue
+
+                        # put data in table (and check if user wants Brazil, because that is an exception)
+                        if atlas in ["Brazil"]:
+                            results_array = response_json[0]['fieldResult']
+                        else:
+                            results_array = response_json['facetResults'][0]['fieldResult']
+
+                        # loop over each entry in the results
+                        for entry in results_array:
+
+                            # put entries in dictionary
+                            if entry['fq'].split(":")[0] == group_by[0]:
+                                name2,value2 = entry['fq'].split(":")
+                                value2 = value2.replace('"', '')
+                                if value2.isdigit():
+                                    value2 = int(value2)
+                                dict_values[name2].append(value2)
+                                dict_values['count'].append(int(entry['count']))
+                                dict_values[name].append(value)
+                                for key in dict_values:
+                                    if (key != name2) and (key != name) and (key != 'count'):
+                                        dict_values[key].append("-")
 
             # format table
-            return pd.DataFrame(dict_values) #, columns=[*group_by,'count'])
+            counts = pd.DataFrame(dict_values).reset_index(drop=True)
+            counts.sort_values(by=group_by)
+
+            # if user wants total, return total number of rows
+            if total_group_by:
+                return pd.DataFrame({'count': [counts.shape[0]]})
+
+            # return dataFrame with all counts values
+            return counts
 
         # else, expand is False
         else:
 
             # add facets to make sure you get results
             for g in group_by:
 
-                URL += "&facets={}".format(g)
+                if atlas in ["Global","GBIF"]:
+                    URL += "&facet={}".format(g)
+                else:
+                    URL += "&facets={}".format(g)
 
             # round out the URL
-            URL += "&flimit=10000&pageSize=0"
-
+            URL += "&flimit=-1&pageSize=0"
+            
             # check to see if the user wants the URL for querying
             if verbose:
                 print("URL for querying:\n\n{}\n".format(URL))
 
             # tab this if this doesn't work
             response = requests.get(URL)
-            json = response.json()
+            response_json = response.json()
+
+            # set some common variables
+            if atlas in ["Global","GBIF"]:
+                length = len(response_json['facets'])
+                name_results = response_json['facets']
+                field_name = 'counts'
+            elif atlas in ["Brazil"]:
+                length = len(response_json)
+                name_results = response_json 
+                field_name = 'fieldResult' 
+                facet_name = 'fq'
+            else:
+                length = len(response_json['facetResults'])
+                name_results = response_json['facetResults']
+                field_name = 'fieldResult'
 
             # get all counts for each value
             dict_values = {entry: [] for entry in [*group_by,'count']}
-            for i in range(len(json['facetResults'])):
-                for item in json['facetResults'][i]['fieldResult']:
+
+            # loop over the array length
+            for i in range(length):
+
+                # check if atlas is GBIF
+                if atlas in ["Global","GBIF"]:
+
+                    # loop over each group and make sure entry is human readable and have a dash if 
+                    # it doesn't have a count
                     for g in group_by:
-                        if g in item['fq'] and item['fq'].split(':')[0] == g:
-                            name,value=item['fq'].split(':')
-                            value=value.replace('"','')
-                            if value.isdigit():
-                                value = int(value)
-                            dict_values[name].append(value)
-                            dict_values['count'].append(int(item['count']))
-                            for entry in dict_values:
-                                if (entry != name) and (entry != 'count'):
-                                    dict_values[entry].append("-")
+                        if "_" in name_results[i]['field']:
+                            test_name = name_results[i]['field'].split("_")
+                            for k in range(len(test_name)):
+                                test_name[k] = test_name[k].lower()
+                                if k > 0:
+                                    test_name[k] = test_name[k].capitalize()
+                            test_name = "".join(test_name)
+                        else:
+                            test_name = name_results[i]['field'].lower()
+                        if test_name == g:
+                            for item in name_results[i][field_name]:
+                                dict_values[g].append(item['name'])
+                                dict_values['count'].append(int(item['count']))
+                                for entry in dict_values:
+                                    if (entry != g) and (entry != 'count'):
+                                        dict_values[entry].append("-")
+                
+                # otherwise, it's all other atlases
+                else:
+
+                    # loop over entry in results
+                    for item in name_results[i][field_name]:
 
-            counts = pd.DataFrame(dict_values) #, columns=[*group_by,'count'])
+                        # loop over each group and make sure entry is human readable and have a dash if 
+                        # it doesn't have a count
+                        for g in group_by:
+
+                            # check for only itesm you want
+                            if g in item['fq'] and item['fq'].split(':')[0] == g:
+                                name,value=item['fq'].split(':')
+                                value=value.replace('"','')
+                                if value.isdigit():
+                                    value = int(value)
+                                dict_values[name].append(value)
+                                dict_values['count'].append(int(item['count']))
+                                for entry in dict_values:
+                                    if (entry != name) and (entry != 'count'):
+                                        dict_values[entry].append("-")
+
+            # get all counts into a dictionary and sort them
+            counts = pd.DataFrame(dict_values).reset_index(drop=True)
+            counts.sort_values(by=group_by)
+
+            # if user wants total, return total number of rows
+            if total_group_by:
+                return pd.DataFrame({'count': [counts.shape[0]]})
 
             # return dataFrame with all counts values
             return counts
 
     # need to make sure that the filter is a string or a lsid
     else:
         raise TypeError("Your filters need to either be a string (for one filter), or a list of strings.")
```

## galah/galah_select.py

```diff
@@ -16,15 +16,15 @@
     # generate a temporary string for fields to return to another function
     tempstring="fields="
 
     # check if this argument is provided
     if select is None:
         raise ValueError("You need to provide one argument: category(ies) to get from the ALA API as a string or list.")
     
-    # otherwise, create a string and return it
+    # otherwise, create a URL string and return it
     elif type(select) is str or type(select) is list:
         if type(select) is str:
             select=[select]
         for selection in select:
             if selection == "basic":
                 for s in ["decimalLatitude","decimalLongitude","eventDate","scientificName","taxonConceptID","recordID","dataResourceName","occurrenceStatus"]:
                     tempstring+="{}%2C".format(s)
@@ -34,9 +34,10 @@
             elif selection == "media":
                 for s in ["multimedia","multimediaLicence","images","videos","sounds"]:
                     tempstring+="{}%2C".format(s)
             else:
                 tempstring+="{}%2C".format(selection)
         return tempstring
 
+    # else, throw an error
     else:
         raise ValueError("This function only takes strings or lists as its arguments")
```

## galah/get_api_url.py

```diff
@@ -1,9 +1,10 @@
 import configparser,os,urllib
 import pandas as pd
+from .galah_config import galah_config
 
 def readConfig():
     configFile=configparser.ConfigParser()
     inifile = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'config.ini')
     configFile.read(inifile)
     return configFile
 
@@ -23,41 +24,58 @@
     if column1value is None:
         raise ValueError("You need to provide a value for this function to search for in the column")
 
     # first, get specific atlas
     atlasfile = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'node_config.csv')
     atlaslist = pd.read_csv(atlasfile)
     configs = readConfig()
+
+    # check for global atlas and make sure it is named correctly
+    if configs['galahSettings']['atlas'] == "GBIF":
+        galah_config(atlas="Global")
+        configs = readConfig()
+
+    # get specific atlas
     specific_atlas = atlaslist[atlaslist['atlas'] == configs['galahSettings']['atlas']]
 
     # get rows with specific value
-    rows = specific_atlas[specific_atlas[column1].astype(str).str.contains(column1value, case=True, na=False)]
+    rows = specific_atlas[specific_atlas[column1].astype(str).str.contains(column1value, case=True, na=False)]  
 
     # check to see if there are two columns to filter by
     if column2 is None and column2value is None:
+
+        # check if there is more than one entry
         if len(rows.loc[rows[column1].astype(str).str.contains(column1value, case=True, na=False)].index) > 1:
             raise ValueError("There are more than one possible APIs - need to specify column2 and column2value")
+        
+        # else, return the singular URL
         else:
-            # dataFrame.loc[dataFrame[column_name].astype(str).str.contains(assertions, case=True, na=False)].sort_values('name', key=lambda x: x.str.len()))
-            index = rows[rows[column1] == column1value].index[0]
-            baseURL = rows[rows[column1] == column1value]['api_url'][index]
-    # add a test here
+            index = rows[rows[column1].astype(str).str.contains(column1value, case=True, na=False)].index[0]
+            baseURL = rows[rows[column1].astype(str).str.contains(column1value, case=True, na=False)]['api_url'][index]
+    
+    # if there are two columns to filter by, first check for the name and value
     elif column2 is not None and column2value is not None:
+
+        # check if there is more than one entry
         if len(rows[rows[column2].astype(str).str.contains(column2value,case=True,na=False)].index) > 1:
             raise ValueError("There are more than one possible APIs with column2 and column2value - choose this API another way")
+        
+        # else, return the singular URL
         else:
             index = rows[rows[column2].astype(str).str.contains(column2value,case=True,na=False)].index[0]
-            #baseURL = rows[rows[column1] == column1value]['api_url'][index]
             baseURL = rows.loc[rows[column1].astype(str).str.contains(column1value, case=True, na=False)]['api_url'][index]
+    
+    # else, the user has provided something incorrect
     else:
         raise ValueError("A value needs to be provided for both column2 and column2 value")
 
+    # check if the user wants to add their email to the URL
     if add_email:
 
-        # email for querying
+        # get email for querying, among other things
         if configs['galahSettings']['email'] is None:
             raise ValueError("You need to provide a valid email address for this function to be able to download data")
         else:
             if "download" in baseURL:
                 baseURL += "?email={}&dwcHeaders=True".format(urllib.parse.quote(configs['galahSettings']['email']))
                 baseURL += "&reasonTypeId={}".format(configs['galahSettings']['reason'])
             else:
@@ -67,8 +85,9 @@
                 baseURL += "&emailNotify=false&"
             elif configs['galahSettings']['email_notify'].lower() == "true":
                 baseURL += "&emailNotify=true&" # check if this is correct
             else:
                 raise ValueError("email_notify option should be set to either True or False - please set that with "
                                  "galah_config(email_notify=\"False\") or galah_config(email_notify=\"True\")")
 
+    # return the final URL
     return baseURL
```

## galah/node_config.csv

```diff
@@ -20,14 +20,32 @@
 Australia,records,records_occurrences,https://biocache-ws.ala.org.au/ws/occurrences/offline/download,atlas_occurrences,TRUE
 Australia,records,records_query,https://biocache-ws.ala.org.au/ws/webportal/params,atlas_occurrences,TRUE
 Australia,records,records_species,https://biocache-ws.ala.org.au/ws/occurrences/facets/download,atlas_species,TRUE
 Australia,spatial,spatial_layers,https://spatial.ala.org.au/ws/layers,show_all-fields,TRUE
 Australia,species,species_children,https://bie-ws.ala.org.au/ws/childConcepts/{id},atlas_taxonomy,TRUE
 Australia,species,species_lookup,https://bie-ws.ala.org.au/ws/species/{id},atlas_taxonomy,TRUE
 Austria,collections,collections_collections,https://collectory.biodiversityatlas.at/ws/collection,show_all-collections,TRUE
+Austria,collections,collections_datasets,https://collectory.biodiversityatlas.at/ws/dataResource,show_all-datasets,TRUE
+Austria,collections,collections_providers,https://collectory.biodiversityatlas.at/ws/dataProvider,show_all-providers,TRUE
+Austria,images,image_licences,https://images.biodiversityatlas.at/ws/licence,show_all-licences,TRUE
+Austria,images,image_metadata,https://images.biodiversityatlas.at/ws/image/,media_metadata,FALSE
+Austria,lists,lists_all,https://lists.biodiversityatlas.at/ws/speciesList/,show_all-lists,TRUE
+Austria,lists,lists_lookup,https://lists.biodiversityatlas.at/ws/speciesListItems/{list_id},show_values-lists,TRUE
+Austria,logger,logger_reasons,https://logger.biodiversityatlas.at/service/logger/reasons,show_all-reasons,TRUE
+Austria,records,records_assertions,https://biocache-ws.biodiversityatlas.at/assertions/codes,show_all-assertions,TRUE
+Austria,records,records_counts,https://biocache-ws.biodiversityatlas.at/occurrences/search,atlas_counts,TRUE
+Austria,records,records_facets,https://biocache-ws.biodiversityatlas.at/occurrence/facets,"atlas_counts, show_values-fields",TRUE
+Austria,records,records_fields,https://biocache-ws.biodiversityatlas.at/index/fields,show_all-fields,TRUE
+Austria,records,records_occurrences,https://biocache-ws.biodiversityatlas.at/occurrences/offline/download,atlas_occurrences,TRUE
+Austria,records,records_query,https://biocache-ws.biodiversityatlas.at/webportal/params,atlas_occurrences,TRUE
+Austria,records,records_species,https://biocache-ws.biodiversityatlas.at/occurrences/facets/download,atlas_species,TRUE
+Austria,spatial,spatial_layers,https://spatial.biodiversityatlas.at/ws/layers,show_all-fields,TRUE
+Austria,species,names_search_single,https://bie-ws.biodiversityatlas.at/search?q={name}&pageSize=5,search_taxa,TRUE
+Austria,species,species_children,https://bie-ws.biodiversityatlas.at/childConcepts/,atlas_taxonomy,TRUE
+Austria,species,species_lookup,https://bie-ws.biodiversityatlas.at/species,atlas_taxonomy,TRUE
 Brazil,collections,collections_collections,https://collectory.sibbr.gov.br/collectory/ws/collection,show_all-collections,TRUE
 Brazil,collections,collections_datasets,https://collectory.sibbr.gov.br/collectory/ws/dataResource,show_all-datasets,TRUE
 Brazil,collections,collections_providers,https://collectory.sibbr.gov.br/collectory/ws/dataProvider,show_all-providers,TRUE
 Brazil,images,image_licences,https://images.sibbr.gov.br/images/ws/licence,show_all-licences,TRUE
 Brazil,images,image_metadata,https://images.sibbr.gov.br/images/ws/image/,media_metadata,FALSE
 Brazil,lists,lists_all,https://specieslist.sibbr.gov.br/ws/speciesList/,show_all-lists,TRUE
 Brazil,lists,lists_lookup,https://specieslist.sibbr.gov.br/ws/speciesListItems/{list_id},show_values-lists,TRUE
@@ -38,14 +56,34 @@
 Brazil,records,records_occurrences,https://biocache-service.sibbr.gov.br/biocache-service/occurrences/offline/download,atlas_occurrences,TRUE
 Brazil,records,records_query,https://biocache-service.sibbr.gov.br/biocache-service/webportal/params,atlas_occurrences,TRUE
 Brazil,records,records_species,https://biocache-service.sibbr.gov.br/biocache-service/occurrences/facets/download,atlas_species,TRUE
 Brazil,spatial,spatial_layers,https://portal-espacial.sibbr.gov.br/spatial-hub/layers,show_all-fields,TRUE
 Brazil,species,names_search_single,https://bie-webservice.sibbr.gov.br/bie-index/search?q={name}&pageSize=5,search_taxa,TRUE
 Brazil,species,species_children,https://bie-webservice.sibbr.gov.br/bie-index/childConcepts/,atlas_taxonomy,TRUE
 Brazil,species,species_lookup,https://bie-webservice.sibbr.gov.br/bie-index/species,atlas_taxonomy,TRUE
+France,collections,collections_collections,https://openobs.mnhn.fr/collectory/ws/collection,show_all-collections,TRUE
+France,collections,collections_datasets,https://openobs.mnhn.fr/collectory/ws/dataResource,show_all-datasets,TRUE
+France,collections,collections_providers,https://openobs.mnhn.fr/collectory/ws/dataProvider,show_all-providers,TRUE
+France,records,records_assertions,https://openobs.mnhn.fr/biocache-service/assertions/codes,show_all-assertions,TRUE
+France,records,records_counts,https://openobs.mnhn.fr/biocache-service/occurrences/search,atlas_counts,TRUE
+France,records,records_facets,https://openobs.mnhn.fr/biocache-service/occurrence/facets,"atlas_counts, show_values-fields",TRUE
+France,records,records_fields,https://openobs.mnhn.fr/biocache-service/index/fields,show_all-fields,TRUE
+France,records,records_occurrences,https://openobs.mnhn.fr/biocache-service/occurrences/offline/download,atlas_occurrences,TRUE
+France,records,records_query,https://openobs.mnhn.fr/biocache-service/webportal/params,atlas_occurrences,TRUE
+France,records,records_species,https://openobs.mnhn.fr/biocache-service/occurrences/facets/download,atlas_species,TRUE
+France,name-matching,names_lookup,https://taxref.mnhn.fr/api/taxa/,search_identifiers,TRUE
+France,name-matching,names_search_single,https://taxref.mnhn.fr/api/taxa/search?scientificNames={name}&page=1&size=5,search_taxa,TRUE
+Global,species,names_search_single,https://api.gbif.org/v1/species/match?verbose=FALSE&name={name},search_taxa,TRUE
+Global,collections,collections_providers,https://api.gbif.org/v1/organization,show_all-providers,TRUE
+Global,collections,collections_collections,https://api.gbif.org/v1/grscicoll/collection,show_all-collections,TRUE
+Global,collections,collections_collections_search,https://api.gbif.org/v1/grscicoll/search,search_all-collections,TRUE
+Global,collections,collections_datasets,https://api.gbif.org/v1/dataset,show_all-datasets,TRUE
+Global,collections,collections_datasets_search,https://api.gbif.org/v1/dataset/search,search_all-datasets,TRUE
+Global,records,records_counts,https://api.gbif.org/v1/occurrence/search,"atlas_counts, show_values-fields",TRUE
+Global,records,records_occurrences,https://api.gbif.org/v1/occurrence/download/request,atlas_occurrences,TRUE
 Spain,collections,collections_collections,https://colecciones.gbif.es/ws/collection,show_all-collections,TRUE
 Spain,collections,collections_datasets,https://colecciones.gbif.es/ws/dataResource,show_all-datasets,TRUE
 Spain,collections,collections_providers,https://colecciones.gbif.es/ws/dataProvider,show_all-providers,TRUE
 Spain,doi,doi_download,https://doi.gbif.es/doi/{doi_string}/download,doi_download,TRUE
 Spain,images,image_licences,https://imagenes.gbif.es/ws/licence,show_all-licences,TRUE
 Spain,images,image_metadata,https://imagenes.gbif.es/ws/image/,media_metadata,FALSE
 Spain,lists,lists_all,https://listas.gbif.es/ws/speciesList,show_all-lists,TRUE
@@ -61,8 +99,8 @@
 Spain,records,records_facets,https://registros-ws.gbif.es/occurrence/facets,"atlas_counts, show_values-fields",TRUE
 Spain,records,records_fields,https://registros-ws.gbif.es/index/fields,show_all-fields,TRUE
 Spain,records,records_occurrences,https://registros-ws.gbif.es/occurrences/offline/download,atlas_occurrences,TRUE
 Spain,records,records_query,https://registros-ws.gbif.es/webportal/params,atlas_occurrences,TRUE
 Spain,records,records_species,https://registros-ws.gbif.es/occurrences/facets/download,atlas_species,TRUE
 Spain,spatial,spatial_layers,https://espacial.gbif.es/ws/layers,show_all-fields,TRUE
 Spain,species,species_children,https://especies-ws.gbif.es/childConcepts/,atlas_taxonomy,TRUE
-Spain,species,species_lookup,https://especies-ws.gbif.es/species,atlas_taxonomy,TRUE
+Spain,species,species_lookup,https://especies-ws.gbif.es/species,atlas_taxonomy,TRUE
```

## galah/search_all.py

```diff
@@ -1,8 +1,9 @@
 from .show_all import show_all
+from .get_api_url import readConfig
 
 '''
 function is meant to search all values for possible query fields - they are defined as None so you can narrow down the
 large list of all the potential variables to add to your atlas query.  Choosing which column you query is also an option
 '''
 def search_all(assertions=None,
                atlases=None,
@@ -67,227 +68,329 @@
 
     .. program-output:: python -c "import galah; print(galah.search_all(apis=\\\"Australia\\\"))"
     """
 
     # set up the option for getting back multiple values
     return_array=[]
 
+    # configs
+    configs = readConfig()
+
+    # get atlas
+    atlas = configs["galahSettings"]["atlas"]
+
     # search options for assertions
     if assertions is not None:
+
         # call show_all to get all the possible values
         dataFrame = show_all(assertions=True)
-        # check to see if user wants default column name
-        if column_name is None:
-            column_name='description'
+
+        # check for default sort column name
+        if atlas in ["Global","GBIF"]:
+            sort_name = "ID"
+        else:
+            sort_name = "name"
+
+        # check if column_name is None; if it is, set it to default
+        if column_name is None and atlas in ["Global","GBIF"]:
+            column_name = 'Description'
+        elif column_name is None:
+            column_name = 'description'
+
         # throw ValueError if column_name variable is not a string
         elif type(column_name) is not str:
             raise ValueError("Only strings are a valid query for the column_name variable")
+        
         # check to see if the user input the correct variable type; else, throw value error
         if type(assertions) is str:
             return_dataFrame = dataFrame.loc[dataFrame[column_name].astype(str).str.contains(assertions, case=True, na=False)]
-            return_array.append(return_dataFrame.sort_values('name', key=lambda x: x.str.len()))
+            return_array.append(return_dataFrame.sort_values(sort_name, key=lambda x: x.str.len()))
+        
+        # else, throw error because this only takes strings for now
         else:
             raise ValueError("You can only pass one string to your search parameter = run show_all(assertions=True) to get strings to pass")
 
     # search options for atlases
     if atlases is not None:
+
         # call show_all to get all the possible values
         dataFrame = show_all(atlases=True)
+
         # check to see if user wants default column name
         if column_name is None:
             column_name='atlas'
+
         # throw ValueError if column_name variable is not a string
         elif type(column_name) is not str:
             raise ValueError("Only strings are a valid query for the column_name variable")
+        
         # check to see if the user input the correct variable type; else, throw value error
         if type(atlases) is str:
             return_dataFrame = dataFrame.loc[dataFrame[column_name].astype(str).str.contains(atlases, case=True, na=False)]
             return_array.append(return_dataFrame.sort_values('atlas', key=lambda x: x.str.len()))
+        
+        # else, throw error because this only takes strings for now
         else:
             raise ValueError(
                 "You can only pass one string to your search parameter = run show_all(atlases=True) to get strings to pass")
 
     # search options for apis
     if apis is not None:
+        
         # call show_all to get all the possible values
         dataFrame = show_all(apis=True)
+        
         # check to see if user wants default column name
         if column_name is None:
             column_name='atlas'
+        
         # throw ValueError if column_name variable is not a string
         elif type(column_name) is not str:
             raise ValueError("Only strings are a valid query for the column_name variable")
+        
         # check to see if the user input the correct variable type; else, throw value error
         if type(apis) is str:
             return_dataFrame = dataFrame.loc[dataFrame[column_name].astype(str).str.contains(apis, case=True, na=False)]
             return_array.append(return_dataFrame.sort_values('atlas', key=lambda x: x.str.len()))
+        
+        # else, throw error because this only takes strings for now
         else:
             raise ValueError(
                 "You can only pass one string to your search parameter = run show_all(apis=True) to get strings to pass")
 
     # search options for collection
     if collection is not None:
+        
         # call show_all to get all the possible values
         dataFrame = show_all(collection=True)
+        
         # check to see if user wants default column name
         if column_name is None:
             column_name = 'name'
+        
         # throw ValueError if column_name variable is not a string
         elif type(column_name) is not str:
             raise ValueError("Only strings are a valid query for the column_name variable")
+        
         # check to see if the user input the correct variable type; else, throw value error
         if type(collection) is str:
             return_dataFrame = dataFrame.loc[dataFrame[column_name].astype(str).str.contains(collection, case=True, na=False)]
             return_array.append(return_dataFrame.sort_values('name', key=lambda x: x.str.len()))
+        
+        # else, throw error because this only takes strings for now
         else:
             raise ValueError(
                 "You can only pass one string to your search parameter = run show_all(apis=True) to get strings to pass")
 
     # search options for datasets
     if datasets is not None:
+
         # call show_all to get all the possible values
         dataFrame = show_all(datasets=True)
-        # check to see if user wants default column name
-        if column_name is None:
-            column_name = 'name'
+
+        # check for the correct column name
+        if column_name is None and atlas in ["Australia","Austria","Brazil","France","Guatemala","Spain","Sweden"]:
+            column_name = 'name'   
+        elif column_name is None:
+            column_name = 'description'
+
         # throw ValueError if column_name variable is not a string
         elif type(column_name) is not str:
             raise ValueError("Only strings are a valid query for the column_name variable")
-        # check to see if the user input the correct variable type; else, throw value error
+        
+        # check to see if the user input the correct variable type
         if type(datasets) is str:
             return_dataFrame = dataFrame.loc[dataFrame[column_name].astype(str).str.contains(datasets, case=True, na=False)]
-            return_array.append(return_dataFrame.sort_values('name', key=lambda x: x.str.len()))
+            if atlas not in ["Global","GBIF"]:
+                return_array.append(return_dataFrame.sort_values('name', key=lambda x: x.str.len()))
+            else:
+                return_array.append(return_dataFrame.sort_values('title', key=lambda x: x.str.len()))
+        
+        # else, throw error because this only takes strings for now
         else:
             raise ValueError(
                 "You can only pass one string to your search parameter = run show_all(datasets=True) to get strings to pass")
 
     # search options for fields
     if fields is not None:
+
         # call show_all to get all the possible values
         dataFrame = show_all(fields=True)
 
         # check to see if user wants default column name
-        if column_name is None:
+        if column_name is None and atlas in ["Global","GBIF"]:
+            column_name = 'Description'
+        elif column_name is None:
             column_name = 'description'
 
         # throw ValueError if column_name variable is not a string
         elif type(column_name) is not str:
             raise ValueError("Only strings are a valid query for the column_name variable")
+        
+        # check to see if the user input the correct variable type
         if type(fields) is str:
             return_dataFrame = dataFrame.loc[dataFrame[column_name].astype(str).str.contains(fields, case=True, na=False)]
-            return_array.append(return_dataFrame.sort_values('id', key=lambda x: x.str.len()))
+            if  atlas in ["Global","GBIF"]:
+                return_array.append(return_dataFrame.sort_values('Parameter', key=lambda x: x.str.len()))
+            else:
+                return_array.append(return_dataFrame.sort_values('id', key=lambda x: x.str.len()))
+        
+        # else, throw error because this only takes strings for now
         else:
             raise ValueError(
                 "You can only pass one string to your search parameter = run show_all(fields=True) to get strings to pass")
     
     # search options for licences
     if licences is not None:
+
         # call show_all to get all the possible values
         dataFrame = show_all(licences=True)
+
         # check to see if user wants default column name
         if column_name is None:
             column_name = 'name'
+
         # throw ValueError if column_name variable is not a string
         elif type(column_name) is not str:
             raise ValueError("Only strings are a valid query for the column_name variable")
+        
         # check to see if the user input the correct variable type; else, throw value error
         if type(licences) is str:
             return_dataFrame = dataFrame.loc[dataFrame[column_name].astype(str).str.contains(licences, case=True, na=False)]
             return_array.append(return_dataFrame.sort_values('id', key=lambda x: x.astype(str).str.len()))
+        
+        # check to see if the user input the correct variable type
         else:
             raise ValueError(
                 "You can only pass one string to your search parameter = run show_all(licences=True) to get strings to pass")
 
     # search options for lists
     if lists is not None:
+
         # call show_all to get all the possible values
         dataFrame = show_all(lists=True)
+
         # check to see if user wants default column name
         if column_name is None:
             column_name = 'listName'
+
         # throw ValueError if column_name variable is not a string
         elif type(column_name) is not str:
             raise ValueError("Only strings are a valid query for the column_name variable")
+        
         # check to see if the user input the correct variable type; else, throw value error
         if type(lists) is str:
             return_dataFrame = dataFrame.loc[dataFrame[column_name].astype(str).str.contains(lists, case=True, na=False)]
             return_array.append(return_dataFrame.sort_values('listName', key=lambda x: x.str.len()))
+        
+        # check to see if the user input the correct variable type
         else:
             raise ValueError(
                 "You can only pass one string to your search parameter = run show_all(lists=True) to get strings to pass")
 
     # search options for profiles
     if profiles is not None:
+
         # call show_all to get all the possible values
         dataFrame = show_all(profiles=True)
+
         # check to see if user wants default column name
-        if column_name is None:
+        if column_name is None and atlas in ["Global","GBIF"]:
+            column_name = 'Description'
+        elif column_name is None:
             column_name = 'description'
+
         # throw ValueError if column_name variable is not a string
         elif type(column_name) is not str:
             raise ValueError("Only strings are a valid query for the column_name variable")
+        
         # check to see if the user input the correct variable type; else, throw value error
         if type(profiles) is str:
             return_dataFrame = dataFrame.loc[dataFrame[column_name].astype(str).str.contains(profiles, case=True, na=False)]
             return_array.append(return_dataFrame.sort_values('id', key=lambda x: x.astype(str).str.len()))
+       
+        # check to see if the user input the correct variable type
         else:
             raise ValueError(
                 "You can only pass one string to your search parameter = run show_all(profiles=True) to get strings to pass")
 
     # search options for providers
     if providers is not None:
+
         # call show_all to get all the possible values
         dataFrame = show_all(providers=True)
+
         # check to see if user wants default column name
-        if column_name is None:
+        if column_name is None and atlas in ["Global","GBIF"]:
+            column_name = 'title'
+        elif column_name is None:
             column_name = 'name'
+
         # throw ValueError if column_name variable is not a string
         elif type(column_name) is not str:
             raise ValueError("Only strings are a valid query for the column_name variable")
+        
         # check to see if the user input the correct variable type; else, throw value error
         if type(providers) is str:
             return_dataFrame = dataFrame.loc[dataFrame[column_name].astype(str).str.contains(providers, case=True, na=False)]
-            return_array.append(return_dataFrame.sort_values('name', key=lambda x: x.str.len()))
+            if atlas in ["Global","GBIF"]:
+                return_array.append(return_dataFrame.sort_values('title', key=lambda x: x.str.len()))
+            else:
+                return_array.append(return_dataFrame.sort_values('name', key=lambda x: x.str.len()))
+        
+        # check to see if the user input the correct variable type
         else:
             raise ValueError(
                 "You can only pass one string to your search parameter = run show_all(providers=True) to get strings to pass")
 
     # search options for ranks
     if ranks is not None:
+
         # call show_all to get all the possible values
         dataFrame = show_all(ranks=True)
+
         # check to see if user wants default column name
         if column_name is None:
             column_name = 'name'
+
         # throw ValueError if column_name variable is not a string
         elif type(column_name) is not str:
             raise ValueError("Only strings are a valid query for the column_name variable")
+        
         # check to see if the user input the correct variable type; else, throw value error
         if type(ranks) is str:
             return_dataFrame = dataFrame.loc[dataFrame[column_name].astype(str).str.contains(ranks, case=True, na=False)]
             return_array.append(return_dataFrame.sort_values('id', key=lambda x: x.astype(str).str.len()))
+        
+        # check to see if the user input the correct variable type
         else:
             raise ValueError(
                 "You can only pass one string to your search parameter = run show_all(ranks=True) to get strings to pass")
 
     # search options for reasons
     if reasons is not None:
+
         # call show_all to get all the possible values
         dataFrame = show_all(reasons=True)
+        
         # check to see if user wants default column name
         if column_name is None:
             column_name = 'name'
+        
         # throw ValueError if column_name variable is not a string
         elif type(column_name) is not str:
             raise ValueError("Only strings are a valid query for the column_name variable")
+        
         # check to see if the user input the correct variable type; else, throw value error
         if type(reasons) is str:
             return_dataFrame = dataFrame.loc[dataFrame[column_name].astype(str).str.contains(reasons, case=True, na=False)]
             return_array.append(return_dataFrame.sort_values('id', key=lambda x: x.astype(str).str.len()))
+        
+        # check to see if the user input the correct variable type
         else:
             raise ValueError(
                 "You can only pass one string to your search parameter = run show_all(reasons=True) to get strings to pass")
 
     # return a single data frame if only one query was flagged; otherwise, return array
     if len(return_array) == 1:
         return return_array[0]
-    return return_array
+    return return_array
```

## galah/search_taxa.py

```diff
@@ -1,78 +1,191 @@
 import requests
 import pandas as pd
+import urllib
 
-from .get_api_url import get_api_url
-from .get_api_url import readConfig
+from .get_api_url import get_api_url,readConfig
+from .common_dictionaries import SEARCH_TAXA_ENTRIES,SEARCH_TAXA_FIELDS,TAXONCONCEPT_NAMES,VERNACULAR_NAMES,atlases
 
-ATLAS_KEYWORDS = {
-    "Australia": "taxonConceptID",
-    "Austria": "guid",
-    "Brazil": "guid",
-    "Canada": "usageKey",
-    "Estonia": "guid",
-    "France": "usageKey",
-    "Guatemala": "guid",
-    "Portugal": "usageKey",
-    "Spain": "taxonConceptID",
-    "Sweden": "guid",
-    "United Kingdom": "guid",
-}
-
-ATLAS_COMMON_NAMES = {
-    "Australia": "vernacularName",
-    "Austria": "",
-    "Brazil": "commonName",
-    "Canada": "",
-    "Estonia": "",
-    "France": "",
-    "Guatemala": "",
-    "Portugal": "",
-    "Spain": "vernacularName",
-    "Sweden": "",
-    "United Kingdom": "",
-}
-
-atlases = ["Australia","Austria","Brazil","Canada","Estonia","France","Guatemala","Portugal","Sweden","Spain","United Kingdom"]
-
-def search_taxa(taxa):
+def search_taxa(taxa=None,
+                identifiers=None,
+                specific_epithet=None,
+                scientific_name=None,
+                verbose=False):
     """
     Look up taxonomic names before downloading data from the ALA, using ``atlas_occurrences()``, ``atlas_species()`` or 
     ``atlas_counts()``. Taxon information returned by ``search_taxa()`` may be passed to the ``taxa`` argument of ``atlas`` 
     functions. 
     
     ``search_taxa()`` allows users to disambiguate homonyms (i.e. where the same name refers to taxa in different 
     clades) prior to downloading data.
 
     Parameters
     ----------
         taxa : string
             one or more scientific names to search.  
+        identifiers : string / list
+            one or more taxonomic identifiers (such as guid or taxonConceptID) to search.  
+        specific_epithet : list
+            search taxonomic levels by using the argument "specificEpithet".
+        scientific_name : dictionary
+            search taxonomic levels by using the argument "scientificName".   
+        verbose : logical
+            If ``True``, galah gives more information like URLs of your queries. Defaults to ``False``
 
     Returns
     -------
         An object of class ``pandas.DataFrame``.
 
     Examples
     --------
 
+    Get taxonomic identifiers for "Vulpes vulpes"
+
     .. prompt:: python
 
         import galah
         galah.search_taxa(taxa="Vulpes vulpes")
 
     .. program-output:: python -c "import galah; print(galah.search_taxa(taxa=\\\"Vulpes vulpes\\\"))"
+
+    Get the species name from a taxonomic identifier
+
+    .. prompt:: python
+
+        import galah
+        galah.search_taxa(identifiers="https://id.biodiversity.org.au/node/apni/2914510")
+
+    .. program-output:: python -c "import galah; print(galah.search_taxa(identifiers=\\\"https://id.biodiversity.org.au/node/apni/2914510\\\"))"
+
+    Search taxonomic levels by using the key word "specificEpithet"
+
+    .. prompt:: python
+
+        import galah
+        galah.search_taxa(specific_epithet=["class=aves","family=pardalotidae","genus=pardalotus","specificEpithet=punctatus"])
+
+    .. program-output:: python -c "import galah; print(galah.search_taxa(specific_epithet=[\\\"class=aves\\\",\\\"family=pardalotidae\\\",\\\"genus=pardalotus\\\",\\\"specificEpithet=punctatus\\\"]))"
+    
+    Search taxonomic levels by using the key word "scientificName"
+
+    .. prompt:: python
+
+        import galah
+        galah.search_taxa(scientific_name=scientific_ name={"family": ["pardalotidae","maluridae"],"scientificName": ["pardolatus striatus","malurus cyaneus"]})
+
+    .. program-output:: python -c "import galah; print(galah.search_taxa(scientific_name={\\\"family\\\": [\\\"pardalotidae\\\",\\\"maluridae\\\"],\\\"scientificName\\\": [\\\"pardolatus striatus\\\",\\\"malurus cyaneus\\\"]}))"
     """
 
     # get configuration
     configs = readConfig()
 
+    # get atlas
+    atlas = configs['galahSettings']['atlas']
+
+    # check for identifiers or specific epithets
+    if identifiers is not None or specific_epithet is not None:
+
+        # first, check if the atlas is Australian; if not, functionality not supported (yet?)
+        if atlas in ["Australia","ALA"]:
+
+            # check for specific epithet
+            if specific_epithet is not None:
+                
+                # if keyword is not correct, raise error
+                if not any("specificEpithet" in se for se in specific_epithet):
+                    raise ValueError("you need to include a search term titled \"specificEpithet\"")
+                
+                # if keyword correct, add to URL
+                else:
+                    baseURL = get_api_url(column1='called_by',column1value='search_taxa',column2='api_name',column2value='names_search_multiple')
+                    URL = baseURL + "?" + "&".join(specific_epithet)        
+            
+            # check for identifiers from user
+            elif identifiers is not None:
+                baseURL = get_api_url(column1='called_by',column1value='search_identifiers',column2='api_name',column2value='names_lookup')
+                URL = baseURL + "?taxonID=" + urllib.parse.quote(identifiers)
+            
+            # else, something wasn't put into the argyments correctly
+            else:
+                raise ValueError("Something isn't right with identifiers or specific_epithet:\nidentifiers: {}\nspecific_epithet: {}\n".format(identifiers,specific_epithet))
+        else:
+            raise ValueError("identifiers and specific_epithet are only available for the Australian atlas.")
+        
+        # get response from URL
+        response = requests.get(URL)
+        response_json = response.json()
+
+        # initialise data dictionary
+        data={}
+
+        # check for relevant data in response
+        for entry in response_json:
+            if entry in SEARCH_TAXA_FIELDS[atlas]:
+                if type(response_json[entry]) is str:
+                    data[entry] = response_json[entry]
+                elif type(response_json[entry]) is list:
+                    data[entry] = ", ".join(response_json[entry])
+                else:
+                    raise ValueError("The type of variable for entry {} is {}".format(entry,type(response_json[entry])))
+        
+        # return data frame with all information
+        return pd.DataFrame(data,index=[0])
+
+    # check to see if scientific name was an argument
+    if scientific_name is not None:
+
+        # check if they are in the Australian atlas
+        if atlas in ["Australia","ALA"]:
+            
+            # get base URL before adding anything onto it 
+            baseURL = get_api_url(column1='called_by',column1value='search_taxa',column2='api_name',column2value='names_search_multiple')
+            
+            # check to see if the correct information and type of variables is available
+            if not any("scientificName" in sn for sn in list(scientific_name.keys())):
+                raise ValueError("you need to include a search term titled \"scientificName\"")
+            elif type(scientific_name) is not dict:
+                raise ValueError("You need to pass a dictionary value to scientific_name")
+            
+            # get length of the arrays in the dictionary
+            lens = map(len,scientific_name.values())
+            len_dict = list(set(list(lens)))
+
+            # throw error if dictionary values are not the same length
+            if len(len_dict) != 1:
+                raise ValueError("All of your dictionary values need to be the same length")
+            
+            # initialise empty data frame 
+            df = pd.DataFrame()
+
+            # loop over all entries in scientific name dictionary and concatenate them to data frame
+            for i in range(len_dict[0]):
+                URL = baseURL + "?" + "&".join(["=".join([key,urllib.parse.quote(scientific_name[key][i])]) for key in scientific_name])
+                response = requests.get(URL)
+                response_json = response.json()
+                data={}
+                for entry in response_json:
+                    if entry in SEARCH_TAXA_FIELDS[atlas]:
+                        if type(response_json[entry]) is str:
+                            data[entry] = response_json[entry]
+                        elif type(response_json[entry]) is list:
+                            data[entry] = ", ".join(response_json[entry])
+                        else:
+                            raise ValueError("The type of variable for entry {} is {}".format(entry,type(response_json[entry])))
+                df = pd.concat([df,pd.DataFrame(data,index=[0])])
+            
+            # return data frame
+            return df
+        
+        # else, throw error saying this is only avaiable for Australian atlas (now)
+        else:
+            raise ValueError("scientific_name is only available for the Australian atlas.")
+
     # first, check if someone actually entered a taxa name
     if taxa is None:
-        raise Exception("You need to specify a taxa")
+        raise Exception("You need to specify one of the following:\n\ntaxa\nidentifiers\nspecific_epithet\nscientific_name\n")
 
     # get base URL for querying
     baseURL = get_api_url(column1='called_by',column1value='search_taxa',column2='api_name',column2value='names_search_single')
 
     # third, add fq=<search term> and converting it to URL
     if type(taxa) is list or type(taxa) is str:
 
@@ -84,50 +197,89 @@
         dataFrame = pd.DataFrame()
 
         # currently only return information above kingdom
         for name in taxa:
 
             # create URL, get result and concatenate result onto dataFrame
             # make sure all the atlases are checked
-            if configs['galahSettings']['atlas'] in atlases:
+            if atlas in atlases:
                 URL = baseURL.replace("{name}","%20".join(name.split(" ")))
             else:
-                raise ValueError("Atlas {} is not taken into account".format(configs['galahSettings']['atlas']))
+                raise ValueError("Atlas {} is not taken into account".format(atlas))
+            
+            if verbose:
+                print("URL for querying:\n\n{}\n".format(URL))
+        
+            # get the response
             response = requests.get(URL)
+            response_json = response.json()
 
-            # get the response
-            json = response.json()
-            if configs['galahSettings']['atlas'] in ["Brazil"]:
+            # Check for the Swedish atlas
+            if atlas in ["Sweden"]: 
+                raw_data = [] 
+                if SEARCH_TAXA_ENTRIES[atlas][0] in response_json:
+                    for item in response_json[SEARCH_TAXA_ENTRIES[atlas][0]][SEARCH_TAXA_ENTRIES[atlas][1]]:
+                        if name.lower() in item['scientificName'].lower():
+                            raw_data = item
+                            break
+                if raw_data is None:
+                    continue
+
+            # check for Austrian, Brazilian, French or Guatemalan atlas
+            elif atlas in ["Austria","Brazil","France", "Guatemala"]:
                 raw_data = None
-                for i,item in enumerate(json['searchResults']['results']):
-                    if item['scientificName'].lower() == name.lower():
-                        raw_data = json['searchResults']['results'][i]
+                if SEARCH_TAXA_ENTRIES[atlas][0] in response_json:
+                    for item in response_json[SEARCH_TAXA_ENTRIES[atlas][0]][SEARCH_TAXA_ENTRIES[atlas][1]]:
+                        if name.lower() == item['scientificName'].lower():
+                            raw_data = item
+                            break
                 if raw_data is None:
-                    continue # return pd.DataFrame()
-            elif configs['galahSettings']['atlas'] in ["Australia","Spain"]:
-                raw_data = json
+                    continue
+            
+            # check for Australian, Global, or Spanish atlas
+            elif atlas in ["Australia","Global","GBIF","Spain"]:
+                raw_data = response_json
+                if atlas in ["Global","GBIF"]:
+                    response_vernacular = requests.get("https://api.gbif.org/v1/species/{}/vernacularNames".format(raw_data[TAXONCONCEPT_NAMES[atlas]["guid"]]))
+                    array_vernacular = response_vernacular.json()['results']
+            
+            # else, throw an error saying this atlas is not taken into account
             else:
-                raise ValueError("The atlas {} is not taken into account".format(configs['galahSettings']['atlas']))
+                raise ValueError("The atlas {} is not taken into account".format(atlas))
 
             # check to see if the taxa was successfully returned
-            if configs['galahSettings']['atlas'] in ["Australia","Spain"] and not json['success']:
+            if atlas in ["Australia","Spain"] and not response_json['success']:
                 continue
+
+            # process information and put it into a data frame
             else:
+
+                # initialise dictionary
                 data={}
+
+                # loop over data
                 for item in raw_data: 
-                    if item in ['scientificName', 'scientificNameAuthorship', ATLAS_KEYWORDS[configs['galahSettings']['atlas']],
-                                'rank', 'match_type','kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species', 'issues', 
-                                ATLAS_COMMON_NAMES[configs['galahSettings']['atlas']]]:
+                    if item in SEARCH_TAXA_FIELDS[atlas]:
                         data[item] = raw_data[item] 
 
-            # add every instance of 
+                # check if the atlas is GBIF and get vernacular names accordingly
+                if atlas in ["Global","GBIF"]:
+                    vernacular_name=""
+                    for item in array_vernacular:
+                        for key in item.keys():
+                            if key in SEARCH_TAXA_FIELDS[atlas]:
+                                vernacular_name += item[key] + ", "
+                    vernacular_name = vernacular_name[:-2]
+                    data[VERNACULAR_NAMES[atlas][1]] = vernacular_name
+
+            # add every taxon to dataframe
             tempdf = pd.DataFrame(data,index=[1])
             dataFrame = pd.concat([dataFrame,tempdf],ignore_index=True)
 
         # return dataFrame with all data
         return dataFrame
 
-    # else, let the user know that the taxa argument can only be a string or a lsit
+    # else, let the user know that the taxa argument can only be a string or a list
     else:
         raise TypeError("The taxa argument can only be a string or a list."
                         "\nExample: search_taxa(\"Vulpes vulpes\")"
                         "\n         search_taxa([\"Osphranter rufus\",\"Vulpes vulpes\",\"Macropus giganteus\",\"Phascolarctos cinereus\"])")
```

## galah/search_values.py

```diff
@@ -1,8 +1,9 @@
 from .show_values import show_values
+from .get_api_url import readConfig
 
 def search_values(field=None,
                   value=None,
                   column_name=None):
     """
     Users may wish to see the specific values within a chosen field, profile or list to narrow queries or understand 
     more about the information of interest. ``search_values()`` allows users for search for specific values within 
@@ -32,21 +33,25 @@
     .. program-output:: python -c "import galah; print(galah.search_values(field=\\\"basisOfRecord\\\",value=\\\"OBS\\\"))"
     """
 
     if value is None:
         raise ValueError("Please specify the field you want to see query-able values for, i.e. field=\"basisOfRecord\"")
     elif type(value) is not str:
         raise TypeError("show_values() only takes a single string as the field argument, i.e. field=\"basisOfRecord\"")
-
+    
+    # get initial data frame
     dataFrame = show_values(field=field)
     
+    # check for column name to search by
     if column_name is None:
         column_name = dataFrame.columns[-1]
+
     # throw ValueError if column_name variable is not a string
     elif type(column_name) is not str:
         raise ValueError("Only strings are a valid query for the column_name variable")
+    
     # check to see if the user input the correct variable type; else, throw value error
     if type(value) is str:
         return dataFrame.loc[dataFrame[column_name].astype(str).str.contains(value,case=False, na=False)].sort_values(column_name,key=lambda x: x.str.len())
     else:
         raise ValueError(
             "You can only pass one string to your search parameter = run show_all(assertions=True) to get strings to pass")
```

## galah/show_all.py

```diff
@@ -1,12 +1,11 @@
 import requests,os
 import pandas as pd
 
-from .get_api_url import get_api_url
-from .get_api_url import readConfig
+from .get_api_url import get_api_url, readConfig
 
 '''
 function is meant to show all values for possible query fields - they are defined as a boolean variable so you can see
 the large list of all the potential variables to add to your atlas query.
 '''
 def show_all(assertions=False,
              atlases=False,
@@ -71,144 +70,222 @@
 
     # get configurations for different atlases
     configs = readConfig()
 
     # set up the option for getting back multiple values
     return_array=[]
 
+
     # check for assertions boolean
     if type(assertions) is bool and assertions:
-        if configs['galahSettings']['atlas'] in ["Australia","Brazil","Spain"]:
+
+        # set returned to False for GBIF
+        returned = False
+
+        # check for all atlases not named GBIF
+        if configs['galahSettings']['atlas'] not in ["Global","GBIF"]:
             response = requests.get(get_api_url(column1='called_by',column1value='show_all-assertions'))
+        
+        # then check for GBIF atlas
+        elif configs['galahSettings']['atlas'] in ["Global","GBIF"]:
+            json = pd.read_csv(os.path.join(os.path.dirname(os.path.abspath(__file__)), 'gbif_assertions.csv'))
+            json.reset_index(drop=True,inplace=True)
+            return_array.append(json)
+            returned = True
+
+        # otherwise, return this default
         else:
             response = requests.get(get_api_url(column1='called_by',column1value='show_all_assertions'))
-        # create a dataFrame from the response
-        json = pd.DataFrame.from_dict(response.json())
-        # set default value for the 'type' column
-        json['type'] = assertions
-        # append this data frame to the return_array
-        return_array.append(json[['name','description','category','type']])
+
+        # if the response hasn't been added to the return array, add it here 
+        if not returned:
+
+            # get the response in a data frame
+            df = pd.DataFrame.from_dict(response.json())
+
+            # set default value for the 'type' column
+            df['type'] = assertions
+
+            # append this data frame to the return_array
+            return_array.append(df[['name','description','category','type']])
+    
+    # else, pass 
     elif not assertions:
         pass
+
+    # else, there is an incorrect argument for assertions
     else:
         raise ValueError("You can only specify True for assertions (default=False)")
 
+
     # return all the atlases you can query
     if type(atlases) is bool and atlases:
+
         # dictionary of all atlases galah currently supports
         '''
-         "atlas": ["Australia","Austria","Brazil","Canada","Estonia","France","Guatemala","Portugal","Spain","Sweden","United Kingdom"],
-            "institution": ["Atlas of Living Australia","Biodiversitäts-Atlas Österreich","Sistemas de Informações sobre a Biodiversidade Brasileira",
-                            "Canadensys", "eElurikkus","Inventaire National du Patrimoine Naturel","Sistema Nacional de Información sobre Diversidad Biológica de Guatemala",
-                            "GBIF Portugal","GBIF Spain","Swedish Biodiversity Data Infrastructure","National Biodiversity Network"],
-            "acronym": ["ALA","BAO","SiBBr","<NA>","<NA>","INPN","SNIBgt","GBIF.pt","GBIF.es","SBDI","NBN",],
-            "url": ["https://www.ala.org.au","https://biodiversityatlas.at","https://sibbr.gov.br","http://www.canadensys.net/",
-                    "https://elurikkus.ee","https://inpn.mnhn.fr","https://snib.conap.gob.gt","https://www.gbif.pt",
-                    "https://www.gbif.es","https://biodiversitydata.se","https://nbn.org.uk"],
+         "atlas": ["Canada","Estonia","Guatemala","Portugal","Sweden","United Kingdom"],
+            "institution": ["Canadensys", "eElurikkus","Sistema Nacional de Información sobre Diversidad Biológica de Guatemala",
+                            "GBIF Portugal","Swedish Biodiversity Data Infrastructure","National Biodiversity Network"],
+            "acronym": ["<NA>","<NA>","SNIBgt","GBIF.pt","SBDI","NBN",],
+            "url": ["http://www.canadensys.net/",
+                    "https://elurikkus.ee","https://snib.conap.gob.gt","https://www.gbif.pt",
+                    "https://biodiversitydata.se","https://nbn.org.uk"],
         '''
+
+        # data of all the atlases galah currently supports
         data = {
-            "atlas": ["Australia","Brazil","Spain"],
-            "institution": ["Atlas of Living Australia","Sistemas de Informações sobre a Biodiversidade Brasileira",
-                            "GBIF Spain"],
-            "acronym": ["ALA","SiBBr","GBIF.es"],
-            "url": ["https://www.ala.org.au","https://sibbr.gov.br","https://www.gbif.es"],
+            "atlas": ["Australia","Austria","Brazil","France","Global","Spain"],
+            "institution": ["Atlas of Living Australia","Biodiversitäts-Atlas Österreich","Sistemas de Informações sobre a Biodiversidade Brasileira",
+                            "Inventaire National du Patrimoine Naturel","Global Biodiversity Information Facility", "GBIF Spain"],
+            "acronym": ["ALA","BAO","SiBBr","INPN","GBIF","GBIF.es"],
+            "url": ["https://www.ala.org.au","https://biodiversityatlas.at","https://sibbr.gov.br","https://inpn.mnhn.fr","https://gbif.org",
+                    "https://www.gbif.es"],
         }
+
         # append this data frame to the return_array
         return_array.append(pd.DataFrame.from_dict(data))
+
+    # else, user doesn't want atlases
     elif not atlases:
         pass
+
+    # else, user has input something incorrectly
     else:
         raise ValueError("You can only specify True for atlases (default=False)")
 
+
     # return all the possible apis you could query
     if type(apis) is bool and apis:
+
         # append the full atlaslist to return_array
         atlasfile = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'node_config.csv')
         atlaslist = pd.read_csv(atlasfile)
+        
+        # at this atlas list to return array
         return_array.append(atlaslist)
+    
+    # else, user doesn't want apis
     elif not apis:
         pass
+    
+    # else, user has input something incorrectly 
     else:
         raise ValueError("You can only specify True for apis (default=False)")
 
+
     # check for collection
     if type(collection) is bool and collection:
-        if configs['galahSettings']['atlas'] in ["Australia","Brazil","Spain"]:
+        
+        # check which atlas user has set
+        if configs['galahSettings']['atlas'] not in ["Global","GBIF"]:
             response = requests.get(get_api_url(column1='called_by',column1value='show_all-collections'))
+        elif configs['galahSettings']['atlas'] in ["Global","GBIF"]:
+            raise ValueError("{} altas does not have a list of collections".format(configs['galahSettings']['atlas']))
         else:
             response = requests.get(get_api_url(column1='called_by', column1value='show_all_collections'))
+        
         # append data frame to return_array
         return_array.append(pd.DataFrame.from_dict(response.json()))
+
+    # user doesn't want collections
     elif not collection:
         pass
+
+    # user has input something incorrectly
     else:
         raise ValueError("You can only specify True for collection (default=False)")
 
+
     # check for datasets
     if type(datasets) is bool and datasets:
-        if configs['galahSettings']['atlas'] in ["Australia","Brazil","Spain"]:
+
+        # first, check atlas
+        if configs['galahSettings']['atlas'] not in ["Global","GBIF"]:
+            response = requests.get(get_api_url(column1='called_by',column1value='show_all-datasets'))
+            datasets_list = pd.DataFrame.from_dict(response.json())
+        elif configs['galahSettings']['atlas'] in ["Global","GBIF"]:
             response = requests.get(get_api_url(column1='called_by',column1value='show_all-datasets'))
+            datasets_list = pd.DataFrame.from_dict(response.json()['results'])
         else:
             response = requests.get(get_api_url(column1='called_by', column1value='show_all_datasets'))
+            datasets_list = pd.DataFrame.from_dict(response.json())
+
         # append data frame to return_array
-        return_array.append(pd.DataFrame.from_dict(response.json()))
+        return_array.append(datasets_list)
+
+    # user doesn't want datasets
     elif not datasets:
         pass
+
+    # user has input something incorrectly
     else:
         raise ValueError("You can only specify True for datasets (default=False)")
 
+
     # get all fields from the API
     if type(fields) is bool and fields:
 
         # get all possible fields
-        if configs['galahSettings']['atlas'] in ["Australia","Brazil","Spain"]:
+        if configs['galahSettings']['atlas'] not in ["Global","GBIF"]:
+            
+            # get response and values
             response = requests.get(get_api_url(column1='called_by',column1value='show_all-fields',column2='api_name',column2value='records_fields'))
             fields_values = pd.DataFrame.from_dict(response.json())
 
             # remove anything with "Contextual" or "Environmental" from the options for Australian atlas
-            ### TODO: Check for Spanish and Brazilian atlases
             if configs['galahSettings']['atlas'] in ["Australia","Brazil","Spain"]:
                 fields_values = fields_values[~fields_values["classs"].astype(str).str.contains("Contextual|Environmental")]
 
             # select only the columns titled 'name', 'info', (and) 'infoUrl'
             if configs['galahSettings']['atlas'] in ["Australia","Spain"]:
                 fields_select = fields_values[['name', 'info', 'infoUrl']]
                 dataFrame = fields_select.rename(columns={"name": "id","info": "description", "infoUrl": "link"})
                 dataFrame.insert(loc=2,column="type", value="field")
             elif configs['galahSettings']['atlas'] in ["Austria","Brazil","Canada","Estonia","France","Guatemala","Sweden","United Kingdom"]:
                 fields_select = fields_values[['name', 'info']]
                 dataFrame = fields_select.rename(columns={"name": "id","info": "description"}) #, inplace=True)
                 dataFrame["type"] = "field"
                 dataFrame["link"] = ""
+
+        # check if atlas is GBIF
+        elif configs['galahSettings']['atlas'] in ["Global","GBIF"]:
+            dataFrame = pd.read_csv(os.path.join(os.path.dirname(os.path.abspath(__file__)), 'gbif_fields.csv'))
         
+        # else, atlas not taken into account
         else:
             raise ValueError("Atlas {} not taken into account.".format(configs['galahSettings']['atlas']))
         
         # second: get spatial layers
         if configs['galahSettings']['atlas'] in ["Australia","Spain"]:
             # get data from API
             response = requests.get(get_api_url(column1='called_by',column1value='show_all-fields',column2='api_name',column2value='spatial_layers'))
             spatial_values = pd.DataFrame.from_dict(response.json())
             spatial_layers = pd.DataFrame()
 
             # select only the columns titled 'name', 'info', (and) 'infoUrl'
             if configs['galahSettings']['atlas'] in ["Australia","Spain"]:
+
                 # build layer id from this
                 spatial_values["type"].replace("Contextual","cl",inplace=True)
                 spatial_values["type"].replace("Environmental","el",inplace=True)
                 spatial_layers["id"] =  spatial_values["type"].astype(str) + spatial_values["id"].astype(str)
+
                 # build descriptions from these
                 spatial_layers["description"] = spatial_values['displayname'] + " " + spatial_values['description']
                 spatial_layers["type"] = "layers"
                 spatial_layers["link"] = ""
+
+            # look only into these atlases 
             elif configs['galahSettings']['atlas'] in ["Austria","Brazil","Canada","Estonia","France","Guatemala","Sweden","United Kingdom"]:
                 layers_select = spatial_values[['name', 'info']]
                 spatial_layers = layers_select.rename(columns={"name": "id","info": "description"})
                 spatial_layers["type"] = "layers"
                 spatial_layers["link"] = ""
+
+            # else, need to add another atlas
             else:
                 raise ValueError("Atlas {} not taken into account".format(configs['galahSettings']['atlas']))
         
         # Australia has more things than other atlases; take that into account
         if configs['galahSettings']['atlas'] in ["Australia","Spain"]:
 
             # third: get media
@@ -231,100 +308,166 @@
 
             # reset index
             dataFrame.reset_index(drop = True, inplace = True)
 
             # return final thing
             return_array.append(dataFrame)
 
+    # user doesn't want fields
     elif not fields:
         pass
+
+    # else, user input something incorrectly
     else:
         raise ValueError("You can only specify True for fields (default=False)")
 
+
     # get all licences from the API
     if type(licences) is bool and licences:
-        if configs['galahSettings']['atlas'] in ["Austria","Canada","Estonia","France"]:
+
+        # check for atlases that don't have licences
+        if configs['galahSettings']['atlas'] in ["Canada","Estonia","France","Global","GBIF"]:
             raise ValueError("The {} atlas does not have a list of licences".format(configs['galahSettings']['atlas']))
-        elif configs['galahSettings']['atlas'] in ["Australia","Spain"]:
+        
+        # check for atlases that do have license
+        elif configs['galahSettings']['atlas'] in ["Australia","Guatemala","Spain","Sweden"]:
             response = requests.get(get_api_url(column1='called_by',column1value='show_all-licences'))
-        elif configs['galahSettings']['atlas'] in ["Brazil"]:
-            raise ValueError("Brazil has an API endpoint for licences, but it is empty.")
+        
+        # check for atlases that have an endpoint but no data
+        elif configs['galahSettings']['atlas'] in ["Austria","Brazil"]:
+            raise ValueError("{} has an API endpoint for licences, but it is empty.".format(configs['galahSettings']['atlas']))
+        
+        # otherwise, do default call
         else:
             response = requests.get(get_api_url(column1='called_by', column1value='show_all_licences'))
+        
+        # check to see if this URL is not working
         if response.status_code == 404:
             raise ValueError("The licences URL for the {} atlas is not working.".format(configs['galahSettings']['atlas']))
+        
         # create a data frame from the API response
         json = pd.DataFrame.from_dict(response.json())
+        
         # append data frame with only the column names 'id', 'name', 'acronym' and 'url'
         return_array.append(json[['id','name','acronym','url']])
+    
+    # user doesn't want licences
     elif not licences:
         pass
+
+    # user has input something incorrectly
     else:
         raise ValueError("You can only specify True for licences (default=False)")
 
+
     # get all lists from the API
     if type(lists) is bool and lists:
-        if  configs['galahSettings']['atlas'] in ["Canada","Estonia","France","Guatemala","Portugal","Sweden"]:
+        
+        # first, check for APIs that do not have lists
+        if  configs['galahSettings']['atlas'] in ["Canada","Estonia","France","GBIF","Global","Guatemala","Portugal","Sweden"]:
             raise ValueError("The {} atlas does not have a lists API.".format(configs['galahSettings']['atlas']))
+        
+        # then, look for lists and set offsets
         for i,maxoffsets in enumerate(["?max=1000&offset=0","?max=1000&offset=1000","?max=1000&offset=2000"]):
-            if configs['galahSettings']['atlas'] in ["Australia","Brazil","Spain"]:
+            if configs['galahSettings']['atlas'] in ["Australia","Austria","Brazil","Spain"]:
                 response = requests.get(get_api_url(column1='called_by',column1value='show_all-lists'))
             else:
                 response = requests.get(
                     "{}{}".format(get_api_url(column1='called_by', column1value='show_all_lists'),maxoffsets))
             if i == 0:
                 json = pd.DataFrame.from_dict(response.json())
             else:
                 json = pd.concat([json,pd.DataFrame.from_dict(response.json())], ignore_index=True)
+
         # loop over the dictionaries in the 'list' column to get the actual list parameters
         #    if this is the first list, create a new dataframe
         #    if it is not the first list, concatenate a new data frame onto the old data frame
         for i,l in enumerate(json['lists']):
             if i == 0:
                 df = pd.DataFrame(l,index=[0])
             else:
                 df = pd.concat([df, pd.DataFrame(l,index=[0])], ignore_index=True)
+
         # append data frame to return_array
         return_array.append(df)
+
+    # user doesn't want lists
     elif not lists:
         pass
+
+    # user input something incorrectly
     else:
         raise ValueError("You can only specify True for lists (default=False)")
 
+
     # get all profiles from the API
     if type(profiles) is bool and profiles:
+
+        # check for only aPIs that have data quality profiles
         if configs['galahSettings']['atlas'] in ["Australia","Spain"]:
+
+            # get response from API
             response = requests.get(get_api_url(column1='called_by',column1value='show_all-profiles'))
-            json = pd.DataFrame.from_dict(response.json())
+            df = pd.DataFrame.from_dict(response.json())
+
             # append data frame with only the column names 'id', 'name', 'shortName' and 'description'
             if configs['galahSettings']['atlas'] in ["Spain"]:
                 print("WARNING: The Spanish atlas has data quality profiles, but they are not yet linked to the biocache yet")
-            return_array.append(json[['id','name','shortName','description']])
+            
+            # return data frame
+            return_array.append(df[['id','name','shortName','description']])
+        
+        # else, raise value error
         else:
             raise ValueError("Only the Australian atlas has data quality profiles you can use.")
+        
+    # user doesn't want profiles
     elif not profiles:
         pass
+
+    # user has input someting incorrectly
     else:
         raise ValueError("You can only specify True for profiles (default=False)")
 
     # get all providers from the API
     if type(providers) is bool and providers:
-        if configs['galahSettings']['atlas'] in ["Australia","Brazil","Spain"]:
+
+        # check for atlases with providers
+        if configs['galahSettings']['atlas'] in ["Australia","Austria","Brazil","Guatemala","Spain","Sweden"]:
+            response = requests.get(get_api_url(column1='called_by',column1value='show_all-providers'))
+            providers_list = pd.DataFrame.from_dict(response.json())
+        
+        # check for GBIF
+        elif configs['galahSettings']['atlas'] in ["Global","GBIF"]:
             response = requests.get(get_api_url(column1='called_by',column1value='show_all-providers'))
+            providers_list = pd.DataFrame.from_dict(response.json()['results'])
+
+        # raise an exception specific to France, as their providers are empty
+        elif configs['galahSettings']['atlas'] in ["France"]:
+            raise ValueError("{} has an API endpoint for providers, but it is empty.".format(configs['galahSettings']['atlas']))
+        
+        # else, do default call
         else:
             response = requests.get(get_api_url(column1='called_by', column1value='show_all_providers'))
+            providers_list = pd.DataFrame.from_dict(response.json())
+
         # append data frame to return_array
-        return_array.append(pd.DataFrame.from_dict(response.json()))
+        return_array.append(providers_list)
+
+    # user does nto want providers
     elif not providers:
         pass
+
+    # user input something incorrectly
     else:
         raise ValueError("You can only specify True for providers (default=False)")
 
     # get all ranks from the API
     if type(ranks) is bool and ranks:
+
         # extended ranks dictionary
         if configs["galahSettings"]["ranks"] == "all":
             all_ranks = {
                 'id': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28,
                     29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,
                     55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69],
                 'name': ['root',"superkingdom", "kingdom", "subkingdom", "superphylum", "phylum", "subphylum", "superclass",
@@ -335,44 +478,60 @@
                         "subfamily", "infrafamily", "supertribe", "tribe", "subtribe", "supergenus", "genus group",
                         "genus", "nothogenus", "subgenus", "supersection botany", "section botany", "subsection botany",
                         "superseries botany", "series botany", "subseries botany", "species group", "superspecies",
                         "species subgroup", "species", "nothospecies", "holomorph", "anamorph", "teleomorph", "subspecies",
                         "nothosubspecies", "infraspecies", "variety", "nothovariety", "subvariety", "form", "nothoform",
                         "subform", "biovar", "serovar", "cultivar", "pathovar", "infraspecific"]
             }
-            # add this in with configuration file
-            # short ranks dictionary
+            return_array.append(pd.DataFrame.from_dict(all_ranks))
+        
+        # check for reduced ranks
         elif configs["galahSettings"]["ranks"] == "gbif":
             gbif_ranks = {
                 'id': [1, 2, 3, 4, 5, 6, 7, 8, 9],
                 'name': ["kingdom", "phylum", "class", "order", "family", "genus", "species", "subspecies", "infraspecific"]
             }
+            return_array.append(pd.DataFrame.from_dict(gbif_ranks))
         else:
             raise ValueError("For ranks, you can only have two values currently:\n\nall: all possible ranks\ngbif: only the nine major ranks\n")
-        # append data frame to return_array
-        return_array.append(pd.DataFrame.from_dict(all_ranks))
+        
+    # user does not want ranks 
     elif not ranks:
         pass
+
+    # user input something incorrectly
     else:
         raise ValueError("You can only specify True for ranks (default=False)")
 
     # check for reasons
     if type(reasons) is bool and reasons:
-        if  configs['galahSettings']['atlas'] in ["Brazil","Estonia","France"]:
+
+        # check for atlases that don't have a reasons API
+        if  configs['galahSettings']['atlas'] in ["Brazil","Estonia","France","Global","GBIF","Guatemala","Sweden"]:
             raise ValueError("The {} atlas does not have a reasons API.".format(configs['galahSettings']['atlas']))
-        elif configs['galahSettings']['atlas'] in ["Australia","Spain"]:
+        
+        # check for ones that do
+        elif configs['galahSettings']['atlas'] in ["Australia","Austria","Spain"]:
             response = requests.get(get_api_url(column1='called_by',column1value='show_all-reasons'))
+        
+        # else, make default call
         else:
             response = requests.get(get_api_url(column1='called_by', column1value='show_all_reasons'))
+
         # create a data frame from the API response
         json = pd.DataFrame.from_dict(response.json())
+
         # append data frame with only the column names 'id', 'name', sort values by 'id', and renumber indices
         return_array.append(json[['id','name']].sort_values('id').reset_index(drop=True))
+
+    # user does not want reasons
     elif not reasons:
         pass
+
+    # user input something incorrectly
     else:
         raise ValueError("You can only specify True for reasons (default=False)")
 
     # if there is only a singular data frame in the return_array, return only this; otherwise, return list
     if len(return_array) == 1:
         return return_array[0]
     return return_array
```

## galah/show_values.py

```diff
@@ -1,11 +1,11 @@
 import requests
 import pandas as pd
 
-from .get_api_url import get_api_url
+from .get_api_url import get_api_url,readConfig
 
 # comment on what this function does later
 def show_values(field=None,
                 verbose=False):
     """
     Users may wish to see the specific values within a chosen field, profile or list to narrow queries or understand 
     more about the information of interest. ``show_values()`` provides users with these values. 
@@ -28,25 +28,32 @@
 
         import galah
         galah.show_values(field="basisOfRecord")
 
     .. program-output:: python -c "import galah; print(galah.show_values(field=\\\"basisOfRecord\\\"))"
     """
 
+    # check to see if field is input correctly
     if field is None:
         raise ValueError("Please specify the field you want to see query-able values for, i.e. field=\"basisOfRecord\"")
     elif type(field) is not str:
         raise TypeError("show_values() only takes a single string as the field argument, i.e. field=\"basisOfRecord\"")
 
+    # get configurations
+    configs = readConfig()
+
     # get base URL for querying
-    baseURL = get_api_url(column1='api_name',column1value='records_facets')
+    if configs['galahSettings']['atlas'] in ["Global","GBIF"]:
+        baseURL = get_api_url(column1='api_name',column1value='records_counts')
+    else:
+        baseURL = get_api_url(column1='api_name',column1value='records_facets')
 
     '''
-    # add a buttload of checks to make sure that the field they entered actually is something they can query
-    ### TODO: talk to Martin about this
+    # add checks to make sure that the field they entered actually is something they can query
+    ### TODO: brainstorm
     # "field", "profile", "list", "collection", "dataset", "provider")
     #collection,datasets,fields,lists,profiles,providers
     raw_valid_values = show_all(collection=True,datasets=True,fields=True,lists=True,profiles=True,providers=True)
     for i,df in enumerate(raw_valid_values):
         if i == 0:
             if 'listName' in list(df.columns):
                 valid_values = df['listName']
@@ -67,32 +74,56 @@
 
     if field not in valid_values:
         raise ValueError("{} is not a valid field query.  Use the show_all() function with any of the following set as"
                          "True to show valid values:\n\n"
                          "collection, datasets, fields, lists, profiles, providers\n")
     '''
     # add the field
-    URL = baseURL + "?facets=" + field + "&flimit=10000"
+    if configs['galahSettings']['atlas'] in ["Global","GBIF"]:
+        URL = baseURL + "facets?facet=" + field # + "&limit=0&facetLimit=10000"
+    else:
+        URL = baseURL + "?facets=" + field
 
     # check to see if the user wants the URL for querying
     if verbose:
         print("URL for querying:\n\n{}\n".format(URL))
 
     # query the API
     response = requests.get(URL)
     json = response.json()
-
+    
     # create empty dataFrame to concatenate results to
     dataFrame = pd.DataFrame()
 
-    # loop over results and create dataFrame
-    for i,entry in enumerate(json[0]['fieldResult']):
-        # check if last character is a full stop
-        if entry['i18nCode'][-1] == ".":
-            tempdf = pd.DataFrame([entry['i18nCode'][0:-1].split('.')], columns=['field', 'category'])
-            dataFrame = pd.concat([dataFrame, tempdf], ignore_index=True)
-        else:
-            tempdf = pd.DataFrame([entry['i18nCode'].split('.')],columns=['field','category'])
+    # loop over results - look to see if GBIF is being used
+    if configs['galahSettings']['atlas'] in ["Global","GBIF"]:
+        # result = json['facets'][0]['counts']
+        result = json['results'][0]['counts']
+        for entry in result:
+            tempdf = pd.DataFrame({'field': json['results'][0]['field'], 'category': entry['name']},index=[0]) # facets
             dataFrame = pd.concat([dataFrame,tempdf],ignore_index=True)
+    
+    # otherwise, assume it is other atlases
+    else:
+        result = json[0]['fieldResult']
+        for i,entry in enumerate(result):
+            # check if last character is a full stop
+            if entry['i18nCode'][-1] == ".":
+                # check to see if the length is more than 2
+                if len(entry['i18nCode'].split('.')) > 2:
+                    temparray = entry['i18nCode'].split('.')
+                    name = " ".join(temparray[1:])
+                    tempdf = pd.DataFrame([[temparray[0],name]],columns=['field','category'])
+                else:
+                    tempdf = pd.DataFrame([entry['i18nCode'][0:-1].split('.')], columns=['field', 'category'])
+                dataFrame = pd.concat([dataFrame, tempdf], ignore_index=True)
+            elif len(entry['i18nCode'].split('.')) > 2:
+                temparray = entry['i18nCode'].split('.')
+                name = " ".join(temparray[1:])
+                tempdf = pd.DataFrame([[temparray[0],name]],columns=['field','category'])
+                dataFrame = pd.concat([dataFrame,tempdf],ignore_index=True)
+            else:
+                tempdf = pd.DataFrame([entry['i18nCode'].split('.')],columns=['field','category'])
+                dataFrame = pd.concat([dataFrame,tempdf],ignore_index=True)
 
     # return dataFrame
     return dataFrame
```

## Comparing `galah_python-0.1.0.dist-info/RECORD` & `galah_python-0.5.0.dist-info/RECORD`

 * *Files 20% similar despite different names*

```diff
@@ -1,25 +1,30 @@
-galah/__init__.py,sha256=8EpSVjJj9V5yIdUBC0YCKlV0Gt-XWMg-l5QTxVl8DUU,1333
-galah/apply_data_profile.py,sha256=IAeUGvAyIhnohOvv4JAo7ZtvVuUQXNVrX5umqrvo4rk,1963
+galah/__init__.py,sha256=m7CEtSKUlKvJw8NCFl4q_pM7tSTfG2JAL8CMaDuGuVw,1378
+galah/apply_data_profile.py,sha256=nmT1RlEiGrxs_qSUZYplozW1bDKAhLMAgMeaUyvM1Tw,2024
 galah/atlas_citation.py,sha256=vRWFt-06YoYPcMcThsWnfGbFy62pl8eeM0I-PlshaV4,2503
-galah/atlas_counts.py,sha256=2OQuh0aOh6UQ9eprnh8W1yyxFQ7JrCJND4qhxH43zLE,10381
-galah/atlas_media.py,sha256=z2K2EyRrYzUF7r6_bbNsPn0BKBeTFvtFnhvz0gChl2o,11072
-galah/atlas_occurrences.py,sha256=KZ3afwW3fak2DiPQ-T69l8Ut77z-pcTmU_ezqoc7oRk,16453
-galah/atlas_species.py,sha256=kjDhahOAYImyWh7DLmHjGM8Lyj4T_wWk73Zq_8Ks0Oc,7337
+galah/atlas_counts.py,sha256=IcpsIBM18Ld_fdGjs6XZOoOdz3kXKsRFuxIm1sR1OsA,12317
+galah/atlas_media.py,sha256=6niO4BRSaKzGebx5jFbqVmLAgMrNrf8e2_mQPEnHm2Y,11183
+galah/atlas_occurrences.py,sha256=LaB-ECFfrE7hiSaApcCgQGrJkMDar7doekIbI2osx3k,23774
+galah/atlas_species.py,sha256=kkKymC59OvIrvOyIJfpTnCdW8MygxHDDlV_wryfDzCI,6969
 galah/atlas_taxonomy.py,sha256=YTxutynekCk_6DHwBD-znxHAuUIg1qkEmIUoRcaOTFA,4408
-galah/config.ini,sha256=-gz72RdGIAZ6vzqalHWmelANw4eHOViEVRxs1R9ivD4,119
-galah/galah_config.py,sha256=xdczUFjmS5rUSLaN9F1Ejaf4eSaZw9GxxJpE4eHirH4,3852
-galah/galah_filter.py,sha256=WKDcLpk9oOx2zgloFZLgR4UCIe3cN9uKauErYWxKCr4,5009
-galah/galah_geolocate.py,sha256=Y8qPxxpnIpWtJ1xxqSmHhMF-nGrc0ad5V4wq5jpt10Y,53
-galah/galah_group_by.py,sha256=PwzZPY5Jt-uwPb-vn3NchsZQ3-MQGICr-dqn4Kk4Oks,6839
-galah/galah_select.py,sha256=Q0DnAmWgkLY-s7fhGog5M-RCjuH0kwF00YEECHFIu2Q,1669
-galah/get_api_url.py,sha256=zdGTT0zbvNpQytxjCKlIzL-H9Vv60WE4t2LV0iig1_0,4007
-galah/node_config.csv,sha256=W6ql0rRNITTlOVe0uc8Xn3mnWAx6f2SbaGeT6uvyV2c,7358
-galah/search_all.py,sha256=oR23slRUT1tKYafvK2CoautDKQZaXb6JEpEQKoo2-mg,14916
-galah/search_taxa.py,sha256=5k8SEwqBtHoXt8mpQ36HEPV-IzNg1EsRrwaeA8gGIqY,4841
-galah/search_values.py,sha256=8ISZ82QRA3Bk9ZNEzrdtFVkvKGcFWtusssV8lhBw_gM,2161
-galah/show_all.py,sha256=ZPyDV6-1gp3epA-ielUdp4NcYjwhly-Bc9pEpU0QQT4,19967
-galah/show_values.py,sha256=GpHKrk-0HrNaRh3EwsrWoWhG9mH6jfpIaXQPVFg1NNA,3955
-galah_python-0.1.0.dist-info/METADATA,sha256=oWppsthkBPeurL5SautWWp01ZYup0oC1qF_TqY5q-kY,450
-galah_python-0.1.0.dist-info/WHEEL,sha256=OqRkF0eY5GHssMorFjlbTIq072vpHpF60fIQA6lS9xA,92
-galah_python-0.1.0.dist-info/top_level.txt,sha256=kfWzvRF4Ut3of0Zi7jXK3dzb-mTOEvaXSdtCyXkrQkI,6
-galah_python-0.1.0.dist-info/RECORD,,
+galah/common_dictionaries.py,sha256=guBWds1qtAPeMtD1SJdh4_bgm_KJ2Bn4ixPMzZvv8dU,8768
+galah/common_functions.py,sha256=FO7wLnHHGxc_bXnJbBPl2fkVcj9Lv53yez904AwgQDI,1587
+galah/config.ini,sha256=Ot853DX2DoaSRvNsxtRS5x8x1jDzLAXlA6WImZ9NYS4,187
+galah/galah_config.py,sha256=qDVUQ64xQWPl3UqWnVvZh6_K54BAEEDBtCyGLKD6SM4,4314
+galah/galah_filter.py,sha256=0TaZlG6Sa7yKypR-xpq-1WzFnIzV6bk8gUvWB84L6Ck,8191
+galah/galah_gbif_filters.py,sha256=tlw31O0t5YX8Lhk8pfchXZRdoDI4x8c00VXJy8CoEzA,2785
+galah/galah_geolocate.py,sha256=1sQK1CFvxdBc7Xso2wuKkF0dy7TKVZy4Yd8Cpe-9gXc,2679
+galah/galah_group_by.py,sha256=Do3eDfmZRL9c2aFpunJIh4QizsG-NX2oplJNpHOV8cI,13133
+galah/galah_select.py,sha256=efJhlPxdYcWbkw3rIpmIUwkP8WMbf43jIRxJ5nY_r8w,1700
+galah/gbif_assertions.csv,sha256=01vlw2yW6LyafSqLswlzmzqOL3XwrJ6Msd1KwHCZwJQ,6550
+galah/gbif_fields.csv,sha256=VsfMl5JmbPhYPxnEYcRio1P4pNm26tEn9JK4duEJ3HQ,11155
+galah/get_api_url.py,sha256=3EZm7zjV9OxmAqfKSf3O5ij5r-25XV7RATW42vGqWZs,4542
+galah/node_config.csv,sha256=OB_xiUquau1WsMhnd9UdG4_gEKUtL6WxdAAuz7ZydC0,11622
+galah/search_all.py,sha256=NpuQxOHdHe20ZypdkxvDuCXxxUanG5Kp07bHkNudK4k,17468
+galah/search_taxa.py,sha256=1l0xJ8eI23Lv5EPcEq16HJbRQ-TO8WrMEtlb-mbxgKE,12787
+galah/search_values.py,sha256=IuHRcTJEITBffy1jT1CXjyeAQkp73wDfndi6oTzYQWs,2277
+galah/show_all.py,sha256=0FkNd6EhKiNzLntHCPtoPCAqjr4wCbLcJI1Nt2y8jjA,24134
+galah/show_values.py,sha256=q0WU35VIJ5YaQtcxB03pZpgsVe5cVo8cDPSAsTnNPaI,5520
+galah_python-0.5.0.dist-info/METADATA,sha256=8MVOGFOFQS2eF4-vwPk4BFg_DhQz2vizu7GRU2e2N_A,699
+galah_python-0.5.0.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+galah_python-0.5.0.dist-info/top_level.txt,sha256=kfWzvRF4Ut3of0Zi7jXK3dzb-mTOEvaXSdtCyXkrQkI,6
+galah_python-0.5.0.dist-info/RECORD,,
```

