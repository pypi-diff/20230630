# Comparing `tmp/pgmpy-0.1.7.tar.gz` & `tmp/pgmpy-0.1.9.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "dist/pgmpy-0.1.7.tar", last modified: Tue Jan 15 08:18:30 2019, max compression
+gzip compressed data, was "dist/pgmpy-0.1.9.tar", last modified: Sat Nov  9 09:37:59 2019, max compression
```

## Comparing `pgmpy-0.1.7.tar` & `pgmpy-0.1.9.tar`

### file list

```diff
@@ -1,164 +1,178 @@
-drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-01-15 08:18:30.000000 pgmpy-0.1.7/
--rw-rw-r--   0 ankur     (1000) ankur     (1000)       38 2019-01-15 08:18:30.000000 pgmpy-0.1.7/setup.cfg
-drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-01-15 08:18:30.000000 pgmpy-0.1.7/pgmpy/
-drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-01-15 08:18:30.000000 pgmpy-0.1.7/pgmpy/sampling/
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    16349 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/sampling/base.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    28329 2019-01-15 02:43:26.000000 pgmpy-0.1.7/pgmpy/sampling/NUTS.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)      679 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/sampling/__init__.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    17725 2019-01-15 02:43:26.000000 pgmpy-0.1.7/pgmpy/sampling/Sampling.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    24100 2019-01-15 02:43:26.000000 pgmpy-0.1.7/pgmpy/sampling/HMC.py
-drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-01-15 08:18:30.000000 pgmpy-0.1.7/pgmpy/readwrite/
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    15061 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/readwrite/XMLBeliefNetwork.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    14437 2018-10-18 09:13:14.000000 pgmpy-0.1.7/pgmpy/readwrite/UAI.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    24969 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/readwrite/PomdpX.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    15497 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/readwrite/XMLBIF.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    19929 2018-10-18 09:13:14.000000 pgmpy-0.1.7/pgmpy/readwrite/BIF.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    44399 2018-10-18 09:13:14.000000 pgmpy-0.1.7/pgmpy/readwrite/ProbModelXML.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)      903 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/readwrite/__init__.py
-drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-01-15 08:18:30.000000 pgmpy-0.1.7/pgmpy/utils/
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     2723 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/utils/mathext.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     7278 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/utils/state_name.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)      866 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/utils/check_functions.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)      366 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/utils/__init__.py
-drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-01-15 08:18:30.000000 pgmpy-0.1.7/pgmpy/factors/
-drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-01-15 08:18:30.000000 pgmpy-0.1.7/pgmpy/factors/continuous/
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     8295 2018-10-18 09:13:14.000000 pgmpy-0.1.7/pgmpy/factors/continuous/discretize.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    14114 2018-10-18 09:13:14.000000 pgmpy-0.1.7/pgmpy/factors/continuous/ContinuousFactor.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     8361 2018-10-18 09:13:14.000000 pgmpy-0.1.7/pgmpy/factors/continuous/LinearGaussianCPD.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)      471 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/factors/continuous/__init__.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    15812 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/factors/FactorSet.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     3183 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/factors/base.py
-drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-01-15 08:18:30.000000 pgmpy-0.1.7/pgmpy/factors/distributions/
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    18462 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/factors/distributions/CustomDistribution.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    20036 2018-10-18 09:13:14.000000 pgmpy-0.1.7/pgmpy/factors/distributions/GaussianDistribution.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)      840 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/factors/distributions/base.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    19128 2018-10-18 09:13:14.000000 pgmpy-0.1.7/pgmpy/factors/distributions/CanonicalDistribution.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)      254 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/factors/distributions/__init__.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)      275 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/factors/__init__.py
-drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-01-15 08:18:30.000000 pgmpy-0.1.7/pgmpy/factors/discrete/
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    17990 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/factors/discrete/CPD.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    28928 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/factors/discrete/DiscreteFactor.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    16001 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/factors/discrete/JointProbabilityDistribution.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)      236 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/factors/discrete/__init__.py
-drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-01-15 08:18:30.000000 pgmpy-0.1.7/pgmpy/models/
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    28964 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/models/MarkovModel.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    39729 2018-12-10 16:09:57.000000 pgmpy-0.1.7/pgmpy/models/BayesianModel.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    24431 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/models/DynamicBayesianNetwork.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    12580 2018-10-18 09:13:14.000000 pgmpy-0.1.7/pgmpy/models/ClusterGraph.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     9206 2018-10-18 09:13:14.000000 pgmpy-0.1.7/pgmpy/models/LinearGaussianBayesianNetwork.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     6910 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/models/NaiveBayes.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    19281 2018-10-18 09:13:14.000000 pgmpy-0.1.7/pgmpy/models/MarkovChain.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    16511 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/models/FactorGraph.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     5613 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/models/NoisyOrModel.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)      730 2019-01-15 02:42:42.000000 pgmpy-0.1.7/pgmpy/models/__init__.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     4314 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/models/JunctionTree.py
-drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-01-15 08:18:30.000000 pgmpy-0.1.7/pgmpy/inference/
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    32556 2018-10-18 09:13:14.000000 pgmpy-0.1.7/pgmpy/inference/ExactInference.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    26017 2018-10-18 09:13:14.000000 pgmpy-0.1.7/pgmpy/inference/mplp.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    18661 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/inference/dbn_inference.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     3954 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/inference/base.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     5404 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/inference/EliminationOrder.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)      411 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/inference/__init__.py
-drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-01-15 08:18:30.000000 pgmpy-0.1.7/pgmpy/tests/
-drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-01-15 08:18:30.000000 pgmpy-0.1.7/pgmpy/tests/test_base/
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     5710 2018-12-12 10:20:43.000000 pgmpy-0.1.7/pgmpy/tests/test_base/test_DirectedGraph.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     4140 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_base/test_UndirectedGraph.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)        0 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_base/__init__.py
-drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-01-15 08:18:30.000000 pgmpy-0.1.7/pgmpy/tests/test_independencies/
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     7896 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_independencies/test_Independencies.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)        0 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_independencies/__init__.py
-drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-01-15 08:18:30.000000 pgmpy-0.1.7/pgmpy/tests/test_inference/
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     5723 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_inference/test_dbn_inference.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     4644 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_inference/test_Inference.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     4833 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_inference/test_Mplp.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     5824 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_inference/test_elimination_order.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    21451 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_inference/test_ExactInference.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)        0 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_inference/__init__.py
-drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-01-15 08:18:30.000000 pgmpy-0.1.7/pgmpy/tests/test_factors/
-drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-01-15 08:18:30.000000 pgmpy-0.1.7/pgmpy/tests/test_factors/test_continuous/
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     9678 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_factors/test_continuous/test_Canonical_Factor.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     3588 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_factors/test_continuous/test_Linear_Gaussain_CPD.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    11543 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_factors/test_continuous/test_JointGaussianDistribution.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    19064 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_factors/test_continuous/test_ContinuousFactor.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)        0 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_factors/test_continuous/__init__.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    20285 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_factors/test_continuous/test_CustomDistribution.py
-drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-01-15 08:18:30.000000 pgmpy-0.1.7/pgmpy/tests/test_factors/test_discrete/
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    56232 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_factors/test_discrete/test_Factor.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)        0 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_factors/test_discrete/__init__.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     4687 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_factors/test_FactorSet.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)        0 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_factors/__init__.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)      150 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/help_functions.py
-drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-01-15 08:18:30.000000 pgmpy-0.1.7/pgmpy/tests/test_models/
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     6616 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_models/test_JunctionTree.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    16010 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_models/test_DynamicBayesianNetwork.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     9313 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_models/test_ClusterGraph.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     3581 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_models/test_LinearGaussianBayesianNetwork.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    29142 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_models/test_MarkovModel.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    32015 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_models/test_BayesianModel.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     4708 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_models/test_NoisyOrModels.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    13227 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_models/test_MarkovChain.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    13471 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_models/test_FactorGraph.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     8843 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_models/test_NaiveBayes.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)        0 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_models/__init__.py
-drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-01-15 08:18:30.000000 pgmpy-0.1.7/pgmpy/tests/test_estimators/
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     1426 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_estimators/test_K2Score.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     2743 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_estimators/test_ScoreCache.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     4536 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_estimators/test_MaximumLikelihoodEstimator.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     4559 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_estimators/test_BayesianEstimator.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     1464 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_estimators/test_BdeuScore.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     4392 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_estimators/test_HillClimbSearch.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     5805 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_estimators/test_ExhaustiveSearch.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     3125 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_estimators/test_BaseEstimator.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     1435 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_estimators/test_ParameterEstimator.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     1432 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_estimators/test_BicScore.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)        0 2019-01-10 21:15:48.000000 pgmpy-0.1.7/pgmpy/tests/test_estimators/__init__.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     6515 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_estimators/test_ConstraintBasedEstimator.py
-drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-01-15 08:18:30.000000 pgmpy-0.1.7/pgmpy/tests/test_readwrite/
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    64565 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_readwrite/test_PomdpX.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    21368 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_readwrite/test_XMLBeliefNetwork.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     6462 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_readwrite/test_UAI.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    11581 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_readwrite/test_BIF.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    13471 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_readwrite/test_XMLBIF.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    63931 2018-10-18 09:13:14.000000 pgmpy-0.1.7/pgmpy/tests/test_readwrite/test_ProbModelXML.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)        0 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_readwrite/__init__.py
-drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-01-15 08:18:30.000000 pgmpy-0.1.7/pgmpy/tests/test_sampling/
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     5935 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_sampling/test_base_continuous.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     8812 2018-12-10 16:09:57.000000 pgmpy-0.1.7/pgmpy/tests/test_sampling/test_Sampling.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     8097 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_sampling/test_continuous_sampling.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)        0 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/test_sampling/__init__.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)       29 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/tests/__init__.py
-drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-01-15 08:18:30.000000 pgmpy-0.1.7/pgmpy/base/
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    10043 2018-12-10 16:09:57.000000 pgmpy-0.1.7/pgmpy/base/DirectedGraph.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     8848 2018-12-10 16:09:57.000000 pgmpy-0.1.7/pgmpy/base/UndirectedGraph.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)      147 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/base/__init__.py
-drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-01-15 08:18:30.000000 pgmpy-0.1.7/pgmpy/estimators/
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    24198 2019-01-03 22:54:45.000000 pgmpy-0.1.7/pgmpy/estimators/ConstraintBasedEstimator.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     3104 2018-10-18 09:13:14.000000 pgmpy-0.1.7/pgmpy/estimators/StructureScore.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     3421 2019-01-03 22:54:45.000000 pgmpy-0.1.7/pgmpy/estimators/BdeuScore.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     8074 2019-01-10 11:35:47.000000 pgmpy-0.1.7/pgmpy/estimators/BayesianEstimator.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     2651 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/estimators/K2Score.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     2976 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/estimators/BicScore.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    16748 2019-01-15 07:35:07.000000 pgmpy-0.1.7/pgmpy/estimators/base.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     8415 2018-10-18 09:13:14.000000 pgmpy-0.1.7/pgmpy/estimators/HillClimbSearch.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     8140 2018-10-18 09:13:14.000000 pgmpy-0.1.7/pgmpy/estimators/ExhaustiveSearch.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     4150 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/estimators/ScoreCache.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)      893 2019-01-15 02:42:42.000000 pgmpy-0.1.7/pgmpy/estimators/__init__.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     6091 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/estimators/MLE.py
-drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-01-15 08:18:30.000000 pgmpy-0.1.7/pgmpy/independencies/
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    15273 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/independencies/Independencies.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)      132 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/independencies/__init__.py
-drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-01-15 08:18:30.000000 pgmpy-0.1.7/pgmpy/extern/
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    31425 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/extern/tabulate.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)    30888 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/extern/six.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)       55 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/extern/__init__.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)       87 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/__init__.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)      146 2018-09-11 14:08:16.000000 pgmpy-0.1.7/pgmpy/Dependencies.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)      967 2019-01-15 08:17:33.000000 pgmpy-0.1.7/setup.py
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     3665 2019-01-15 07:35:07.000000 pgmpy-0.1.7/README.md
-drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-01-15 08:18:30.000000 pgmpy-0.1.7/pgmpy.egg-info/
--rw-rw-r--   0 ankur     (1000) ankur     (1000)       48 2019-01-15 08:18:30.000000 pgmpy-0.1.7/pgmpy.egg-info/requires.txt
--rw-rw-r--   0 ankur     (1000) ankur     (1000)     5124 2019-01-15 08:18:30.000000 pgmpy-0.1.7/pgmpy.egg-info/SOURCES.txt
--rw-rw-r--   0 ankur     (1000) ankur     (1000)        6 2019-01-15 08:18:30.000000 pgmpy-0.1.7/pgmpy.egg-info/top_level.txt
--rw-rw-r--   0 ankur     (1000) ankur     (1000)        1 2019-01-15 08:18:30.000000 pgmpy-0.1.7/pgmpy.egg-info/dependency_links.txt
--rw-rw-r--   0 ankur     (1000) ankur     (1000)      698 2019-01-15 08:18:30.000000 pgmpy-0.1.7/pgmpy.egg-info/PKG-INFO
--rw-rw-r--   0 ankur     (1000) ankur     (1000)      698 2019-01-15 08:18:30.000000 pgmpy-0.1.7/PKG-INFO
+drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-11-09 09:37:59.000000 pgmpy-0.1.9/
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)       38 2019-11-09 09:37:59.000000 pgmpy-0.1.9/setup.cfg
+drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-11-09 09:37:59.000000 pgmpy-0.1.9/pgmpy/
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)      386 2019-08-04 11:46:51.000000 pgmpy-0.1.9/pgmpy/global_vars.py
+drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-11-09 09:37:59.000000 pgmpy-0.1.9/pgmpy/sampling/
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    16543 2019-08-04 11:46:51.000000 pgmpy-0.1.9/pgmpy/sampling/base.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    31629 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/sampling/NUTS.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)      617 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/sampling/__init__.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    18238 2019-10-14 14:42:20.000000 pgmpy-0.1.9/pgmpy/sampling/Sampling.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    24761 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/sampling/HMC.py
+drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-11-09 09:37:59.000000 pgmpy-0.1.9/pgmpy/readwrite/
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    15910 2019-10-26 11:33:05.000000 pgmpy-0.1.9/pgmpy/readwrite/XMLBeliefNetwork.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    14927 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/readwrite/UAI.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    25204 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/readwrite/PomdpX.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    15936 2019-10-26 11:33:05.000000 pgmpy-0.1.9/pgmpy/readwrite/XMLBIF.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    21956 2019-10-26 11:33:05.000000 pgmpy-0.1.9/pgmpy/readwrite/BIF.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    46841 2019-10-26 11:33:05.000000 pgmpy-0.1.9/pgmpy/readwrite/ProbModelXML.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)      802 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/readwrite/__init__.py
+drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-11-09 09:37:59.000000 pgmpy-0.1.9/pgmpy/utils/
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     2747 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/utils/mathext.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     3599 2019-09-19 16:39:45.000000 pgmpy-0.1.9/pgmpy/utils/state_name.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)      275 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/utils/decorators.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     3698 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/utils/optimizer.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)      918 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/utils/check_functions.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     1179 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/utils/sets.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)      369 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/utils/__init__.py
+drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-11-09 09:37:59.000000 pgmpy-0.1.9/pgmpy/data/
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     8104 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/data/Data.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)       43 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/data/__init__.py
+drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-11-09 09:37:59.000000 pgmpy-0.1.9/pgmpy/factors/
+drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-11-09 09:37:59.000000 pgmpy-0.1.9/pgmpy/factors/continuous/
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     8519 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/factors/continuous/discretize.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    14141 2019-07-12 12:32:29.000000 pgmpy-0.1.9/pgmpy/factors/continuous/ContinuousFactor.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     8540 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/factors/continuous/LinearGaussianCPD.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)      427 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/factors/continuous/__init__.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    15877 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/factors/FactorSet.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     3145 2019-07-12 12:32:29.000000 pgmpy-0.1.9/pgmpy/factors/base.py
+drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-11-09 09:37:59.000000 pgmpy-0.1.9/pgmpy/factors/distributions/
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    18747 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/factors/distributions/CustomDistribution.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    20078 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/factors/distributions/GaussianDistribution.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)      841 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/factors/distributions/base.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    19441 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/factors/distributions/CanonicalDistribution.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)      220 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/factors/distributions/__init__.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)      242 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/factors/__init__.py
+drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-11-09 09:37:59.000000 pgmpy-0.1.9/pgmpy/factors/discrete/
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    18697 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/factors/discrete/CPD.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    29700 2019-08-04 11:46:51.000000 pgmpy-0.1.9/pgmpy/factors/discrete/DiscreteFactor.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    16361 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/factors/discrete/JointProbabilityDistribution.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)      202 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/factors/discrete/__init__.py
+drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-11-09 09:37:59.000000 pgmpy-0.1.9/pgmpy/models/
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    29029 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/models/MarkovModel.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    54281 2019-08-04 11:46:51.000000 pgmpy-0.1.9/pgmpy/models/SEM.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    29857 2019-10-26 11:33:05.000000 pgmpy-0.1.9/pgmpy/models/BayesianModel.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    25510 2019-10-14 14:13:31.000000 pgmpy-0.1.9/pgmpy/models/DynamicBayesianNetwork.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    12741 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/models/ClusterGraph.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     9774 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/models/LinearGaussianBayesianNetwork.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     7065 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/models/NaiveBayes.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    19949 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/models/MarkovChain.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    16629 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/models/FactorGraph.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     5790 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/models/NoisyOrModel.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)      754 2019-07-12 12:32:29.000000 pgmpy-0.1.9/pgmpy/models/__init__.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     4331 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/models/JunctionTree.py
+drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-11-09 09:37:59.000000 pgmpy-0.1.9/pgmpy/inference/
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    36198 2019-09-19 16:39:45.000000 pgmpy-0.1.9/pgmpy/inference/ExactInference.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    26882 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/inference/mplp.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    19373 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/inference/dbn_inference.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     3955 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/inference/base.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    13255 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/inference/CausalInference.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     5914 2019-09-19 16:39:45.000000 pgmpy-0.1.9/pgmpy/inference/EliminationOrder.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)      369 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/inference/__init__.py
+drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-11-09 09:37:59.000000 pgmpy-0.1.9/pgmpy/tests/
+drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-11-09 09:37:59.000000 pgmpy-0.1.9/pgmpy/tests/test_base/
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     4089 2019-10-26 11:33:05.000000 pgmpy-0.1.9/pgmpy/tests/test_base/test_UndirectedGraph.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     6571 2019-10-26 11:33:05.000000 pgmpy-0.1.9/pgmpy/tests/test_base/test_DAG.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)        0 2019-02-10 00:20:01.000000 pgmpy-0.1.9/pgmpy/tests/test_base/__init__.py
+drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-11-09 09:37:59.000000 pgmpy-0.1.9/pgmpy/tests/test_independencies/
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     8047 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/tests/test_independencies/test_Independencies.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)        0 2019-02-10 00:20:01.000000 pgmpy-0.1.9/pgmpy/tests/test_independencies/__init__.py
+drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-11-09 09:37:59.000000 pgmpy-0.1.9/pgmpy/tests/test_inference/
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     5134 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/tests/test_inference/test_dbn_inference.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     4446 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/tests/test_inference/test_CausalInference.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     4315 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/tests/test_inference/test_Inference.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     5163 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/tests/test_inference/test_Mplp.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     4895 2019-07-12 12:32:29.000000 pgmpy-0.1.9/pgmpy/tests/test_inference/test_elimination_order.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    21483 2019-09-17 11:09:36.000000 pgmpy-0.1.9/pgmpy/tests/test_inference/test_ExactInference.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)        0 2019-02-10 00:20:01.000000 pgmpy-0.1.9/pgmpy/tests/test_inference/__init__.py
+drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-11-09 09:37:59.000000 pgmpy-0.1.9/pgmpy/tests/test_factors/
+drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-11-09 09:37:59.000000 pgmpy-0.1.9/pgmpy/tests/test_factors/test_continuous/
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     9589 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/tests/test_factors/test_continuous/test_Canonical_Factor.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     3451 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/tests/test_factors/test_continuous/test_Linear_Gaussain_CPD.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    11784 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/tests/test_factors/test_continuous/test_JointGaussianDistribution.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    19854 2019-07-12 12:32:29.000000 pgmpy-0.1.9/pgmpy/tests/test_factors/test_continuous/test_ContinuousFactor.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)        0 2019-02-10 00:20:01.000000 pgmpy-0.1.9/pgmpy/tests/test_factors/test_continuous/__init__.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    20141 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/tests/test_factors/test_continuous/test_CustomDistribution.py
+drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-11-09 09:37:59.000000 pgmpy-0.1.9/pgmpy/tests/test_factors/test_discrete/
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    65094 2019-09-19 16:39:45.000000 pgmpy-0.1.9/pgmpy/tests/test_factors/test_discrete/test_Factor.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)        0 2019-02-10 00:20:01.000000 pgmpy-0.1.9/pgmpy/tests/test_factors/test_discrete/__init__.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     4655 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/tests/test_factors/test_FactorSet.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)        0 2019-02-10 00:20:01.000000 pgmpy-0.1.9/pgmpy/tests/test_factors/__init__.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)      168 2019-06-07 14:33:57.000000 pgmpy-0.1.9/pgmpy/tests/help_functions.py
+drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-11-09 09:37:59.000000 pgmpy-0.1.9/pgmpy/tests/test_models/
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     6691 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/tests/test_models/test_JunctionTree.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    17079 2019-10-14 14:13:31.000000 pgmpy-0.1.9/pgmpy/tests/test_models/test_DynamicBayesianNetwork.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     9315 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/tests/test_models/test_ClusterGraph.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     3559 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/tests/test_models/test_LinearGaussianBayesianNetwork.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    61472 2019-07-12 12:32:29.000000 pgmpy-0.1.9/pgmpy/tests/test_models/test_SEM.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    28184 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/tests/test_models/test_MarkovModel.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    38801 2019-10-26 11:33:05.000000 pgmpy-0.1.9/pgmpy/tests/test_models/test_BayesianModel.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     3574 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/tests/test_models/test_NoisyOrModels.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    13657 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/tests/test_models/test_MarkovChain.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    12982 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/tests/test_models/test_FactorGraph.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     9146 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/tests/test_models/test_NaiveBayes.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)        0 2019-02-10 00:20:01.000000 pgmpy-0.1.9/pgmpy/tests/test_models/__init__.py
+drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-11-09 09:37:59.000000 pgmpy-0.1.9/pgmpy/tests/test_estimators/
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     1468 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/tests/test_estimators/test_K2Score.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     2879 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/tests/test_estimators/test_ScoreCache.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     4951 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/tests/test_estimators/test_MaximumLikelihoodEstimator.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     5481 2019-08-04 11:46:51.000000 pgmpy-0.1.9/pgmpy/tests/test_estimators/test_SEMEstimator.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     4778 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/tests/test_estimators/test_BayesianEstimator.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     1505 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/tests/test_estimators/test_BdeuScore.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     4339 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/tests/test_estimators/test_HillClimbSearch.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     6369 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/tests/test_estimators/test_ExhaustiveSearch.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     2904 2019-10-14 14:13:31.000000 pgmpy-0.1.9/pgmpy/tests/test_estimators/test_BaseEstimator.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     1627 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/tests/test_estimators/test_ParameterEstimator.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     1474 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/tests/test_estimators/test_BicScore.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     1491 2019-10-14 14:13:31.000000 pgmpy-0.1.9/pgmpy/tests/test_estimators/test_CITests.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)        0 2019-02-10 00:20:01.000000 pgmpy-0.1.9/pgmpy/tests/test_estimators/__init__.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     6595 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/tests/test_estimators/test_ConstraintBasedEstimator.py
+drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-11-09 09:37:59.000000 pgmpy-0.1.9/pgmpy/tests/test_readwrite/
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    64001 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/tests/test_readwrite/test_PomdpX.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    20962 2019-10-26 11:33:05.000000 pgmpy-0.1.9/pgmpy/tests/test_readwrite/test_XMLBeliefNetwork.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     7064 2019-10-26 11:33:05.000000 pgmpy-0.1.9/pgmpy/tests/test_readwrite/test_UAI.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    13302 2019-10-26 11:33:05.000000 pgmpy-0.1.9/pgmpy/tests/test_readwrite/test_BIF.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    12137 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/tests/test_readwrite/test_XMLBIF.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    65427 2019-10-26 11:33:05.000000 pgmpy-0.1.9/pgmpy/tests/test_readwrite/test_ProbModelXML.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)        0 2019-02-10 00:20:01.000000 pgmpy-0.1.9/pgmpy/tests/test_readwrite/__init__.py
+drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-11-09 09:37:59.000000 pgmpy-0.1.9/pgmpy/tests/test_sampling/
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     6897 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/tests/test_sampling/test_base_continuous.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     8966 2019-10-14 14:42:20.000000 pgmpy-0.1.9/pgmpy/tests/test_sampling/test_Sampling.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     8572 2019-11-09 09:32:02.000000 pgmpy-0.1.9/pgmpy/tests/test_sampling/test_continuous_sampling.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)        0 2019-02-10 00:20:01.000000 pgmpy-0.1.9/pgmpy/tests/test_sampling/__init__.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)       29 2019-02-10 00:20:01.000000 pgmpy-0.1.9/pgmpy/tests/__init__.py
+drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-11-09 09:37:59.000000 pgmpy-0.1.9/pgmpy/base/
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     9132 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/base/UndirectedGraph.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    23841 2019-10-29 10:19:45.000000 pgmpy-0.1.9/pgmpy/base/DAG.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)      104 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/base/__init__.py
+drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-11-09 09:37:59.000000 pgmpy-0.1.9/pgmpy/estimators/
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    24350 2019-10-14 14:13:31.000000 pgmpy-0.1.9/pgmpy/estimators/ConstraintBasedEstimator.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     3104 2019-02-10 00:20:01.000000 pgmpy-0.1.9/pgmpy/estimators/StructureScore.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    14881 2019-08-04 11:46:51.000000 pgmpy-0.1.9/pgmpy/estimators/SEMEstimator.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     3560 2019-09-24 13:14:37.000000 pgmpy-0.1.9/pgmpy/estimators/BdeuScore.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     8270 2019-11-09 09:32:02.000000 pgmpy-0.1.9/pgmpy/estimators/BayesianEstimator.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     2703 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/estimators/K2Score.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     3001 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/estimators/BicScore.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    11864 2019-10-26 11:33:05.000000 pgmpy-0.1.9/pgmpy/estimators/base.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     7774 2019-10-14 14:13:31.000000 pgmpy-0.1.9/pgmpy/estimators/CITests.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     8902 2019-07-12 12:32:29.000000 pgmpy-0.1.9/pgmpy/estimators/HillClimbSearch.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     8136 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/estimators/ExhaustiveSearch.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     4192 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/estimators/ScoreCache.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     1103 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/estimators/LinearModel.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     1014 2019-09-24 13:14:37.000000 pgmpy-0.1.9/pgmpy/estimators/__init__.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     6462 2019-11-09 09:32:02.000000 pgmpy-0.1.9/pgmpy/estimators/MLE.py
+drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-11-09 09:37:59.000000 pgmpy-0.1.9/pgmpy/independencies/
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    15718 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/independencies/Independencies.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)      121 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/independencies/__init__.py
+drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-11-09 09:37:59.000000 pgmpy-0.1.9/pgmpy/extern/
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    30258 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/extern/tabulate.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)    30962 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/extern/six.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)       55 2019-06-19 11:19:02.000000 pgmpy-0.1.9/pgmpy/extern/__init__.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)      103 2019-11-09 09:34:59.000000 pgmpy-0.1.9/pgmpy/__init__.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)      899 2019-07-12 12:32:29.000000 pgmpy-0.1.9/setup.py
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     3952 2019-10-14 14:13:31.000000 pgmpy-0.1.9/README.md
+drwxrwxr-x   0 ankur     (1000) ankur     (1000)        0 2019-11-09 09:37:59.000000 pgmpy-0.1.9/pgmpy.egg-info/
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)     5516 2019-11-09 09:37:59.000000 pgmpy-0.1.9/pgmpy.egg-info/SOURCES.txt
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)        6 2019-11-09 09:37:59.000000 pgmpy-0.1.9/pgmpy.egg-info/top_level.txt
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)        1 2019-11-09 09:37:59.000000 pgmpy-0.1.9/pgmpy.egg-info/dependency_links.txt
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)      698 2019-11-09 09:37:59.000000 pgmpy-0.1.9/pgmpy.egg-info/PKG-INFO
+-rw-rw-r--   0 ankur     (1000) ankur     (1000)      698 2019-11-09 09:37:59.000000 pgmpy-0.1.9/PKG-INFO
```

### Comparing `pgmpy-0.1.7/pgmpy/sampling/base.py` & `pgmpy-0.1.9/pgmpy/sampling/base.py`

 * *Files 2% similar despite different names*

```diff
@@ -48,16 +48,23 @@
     -0.4054597701149426
     >>> grad_logp
     array([ 0.90229885, -0.01149425])
     """
 
     def __init__(self, variable_assignments, model):
 
-        self.variable_assignments = _check_1d_array_object(variable_assignments, 'variable_assignments')
-        _check_length_equal(variable_assignments, model.variables, 'variable_assignments', 'model.variables')
+        self.variable_assignments = _check_1d_array_object(
+            variable_assignments, "variable_assignments"
+        )
+        _check_length_equal(
+            variable_assignments,
+            model.variables,
+            "variable_assignments",
+            "model.variables",
+        )
 
         self.model = model
 
         # The gradient log of probability distribution at position
         self.grad_log = None
 
         # The gradient log of probability distribution at position
@@ -128,15 +135,15 @@
         self.grad_log, self.log_pdf = self._get_gradient_log_pdf()
 
     def _get_gradient_log_pdf(self):
         """
         Method that finds gradient and its log at position
         """
         sub_vec = self.variable_assignments - self.model.mean.flatten()
-        grad = - np.dot(self.model.precision_matrix, sub_vec)
+        grad = -np.dot(self.model.precision_matrix, sub_vec)
         log_pdf = 0.5 * np.dot(sub_vec, grad)
 
         return grad, log_pdf
 
 
 class BaseSimulateHamiltonianDynamics(object):
     """
@@ -195,33 +202,41 @@
     array([0.9375, 1.875])
     >>> new_momentum
     array([-0.25, -0.5])
     >>> new_grad
     array([-0.9375, -1.875])
     """
 
-    def __init__(self, model, position, momentum, stepsize, grad_log_pdf, grad_log_position=None):
+    def __init__(
+        self, model, position, momentum, stepsize, grad_log_pdf, grad_log_position=None
+    ):
 
-        position = _check_1d_array_object(position, 'position')
+        position = _check_1d_array_object(position, "position")
 
-        momentum = _check_1d_array_object(momentum, 'momentum')
+        momentum = _check_1d_array_object(momentum, "momentum")
 
         if not issubclass(grad_log_pdf, BaseGradLogPDF):
-            raise TypeError("grad_log_pdf must be an instance" +
-                            " of pgmpy.inference.continuous.base.BaseGradLogPDF")
+            raise TypeError(
+                "grad_log_pdf must be an instance"
+                + " of pgmpy.inference.continuous.base.BaseGradLogPDF"
+            )
 
-        _check_length_equal(position, momentum, 'position', 'momentum')
-        _check_length_equal(position, model.variables, 'position', 'model.variables')
+        _check_length_equal(position, momentum, "position", "momentum")
+        _check_length_equal(position, model.variables, "position", "model.variables")
 
         if grad_log_position is None:
             grad_log_position, _ = grad_log_pdf(position, model).get_gradient_log_pdf()
 
         else:
-            grad_log_positon = _check_1d_array_object(grad_log_position, 'grad_log_position')
-            _check_length_equal(grad_log_position, position, 'grad_log_position', 'position')
+            grad_log_positon = _check_1d_array_object(
+                grad_log_position, "grad_log_position"
+            )
+            _check_length_equal(
+                grad_log_position, position, "grad_log_position", "position"
+            )
 
         self.position = position
         self.momentum = momentum
         self.stepsize = stepsize
         self.model = model
         self.grad_log_pdf = grad_log_pdf
         self.grad_log_position = grad_log_position
@@ -308,20 +323,25 @@
     array([ 70., -19.])
     >>> new_momentum
     array([  99., -121.])
     >>> new_grad
     array([ 41., -58.])
     """
 
-    def __init__(self, model, position, momentum, stepsize, grad_log_pdf, grad_log_position=None):
-
-        BaseSimulateHamiltonianDynamics.__init__(self, model, position, momentum,
-                                                 stepsize, grad_log_pdf, grad_log_position)
-
-        self.new_position, self.new_momentum, self.new_grad_logp = self._get_proposed_values()
+    def __init__(
+        self, model, position, momentum, stepsize, grad_log_pdf, grad_log_position=None
+    ):
+
+        BaseSimulateHamiltonianDynamics.__init__(
+            self, model, position, momentum, stepsize, grad_log_pdf, grad_log_position
+        )
+
+        self.new_position, self.new_momentum, self.new_grad_logp = (
+            self._get_proposed_values()
+        )
 
     def _get_proposed_values(self):
         """
         Method to perform time splitting using leapfrog
         """
         # Take half step in time for updating momentum
         momentum_bar = self.momentum + 0.5 * self.stepsize * self.grad_log_position
@@ -380,20 +400,25 @@
     array([2.125, 1.1875])
     >>> new_momentum
     array([0.5, 0.75])
     >>> new_grad
     array([-2.125, -1.1875])
     """
 
-    def __init__(self, model, position, momentum, stepsize, grad_log_pdf, grad_log_position=None):
-
-        BaseSimulateHamiltonianDynamics.__init__(self, model, position, momentum,
-                                                 stepsize, grad_log_pdf, grad_log_position)
-
-        self.new_position, self.new_momentum, self.new_grad_logp = self._get_proposed_values()
+    def __init__(
+        self, model, position, momentum, stepsize, grad_log_pdf, grad_log_position=None
+    ):
+
+        BaseSimulateHamiltonianDynamics.__init__(
+            self, model, position, momentum, stepsize, grad_log_pdf, grad_log_position
+        )
+
+        self.new_position, self.new_momentum, self.new_grad_logp = (
+            self._get_proposed_values()
+        )
 
     def _get_proposed_values(self):
         """
         Method to perform time splitting using Modified euler method
         """
         # Take full step in time and update momentum
         momentum_bar = self.momentum + self.stepsize * self.grad_log_position
```

### Comparing `pgmpy-0.1.7/pgmpy/sampling/NUTS.py` & `pgmpy-0.1.9/pgmpy/sampling/NUTS.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 # -*- coding: utf-8 -*-
 
 from __future__ import division
 
 import numpy as np
+from tqdm import tqdm
 
 from pgmpy.sampling import HamiltonianMCDA, LeapFrog, _return_samples
 from pgmpy.utils import _check_1d_array_object, _check_length_equal
 
 
 class NoUTurnSampler(HamiltonianMCDA):
     """
@@ -56,42 +57,56 @@
     Setting Path Lengths in Hamiltonian Monte Carlo. Journal of
     Machine Learning Research 15 (2014) 1351-1381
     Algorithm 3 : Efficient No-U-Turn Sampler
     """
 
     def __init__(self, model, grad_log_pdf, simulate_dynamics=LeapFrog):
 
-        super(NoUTurnSampler, self).__init__(model=model, grad_log_pdf=grad_log_pdf,
-                                             simulate_dynamics=simulate_dynamics)
+        super(NoUTurnSampler, self).__init__(
+            model=model, grad_log_pdf=grad_log_pdf, simulate_dynamics=simulate_dynamics
+        )
 
     def _initalize_tree(self, position, momentum, slice_var, stepsize):
         """
         Initalizes root node of the tree, i.e depth = 0
         """
 
-        position_bar, momentum_bar, _ = self.simulate_dynamics(self.model, position, momentum, stepsize,
-                                                               self.grad_log_pdf).get_proposed_values()
+        position_bar, momentum_bar, _ = self.simulate_dynamics(
+            self.model, position, momentum, stepsize, self.grad_log_pdf
+        ).get_proposed_values()
 
         _, logp_bar = self.grad_log_pdf(position_bar, self.model).get_gradient_log_pdf()
 
         hamiltonian = logp_bar - 0.5 * np.dot(momentum_bar, momentum_bar)
 
         candidate_set_size = slice_var < np.exp(hamiltonian)
         accept_set_bool = hamiltonian > np.log(slice_var) - 10000  # delta_max = 10000
 
         return position_bar, momentum_bar, candidate_set_size, accept_set_bool
 
-    def _update_acceptance_criteria(self, position_forward, position_backward, momentum_forward, momentum_backward,
-                                    accept_set_bool, candidate_set_size, candidate_set_size2):
+    def _update_acceptance_criteria(
+        self,
+        position_forward,
+        position_backward,
+        momentum_forward,
+        momentum_backward,
+        accept_set_bool,
+        candidate_set_size,
+        candidate_set_size2,
+    ):
 
         # criteria1 = I[(θ+ − θ−)·r− ≥ 0]
-        criteria1 = np.dot((position_forward - position_backward), momentum_backward) >= 0
+        criteria1 = (
+            np.dot((position_forward - position_backward), momentum_backward) >= 0
+        )
 
         # criteira2 = I[(θ+ − θ− )·r+ ≥ 0]
-        criteria2 = np.dot((position_forward - position_backward), momentum_forward) >= 0
+        criteria2 = (
+            np.dot((position_forward - position_backward), momentum_forward) >= 0
+        )
 
         accept_set_bool = accept_set_bool and criteria1 and criteria2
         candidate_set_size += candidate_set_size2
 
         return accept_set_bool, candidate_set_size
 
     def _build_tree(self, position, momentum, slice_var, direction, depth, stepsize):
@@ -100,47 +115,103 @@
         """
 
         # Parameter names in algorithm (here -> representation in algorithm)
         # position -> theta, momentum -> r, slice_var -> u, direction -> v, depth ->j, stepsize -> epsilon
         # candidate_set_size -> n, accept_set_bool -> s
         if depth == 0:
             # Take single leapfrog step in the given direction (direction * stepsize)
-            position_bar, momentum_bar, candidate_set_size, accept_set_bool =\
-                self._initalize_tree(position, momentum, slice_var, direction * stepsize)
-
-            return (position_bar, momentum_bar, position_bar, momentum_bar, position_bar,
-                    candidate_set_size, accept_set_bool)
+            position_bar, momentum_bar, candidate_set_size, accept_set_bool = self._initalize_tree(
+                position, momentum, slice_var, direction * stepsize
+            )
+
+            return (
+                position_bar,
+                momentum_bar,
+                position_bar,
+                momentum_bar,
+                position_bar,
+                candidate_set_size,
+                accept_set_bool,
+            )
 
         else:
             # Build left and right subtrees
-            (position_backward, momentum_backward, position_forward, momentum_forward, position_bar,
-             candidate_set_size, accept_set_bool) = self._build_tree(position, momentum,
-                                                                     slice_var, direction, depth - 1, stepsize)
+            (
+                position_backward,
+                momentum_backward,
+                position_forward,
+                momentum_forward,
+                position_bar,
+                candidate_set_size,
+                accept_set_bool,
+            ) = self._build_tree(
+                position, momentum, slice_var, direction, depth - 1, stepsize
+            )
             if accept_set_bool == 1:
                 if direction == -1:
                     # Build tree in backward direction
-                    (position_backward, momentum_backward, _, _, position_bar2, candidate_set_size2,
-                     accept_set_bool2) = self._build_tree(position_backward, momentum_backward,
-                                                          slice_var, direction, depth - 1, stepsize)
+                    (
+                        position_backward,
+                        momentum_backward,
+                        _,
+                        _,
+                        position_bar2,
+                        candidate_set_size2,
+                        accept_set_bool2,
+                    ) = self._build_tree(
+                        position_backward,
+                        momentum_backward,
+                        slice_var,
+                        direction,
+                        depth - 1,
+                        stepsize,
+                    )
                 else:
                     # Build tree in forward direction
-                    (_, _, position_forward, momentum_forward, position_bar2, candidate_set_size2,
-                     accept_set_bool2) = self._build_tree(position_forward, momentum_forward,
-                                                          slice_var, direction, depth - 1, stepsize)
-
-                if np.random.rand() < candidate_set_size2 / (candidate_set_size2 + candidate_set_size):
+                    (
+                        _,
+                        _,
+                        position_forward,
+                        momentum_forward,
+                        position_bar2,
+                        candidate_set_size2,
+                        accept_set_bool2,
+                    ) = self._build_tree(
+                        position_forward,
+                        momentum_forward,
+                        slice_var,
+                        direction,
+                        depth - 1,
+                        stepsize,
+                    )
+
+                if np.random.rand() < candidate_set_size2 / (
+                    candidate_set_size2 + candidate_set_size
+                ):
                     position_bar = position_bar2
 
-                accept_set_bool, candidate_set_size =\
-                    self._update_acceptance_criteria(position_forward, position_backward, momentum_forward,
-                                                     momentum_backward, accept_set_bool2, candidate_set_size,
-                                                     candidate_set_size2)
-
-            return (position_backward, momentum_backward, position_forward, momentum_forward,
-                    position_bar, candidate_set_size, accept_set_bool)
+                accept_set_bool, candidate_set_size = self._update_acceptance_criteria(
+                    position_forward,
+                    position_backward,
+                    momentum_forward,
+                    momentum_backward,
+                    accept_set_bool2,
+                    candidate_set_size,
+                    candidate_set_size2,
+                )
+
+            return (
+                position_backward,
+                momentum_backward,
+                position_forward,
+                momentum_forward,
+                position_bar,
+                candidate_set_size,
+                accept_set_bool,
+            )
 
     def _sample(self, position, stepsize):
         """
         Returns a sample using a single iteration of NUTS
         """
 
         # Re-sampling momentum
@@ -150,41 +221,74 @@
         depth = 0
         position_backward, position_forward = position, position
         momentum_backward, momentum_forward = momentum, momentum
         candidate_set_size = accept_set_bool = 1
         _, log_pdf = self.grad_log_pdf(position, self.model).get_gradient_log_pdf()
 
         # Resample slice variable `u`
-        slice_var = np.random.uniform(0, np.exp(log_pdf - 0.5 * np.dot(momentum, momentum)))
+        slice_var = np.random.uniform(
+            0, np.exp(log_pdf - 0.5 * np.dot(momentum, momentum))
+        )
 
         while accept_set_bool == 1:
             direction = np.random.choice([-1, 1], p=[0.5, 0.5])
             if direction == -1:
                 # Build a tree in backward direction
-                (position_backward, momentum_backward, _, _, position_bar,
-                 candidate_set_size2, accept_set_bool2) = self._build_tree(position_backward, momentum_backward,
-                                                                           slice_var, direction, depth, stepsize)
+                (
+                    position_backward,
+                    momentum_backward,
+                    _,
+                    _,
+                    position_bar,
+                    candidate_set_size2,
+                    accept_set_bool2,
+                ) = self._build_tree(
+                    position_backward,
+                    momentum_backward,
+                    slice_var,
+                    direction,
+                    depth,
+                    stepsize,
+                )
             else:
                 # Build tree in forward direction
-                (_, _, position_forward, momentum_forward, position_bar,
-                 candidate_set_size2, accept_set_bool2) = self._build_tree(position_forward, momentum_forward,
-                                                                           slice_var, direction, depth, stepsize)
+                (
+                    _,
+                    _,
+                    position_forward,
+                    momentum_forward,
+                    position_bar,
+                    candidate_set_size2,
+                    accept_set_bool2,
+                ) = self._build_tree(
+                    position_forward,
+                    momentum_forward,
+                    slice_var,
+                    direction,
+                    depth,
+                    stepsize,
+                )
             if accept_set_bool2 == 1:
                 if np.random.rand() < candidate_set_size2 / candidate_set_size:
                     position = position_bar.copy()
 
-            accept_set_bool, candidate_set_size = self._update_acceptance_criteria(position_forward, position_backward,
-                                                                                   momentum_forward, momentum_backward,
-                                                                                   accept_set_bool2, candidate_set_size,
-                                                                                   candidate_set_size2)
+            accept_set_bool, candidate_set_size = self._update_acceptance_criteria(
+                position_forward,
+                position_backward,
+                momentum_forward,
+                momentum_backward,
+                accept_set_bool2,
+                candidate_set_size,
+                candidate_set_size2,
+            )
             depth += 1
 
         return position
 
-    def sample(self, initial_pos, num_samples, stepsize=None, return_type='dataframe'):
+    def sample(self, initial_pos, num_samples, stepsize=None, return_type="dataframe"):
         """
         Method to return samples using No U Turn Sampler
 
         Parameters
         ----------
         initial_pos: A 1d array like object
             Vector representing values of parameter position, the starting
@@ -226,27 +330,29 @@
         4  0.781338  0.647220 -0.948640
         5  0.040308 -1.391406  0.412201
         6  1.179549 -1.450552  1.105216
         7  1.100320 -1.313926  1.207815
         8  1.484520 -1.349247  0.768599
         9  0.934942 -1.894589  0.471772
         """
-        initial_pos = _check_1d_array_object(initial_pos, 'initial_pos')
-        _check_length_equal(initial_pos, self.model.variables, 'initial_pos', 'model.variables')
+        initial_pos = _check_1d_array_object(initial_pos, "initial_pos")
+        _check_length_equal(
+            initial_pos, self.model.variables, "initial_pos", "model.variables"
+        )
 
         if stepsize is None:
             stepsize = self._find_reasonable_stepsize(initial_pos)
 
-        types = [(var_name, 'float') for var_name in self.model.variables]
+        types = [(var_name, "float") for var_name in self.model.variables]
         samples = np.zeros(num_samples, dtype=types).view(np.recarray)
 
         samples[0] = tuple(initial_pos)
         position_m = initial_pos
 
-        for i in range(1, num_samples):
+        for i in tqdm(range(1, num_samples)):
             # Genrating sample
             position_m = self._sample(position_m, stepsize)
             samples[i] = tuple(position_m)
 
         return _return_samples(return_type, samples)
 
     def generate_sample(self, initial_pos, num_samples, stepsize=None):
@@ -289,16 +395,18 @@
                [ 10.59255762,  -8.48085076],
                [  9.99860242,  -9.47096032],
                [ 10.5733564 ,  -9.83504745],
                [ 11.51302059,  -9.49919523],
                [ 11.31892143,  -8.5873259 ],
                [ 11.29008667,  -0.43809674]])
         """
-        initial_pos = _check_1d_array_object(initial_pos, 'initial_pos')
-        _check_length_equal(initial_pos, self.model.variables, 'initial_pos', 'model.variables')
+        initial_pos = _check_1d_array_object(initial_pos, "initial_pos")
+        _check_length_equal(
+            initial_pos, self.model.variables, "initial_pos", "model.variables"
+        )
 
         if stepsize is None:
             stepsize = self._find_reasonable_stepsize(initial_pos)
 
         position_m = initial_pos
 
         for _ in range(0, num_samples):
@@ -365,66 +473,154 @@
     Machine Learning Research 15 (2014) 1351-1381
     Algorithm 6 : No-U-Turn Sampler with Dual Averaging
     """
 
     def __init__(self, model, grad_log_pdf, simulate_dynamics=LeapFrog, delta=0.65):
 
         if not isinstance(delta, float) or delta > 1.0 or delta < 0.0:
-            raise ValueError(
-                "delta should be a floating value in between 0 and 1")
+            raise ValueError("delta should be a floating value in between 0 and 1")
 
         self.delta = delta
 
-        super(NoUTurnSamplerDA, self).__init__(model=model, grad_log_pdf=grad_log_pdf,
-                                               simulate_dynamics=simulate_dynamics)
-
-    def _build_tree(self, position, momentum, slice_var, direction, depth, stepsize, position0, momentum0):
+        super(NoUTurnSamplerDA, self).__init__(
+            model=model, grad_log_pdf=grad_log_pdf, simulate_dynamics=simulate_dynamics
+        )
+
+    def _build_tree(
+        self,
+        position,
+        momentum,
+        slice_var,
+        direction,
+        depth,
+        stepsize,
+        position0,
+        momentum0,
+    ):
         """
         Recursively builds a tree for proposing new position and momentum
         """
         if depth == 0:
 
-            position_bar, momentum_bar, candidate_set_size, accept_set_bool =\
-                self._initalize_tree(position, momentum, slice_var, direction * stepsize)
-
-            alpha = min(1, self._acceptance_prob(position, position_bar, momentum, momentum_bar))
-
-            return (position_bar, momentum_bar, position_bar, momentum_bar, position_bar,
-                    candidate_set_size, accept_set_bool, alpha, 1)
+            position_bar, momentum_bar, candidate_set_size, accept_set_bool = self._initalize_tree(
+                position, momentum, slice_var, direction * stepsize
+            )
+
+            alpha = min(
+                1, self._acceptance_prob(position, position_bar, momentum, momentum_bar)
+            )
+
+            return (
+                position_bar,
+                momentum_bar,
+                position_bar,
+                momentum_bar,
+                position_bar,
+                candidate_set_size,
+                accept_set_bool,
+                alpha,
+                1,
+            )
 
         else:
-            (position_backward, momentum_backward, position_forward, momentum_forward, position_bar,
-             candidate_set_size, accept_set_bool, alpha, n_alpha) =\
-                self._build_tree(position, momentum, slice_var,
-                                 direction, depth - 1, stepsize, position0, momentum0)
+            (
+                position_backward,
+                momentum_backward,
+                position_forward,
+                momentum_forward,
+                position_bar,
+                candidate_set_size,
+                accept_set_bool,
+                alpha,
+                n_alpha,
+            ) = self._build_tree(
+                position,
+                momentum,
+                slice_var,
+                direction,
+                depth - 1,
+                stepsize,
+                position0,
+                momentum0,
+            )
 
             if accept_set_bool == 1:
                 if direction == -1:
                     # Build tree in backward direction
-                    (position_backward, momentum_backward, _, _, position_bar2, candidate_set_size2, accept_set_bool2,
-                     alpha2, n_alpha2) = self._build_tree(position_backward, momentum_backward, slice_var, direction,
-                                                          depth - 1, stepsize, position0, momentum0)
+                    (
+                        position_backward,
+                        momentum_backward,
+                        _,
+                        _,
+                        position_bar2,
+                        candidate_set_size2,
+                        accept_set_bool2,
+                        alpha2,
+                        n_alpha2,
+                    ) = self._build_tree(
+                        position_backward,
+                        momentum_backward,
+                        slice_var,
+                        direction,
+                        depth - 1,
+                        stepsize,
+                        position0,
+                        momentum0,
+                    )
                 else:
                     # Build tree in forward direction
-                    (_, _, position_forward, momentum_forward, position_bar2, candidate_set_size2, accept_set_bool2,
-                     alpha2, n_alpha2) = self._build_tree(position_forward, momentum_forward, slice_var, direction,
-                                                          depth - 1, stepsize, position0, momentum0)
-
-                if np.random.rand() < candidate_set_size2 / (candidate_set_size2 + candidate_set_size):
+                    (
+                        _,
+                        _,
+                        position_forward,
+                        momentum_forward,
+                        position_bar2,
+                        candidate_set_size2,
+                        accept_set_bool2,
+                        alpha2,
+                        n_alpha2,
+                    ) = self._build_tree(
+                        position_forward,
+                        momentum_forward,
+                        slice_var,
+                        direction,
+                        depth - 1,
+                        stepsize,
+                        position0,
+                        momentum0,
+                    )
+
+                if np.random.rand() < candidate_set_size2 / (
+                    candidate_set_size2 + candidate_set_size
+                ):
                     position_bar = position_bar2
 
                 alpha += alpha2
                 n_alpha += n_alpha2
-                accept_set_bool, candidate_set_size =\
-                    self._update_acceptance_criteria(position_forward, position_backward, momentum_forward,
-                                                     momentum_backward, accept_set_bool2, candidate_set_size,
-                                                     candidate_set_size2)
-
-            return (position_backward, momentum_backward, position_forward, momentum_forward, position_bar,
-                    candidate_set_size, accept_set_bool, alpha, n_alpha)
+                accept_set_bool, candidate_set_size = self._update_acceptance_criteria(
+                    position_forward,
+                    position_backward,
+                    momentum_forward,
+                    momentum_backward,
+                    accept_set_bool2,
+                    candidate_set_size,
+                    candidate_set_size2,
+                )
+
+            return (
+                position_backward,
+                momentum_backward,
+                position_forward,
+                momentum_forward,
+                position_bar,
+                candidate_set_size,
+                accept_set_bool,
+                alpha,
+                n_alpha,
+            )
 
     def _sample(self, position, stepsize):
         """
         Returns a sample using a single iteration of NUTS with dual averaging
         """
 
         # Re-sampling momentum
@@ -435,43 +631,91 @@
         position_backward, position_forward = position, position
         momentum_backward, momentum_forward = momentum, momentum
         candidate_set_size = accept_set_bool = 1
         position_m_1 = position
         _, log_pdf = self.grad_log_pdf(position, self.model).get_gradient_log_pdf()
 
         # Resample slice variable `u`
-        slice_var = np.random.uniform(0, np.exp(log_pdf - 0.5 * np.dot(momentum, momentum)))
+        slice_var = np.random.uniform(
+            0, np.exp(log_pdf - 0.5 * np.dot(momentum, momentum))
+        )
 
         while accept_set_bool == 1:
             direction = np.random.choice([-1, 1], p=[0.5, 0.5])
             if direction == -1:
                 # Build a tree in backward direction
-                (position_backward, momentum_backward, _, _, position_bar, candidate_set_size2, accept_set_bool2,
-                 alpha, n_alpha) = self._build_tree(position_backward, momentum_backward, slice_var, direction,
-                                                    depth, stepsize, position_m_1, momentum)
+                (
+                    position_backward,
+                    momentum_backward,
+                    _,
+                    _,
+                    position_bar,
+                    candidate_set_size2,
+                    accept_set_bool2,
+                    alpha,
+                    n_alpha,
+                ) = self._build_tree(
+                    position_backward,
+                    momentum_backward,
+                    slice_var,
+                    direction,
+                    depth,
+                    stepsize,
+                    position_m_1,
+                    momentum,
+                )
             else:
                 # Build tree in forward direction
-                (_, _, position_forward, momentum_forward, position_bar, candidate_set_size2, accept_set_bool2,
-                 alpha, n_alpha) = self._build_tree(position_forward, momentum_forward, slice_var, direction,
-                                                    depth, stepsize, position_m_1, momentum)
+                (
+                    _,
+                    _,
+                    position_forward,
+                    momentum_forward,
+                    position_bar,
+                    candidate_set_size2,
+                    accept_set_bool2,
+                    alpha,
+                    n_alpha,
+                ) = self._build_tree(
+                    position_forward,
+                    momentum_forward,
+                    slice_var,
+                    direction,
+                    depth,
+                    stepsize,
+                    position_m_1,
+                    momentum,
+                )
 
             if accept_set_bool2 == 1:
                 if np.random.rand() < candidate_set_size2 / candidate_set_size:
                     position = position_bar
 
-            accept_set_bool, candidate_set_size = self._update_acceptance_criteria(position_forward, position_backward,
-                                                                                   momentum_forward, momentum_backward,
-                                                                                   accept_set_bool2, candidate_set_size,
-                                                                                   candidate_set_size2)
+            accept_set_bool, candidate_set_size = self._update_acceptance_criteria(
+                position_forward,
+                position_backward,
+                momentum_forward,
+                momentum_backward,
+                accept_set_bool2,
+                candidate_set_size,
+                candidate_set_size2,
+            )
 
             depth += 1
 
         return position, alpha, n_alpha
 
-    def sample(self, initial_pos, num_adapt, num_samples, stepsize=None, return_type='dataframe'):
+    def sample(
+        self,
+        initial_pos,
+        num_adapt,
+        num_samples,
+        stepsize=None,
+        return_type="dataframe",
+    ):
         """
         Returns samples using No U Turn Sampler with dual averaging
 
         Parameters
         ----------
         initial_pos: A 1d array like object
             Vector representing values of parameter position, the starting
@@ -516,41 +760,45 @@
         4   8.526596 -21.555793
         5  11.343194  -6.353789
         6  -1.583269 -12.802931
         7  12.411957 -11.704859
         8  13.253336 -20.169492
         9  11.295901  -7.665058
         """
-        initial_pos = _check_1d_array_object(initial_pos, 'initial_pos')
-        _check_length_equal(initial_pos, self.model.variables, 'initial_pos', 'model.variables')
+        initial_pos = _check_1d_array_object(initial_pos, "initial_pos")
+        _check_length_equal(
+            initial_pos, self.model.variables, "initial_pos", "model.variables"
+        )
 
         if stepsize is None:
             stepsize = self._find_reasonable_stepsize(initial_pos)
 
         if num_adapt <= 1:
-            return NoUTurnSampler(self.model, self.grad_log_pdf,
-                                  self.simulate_dynamics).sample(initial_pos, num_samples, stepsize)
+            return NoUTurnSampler(
+                self.model, self.grad_log_pdf, self.simulate_dynamics
+            ).sample(initial_pos, num_samples, stepsize)
 
         mu = np.log(10.0 * stepsize)
         stepsize_bar = 1.0
         h_bar = 0.0
 
-        types = [(var_name, 'float') for var_name in self.model.variables]
+        types = [(var_name, "float") for var_name in self.model.variables]
         samples = np.zeros(num_samples, dtype=types).view(np.recarray)
         samples[0] = tuple(initial_pos)
         position_m = initial_pos
 
-        for i in range(1, num_samples):
+        for i in tqdm(range(1, num_samples)):
 
             position_m, alpha, n_alpha = self._sample(position_m, stepsize)
             samples[i] = tuple(position_m)
 
             if i <= num_adapt:
-                stepsize, stepsize_bar, h_bar = self._adapt_params(stepsize, stepsize_bar, h_bar, mu,
-                                                                   i, alpha, n_alpha)
+                stepsize, stepsize_bar, h_bar = self._adapt_params(
+                    stepsize, stepsize_bar, h_bar, mu, i, alpha, n_alpha
+                )
             else:
                 stepsize = stepsize_bar
 
         return _return_samples(return_type, samples)
 
     def generate_sample(self, initial_pos, num_adapt, num_samples, stepsize=None):
         """
@@ -598,23 +846,26 @@
                [-29.97143077, -12.0801625 ],
                [-33.07960829,  -8.90440347],
                [-55.28263496, -17.31718524],
                [-55.28263496, -17.31718524],
                [-56.63440044, -16.03309364],
                [-63.880094  , -19.19981944]])
         """
-        initial_pos = _check_1d_array_object(initial_pos, 'initial_pos')
-        _check_length_equal(initial_pos, self.model.variables, 'initial_pos', 'model.variables')
+        initial_pos = _check_1d_array_object(initial_pos, "initial_pos")
+        _check_length_equal(
+            initial_pos, self.model.variables, "initial_pos", "model.variables"
+        )
 
         if stepsize is None:
             stepsize = self._find_reasonable_stepsize(initial_pos)
 
         if num_adapt <= 1:  # return sample generated using Simple HMC algorithm
-            for sample in NoUTurnSampler(self.model, self.grad_log_pdf,
-                                         self.simulate_dynamics).generate_sample(initial_pos, num_samples, stepsize):
+            for sample in NoUTurnSampler(
+                self.model, self.grad_log_pdf, self.simulate_dynamics
+            ).generate_sample(initial_pos, num_samples, stepsize):
                 yield sample
             return
         mu = np.log(10.0 * stepsize)
 
         stepsize_bar = 1.0
         h_bar = 0.0
 
@@ -622,13 +873,14 @@
         num_adapt += 1
 
         for i in range(1, num_samples + 1):
 
             position_m, alpha, n_alpha = self._sample(position_m, stepsize)
 
             if i <= num_adapt:
-                stepsize, stepsize_bar, h_bar = self._adapt_params(stepsize, stepsize_bar, h_bar, mu,
-                                                                   i, alpha, n_alpha)
+                stepsize, stepsize_bar, h_bar = self._adapt_params(
+                    stepsize, stepsize_bar, h_bar, mu, i, alpha, n_alpha
+                )
             else:
                 stepsize = stepsize_bar
 
             yield position_m
```

### Comparing `pgmpy-0.1.7/pgmpy/sampling/__init__.py` & `pgmpy-0.1.9/pgmpy/sampling/__init__.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,18 +1,26 @@
-from .base import (BaseGradLogPDF, GradLogPDFGaussian, LeapFrog,
-                   ModifiedEuler, BaseSimulateHamiltonianDynamics, _return_samples)
+from .base import (
+    BaseGradLogPDF,
+    GradLogPDFGaussian,
+    LeapFrog,
+    ModifiedEuler,
+    BaseSimulateHamiltonianDynamics,
+    _return_samples,
+)
 from .HMC import HamiltonianMC, HamiltonianMCDA
 from .NUTS import NoUTurnSampler, NoUTurnSamplerDA
 from .Sampling import GibbsSampling, BayesianModelSampling
 
-__all__ = ['LeapFrog',
-           'ModifiedEuler',
-           'BaseSimulateHamiltonianDynamics',
-           'BaseGradLogPDF',
-           'GradLogPDFGaussian',
-           '_return_samples',
-           'HamiltonianMC',
-           'HamiltonianMCDA',
-           'NoUTurnSampler',
-           'NoUTurnSamplerDA',
-           'BayesianModelSampling',
-           'GibbsSampling']
+__all__ = [
+    "LeapFrog",
+    "ModifiedEuler",
+    "BaseSimulateHamiltonianDynamics",
+    "BaseGradLogPDF",
+    "GradLogPDFGaussian",
+    "_return_samples",
+    "HamiltonianMC",
+    "HamiltonianMCDA",
+    "NoUTurnSampler",
+    "NoUTurnSamplerDA",
+    "BayesianModelSampling",
+    "GibbsSampling",
+]
```

### Comparing `pgmpy-0.1.7/pgmpy/sampling/Sampling.py` & `pgmpy-0.1.9/pgmpy/sampling/Sampling.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,22 +1,23 @@
 from collections import namedtuple
 import itertools
 
 import networkx as nx
 import numpy as np
+from tqdm import tqdm
 
 from pgmpy.factors import factor_product
 from pgmpy.inference import Inference
 from pgmpy.models import BayesianModel, MarkovChain, MarkovModel
 from pgmpy.utils.mathext import sample_discrete
 from pgmpy.extern.six.moves import map, range
 from pgmpy.sampling import _return_samples
 
 
-State = namedtuple('State', ['var', 'state'])
+State = namedtuple("State", ["var", "state"])
 
 
 class BayesianModelSampling(Inference):
     """
     Class for sampling methods specific to Bayesian Models
 
     Parameters
@@ -25,22 +26,25 @@
         model on which inference queries will be computed
 
 
     Public Methods
     --------------
     forward_sample(size)
     """
+
     def __init__(self, model):
         if not isinstance(model, BayesianModel):
-            raise TypeError("Model expected type: BayesianModel, got type: ", type(model))
+            raise TypeError(
+                "Model expected type: BayesianModel, got type: ", type(model)
+            )
 
         self.topological_order = list(nx.topological_sort(model))
         super(BayesianModelSampling, self).__init__(model)
 
-    def forward_sample(self, size=1, return_type='dataframe'):
+    def forward_sample(self, size=1, return_type="dataframe"):
         """
         Generates sample(s) from joint distribution of the bayesian network.
 
         Parameters
         ----------
         size: int
             size of sample to be generated
@@ -68,18 +72,20 @@
         ...                ['intel', 'diff'], [2, 2])
         >>> student.add_cpds(cpd_d, cpd_i, cpd_g)
         >>> inference = BayesianModelSampling(student)
         >>> inference.forward_sample(size=2, return_type='recarray')
         rec.array([(0, 0, 1), (1, 0, 2)], dtype=
                   [('diff', '<i8'), ('intel', '<i8'), ('grade', '<i8')])
         """
-        types = [(var_name, 'int') for var_name in self.topological_order]
+        types = [(var_name, "int") for var_name in self.topological_order]
         sampled = np.zeros(size, dtype=types).view(np.recarray)
 
-        for node in self.topological_order:
+        pbar = tqdm(self.topological_order)
+        for node in pbar:
+            pbar.set_description("Generating for node: {node}".format(node=node))
             cpd = self.model.get_cpds(node)
             states = range(self.cardinality[node])
             evidence = cpd.variables[:0:-1]
             if evidence:
                 cached_values = self.pre_compute_reduce(variable=node)
                 evidence = np.vstack([sampled[i] for i in evidence])
                 weights = list(map(lambda t: cached_values[tuple(t)], evidence.T))
@@ -90,17 +96,21 @@
         return _return_samples(return_type, sampled)
 
     def pre_compute_reduce(self, variable):
         variable_cpd = self.model.get_cpds(variable)
         variable_evid = variable_cpd.variables[:0:-1]
         cached_values = {}
 
-        for state_combination in itertools.product(*[range(self.cardinality[var]) for var in variable_evid]):
+        for state_combination in itertools.product(
+            *[range(self.cardinality[var]) for var in variable_evid]
+        ):
             states = list(zip(variable_evid, state_combination))
-            cached_values[state_combination] = variable_cpd.reduce(states, inplace=False).values
+            cached_values[state_combination] = variable_cpd.reduce(
+                states, inplace=False
+            ).values
 
         return cached_values
 
     def rejection_sample(self, evidence=None, size=1, return_type="dataframe"):
         """
         Generates sample(s) from joint distribution of the bayesian network,
         given the evidence.
@@ -138,33 +148,37 @@
         >>> inference.rejection_sample(evidence=evidence, size=2, return_type='dataframe')
                 intel       diff       grade
         0         0          0          1
         1         0          0          1
         """
         if evidence is None:
             return self.forward_sample(size)
-        types = [(var_name, 'int') for var_name in self.topological_order]
+        types = [(var_name, "int") for var_name in self.topological_order]
         sampled = np.zeros(0, dtype=types).view(np.recarray)
         prob = 1
         i = 0
+
+        pbar = tqdm(total=size)
         while i < size:
             _size = int(((size - i) / prob) * 1.5)
-            _sampled = self.forward_sample(_size, 'recarray')
+            _sampled = self.forward_sample(_size, "recarray")
 
             for evid in evidence:
                 _sampled = _sampled[_sampled[evid[0]] == evid[1]]
 
             prob = max(len(_sampled) / _size, 0.01)
             sampled = np.append(sampled, _sampled)[:size]
 
             i += len(_sampled)
+            pbar.update(len(_sampled))
+        pbar.close()
 
         return _return_samples(return_type, sampled)
 
-    def likelihood_weighted_sample(self, evidence=None, size=1, return_type="dataframe"):
+    def likelihood_weighted_sample(self, evidence=[], size=1, return_type="dataframe"):
         """
         Generates weighted sample(s) from joint distribution of the bayesian
         network, that comply with the given evidence.
         'Probabilistic Graphical Model Principles and Techniques', Koller and
         Friedman, Algorithm 12.2 pp 493.
 
         Parameters
@@ -197,40 +211,42 @@
         >>> student.add_cpds(cpd_d, cpd_i, cpd_g)
         >>> inference = BayesianModelSampling(student)
         >>> evidence = [State('diff', 0)]
         >>> inference.likelihood_weighted_sample(evidence=evidence, size=2, return_type='recarray')
         rec.array([(0, 0, 1, 0.6), (0, 0, 2, 0.6)], dtype=
                   [('diff', '<i8'), ('intel', '<i8'), ('grade', '<i8'), ('_weight', '<f8')])
         """
-        types = [(var_name, 'int') for var_name in self.topological_order]
-        types.append(('_weight', 'float'))
+        types = [(var_name, "int") for var_name in self.topological_order]
+        types.append(("_weight", "float"))
         sampled = np.zeros(size, dtype=types).view(np.recarray)
-        sampled['_weight'] = np.ones(size)
+        sampled["_weight"] = np.ones(size)
         evidence_dict = {var: st for var, st in evidence}
 
         for node in self.topological_order:
             cpd = self.model.get_cpds(node)
             states = range(self.cardinality[node])
             evidence = cpd.get_evidence()
 
             if evidence:
                 evidence_values = np.vstack([sampled[i] for i in evidence])
                 cached_values = self.pre_compute_reduce(node)
-                weights = list(map(lambda t: cached_values[tuple(t)], evidence_values.T))
+                weights = list(
+                    map(lambda t: cached_values[tuple(t)], evidence_values.T)
+                )
                 if node in evidence_dict:
                     sampled[node] = evidence_dict[node]
                     for i in range(size):
-                        sampled['_weight'][i] *= weights[i][evidence_dict[node]]
+                        sampled["_weight"][i] *= weights[i][evidence_dict[node]]
                 else:
                     sampled[node] = sample_discrete(states, weights)
             else:
                 if node in evidence_dict:
                     sampled[node] = evidence_dict[node]
                     for i in range(size):
-                        sampled['_weight'][i] *= cpd.values[evidence_dict[node]]
+                        sampled["_weight"][i] *= cpd.values[evidence_dict[node]]
                 else:
                     sampled[node] = sample_discrete(states, cpd.values, size)
 
         return _return_samples(return_type, sampled)
 
 
 class GibbsSampling(MarkovChain):
@@ -255,23 +271,24 @@
     >>> from pgmpy.models import BayesianModel
     >>> intel_cpd = TabularCPD('intel', 2, [[0.7], [0.3]])
     >>> sat_cpd = TabularCPD('sat', 2, [[0.95, 0.2], [0.05, 0.8]], evidence=['intel'], evidence_card=[2])
     >>> student = BayesianModel()
     >>> student.add_nodes_from(['intel', 'sat'])
     >>> student.add_edge('intel', 'sat')
     >>> student.add_cpds(intel_cpd, sat_cpd)
-    >>> from pgmpy.inference import GibbsSampling
+    >>> from pgmpy.sampling import GibbsSampling
     >>> gibbs_chain = GibbsSampling(student)
     Sample from it:
     >>> gibbs_chain.sample(size=3)
        intel  sat
     0      0    0
     1      0    0
     2      1    1
     """
+
     def __init__(self, model=None):
         super(GibbsSampling, self).__init__()
         if isinstance(model, BayesianModel):
             self._get_kernel_from_bayesian_model(model)
         elif isinstance(model, MarkovModel):
             self._get_kernel_from_markov_model(model)
 
@@ -283,15 +300,17 @@
 
         Parameters:
         -----------
         model: BayesianModel
             The model from which probabilities will be computed.
         """
         self.variables = np.array(model.nodes())
-        self.cardinalities = {var: model.get_cpds(var).variable_card for var in self.variables}
+        self.cardinalities = {
+            var: model.get_cpds(var).variable_card for var in self.variables
+        }
 
         for var in self.variables:
             other_vars = [v for v in self.variables if var != v]
             other_cards = [self.cardinalities[v] for v in other_vars]
             cpds = [cpd for cpd in model.cpds if var in cpd.scope()]
             prod_cpd = factor_product(*cpds)
             kernel = {}
@@ -316,26 +335,34 @@
         self.variables = np.array(model.nodes())
         factors_dict = {var: [] for var in self.variables}
         for factor in model.get_factors():
             for var in factor.scope():
                 factors_dict[var].append(factor)
 
         # Take factor product
-        factors_dict = {var: factor_product(*factors) if len(factors) > 1 else factors[0]
-                        for var, factors in factors_dict.items()}
-        self.cardinalities = {var: factors_dict[var].get_cardinality([var])[var] for var in self.variables}
+        factors_dict = {
+            var: factor_product(*factors) if len(factors) > 1 else factors[0]
+            for var, factors in factors_dict.items()
+        }
+        self.cardinalities = {
+            var: factors_dict[var].get_cardinality([var])[var] for var in self.variables
+        }
 
         for var in self.variables:
             other_vars = [v for v in self.variables if var != v]
             other_cards = [self.cardinalities[v] for v in other_vars]
             kernel = {}
             factor = factors_dict[var]
             scope = set(factor.scope())
             for tup in itertools.product(*[range(card) for card in other_cards]):
-                states = [State(first_var, s) for first_var, s in zip(other_vars, tup) if first_var in scope]
+                states = [
+                    State(first_var, s)
+                    for first_var, s in zip(other_vars, tup)
+                    if first_var in scope
+                ]
                 reduced_factor = factor.reduce(states, inplace=False)
                 kernel[tup] = reduced_factor.values / sum(reduced_factor.values)
             self.transition_models[var] = kernel
 
     def sample(self, start_state=None, size=1, return_type="dataframe"):
         """
         Sample from the Markov Chain.
@@ -354,15 +381,15 @@
         -------
         sampled: A pandas.DataFrame or a numpy.recarray object depending upon return_type argument
             the generated samples
 
         Examples:
         ---------
         >>> from pgmpy.factors import DiscreteFactor
-        >>> from pgmpy.inference import GibbsSampling
+        >>> from pgmpy.sampling import GibbsSampling
         >>> from pgmpy.models import MarkovModel
         >>> model = MarkovModel([('A', 'B'), ('C', 'B')])
         >>> factor_ab = DiscreteFactor(['A', 'B'], [2, 2], [1, 2, 3, 4])
         >>> factor_cb = DiscreteFactor(['C', 'B'], [2, 2], [5, 6, 7, 8])
         >>> model.add_factors(factor_ab, factor_cb)
         >>> gibbs = GibbsSampling(model)
         >>> gibbs.sample(size=4, return_tupe='dataframe')
@@ -373,22 +400,24 @@
         3  1  1  1
         """
         if start_state is None and self.state is None:
             self.state = self.random_state()
         elif start_state is not None:
             self.set_start_state(start_state)
 
-        types = [(var_name, 'int') for var_name in self.variables]
+        types = [(var_name, "int") for var_name in self.variables]
         sampled = np.zeros(size, dtype=types).view(np.recarray)
         sampled[0] = tuple([st for var, st in self.state])
-        for i in range(size - 1):
+        for i in tqdm(range(size - 1)):
             for j, (var, st) in enumerate(self.state):
                 other_st = tuple(st for v, st in self.state if var != v)
-                next_st = sample_discrete(list(range(self.cardinalities[var])),
-                                          self.transition_models[var][other_st])[0]
+                next_st = sample_discrete(
+                    list(range(self.cardinalities[var])),
+                    self.transition_models[var][other_st],
+                )[0]
                 self.state[j] = State(var, next_st)
             sampled[i + 1] = tuple([st for var, st in self.state])
 
         return _return_samples(return_type, sampled)
 
     def generate_sample(self, start_state=None, size=1):
         """
@@ -418,11 +447,13 @@
             self.state = self.random_state()
         elif start_state is not None:
             self.set_start_state(start_state)
 
         for i in range(size):
             for j, (var, st) in enumerate(self.state):
                 other_st = tuple(st for v, st in self.state if var != v)
-                next_st = sample_discrete(list(range(self.cardinalities[var])),
-                                          self.transition_models[var][other_st])[0]
+                next_st = sample_discrete(
+                    list(range(self.cardinalities[var])),
+                    self.transition_models[var][other_st],
+                )[0]
                 self.state[j] = State(var, next_st)
             yield self.state[:]
```

### Comparing `pgmpy-0.1.7/pgmpy/sampling/HMC.py` & `pgmpy-0.1.9/pgmpy/sampling/HMC.py`

 * *Files 4% similar despite different names*

```diff
@@ -2,17 +2,23 @@
 """
     A collection of methods for sampling from continuous models in pgmpy
 """
 from __future__ import division
 from math import sqrt
 
 import numpy as np
+from tqdm import tqdm
 
 from pgmpy.utils import _check_1d_array_object, _check_length_equal
-from pgmpy.sampling import LeapFrog, BaseSimulateHamiltonianDynamics, BaseGradLogPDF, _return_samples
+from pgmpy.sampling import (
+    LeapFrog,
+    BaseSimulateHamiltonianDynamics,
+    BaseGradLogPDF,
+    _return_samples,
+)
 
 
 class HamiltonianMC(object):
     """
     Class for performing sampling using simple
     Hamiltonian Monte Carlo
 
@@ -68,20 +74,24 @@
     chapter 5: MCMC Using Hamiltonian Dynamics.
     CRC Press, 2011.
     """
 
     def __init__(self, model, grad_log_pdf, simulate_dynamics=LeapFrog):
 
         if not issubclass(grad_log_pdf, BaseGradLogPDF):
-            raise TypeError("grad_log_pdf must be an instance of " +
-                            "pgmpy.inference.base_continuous.BaseGradLogPDF")
+            raise TypeError(
+                "grad_log_pdf must be an instance of "
+                + "pgmpy.inference.base_continuous.BaseGradLogPDF"
+            )
 
         if not issubclass(simulate_dynamics, BaseSimulateHamiltonianDynamics):
-            raise TypeError("split_time must be an instance of " +
-                            "pgmpy.inference.base_continuous.BaseSimulateHamiltonianDynamics")
+            raise TypeError(
+                "split_time must be an instance of "
+                + "pgmpy.inference.base_continuous.BaseSimulateHamiltonianDynamics"
+            )
 
         self.model = model
         self.grad_log_pdf = grad_log_pdf
         self.simulate_dynamics = simulate_dynamics
         self.accepted_proposals = 0.0
         self.acceptance_rate = 0
 
@@ -92,27 +102,29 @@
 
         # Parameters to help in evaluating Joint distribution P(position, momentum)
         _, logp = self.grad_log_pdf(position, self.model).get_gradient_log_pdf()
         _, logp_bar = self.grad_log_pdf(position_bar, self.model).get_gradient_log_pdf()
 
         # acceptance_prob = P(position_bar, momentum_bar)/ P(position, momentum)
         potential_change = logp_bar - logp  # Negative change
-        kinetic_change = 0.5 * np.float(np.dot(momentum_bar.T, momentum_bar) - np.dot(momentum.T, momentum))
+        kinetic_change = 0.5 * np.float(
+            np.dot(momentum_bar.T, momentum_bar) - np.dot(momentum.T, momentum)
+        )
 
         # acceptance probability
         return np.exp(potential_change - kinetic_change)
 
     def _get_condition(self, acceptance_prob, a):
         """
         Temporary method to fix issue in numpy 0.12 #852
         """
         if a == 1:
-            return (acceptance_prob ** a) > (1/(2**a))
+            return (acceptance_prob ** a) > (1 / (2 ** a))
         else:
-            return (1/(acceptance_prob ** a)) > (2**(-a))
+            return (1 / (acceptance_prob ** a)) > (2 ** (-a))
 
     def _find_reasonable_stepsize(self, position, stepsize_app=1):
         """
         Method for choosing initial value of stepsize
 
         References
         -----------
@@ -121,33 +133,37 @@
         Machine Learning Research 15 (2014) 1351-1381
         Algorithm 4 : Heuristic for choosing an initial value of epsilon
         """
         # momentum = N(0, I)
         momentum = np.reshape(np.random.normal(0, 1, len(position)), position.shape)
 
         # Take a single step in time
-        position_bar, momentum_bar, _ =\
-            self.simulate_dynamics(self.model, position, momentum,
-                                   stepsize_app, self.grad_log_pdf).get_proposed_values()
-
-        acceptance_prob = self._acceptance_prob(position, position_bar, momentum, momentum_bar)
+        position_bar, momentum_bar, _ = self.simulate_dynamics(
+            self.model, position, momentum, stepsize_app, self.grad_log_pdf
+        ).get_proposed_values()
+
+        acceptance_prob = self._acceptance_prob(
+            position, position_bar, momentum, momentum_bar
+        )
 
         # a = 2I[acceptance_prob] -1
         a = 2 * (acceptance_prob > 0.5) - 1
 
         condition = self._get_condition(acceptance_prob, a)
 
         while condition:
             stepsize_app = (2 ** a) * stepsize_app
 
-            position_bar, momentum_bar, _ =\
-                self.simulate_dynamics(self.model, position, momentum,
-                                       stepsize_app, self.grad_log_pdf).get_proposed_values()
-
-            acceptance_prob = self._acceptance_prob(position, position_bar, momentum, momentum_bar)
+            position_bar, momentum_bar, _ = self.simulate_dynamics(
+                self.model, position, momentum, stepsize_app, self.grad_log_pdf
+            ).get_proposed_values()
+
+            acceptance_prob = self._acceptance_prob(
+                position, position_bar, momentum, momentum_bar
+            )
 
             condition = self._get_condition(acceptance_prob, a)
 
         return stepsize_app
 
     def _sample(self, position, trajectory_length, stepsize, lsteps=None):
         """
@@ -162,31 +178,45 @@
         # Number of steps L to simulate dynamics
         if lsteps is None:
             lsteps = int(max(1, round(trajectory_length / stepsize, 0)))
 
         grad_bar, _ = self.grad_log_pdf(position_bar, self.model).get_gradient_log_pdf()
 
         for _ in range(lsteps):
-            position_bar, momentum_bar, grad_bar =\
-                self.simulate_dynamics(self.model, position_bar, momentum_bar,
-                                       stepsize, self.grad_log_pdf, grad_bar).get_proposed_values()
-
-        acceptance_prob = self._acceptance_prob(position, position_bar, momentum, momentum_bar)
+            position_bar, momentum_bar, grad_bar = self.simulate_dynamics(
+                self.model,
+                position_bar,
+                momentum_bar,
+                stepsize,
+                self.grad_log_pdf,
+                grad_bar,
+            ).get_proposed_values()
+
+        acceptance_prob = self._acceptance_prob(
+            position, position_bar, momentum, momentum_bar
+        )
 
         # Metropolis acceptance probability
         alpha = min(1, acceptance_prob)
 
         # Accept or reject the new proposed value of position, i.e position_bar
         if np.random.rand() < alpha:
             position = position_bar.copy()
             self.accepted_proposals += 1.0
 
         return position, alpha
 
-    def sample(self, initial_pos, num_samples, trajectory_length, stepsize=None, return_type='dataframe'):
+    def sample(
+        self,
+        initial_pos,
+        num_samples,
+        trajectory_length,
+        stepsize=None,
+        return_type="dataframe",
+    ):
         """
         Method to return samples using Hamiltonian Monte Carlo
 
         Parameters
         ----------
         initial_pos: A 1d array like object
             Vector representing values of parameter position, the starting
@@ -239,40 +269,46 @@
         >>> np.cov(samples.values.T)
         array([[ 1.00795398,  0.71384233,  0.79802097],
                [ 0.71384233,  1.00633524,  0.21313767],
                [ 0.79802097,  0.21313767,  0.98519017]])
         """
 
         self.accepted_proposals = 1.0
-        initial_pos = _check_1d_array_object(initial_pos, 'initial_pos')
-        _check_length_equal(initial_pos, self.model.variables, 'initial_pos', 'model.variables')
+        initial_pos = _check_1d_array_object(initial_pos, "initial_pos")
+        _check_length_equal(
+            initial_pos, self.model.variables, "initial_pos", "model.variables"
+        )
 
         if stepsize is None:
             stepsize = self._find_reasonable_stepsize(initial_pos)
 
-        types = [(var_name, 'float') for var_name in self.model.variables]
+        types = [(var_name, "float") for var_name in self.model.variables]
         samples = np.zeros(num_samples, dtype=types).view(np.recarray)
 
         # Assigning after converting into tuple because value was being changed after assignment
         # Reason for this is unknown
         samples[0] = tuple(initial_pos)
         position_m = initial_pos
 
         lsteps = int(max(1, round(trajectory_length / stepsize, 0)))
-        for i in range(1, num_samples):
+        for i in tqdm(range(1, num_samples)):
 
             # Genrating sample
-            position_m, _ = self._sample(position_m, trajectory_length, stepsize, lsteps)
+            position_m, _ = self._sample(
+                position_m, trajectory_length, stepsize, lsteps
+            )
             samples[i] = tuple(position_m)
 
         self.acceptance_rate = self.accepted_proposals / num_samples
 
         return _return_samples(return_type, samples)
 
-    def generate_sample(self, initial_pos, num_samples, trajectory_length, stepsize=None):
+    def generate_sample(
+        self, initial_pos, num_samples, trajectory_length, stepsize=None
+    ):
         """
         Method returns a generator type object whose each iteration yields a sample
         using Hamiltonian Monte Carlo
 
         Parameters
         ----------
         initial_pos: A 1d array like object
@@ -319,26 +355,30 @@
         array([[ 2.95692642,  0.4379419 ],
                [ 0.4379419 ,  3.00939434]])
         >>> sampler.acceptance_rate
         0.9969
         """
 
         self.accepted_proposals = 0
-        initial_pos = _check_1d_array_object(initial_pos, 'initial_pos')
-        _check_length_equal(initial_pos, self.model.variables, 'initial_pos', 'model.variables')
+        initial_pos = _check_1d_array_object(initial_pos, "initial_pos")
+        _check_length_equal(
+            initial_pos, self.model.variables, "initial_pos", "model.variables"
+        )
 
         if stepsize is None:
             stepsize = self._find_reasonable_stepsize(initial_pos)
 
         lsteps = int(max(1, round(trajectory_length / stepsize, 0)))
         position_m = initial_pos.copy()
 
         for i in range(0, num_samples):
 
-            position_m, _ = self._sample(position_m, trajectory_length, stepsize, lsteps)
+            position_m, _ = self._sample(
+                position_m, trajectory_length, stepsize, lsteps
+            )
 
             yield position_m
 
         self.acceptance_rate = self.accepted_proposals / num_samples
 
 
 class HamiltonianMCDA(HamiltonianMC):
@@ -390,44 +430,60 @@
     -----------
     Matthew D. Hoffman, Andrew Gelman, The No-U-Turn Sampler: Adaptively
     Setting Path Lengths in Hamiltonian Monte Carlo. Journal of
     Machine Learning Research 15 (2014) 1351-1381
     Algorithm 5 : Hamiltonian Monte Carlo with dual averaging
     """
 
-    def __init__(self, model, grad_log_pdf=None, simulate_dynamics=LeapFrog, delta=0.65):
+    def __init__(
+        self, model, grad_log_pdf=None, simulate_dynamics=LeapFrog, delta=0.65
+    ):
 
         if not isinstance(delta, float) or delta > 1.0 or delta < 0.0:
-            raise ValueError(
-                "delta should be a floating value in between 0 and 1")
+            raise ValueError("delta should be a floating value in between 0 and 1")
 
         self.delta = delta
 
-        super(HamiltonianMCDA, self).__init__(model=model, grad_log_pdf=grad_log_pdf,
-                                              simulate_dynamics=simulate_dynamics)
-
-    def _adapt_params(self, stepsize, stepsize_bar, h_bar, mu, index_i, alpha, n_alpha=1):
+        super(HamiltonianMCDA, self).__init__(
+            model=model, grad_log_pdf=grad_log_pdf, simulate_dynamics=simulate_dynamics
+        )
+
+    def _adapt_params(
+        self, stepsize, stepsize_bar, h_bar, mu, index_i, alpha, n_alpha=1
+    ):
         """
         Run tha adaptation for stepsize for better proposals of position
         """
         gamma = 0.05  # free parameter that controls the amount of shrinkage towards mu
-        t0 = 10.0  # free parameter that stabilizes the initial iterations of the algorithm
+        t0 = (
+            10.0
+        )  # free parameter that stabilizes the initial iterations of the algorithm
         kappa = 0.75
         # See equation (6) section 3.2.1 for details
 
         estimate = 1.0 / (index_i + t0)
         h_bar = (1 - estimate) * h_bar + estimate * (self.delta - alpha / n_alpha)
 
         stepsize = np.exp(mu - sqrt(index_i) / gamma * h_bar)
         i_kappa = index_i ** (-kappa)
-        stepsize_bar = np.exp(i_kappa * np.log(stepsize) + (1 - i_kappa) * np.log(stepsize_bar))
+        stepsize_bar = np.exp(
+            i_kappa * np.log(stepsize) + (1 - i_kappa) * np.log(stepsize_bar)
+        )
 
         return stepsize, stepsize_bar, h_bar
 
-    def sample(self, initial_pos, num_adapt, num_samples, trajectory_length, stepsize=None, return_type='dataframe'):
+    def sample(
+        self,
+        initial_pos,
+        num_adapt,
+        num_samples,
+        trajectory_length,
+        stepsize=None,
+        return_type="dataframe",
+    ):
         """
         Method to return samples using Hamiltonian Monte Carlo
 
         Parameters
         ----------
         initial_pos: A 1d array like object
             Vector representing values of parameter position, the starting
@@ -472,54 +528,62 @@
         array([[ 0.98432155,  0.66517394],
                [ 0.66517394,  2.95449533]])
 
         """
 
         self.accepted_proposals = 1.0
 
-        initial_pos = _check_1d_array_object(initial_pos, 'initial_pos')
-        _check_length_equal(initial_pos, self.model.variables, 'initial_pos', 'model.variables')
+        initial_pos = _check_1d_array_object(initial_pos, "initial_pos")
+        _check_length_equal(
+            initial_pos, self.model.variables, "initial_pos", "model.variables"
+        )
 
         if stepsize is None:
             stepsize = self._find_reasonable_stepsize(initial_pos)
 
         if num_adapt <= 1:  # Return samples genrated using Simple HMC algorithm
-            return HamiltonianMC.sample(self, initial_pos, num_samples, trajectory_length, stepsize)
+            return HamiltonianMC.sample(
+                self, initial_pos, num_samples, trajectory_length, stepsize
+            )
 
         # stepsize is epsilon
         # freely chosen point, after each iteration xt(/position) is shrunk towards it
         mu = np.log(10.0 * stepsize)
         # log(10 * stepsize) large values to save computation
         # stepsize_bar is epsilon_bar
         stepsize_bar = 1.0
         h_bar = 0.0
         # See equation (6) section 3.2.1 for details
 
-        types = [(var_name, 'float') for var_name in self.model.variables]
+        types = [(var_name, "float") for var_name in self.model.variables]
         samples = np.zeros(num_samples, dtype=types).view(np.recarray)
         samples[0] = tuple(initial_pos)
         position_m = initial_pos
 
-        for i in range(1, num_samples):
+        for i in tqdm(range(1, num_samples)):
 
             # Genrating sample
             position_m, alpha = self._sample(position_m, trajectory_length, stepsize)
             samples[i] = tuple(position_m)
 
             # Adaptation of stepsize till num_adapt iterations
             if i <= num_adapt:
-                stepsize, stepsize_bar, h_bar = self._adapt_params(stepsize, stepsize_bar, h_bar, mu, i, alpha)
+                stepsize, stepsize_bar, h_bar = self._adapt_params(
+                    stepsize, stepsize_bar, h_bar, mu, i, alpha
+                )
             else:
                 stepsize = stepsize_bar
 
         self.acceptance_rate = self.accepted_proposals / num_samples
 
         return _return_samples(return_type, samples)
 
-    def generate_sample(self, initial_pos, num_adapt, num_samples, trajectory_length, stepsize=None):
+    def generate_sample(
+        self, initial_pos, num_adapt, num_samples, trajectory_length, stepsize=None
+    ):
         """
         Method returns a generator type object whose each iteration yields a sample
         using Hamiltonian Monte Carlo
 
         Parameters
         ----------
         initial_pos: A 1d array like object
@@ -558,22 +622,26 @@
         ...                                       num_samples = 10000, trajectory_length=2, stepsize=None)
         >>> samples_array = np.array([sample for sample in gen_samples])
         >>> np.cov(samples_array.T)
         array([[ 0.98432155,  0.69517394],
                [ 0.69517394,  2.95449533]])
         """
         self.accepted_proposals = 0
-        initial_pos = _check_1d_array_object(initial_pos, 'initial_pos')
-        _check_length_equal(initial_pos, self.model.variables, 'initial_pos', 'model.variables')
+        initial_pos = _check_1d_array_object(initial_pos, "initial_pos")
+        _check_length_equal(
+            initial_pos, self.model.variables, "initial_pos", "model.variables"
+        )
 
         if stepsize is None:
             stepsize = self._find_reasonable_stepsize(initial_pos)
 
         if num_adapt <= 1:  # return sample generated using Simple HMC algorithm
-            for sample in HamiltonianMC.generate_sample(self, initial_pos, num_samples, trajectory_length, stepsize):
+            for sample in HamiltonianMC.generate_sample(
+                self, initial_pos, num_samples, trajectory_length, stepsize
+            ):
                 yield sample
             return
         mu = np.log(10.0 * stepsize)
 
         stepsize_bar = 1.0
         h_bar = 0.0
 
@@ -581,14 +649,16 @@
         num_adapt += 1
 
         for i in range(1, num_samples + 1):
 
             position_m, alpha = self._sample(position_m, trajectory_length, stepsize)
 
             if i <= num_adapt:
-                stepsize, stepsize_bar, h_bar = self._adapt_params(stepsize, stepsize_bar, h_bar, mu, i, alpha)
+                stepsize, stepsize_bar, h_bar = self._adapt_params(
+                    stepsize, stepsize_bar, h_bar, mu, i, alpha
+                )
             else:
                 stepsize = stepsize_bar
 
             yield position_m
 
         self.acceptance_rate = self.accepted_proposals / num_samples
```

### Comparing `pgmpy-0.1.7/pgmpy/readwrite/XMLBeliefNetwork.py` & `pgmpy-0.1.9/pgmpy/readwrite/XMLBeliefNetwork.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 import numpy as np
+import networkx as nx
 
 try:
     from lxml import etree
 except ImportError:
     try:
         import xml.etree.ElementTree as etree
     except ImportError:
@@ -17,14 +18,15 @@
 from pgmpy.extern.six.moves import map, range
 
 
 class XBNReader(object):
     """
     Base class for reading XML Belief Network File Format.
     """
+
     def __init__(self, path=None, string=None):
         """
         Initializer for XBNReader class.
 
         Parameters
         ----------
         path: str or file
@@ -44,15 +46,15 @@
         if path:
             self.network = etree.parse(path).getroot()
         elif string:
             self.network = etree.fromstring(string)
         else:
             raise ValueError("Must specify either path or string")
 
-        self.bnmodel = self.network.find('BNMODEL')
+        self.bnmodel = self.network.find("BNMODEL")
         self.analysisnotebook = self.get_analysisnotebook_values()
         self.model_name = self.get_bnmodel_name()
         self.static_properties = self.get_static_properties()
         self.variables = self.get_variables()
         self.edges = self.get_edges()
         self.variable_CPD = self.get_distributions()
 
@@ -75,27 +77,30 @@
 
         Examples
         --------
         >>> reader = XBNReader('xbn_test.xml')
         >>> reader.get_bnmodel_name()
         'Cancer'
         """
-        return self.network.find('BNMODEL').get('NAME')
+        return self.network.find("BNMODEL").get("NAME")
 
     def get_static_properties(self):
         """
         Returns a dictionary of STATICPROPERTIES
 
         Examples
         --------
         >>> reader = XBNReader('xbn_test.xml')
         >>> reader.get_static_properties()
         {'FORMAT': 'MSR DTAS XML', 'VERSION': '0.2', 'CREATOR': 'Microsoft Research DTAS'}
         """
-        return {tags.tag: tags.get('VALUE') for tags in self.bnmodel.find('STATICPROPERTIES')}
+        return {
+            tags.tag: tags.get("VALUE")
+            for tags in self.bnmodel.find("STATICPROPERTIES")
+        }
 
     def get_variables(self):
         """
         Returns a list of variables.
 
         Examples
         --------
@@ -109,33 +114,38 @@
                'STATES': ['Present', 'Absent']},
         'c': {....},
         'd': {....},
         'e': {....}
         }
         """
         variables = {}
-        for variable in self.bnmodel.find('VARIABLES'):
-            variables[variable.get('NAME')] = {'TYPE': variable.get('TYPE'),
-                                               'XPOS': variable.get('XPOS'),
-                                               'YPOS': variable.get('YPOS'),
-                                               'DESCRIPTION': variable.find('DESCRIPTION').text,
-                                               'STATES': [state.text for state in variable.findall('STATENAME')]}
+        for variable in self.bnmodel.find("VARIABLES"):
+            variables[variable.get("NAME")] = {
+                "TYPE": variable.get("TYPE"),
+                "XPOS": variable.get("XPOS"),
+                "YPOS": variable.get("YPOS"),
+                "DESCRIPTION": variable.find("DESCRIPTION").text,
+                "STATES": [state.text for state in variable.findall("STATENAME")],
+            }
         return variables
 
     def get_edges(self):
         """
         Returns a list of tuples. Each tuple contains two elements (parent, child) for each edge.
 
         Examples
         --------
         >>> reader = XBNReader('xbn_test.xml')
         >>> reader.get_edges()
         [('a', 'b'), ('a', 'c'), ('b', 'd'), ('c', 'd'), ('c', 'e')]
         """
-        return [(arc.get('PARENT'), arc.get('CHILD')) for arc in self.bnmodel.find('STRUCTURE')]
+        return [
+            (arc.get("PARENT"), arc.get("CHILD"))
+            for arc in self.bnmodel.find("STRUCTURE")
+        ]
 
     def get_distributions(self):
         """
         Returns a dictionary of name and its distribution. Distribution is a ndarray.
 
         The ndarray is stored in the standard way such that the rightmost variable changes most often.
         Consider a CPD of variable 'd' which has parents 'b' and 'c' (distribution['CONDSET'] = ['b', 'c'])
@@ -163,62 +173,80 @@
                  [ 0.05,  0.95]]), 'CONDSET': ['a'], 'CARDINALITY': [2]},
          'd': {'TYPE': 'discrete', 'DPIS': array([[ 0.8 ,  0.2 ],
                  [ 0.9 ,  0.1 ],
                  [ 0.7 ,  0.3 ],
                  [ 0.05,  0.95]]), 'CONDSET': ['b', 'c']}, 'CARDINALITY': [2, 2]}
         """
         distribution = {}
-        for dist in self.bnmodel.find('DISTRIBUTIONS'):
-            variable_name = dist.find('PRIVATE').get('NAME')
-            distribution[variable_name] = {'TYPE': dist.get('TYPE')}
-            if dist.find('CONDSET') is not None:
-                distribution[variable_name]['CONDSET'] = [var.get('NAME') for
-                                                          var in dist.find('CONDSET').findall('CONDELEM')]
-                distribution[variable_name]['CARDINALITY'] = np.array(
-                    [len(set(np.array([list(map(int, dpi.get('INDEXES').split()))
-                                       for dpi in dist.find('DPIS')])[:, i]))
-                     for i in range(len(distribution[variable_name]['CONDSET']))])
-            distribution[variable_name]['DPIS'] = np.array(
-                [list(map(float, dpi.text.split())) for dpi in dist.find('DPIS')])
+        for dist in self.bnmodel.find("DISTRIBUTIONS"):
+            variable_name = dist.find("PRIVATE").get("NAME")
+            distribution[variable_name] = {"TYPE": dist.get("TYPE")}
+            if dist.find("CONDSET") is not None:
+                distribution[variable_name]["CONDSET"] = [
+                    var.get("NAME") for var in dist.find("CONDSET").findall("CONDELEM")
+                ]
+                distribution[variable_name]["CARDINALITY"] = np.array(
+                    [
+                        len(
+                            set(
+                                np.array(
+                                    [
+                                        list(map(int, dpi.get("INDEXES").split()))
+                                        for dpi in dist.find("DPIS")
+                                    ]
+                                )[:, i]
+                            )
+                        )
+                        for i in range(len(distribution[variable_name]["CONDSET"]))
+                    ]
+                )
+            distribution[variable_name]["DPIS"] = np.array(
+                [list(map(float, dpi.text.split())) for dpi in dist.find("DPIS")]
+            )
 
         return distribution
 
     def get_model(self):
         """
         Returns an instance of Bayesian Model.
         """
         model = BayesianModel()
         model.add_nodes_from(self.variables)
         model.add_edges_from(self.edges)
         model.name = self.model_name
 
         tabular_cpds = []
         for var, values in self.variable_CPD.items():
-            evidence = values['CONDSET'] if 'CONDSET' in values else []
-            cpd = values['DPIS']
-            evidence_card = values['CARDINALITY'] if 'CARDINALITY' in values else []
-            states = self.variables[var]['STATES']
-            cpd = TabularCPD(var, len(states), cpd,
-                             evidence=evidence,
-                             evidence_card=evidence_card)
+            evidence = values["CONDSET"] if "CONDSET" in values else []
+            cpd = values["DPIS"]
+            evidence_card = values["CARDINALITY"] if "CARDINALITY" in values else []
+            states = self.variables[var]["STATES"]
+            cpd = TabularCPD(
+                var, len(states), cpd, evidence=evidence, evidence_card=evidence_card
+            )
             tabular_cpds.append(cpd)
 
         model.add_cpds(*tabular_cpds)
 
-        for var, properties in self.variables.items():
-            model.node[var] = properties
+        if nx.__version__.startswith("1"):
+            for var, properties in self.variables.items():
+                model.nodes[var] = properties
+        else:
+            for var, properties in self.variables.items():
+                model._node[var] = properties
 
         return model
 
 
 class XBNWriter(object):
     """
     Base class for writing XML Belief Network file format.
     """
-    def __init__(self, model, encoding='utf-8', prettyprint=True):
+
+    def __init__(self, model, encoding="utf-8", prettyprint=True):
         """
         Initializer for XBNWriter class
 
         Parameters
         ----------
         model: BayesianModel Instance
             Model to write
@@ -238,20 +266,20 @@
         if not isinstance(model, BayesianModel):
             raise TypeError("Model must be an instance of Bayesian Model.")
         self.model = model
 
         self.encoding = encoding
         self.prettyprint = prettyprint
 
-        self.network = etree.Element('ANALYSISNOTEBOOK')
-        self.bnmodel = etree.SubElement(self.network, 'BNMODEL')
+        self.network = etree.Element("ANALYSISNOTEBOOK")
+        self.bnmodel = etree.SubElement(self.network, "BNMODEL")
         if self.model.name:
-            etree.SubElement(self.bnmodel, 'NAME').text = self.model.name
+            etree.SubElement(self.bnmodel, "NAME").text = self.model.name
 
-        self.variables = self.set_variables(self.model.node)
+        self.variables = self.set_variables(self.model.nodes)
         self.structure = self.set_edges(sorted(self.model.edges()))
         self.distribution = self.set_distributions()
 
     def __str__(self):
         """
         Return the XML as string.
         """
@@ -259,22 +287,22 @@
             self.indent(self.network)
         return etree.tostring(self.network, encoding=self.encoding)
 
     def indent(self, elem, level=0):
         """
         Inplace prettyprint formatter.
         """
-        i = "\n" + level*"  "
+        i = "\n" + level * "  "
         if len(elem):
             if not elem.text or not elem.text.strip():
                 elem.text = i + "  "
             if not elem.tail or not elem.tail.strip():
                 elem.tail = i
             for elem in elem:
-                self.indent(elem, level+1)
+                self.indent(elem, level + 1)
             if not elem.tail or not elem.tail.strip():
                 elem.tail = i
         else:
             if level and (not elem.tail or not elem.tail.strip()):
                 elem.tail = i
 
     def set_analysisnotebook(self, **data):
@@ -307,15 +335,15 @@
 
         Examples
         --------
         >>> from pgmpy.readwrite.XMLBeliefNetwork import XBNWriter
         >>> writer = XBNWriter()
         >>> writer.set_bnmodel_name("Cancer")
         """
-        self.bnmodel.set('NAME', str(name))
+        self.bnmodel.set("NAME", str(name))
 
     def set_static_properties(self, **data):
         """
         Set STATICPROPERTIES tag for the network
 
         Parameters
         ----------
@@ -324,17 +352,17 @@
 
         Examples
         --------
         >>> from pgmpy.readwrite.XMLBeliefNetwork import XBNWriter
         >>> writer = XBNWriter()
         >>> writer.set_static_properties(FORMAT="MSR DTAS XML", VERSION="0.2", CREATOR="Microsoft Research DTAS")
         """
-        static_prop = etree.SubElement(self.bnmodel, 'STATICPROPERTIES')
+        static_prop = etree.SubElement(self.bnmodel, "STATICPROPERTIES")
         for key, value in data.items():
-            etree.SubElement(static_prop, key, attrib={'VALUE': value})
+            etree.SubElement(static_prop, key, attrib={"VALUE": value})
 
     def set_variables(self, data):
         """
         Set variables for the network.
 
         Parameters
         ----------
@@ -350,19 +378,31 @@
         ...                             'STATES': ['Present', 'Absent']}
         ...                       'b': {'TYPE': 'discrete', 'XPOS': '11290',
         ...                             'YPOS': '11965', 'DESCRIPTION': '(b) Serum Calcium Increase',
         ...                             'STATES': ['Present', 'Absent']}})
         """
         variables = etree.SubElement(self.bnmodel, "VARIABLES")
         for var in sorted(data):
-            variable = etree.SubElement(variables, 'VAR', attrib={'NAME': var, 'TYPE': data[var]['TYPE'],
-                                                                  'XPOS': data[var]['XPOS'], 'YPOS': data[var]['YPOS']})
-            etree.SubElement(variable, 'DESCRIPTION', attrib={'DESCRIPTION': data[var]['DESCRIPTION']})
-            for state in data[var]['STATES']:
-                etree.SubElement(variable, 'STATENAME').text = state
+            variable = etree.SubElement(
+                variables,
+                "VAR",
+                attrib={
+                    "NAME": var,
+                    "TYPE": data[var]["TYPE"],
+                    "XPOS": data[var]["XPOS"],
+                    "YPOS": data[var]["YPOS"],
+                },
+            )
+            etree.SubElement(
+                variable,
+                "DESCRIPTION",
+                attrib={"DESCRIPTION": data[var]["DESCRIPTION"]},
+            )
+            for state in data[var]["STATES"]:
+                etree.SubElement(variable, "STATENAME").text = state
 
     def set_edges(self, edge_list):
         """
         Set edges/arc in the network.
 
         Parameters
         ----------
@@ -371,42 +411,53 @@
 
         Examples
         --------
         >>> from pgmpy.readwrite.XMLBeliefNetwork import XBNWriter
         >>> writer = XBNWriter()
         >>> writer.set_edges([('a', 'b'), ('a', 'c'), ('b', 'd'), ('c', 'd'), ('c', 'e')])
         """
-        structure = etree.SubElement(self.bnmodel, 'STRUCTURE')
+        structure = etree.SubElement(self.bnmodel, "STRUCTURE")
         for edge in edge_list:
-            etree.SubElement(structure, 'ARC', attrib={'PARENT': edge[0], 'CHILD': edge[1]})
+            etree.SubElement(
+                structure, "ARC", attrib={"PARENT": edge[0], "CHILD": edge[1]}
+            )
 
     def set_distributions(self):
         """
         Set distributions in the network.
 
         Examples
         --------
         >>> from pgmpy.readwrite.XMLBeliefNetwork import XBNWriter
         >>> writer =XBNWriter()
         >>> writer.set_distributions()
         """
-        distributions = etree.SubElement(self.bnmodel, 'DISTRIBUTIONS')
+        distributions = etree.SubElement(self.bnmodel, "DISTRIBUTIONS")
 
         cpds = self.model.get_cpds()
         cpds.sort(key=lambda x: x.variable)
         for cpd in cpds:
             cpd_values = cpd.values.ravel()
             var = cpd.variable
-            dist = etree.SubElement(distributions, 'DIST', attrib={'TYPE': self.model.node[var]['TYPE']})
-            etree.SubElement(dist, 'PRIVATE', attrib={'NAME': var})
-            dpis = etree.SubElement(dist, 'DPIS')
+            dist = etree.SubElement(
+                distributions, "DIST", attrib={"TYPE": self.model.nodes[var]["TYPE"]}
+            )
+            etree.SubElement(dist, "PRIVATE", attrib={"NAME": var})
+            dpis = etree.SubElement(dist, "DPIS")
             evidence = cpd.variables[:0:-1]
             if evidence:
-                condset = etree.SubElement(dist, 'CONDSET')
+                condset = etree.SubElement(dist, "CONDSET")
                 for condelem in sorted(evidence):
-                    etree.SubElement(condset, 'CONDELEM', attrib={'NAME': condelem})
+                    etree.SubElement(condset, "CONDELEM", attrib={"NAME": condelem})
                 # TODO: Get Index value.
                 for val in range(0, len(cpd_values), 2):
-                    etree.SubElement(dpis, "DPI", attrib={'INDEXES': ' '}).text = \
-                        " " + str(cpd_values[val]) + " " + str(cpd_values[val+1]) + " "
+                    etree.SubElement(dpis, "DPI", attrib={"INDEXES": " "}).text = (
+                        " "
+                        + str(cpd_values[val])
+                        + " "
+                        + str(cpd_values[val + 1])
+                        + " "
+                    )
             else:
-                etree.SubElement(dpis, "DPI").text = ' ' + ' '.join(map(str, cpd_values))
+                etree.SubElement(dpis, "DPI").text = " " + " ".join(
+                    map(str, cpd_values)
+                )
```

### Comparing `pgmpy-0.1.7/pgmpy/readwrite/UAI.py` & `pgmpy-0.1.9/pgmpy/readwrite/UAI.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,14 +9,15 @@
 from pgmpy.extern.six.moves import map, range
 
 
 class UAIReader(object):
     """
     Class for reading UAI file format from files or strings.
     """
+
     def __init__(self, path=None, string=None):
         """
         Initialize an instance of UAI reader class
 
         Parameters
         ----------
         path : file or str
@@ -47,37 +48,51 @@
         self.edges = self.get_edges()
         self.tables = self.get_tables()
 
     def get_grammar(self):
         """
         Returns the grammar of the UAI file.
         """
-        network_name = Word(alphas).setResultsName('network_name')
-        no_variables = Word(nums).setResultsName('no_variables')
+        network_name = Word(alphas).setResultsName("network_name")
+        no_variables = Word(nums).setResultsName("no_variables")
         grammar = network_name + no_variables
-        self.no_variables = int(grammar.parseString(self.network)['no_variables'])
-        domain_variables = (Word(nums)*self.no_variables).setResultsName('domain_variables')
+        self.no_variables = int(grammar.parseString(self.network)["no_variables"])
+        domain_variables = (Word(nums) * self.no_variables).setResultsName(
+            "domain_variables"
+        )
         grammar += domain_variables
-        no_functions = Word(nums).setResultsName('no_functions')
+        no_functions = Word(nums).setResultsName("no_functions")
         grammar += no_functions
-        self.no_functions = int(grammar.parseString(self.network)['no_functions'])
+        self.no_functions = int(grammar.parseString(self.network)["no_functions"])
         integer = Word(nums).setParseAction(lambda t: int(t[0]))
         for function in range(0, self.no_functions):
-            scope_grammar = Word(nums).setResultsName('fun_scope_' + str(function))
+            scope_grammar = Word(nums).setResultsName("fun_scope_" + str(function))
             grammar += scope_grammar
-            function_scope = grammar.parseString(self.network)['fun_scope_' + str(function)]
-            function_grammar = ((integer)*int(function_scope)).setResultsName('fun_' + str(function))
+            function_scope = grammar.parseString(self.network)[
+                "fun_scope_" + str(function)
+            ]
+            function_grammar = ((integer) * int(function_scope)).setResultsName(
+                "fun_" + str(function)
+            )
             grammar += function_grammar
 
-        floatnumber = Combine(Word(nums) + Optional(Literal(".") + Optional(Word(nums))))
+        floatnumber = Combine(
+            Word(nums) + Optional(Literal(".") + Optional(Word(nums)))
+        )
         for function in range(0, self.no_functions):
-            no_values_grammar = Word(nums).setResultsName('fun_no_values_' + str(function))
+            no_values_grammar = Word(nums).setResultsName(
+                "fun_no_values_" + str(function)
+            )
             grammar += no_values_grammar
-            no_values = grammar.parseString(self.network)['fun_no_values_' + str(function)]
-            values_grammar = ((floatnumber)*int(no_values)).setResultsName('fun_values_' + str(function))
+            no_values = grammar.parseString(self.network)[
+                "fun_no_values_" + str(function)
+            ]
+            values_grammar = ((floatnumber) * int(no_values)).setResultsName(
+                "fun_values_" + str(function)
+            )
             grammar += values_grammar
         return grammar
 
     def get_network_type(self):
         """
         Returns the type of network defined by the file.
 
@@ -89,15 +104,15 @@
         Example
         -------
         >>> reader = UAIReader('TestUAI.uai')
         >>> reader.get_network_type()
         'MARKOV'
         """
         network_type = self.grammar.parseString(self.network)
-        return network_type['network_name']
+        return network_type["network_name"]
 
     def get_variables(self):
         """
         Returns a list of variables.
         Each variable is represented by an index of list.
         For example if the no of variables are 4 then the list will be
         [var_0, var_1, var_2, var_3]
@@ -130,15 +145,15 @@
         Example
         -------
         >>> reader = UAIReader('TestUAI.uai')
         >>> reader.get_domain()
         {'var_0': '2', 'var_1': '2', 'var_2': '3'}
         """
         domain = {}
-        var_domain = self.grammar.parseString(self.network)['domain_variables']
+        var_domain = self.grammar.parseString(self.network)["domain_variables"]
         for var in range(0, len(var_domain)):
             domain["var_" + str(var)] = var_domain[var]
         return domain
 
     def get_edges(self):
         """
         Returns the edges of the network.
@@ -151,18 +166,20 @@
         -------
         >>> reader = UAIReader('TestUAI.uai')
         >>> reader.get_edges()
         {('var_0', 'var_1'), ('var_0', 'var_2'), ('var_1', 'var_2')}
         """
         edges = []
         for function in range(0, self.no_functions):
-            function_variables = self.grammar.parseString(self.network)['fun_' + str(function)]
+            function_variables = self.grammar.parseString(self.network)[
+                "fun_" + str(function)
+            ]
             if isinstance(function_variables, int):
                 function_variables = [function_variables]
-            if self.network_type == 'BAYES':
+            if self.network_type == "BAYES":
                 child_var = "var_" + str(function_variables[-1])
                 function_variables = function_variables[:-1]
                 for var in function_variables:
                     edges.append((child_var, "var_" + str(var)))
             elif self.network_type == "MARKOV":
                 function_variables = ["var_" + str(var) for var in function_variables]
                 edges.extend(list(combinations(function_variables, 2)))
@@ -185,24 +202,30 @@
         [(['var_0', 'var_1'], ['4.000', '2.400', '1.000', '0.000']),
          (['var_0', 'var_1', 'var_2'],
           ['2.2500', '3.2500', '3.7500', '0.0000', '0.0000', '10.0000',
            '1.8750', '4.0000', '3.3330', '2.0000', '2.0000', '3.4000'])]
         """
         tables = []
         for function in range(0, self.no_functions):
-            function_variables = self.grammar.parseString(self.network)['fun_' + str(function)]
+            function_variables = self.grammar.parseString(self.network)[
+                "fun_" + str(function)
+            ]
             if isinstance(function_variables, int):
                 function_variables = [function_variables]
-            if self.network_type == 'BAYES':
+            if self.network_type == "BAYES":
                 child_var = "var_" + str(function_variables[-1])
-                values = self.grammar.parseString(self.network)['fun_values_' + str(function)]
+                values = self.grammar.parseString(self.network)[
+                    "fun_values_" + str(function)
+                ]
                 tables.append((child_var, list(values)))
             elif self.network_type == "MARKOV":
                 function_variables = ["var_" + str(var) for var in function_variables]
-                values = self.grammar.parseString(self.network)['fun_values_' + str(function)]
+                values = self.grammar.parseString(self.network)[
+                    "fun_values_" + str(function)
+                ]
                 tables.append((function_variables, list(values)))
         return tables
 
     def get_model(self):
         """
         Returns an instance of Bayesian Model or Markov Model.
         Varibles are in the pattern var_0, var_1, var_2 where var_0 is
@@ -213,15 +236,15 @@
         model: an instance of Bayesian or Markov Model.
 
         Examples
         --------
         >>> reader = UAIReader('TestUAI.uai')
         >>> reader.get_model()
         """
-        if self.network_type == 'BAYES':
+        if self.network_type == "BAYES":
             model = BayesianModel()
             model.add_nodes_from(self.variables)
             model.add_edges_from(self.edges)
 
             tabular_cpds = []
             for cpd in self.tables:
                 child_var = cpd[0]
@@ -230,33 +253,36 @@
                 values = np.array(arr)
                 values = values.reshape(states, values.size // states)
                 tabular_cpds.append(TabularCPD(child_var, states, values))
 
             model.add_cpds(*tabular_cpds)
             return model
 
-        elif self.network_type == 'MARKOV':
+        elif self.network_type == "MARKOV":
             model = MarkovModel(self.edges)
 
             factors = []
             for table in self.tables:
                 variables = table[0]
                 cardinality = [int(self.domain[var]) for var in variables]
                 value = list(map(float, table[1]))
-                factor = DiscreteFactor(variables=variables, cardinality=cardinality, values=value)
+                factor = DiscreteFactor(
+                    variables=variables, cardinality=cardinality, values=value
+                )
                 factors.append(factor)
 
             model.add_factors(*factors)
             return model
 
 
 class UAIWriter(object):
     """
     Class for writing models in UAI.
     """
+
     def __init__(self, model):
         """
         Initialize an instance of UAI writer class
 
         Parameters
         ----------
         model: A Bayesian or Markov model
@@ -345,25 +371,31 @@
             cpds = self.model.get_cpds()
             cpds.sort(key=lambda x: x.variable)
             variables = sorted(self.domain.items(), key=lambda x: (x[1], x[0]))
             functions = []
             for cpd in cpds:
                 child_var = cpd.variable
                 evidence = cpd.variables[:0:-1]
-                function = [str(variables.index((var, self.domain[var]))) for var in evidence]
-                function.append(str(variables.index((child_var, self.domain[child_var]))))
+                function = [
+                    str(variables.index((var, self.domain[var]))) for var in evidence
+                ]
+                function.append(
+                    str(variables.index((child_var, self.domain[child_var])))
+                )
                 functions.append(function)
             return functions
         elif isinstance(self.model, MarkovModel):
             factors = self.model.get_factors()
             functions = []
             variables = sorted(self.domain.items(), key=lambda x: (x[1], x[0]))
             for factor in factors:
                 scope = factor.scope()
-                function = [str(variables.index((var, self.domain[var]))) for var in scope]
+                function = [
+                    str(variables.index((var, self.domain[var]))) for var in scope
+                ]
                 functions.append(function)
             return functions
         else:
             raise TypeError("Model must be an instance of Markov or Bayesian model.")
 
     def get_tables(self):
         """
@@ -402,9 +434,9 @@
 
         Examples
         -------
         >>> writer = UAIWriter(model)
         >>> writer.write_xmlbif(test_file)
         """
         writer = self.__str__()
-        with open(filename, 'w') as fout:
+        with open(filename, "w") as fout:
             fout.write(writer)
```

### Comparing `pgmpy-0.1.7/pgmpy/readwrite/PomdpX.py` & `pgmpy-0.1.9/pgmpy/readwrite/PomdpX.py`

 * *Files 9% similar despite different names*

```diff
@@ -65,27 +65,27 @@
         --------
         >>> reader = PomdpXReader('Test_PomdpX.xml')
         >>> reader.get_description()
         'RockSample problem for map size 1 x 3.
          Rock is at 0, Rover’s initial position is at 1.
          Exit is at 2.'
         """
-        return self.network.find('Description').text
+        return self.network.find("Description").text
 
     def get_discount(self):
         """
         Returns the discount factor for the problem
 
         Example
         --------
         >>> reader = PomdpXReader('Test_PomdpX.xml')
         >>> reader.get_discount()
         0.95
         """
-        return float(self.network.find('Discount').text)
+        return float(self.network.find("Discount").text)
 
     def get_variables(self):
         """
         Returns list of variables of the network
 
         Example
         -------
@@ -105,51 +105,48 @@
                         'RewardVar': [{'vname': 'reward_rover'}],
                         'ActionVar': [{'vname': 'action_rover',
                                        'ValueEnum': ['amw', 'ame',
                                                      'ac', 'as']}]
                         }
         """
         self.variables = defaultdict(list)
-        for variable in self.network.findall('Variable'):
+        for variable in self.network.findall("Variable"):
             _variables = defaultdict(list)
-            for var in variable.findall('StateVar'):
+            for var in variable.findall("StateVar"):
                 state_variables = defaultdict(list)
-                state_variables['vnamePrev'] = var.get('vnamePrev')
-                state_variables['vnameCurr'] = var.get('vnameCurr')
-                if var.get('fullyObs'):
-                    state_variables['fullyObs'] = True
+                state_variables["vnamePrev"] = var.get("vnamePrev")
+                state_variables["vnameCurr"] = var.get("vnameCurr")
+                if var.get("fullyObs"):
+                    state_variables["fullyObs"] = True
                 else:
-                    state_variables['fullyObs'] = False
-                state_variables['ValueEnum'] = []
-                if var.find('NumValues') is not None:
-                    for i in range(0, int(var.find('NumValues').text)):
-                        state_variables['ValueEnum'].append('s' + str(i))
-                if var.find('ValueEnum') is not None:
-                    state_variables['ValueEnum'] = \
-                        var.find('ValueEnum').text.split()
-                _variables['StateVar'].append(state_variables)
+                    state_variables["fullyObs"] = False
+                state_variables["ValueEnum"] = []
+                if var.find("NumValues") is not None:
+                    for i in range(0, int(var.find("NumValues").text)):
+                        state_variables["ValueEnum"].append("s" + str(i))
+                if var.find("ValueEnum") is not None:
+                    state_variables["ValueEnum"] = var.find("ValueEnum").text.split()
+                _variables["StateVar"].append(state_variables)
 
-            for var in variable.findall('ObsVar'):
+            for var in variable.findall("ObsVar"):
                 obs_variables = defaultdict(list)
-                obs_variables['vname'] = var.get('vname')
-                obs_variables['ValueEnum'] = \
-                    var.find('ValueEnum').text.split()
-                _variables['ObsVar'].append(obs_variables)
+                obs_variables["vname"] = var.get("vname")
+                obs_variables["ValueEnum"] = var.find("ValueEnum").text.split()
+                _variables["ObsVar"].append(obs_variables)
 
-            for var in variable.findall('ActionVar'):
+            for var in variable.findall("ActionVar"):
                 action_variables = defaultdict(list)
-                action_variables['vname'] = var.get('vname')
-                action_variables['ValueEnum'] = \
-                    var.find('ValueEnum').text.split()
-                _variables['ActionVar'].append(action_variables)
+                action_variables["vname"] = var.get("vname")
+                action_variables["ValueEnum"] = var.find("ValueEnum").text.split()
+                _variables["ActionVar"].append(action_variables)
 
-            for var in variable.findall('RewardVar'):
+            for var in variable.findall("RewardVar"):
                 reward_variables = defaultdict(list)
-                reward_variables['vname'] = var.get('vname')
-                _variables['RewardVar'].append(reward_variables)
+                reward_variables["vname"] = var.get("vname")
+                _variables["RewardVar"].append(reward_variables)
 
             self.variables.update(_variables)
 
         return self.variables
 
     def get_initial_beliefs(self):
         """
@@ -169,24 +166,24 @@
          },
          {'Var': '',
           '...': ...,'
           '...': '...',
           }]
         """
         initial_state_belief = []
-        for variable in self.network.findall('InitialStateBelief'):
-            for var in variable.findall('CondProb'):
+        for variable in self.network.findall("InitialStateBelief"):
+            for var in variable.findall("CondProb"):
                 cond_prob = defaultdict(list)
-                cond_prob['Var'] = var.find('Var').text
-                cond_prob['Parent'] = var.find('Parent').text.split()
-                if not var.find('Parameter').get('type'):
-                    cond_prob['Type'] = 'TBL'
+                cond_prob["Var"] = var.find("Var").text
+                cond_prob["Parent"] = var.find("Parent").text.split()
+                if not var.find("Parameter").get("type"):
+                    cond_prob["Type"] = "TBL"
                 else:
-                    cond_prob['Type'] = var.find('Parameter').get('type')
-                cond_prob['Parameter'] = self.get_parameter(var)
+                    cond_prob["Type"] = var.find("Parameter").get("type")
+                cond_prob["Parameter"] = self.get_parameter(var)
                 initial_state_belief.append(cond_prob)
 
         return initial_state_belief
 
     def get_state_transition_function(self):
         """
         Returns the transition of the state variables as nested dict in the
@@ -205,24 +202,24 @@
                          {'Instance': ['amw', 's1', 's0'],
                          'ProbTable': ['1.0']},
                          ...
                         ]
         }]
         """
         state_transition_function = []
-        for variable in self.network.findall('StateTransitionFunction'):
-            for var in variable.findall('CondProb'):
+        for variable in self.network.findall("StateTransitionFunction"):
+            for var in variable.findall("CondProb"):
                 cond_prob = defaultdict(list)
-                cond_prob['Var'] = var.find('Var').text
-                cond_prob['Parent'] = var.find('Parent').text.split()
-                if not var.find('Parameter').get('type'):
-                    cond_prob['Type'] = 'TBL'
+                cond_prob["Var"] = var.find("Var").text
+                cond_prob["Parent"] = var.find("Parent").text.split()
+                if not var.find("Parameter").get("type"):
+                    cond_prob["Type"] = "TBL"
                 else:
-                    cond_prob['Type'] = var.find('Parameter').get('type')
-                cond_prob['Parameter'] = self.get_parameter(var)
+                    cond_prob["Type"] = var.find("Parameter").get("type")
+                cond_prob["Parameter"] = self.get_parameter(var)
                 state_transition_function.append(cond_prob)
 
         return state_transition_function
 
     def get_obs_function(self):
         """
         Returns the observation function as nested dict in the case of table-
@@ -239,24 +236,24 @@
               'Parameter': [{'Instance': ['amw', '*', '*', '-'],
                              'ProbTable': ['1.0', '0.0']},
                          ...
                         ]
         }]
         """
         obs_function = []
-        for variable in self.network.findall('ObsFunction'):
-            for var in variable.findall('CondProb'):
+        for variable in self.network.findall("ObsFunction"):
+            for var in variable.findall("CondProb"):
                 cond_prob = defaultdict(list)
-                cond_prob['Var'] = var.find('Var').text
-                cond_prob['Parent'] = var.find('Parent').text.split()
-                if not var.find('Parameter').get('type'):
-                    cond_prob['Type'] = 'TBL'
+                cond_prob["Var"] = var.find("Var").text
+                cond_prob["Parent"] = var.find("Parent").text.split()
+                if not var.find("Parameter").get("type"):
+                    cond_prob["Type"] = "TBL"
                 else:
-                    cond_prob['Type'] = var.find('Parameter').get('type')
-                cond_prob['Parameter'] = self.get_parameter(var)
+                    cond_prob["Type"] = var.find("Parameter").get("type")
+                cond_prob["Parameter"] = self.get_parameter(var)
                 obs_function.append(cond_prob)
 
         return obs_function
 
     def get_reward_function(self):
         """
         Returns the reward function as nested dict in the case of table-
@@ -273,116 +270,115 @@
               'Parameter': [{'Instance': ['ame', 's1', '*'],
                              'ValueTable': ['10']},
                          ...
                         ]
         }]
         """
         reward_function = []
-        for variable in self.network.findall('RewardFunction'):
-            for var in variable.findall('Func'):
+        for variable in self.network.findall("RewardFunction"):
+            for var in variable.findall("Func"):
                 func = defaultdict(list)
-                func['Var'] = var.find('Var').text
-                func['Parent'] = var.find('Parent').text.split()
-                if not var.find('Parameter').get('type'):
-                    func['Type'] = 'TBL'
+                func["Var"] = var.find("Var").text
+                func["Parent"] = var.find("Parent").text.split()
+                if not var.find("Parameter").get("type"):
+                    func["Type"] = "TBL"
                 else:
-                    func['Type'] = var.find('Parameter').get('type')
-                func['Parameter'] = self.get_parameter(var)
+                    func["Type"] = var.find("Parameter").get("type")
+                func["Parameter"] = self.get_parameter(var)
                 reward_function.append(func)
 
         return reward_function
 
     def get_parameter(self, var):
         """
         This method supports the functional tags by providing the actual
         values in the function as list of dict in case of table type parameter or as
         nested dict in case of decision diagram
         """
         parameter = []
 
-        for parameter_tag in var.findall('Parameter'):
-            parameter_type = 'TBL'
-            if parameter_tag.get('type') is not None:
-                parameter_type = parameter_tag.get('type')
-            if parameter_type == 'TBL':
+        for parameter_tag in var.findall("Parameter"):
+            parameter_type = "TBL"
+            if parameter_tag.get("type") is not None:
+                parameter_type = parameter_tag.get("type")
+            if parameter_type == "TBL":
                 parameter = self.get_parameter_tbl(parameter_tag)
-            elif parameter_type == 'DD':
+            elif parameter_type == "DD":
                 parameter = defaultdict(list)
                 parameter = self.get_parameter_dd(parameter_tag)
 
         return parameter
 
     def get_parameter_tbl(self, parameter):
         """
         This method returns parameters as list of dict in case of table type
         parameter
         """
         par = []
-        for entry in parameter.findall('Entry'):
+        for entry in parameter.findall("Entry"):
             instance = defaultdict(list)
-            instance['Instance'] = entry.find('Instance').text.split()
-            if entry.find('ProbTable') is None:
-                instance['ValueTable'] = entry.find('ValueTable').text.split()
+            instance["Instance"] = entry.find("Instance").text.split()
+            if entry.find("ProbTable") is None:
+                instance["ValueTable"] = entry.find("ValueTable").text.split()
             else:
-                instance['ProbTable'] = entry.find('ProbTable').text.split()
+                instance["ProbTable"] = entry.find("ProbTable").text.split()
             par.append(instance)
         return par
 
     def get_parameter_dd(self, parameter):
         """
         This method returns parameters as nested dicts in case of decision
         diagram parameter.
         """
         dag = defaultdict(list)
-        dag_elem = parameter.find('DAG')
-        node = dag_elem.find('Node')
-        root = node.get('var')
+        dag_elem = parameter.find("DAG")
+        node = dag_elem.find("Node")
+        root = node.get("var")
 
         def get_param(node):
             edges = defaultdict(list)
-            for edge in node.findall('Edge'):
-                if edge.find('Terminal') is not None:
-                    edges[edge.get('val')] = edge.find('Terminal').text
-                elif edge.find('Node') is not None:
+            for edge in node.findall("Edge"):
+                if edge.find("Terminal") is not None:
+                    edges[edge.get("val")] = edge.find("Terminal").text
+                elif edge.find("Node") is not None:
                     node_cpd = defaultdict(list)
-                    node_cpd[edge.find('Node').get('var')] = \
-                        get_param(edge.find('Node'))
-                    edges[edge.get('val')] = node_cpd
-                elif edge.find('SubDAG') is not None:
+                    node_cpd[edge.find("Node").get("var")] = get_param(
+                        edge.find("Node")
+                    )
+                    edges[edge.get("val")] = node_cpd
+                elif edge.find("SubDAG") is not None:
                     subdag_attribute = defaultdict(list)
-                    subdag_attribute['type'] = edge.find('SubDAG').get('type')
-                    if subdag_attribute['type'] == 'template':
-                        subdag_attribute['idref'] = \
-                            edge.find('SubDAG').get('idref')
-                    if edge.find('SubDAG').get('var'):
-                        subdag_attribute['var'] = \
-                            edge.find('SubDAG').get('var')
-                    if edge.find('SubDAG').get('val'):
-                        subdag_attribute['val'] = \
-                            edge.find('SubDAG').get('val')
-                    edges[edge.get('val')] = subdag_attribute
+                    subdag_attribute["type"] = edge.find("SubDAG").get("type")
+                    if subdag_attribute["type"] == "template":
+                        subdag_attribute["idref"] = edge.find("SubDAG").get("idref")
+                    if edge.find("SubDAG").get("var"):
+                        subdag_attribute["var"] = edge.find("SubDAG").get("var")
+                    if edge.find("SubDAG").get("val"):
+                        subdag_attribute["val"] = edge.find("SubDAG").get("val")
+                    edges[edge.get("val")] = subdag_attribute
             return edges
 
-        if parameter.find('SubDAGTemplate') is not None:
-            SubDAGTemplate = parameter.find('SubDAGTemplate')
-            subdag_root = SubDAGTemplate.find('Node')
-            subdag_node = subdag_root.get('var')
+        if parameter.find("SubDAGTemplate") is not None:
+            SubDAGTemplate = parameter.find("SubDAGTemplate")
+            subdag_root = SubDAGTemplate.find("Node")
+            subdag_node = subdag_root.get("var")
             subdag_dict = defaultdict(list)
             subdag_dict[subdag_node] = get_param(subdag_root)
-            dag['SubDAGTemplate'] = subdag_dict
-            dag['id'] = SubDAGTemplate.get('id')
+            dag["SubDAGTemplate"] = subdag_dict
+            dag["id"] = SubDAGTemplate.get("id")
         dag[root] = get_param(node)
         return dag
 
 
 class PomdpXWriter(object):
     """
     Class for writing models in PomdpX
     """
-    def __init__(self, model_data, encoding='utf-8', prettyprint=True):
+
+    def __init__(self, model_data, encoding="utf-8", prettyprint=True):
         """
         Initialise a PomdpXWriter Object
 
         Parameters
         ---------------
         model: A Bayesian of Markov Model
             The model to write
@@ -392,43 +388,43 @@
             Indentation in output XML if true
         """
         self.model = model_data
 
         self.encoding = encoding
         self.prettyprint = prettyprint
 
-        self.xml = etree.Element("pomdpx", attrib={'version': '1.0'})
-        self.description = etree.SubElement(self.xml, 'Description')
-        self.discount = etree.SubElement(self.xml, 'Discount')
-        self.variable = etree.SubElement(self.xml, 'Variable')
-        self.initial_belief = etree.SubElement(self.xml, 'InitialStateBelief')
-        self.transition_function = etree.SubElement(self.xml, 'StateTransitionFunction')
-        self.observation_function = etree.SubElement(self.xml, 'ObsFunction')
-        self.reward_function = etree.SubElement(self.xml, 'RewardFunction')
+        self.xml = etree.Element("pomdpx", attrib={"version": "1.0"})
+        self.description = etree.SubElement(self.xml, "Description")
+        self.discount = etree.SubElement(self.xml, "Discount")
+        self.variable = etree.SubElement(self.xml, "Variable")
+        self.initial_belief = etree.SubElement(self.xml, "InitialStateBelief")
+        self.transition_function = etree.SubElement(self.xml, "StateTransitionFunction")
+        self.observation_function = etree.SubElement(self.xml, "ObsFunction")
+        self.reward_function = etree.SubElement(self.xml, "RewardFunction")
 
     def __str__(self, xml):
         """
         Return the XML as string.
         """
         if self.prettyprint:
             self.indent(xml)
         return etree.tostring(xml, encoding=self.encoding)
 
     def indent(self, elem, level=0):
         """
         Inplace prettyprint formatter.
         """
-        i = "\n" + level*"  "
+        i = "\n" + level * "  "
         if len(elem):
             if not elem.text or not elem.text.strip():
                 elem.text = i + "  "
             if not elem.tail or not elem.tail.strip():
                 elem.tail = i
             for elem in elem:
-                self.indent(elem, level+1)
+                self.indent(elem, level + 1)
             if not elem.tail or not elem.tail.strip():
                 elem.tail = i
         else:
             if level and (not elem.tail or not elem.tail.strip()):
                 elem.tail = i
 
     def _add_value_enum(self, var, tag):
@@ -440,53 +436,62 @@
         var: The SubElement variable
         tag: The SubElement tag to which enum value is to be added
 
         Return
         ---------------
         None
         """
-        if var['ValueEnum'][0] == 's0':
-                numvalues_tag = etree.SubElement(tag, 'NumValues')
-                numvalues_tag.text = str(int(var['ValueEnum'][-1][-1]) + 1)
+        if var["ValueEnum"][0] == "s0":
+            numvalues_tag = etree.SubElement(tag, "NumValues")
+            numvalues_tag.text = str(int(var["ValueEnum"][-1][-1]) + 1)
         else:
-            valueenum_tag = etree.SubElement(tag, 'ValueEnum')
-            valueenum_tag.text = ''
-            for value in var['ValueEnum']:
-                valueenum_tag.text += value + ' '
+            valueenum_tag = etree.SubElement(tag, "ValueEnum")
+            valueenum_tag.text = ""
+            for value in var["ValueEnum"]:
+                valueenum_tag.text += value + " "
             valueenum_tag.text = valueenum_tag.text[:-1]
 
     def get_variables(self):
         """
         Add variables to PomdpX
 
         Return
         ---------------
         xml containing variables tag
         """
-        state_variables = self.model['variables']['StateVar']
+        state_variables = self.model["variables"]["StateVar"]
         for var in state_variables:
-            state_var_tag = etree.SubElement(self.variable, 'StateVar', attrib={'vnamePrev': var['vnamePrev'],
-                                                                                'vnameCurr': var['vnameCurr'],
-                                                                                'fullyObs': 'true' if var['fullyObs']
-                                                                                else 'false'})
+            state_var_tag = etree.SubElement(
+                self.variable,
+                "StateVar",
+                attrib={
+                    "vnamePrev": var["vnamePrev"],
+                    "vnameCurr": var["vnameCurr"],
+                    "fullyObs": "true" if var["fullyObs"] else "false",
+                },
+            )
             self._add_value_enum(var, state_var_tag)
 
-        obs_variables = self.model['variables']['ObsVar']
+        obs_variables = self.model["variables"]["ObsVar"]
         for var in obs_variables:
-            obs_var_tag = etree.SubElement(self.variable, 'ObsVar', attrib={'vname': var['vname']})
+            obs_var_tag = etree.SubElement(
+                self.variable, "ObsVar", attrib={"vname": var["vname"]}
+            )
             self._add_value_enum(var, obs_var_tag)
 
-        action_variables = self.model['variables']['ActionVar']
+        action_variables = self.model["variables"]["ActionVar"]
         for var in action_variables:
-            action_var_tag = etree.SubElement(self.variable, 'ActionVar', attrib={'vname': var['vname']})
+            action_var_tag = etree.SubElement(
+                self.variable, "ActionVar", attrib={"vname": var["vname"]}
+            )
             self._add_value_enum(var, action_var_tag)
 
-        reward_var = self.model['variables']['RewardVar']
+        reward_var = self.model["variables"]["RewardVar"]
         for var in reward_var:
-            etree.SubElement(self.variable, 'RewardVar', attrib={'vname': var['vname']})
+            etree.SubElement(self.variable, "RewardVar", attrib={"vname": var["vname"]})
 
         return self.__str__(self.variable)[:-1]
 
     def add_parameter_dd(self, dag_tag, node_dict):
         """
         helper function for adding parameters in condition
 
@@ -498,30 +503,47 @@
                    the decision diagram dictionary
 
         Return
         ---------------
         None
         """
         if isinstance(node_dict, defaultdict) or isinstance(node_dict, dict):
-            node_tag = etree.SubElement(dag_tag, 'Node', attrib={'var': next(iter(node_dict.keys()))})
+            node_tag = etree.SubElement(
+                dag_tag, "Node", attrib={"var": next(iter(node_dict.keys()))}
+            )
             edge_dict = next(iter(node_dict.values()))
             for edge in sorted(edge_dict.keys(), key=tuple):
-                edge_tag = etree.SubElement(node_tag, 'Edge', attrib={'val': edge})
+                edge_tag = etree.SubElement(node_tag, "Edge", attrib={"val": edge})
                 value = edge_dict.get(edge)
                 if isinstance(value, six.string_types):
-                    terminal_tag = etree.SubElement(edge_tag, 'Terminal')
+                    terminal_tag = etree.SubElement(edge_tag, "Terminal")
                     terminal_tag.text = value
-                elif 'type' in value:
-                    if 'val' in value:
-                        etree.SubElement(edge_tag, 'SubDAG',
-                                         attrib={'type': value['type'], 'var': value['var'], 'val': value['val']})
-                    elif 'idref' in value:
-                        etree.SubElement(edge_tag, 'SubDAG', attrib={'type': value['type'], 'idref': value['idref']})
+                elif "type" in value:
+                    if "val" in value:
+                        etree.SubElement(
+                            edge_tag,
+                            "SubDAG",
+                            attrib={
+                                "type": value["type"],
+                                "var": value["var"],
+                                "val": value["val"],
+                            },
+                        )
+                    elif "idref" in value:
+                        etree.SubElement(
+                            edge_tag,
+                            "SubDAG",
+                            attrib={"type": value["type"], "idref": value["idref"]},
+                        )
                     else:
-                        etree.SubElement(edge_tag, 'SubDAG', attrib={'type': value['type'], 'var': value['var']})
+                        etree.SubElement(
+                            edge_tag,
+                            "SubDAG",
+                            attrib={"type": value["type"], "var": value["var"]},
+                        )
                 else:
                     self.add_parameter_dd(edge_tag, value)
 
     def add_conditions(self, condition, condprob):
         """
         helper function for adding probability conditions for model\
 
@@ -533,106 +555,112 @@
         condprob:   etree SubElement
                     the tag to which condition is added
 
         Return
         ---------------
         None
         """
-        var_tag = etree.SubElement(condprob, 'Var')
-        var_tag.text = condition['Var']
-        parent_tag = etree.SubElement(condprob, 'Parent')
-        parent_tag.text = ''
-        for parent in condition['Parent']:
-            parent_tag.text += parent + ' '
+        var_tag = etree.SubElement(condprob, "Var")
+        var_tag.text = condition["Var"]
+        parent_tag = etree.SubElement(condprob, "Parent")
+        parent_tag.text = ""
+        for parent in condition["Parent"]:
+            parent_tag.text += parent + " "
         parent_tag.text = parent_tag.text[:-1]
-        parameter_tag = etree.SubElement(condprob, 'Parameter', attrib={'type': condition['Type']
-                                                                        if condition['Type'] is not None
-                                                                        else 'TBL'})
-        if condition['Type'] == 'DD':
-            dag_tag = etree.SubElement(parameter_tag, 'DAG')
-            parameter_dict = condition['Parameter']
-            if 'SubDAGTemplate' in parameter_dict:
-                subdag_tag = etree.SubElement(parameter_tag, 'SubDAGTemplate', attrib={'id': parameter_dict['id']})
-                self.add_parameter_dd(subdag_tag, parameter_dict['SubDAGTemplate'])
-                del parameter_dict['SubDAGTemplate']
-                del parameter_dict['id']
+        parameter_tag = etree.SubElement(
+            condprob,
+            "Parameter",
+            attrib={
+                "type": condition["Type"] if condition["Type"] is not None else "TBL"
+            },
+        )
+        if condition["Type"] == "DD":
+            dag_tag = etree.SubElement(parameter_tag, "DAG")
+            parameter_dict = condition["Parameter"]
+            if "SubDAGTemplate" in parameter_dict:
+                subdag_tag = etree.SubElement(
+                    parameter_tag, "SubDAGTemplate", attrib={"id": parameter_dict["id"]}
+                )
+                self.add_parameter_dd(subdag_tag, parameter_dict["SubDAGTemplate"])
+                del parameter_dict["SubDAGTemplate"]
+                del parameter_dict["id"]
                 self.add_parameter_dd(dag_tag, parameter_dict)
             else:
                 self.add_parameter_dd(dag_tag, parameter_dict)
         else:
-            for parameter in condition['Parameter']:
-                entry = etree.SubElement(parameter_tag, 'Entry')
-                instance = etree.SubElement(entry, 'Instance')
-                instance.text = ''
-                for instance_var in parameter['Instance']:
-                    instance.text += instance_var + ' '
-                length_instance = len(parameter['Instance'])
-                if len(parameter['Instance'][length_instance - 1]) > 1:
-                        instance.text = instance.text[:-1]
-                if len(parameter['Instance']) == 1:
-                    instance.text = ' ' + instance.text
-                if condprob.tag == 'Func':
-                    table = 'ValueTable'
+            for parameter in condition["Parameter"]:
+                entry = etree.SubElement(parameter_tag, "Entry")
+                instance = etree.SubElement(entry, "Instance")
+                instance.text = ""
+                for instance_var in parameter["Instance"]:
+                    instance.text += instance_var + " "
+                length_instance = len(parameter["Instance"])
+                if len(parameter["Instance"][length_instance - 1]) > 1:
+                    instance.text = instance.text[:-1]
+                if len(parameter["Instance"]) == 1:
+                    instance.text = " " + instance.text
+                if condprob.tag == "Func":
+                    table = "ValueTable"
                 else:
-                    table = 'ProbTable'
+                    table = "ProbTable"
                 prob_table = parameter[table]
                 prob_table_tag = etree.SubElement(entry, table)
-                prob_table_tag.text = ''
+                prob_table_tag.text = ""
                 for probability in prob_table:
-                    prob_table_tag.text += probability + ' '
+                    prob_table_tag.text += probability + " "
                 prob_table_tag.text = prob_table_tag.text[:-1]
 
     def add_initial_belief(self):
         """
         add initial belief tag to pomdpx model
 
         Return
         ---------------
         string containing the xml for initial belief tag
         """
-        initial_belief = self.model['initial_state_belief']
+        initial_belief = self.model["initial_state_belief"]
         for condition in initial_belief:
-            condprob = etree.SubElement(self.initial_belief, 'CondProb')
+            condprob = etree.SubElement(self.initial_belief, "CondProb")
             self.add_conditions(condition, condprob)
         return self.__str__(self.initial_belief)[:-1]
 
     def add_state_transition_function(self):
         """
         add state transition function tag to pomdpx model
 
         Return
         ---------------
         string containing the xml for state transition tag
         """
-        state_transition_function = self.model['state_transition_function']
+        state_transition_function = self.model["state_transition_function"]
         for condition in state_transition_function:
-            condprob = etree.SubElement(self.transition_function, 'CondProb')
+            condprob = etree.SubElement(self.transition_function, "CondProb")
             self.add_conditions(condition, condprob)
         return self.__str__(self.transition_function)[:-1]
 
     def add_obs_function(self):
         """
         add observation function tag to pomdpx model
 
         Return
         ---------------
         string containing the xml for observation function tag
         """
-        obs_function = self.model['obs_function']
+        obs_function = self.model["obs_function"]
         for condition in obs_function:
-            condprob = etree.SubElement(self.observation_function, 'CondProb')
+            condprob = etree.SubElement(self.observation_function, "CondProb")
             self.add_conditions(condition, condprob)
         return self.__str__(self.observation_function)[:-1]
 
     def add_reward_function(self):
         """
         add reward function tag to pomdpx model
 
         Return
         ---------------
         string containing the xml for reward function tag
         """
-        reward_function = self.model['reward_function']
+        reward_function = self.model["reward_function"]
         for condition in reward_function:
-            condprob = etree.SubElement(self.reward_function, 'Func')
+            condprob = etree.SubElement(self.reward_function, "Func")
             self.add_conditions(condition, condprob)
         return self.__str__(self.reward_function)[:-1]
```

### Comparing `pgmpy-0.1.7/pgmpy/readwrite/XMLBIF.py` & `pgmpy-0.1.9/pgmpy/readwrite/XMLBIF.py`

 * *Files 3% similar despite different names*

```diff
@@ -46,20 +46,20 @@
         Examples
         --------
         # xmlbif_test.xml is the file present in
         # http://www.cs.cmu.edu/~fgcozman/Research/InterchangeFormat/
         >>> reader = XMLBIFReader("xmlbif_test.xml")
         """
         if path:
-            self.network = etree.ElementTree(file=path).getroot().find('NETWORK')
+            self.network = etree.ElementTree(file=path).getroot().find("NETWORK")
         elif string:
-            self.network = etree.fromstring(string.encode('utf-8')).find('NETWORK')
+            self.network = etree.fromstring(string.encode("utf-8")).find("NETWORK")
         else:
             raise ValueError("Must specify either path or string")
-        self.network_name = self.network.find('NAME').text
+        self.network_name = self.network.find("NAME").text
         self.variables = self.get_variables()
         self.variable_parents = self.get_parents()
         self.edge_list = self.get_edges()
         self.variable_states = self.get_states()
         self.variable_CPD = self.get_values()
         self.variable_property = self.get_property()
 
@@ -69,15 +69,17 @@
 
         Examples
         --------
         >>> reader = XMLBIF.XMLBIFReader("xmlbif_test.xml")
         >>> reader.get_variables()
         ['light-on', 'bowel-problem', 'dog-out', 'hear-bark', 'family-out']
         """
-        variables = [variable.find('NAME').text for variable in self.network.findall('VARIABLE')]
+        variables = [
+            variable.find("NAME").text for variable in self.network.findall("VARIABLE")
+        ]
         return variables
 
     def get_edges(self):
         """
         Returns the edges of the network
 
         Examples
@@ -85,16 +87,19 @@
         >>> reader = XMLBIF.XMLBIFReader("xmlbif_test.xml")
         >>> reader.get_edges()
         [['family-out', 'light-on'],
          ['family-out', 'dog-out'],
          ['bowel-problem', 'dog-out'],
          ['dog-out', 'hear-bark']]
         """
-        edge_list = [[value, key] for key in self.variable_parents
-                     for value in self.variable_parents[key]]
+        edge_list = [
+            [value, key]
+            for key in self.variable_parents
+            for value in self.variable_parents[key]
+        ]
         return edge_list
 
     def get_states(self):
         """
         Returns the states of variables present in the network
 
         Examples
@@ -103,16 +108,20 @@
         >>> reader.get_states()
         {'bowel-problem': ['true', 'false'],
          'dog-out': ['true', 'false'],
          'family-out': ['true', 'false'],
          'hear-bark': ['true', 'false'],
          'light-on': ['true', 'false']}
         """
-        variable_states = {variable.find('NAME').text: [outcome.text for outcome in variable.findall('OUTCOME')]
-                           for variable in self.network.findall('VARIABLE')}
+        variable_states = {
+            variable.find("NAME").text: [
+                outcome.text for outcome in variable.findall("OUTCOME")
+            ]
+            for variable in self.network.findall("VARIABLE")
+        }
         return variable_states
 
     def get_parents(self):
         """
         Returns the parents of the variables present in the network
 
         Examples
@@ -121,16 +130,20 @@
         >>> reader.get_parents()
         {'bowel-problem': [],
          'dog-out': ['family-out', 'bowel-problem'],
          'family-out': [],
          'hear-bark': ['dog-out'],
          'light-on': ['family-out']}
         """
-        variable_parents = {definition.find('FOR').text: [edge.text for edge in definition.findall('GIVEN')]
-                            for definition in self.network.findall('DEFINITION')}
+        variable_parents = {
+            definition.find("FOR").text: [
+                edge.text for edge in definition.findall("GIVEN")
+            ]
+            for definition in self.network.findall("DEFINITION")
+        }
         return variable_parents
 
     def get_values(self):
         """
         Returns the CPD of the variables present in the network
 
         Examples
@@ -144,21 +157,28 @@
          'family-out': array([[ 0.15],
                               [ 0.85]]),
          'hear-bark': array([[ 0.7 ,  0.3 ],
                              [ 0.01,  0.99]]),
          'light-on': array([[ 0.6 ,  0.4 ],
                             [ 0.05,  0.95]])}
         """
-        variable_CPD = {definition.find('FOR').text: list(map(float, table.text.split()))
-                        for definition in self.network.findall('DEFINITION')
-                        for table in definition.findall('TABLE')}
+        variable_CPD = {
+            definition.find("FOR").text: list(map(float, table.text.split()))
+            for definition in self.network.findall("DEFINITION")
+            for table in definition.findall("TABLE")
+        }
         for variable in variable_CPD:
             arr = np.array(variable_CPD[variable])
-            arr = arr.reshape((len(self.variable_states[variable]),
-                               arr.size // len(self.variable_states[variable])), order='F')
+            arr = arr.reshape(
+                (
+                    len(self.variable_states[variable]),
+                    arr.size // len(self.variable_states[variable]),
+                ),
+                order="F",
+            )
             variable_CPD[variable] = arr
         return variable_CPD
 
     def get_property(self):
         """
         Returns the property of the variable
 
@@ -168,49 +188,61 @@
         >>> reader.get_property()
         {'bowel-problem': ['position = (190, 69)'],
          'dog-out': ['position = (155, 165)'],
          'family-out': ['position = (112, 69)'],
          'hear-bark': ['position = (154, 241)'],
          'light-on': ['position = (73, 165)']}
         """
-        variable_property = {variable.find('NAME').text: [property.text for property in variable.findall('PROPERTY')]
-                             for variable in self.network.findall('VARIABLE')}
+        variable_property = {
+            variable.find("NAME").text: [
+                property.text for property in variable.findall("PROPERTY")
+            ]
+            for variable in self.network.findall("VARIABLE")
+        }
         return variable_property
 
     def get_model(self):
         model = BayesianModel()
         model.add_nodes_from(self.variables)
         model.add_edges_from(self.edge_list)
         model.name = self.network_name
 
         tabular_cpds = []
         for var, values in self.variable_CPD.items():
-            evidence_card = [len(self.variable_states[evidence_var]) for evidence_var in self.variable_parents[var]]
-            cpd = TabularCPD(var, len(self.variable_states[var]), values,
-                             evidence=self.variable_parents[var],
-                             evidence_card=evidence_card, state_names=self.get_states())
+            evidence_card = [
+                len(self.variable_states[evidence_var])
+                for evidence_var in self.variable_parents[var]
+            ]
+            cpd = TabularCPD(
+                var,
+                len(self.variable_states[var]),
+                values,
+                evidence=self.variable_parents[var],
+                evidence_card=evidence_card,
+                state_names=self.get_states(),
+            )
             tabular_cpds.append(cpd)
 
         model.add_cpds(*tabular_cpds)
 
         for node, properties in self.variable_property.items():
             for prop in properties:
                 if prop is not None:
-                    prop_name, prop_value = map(lambda t: t.strip(), prop.split('='))
-                    model.node[node][prop_name] = prop_value
+                    prop_name, prop_value = map(lambda t: t.strip(), prop.split("="))
+                    model.nodes[node][prop_name] = prop_value
 
         return model
 
 
 class XMLBIFWriter(object):
     """
     Base class for writing XMLBIF network file format.
     """
 
-    def __init__(self, model, encoding='utf-8', prettyprint=True):
+    def __init__(self, model, encoding="utf-8", prettyprint=True):
         """
         Initialise a XMLBIFWriter object.
 
         Parameters
         ----------
         model: BayesianModel Instance
             Model to write
@@ -226,20 +258,20 @@
         if not isinstance(model, BayesianModel):
             raise TypeError("model must an instance of BayesianModel")
         self.model = model
 
         self.encoding = encoding
         self.prettyprint = prettyprint
 
-        self.xml = etree.Element("BIF", attrib={'VERSION': '0.3'})
-        self.network = etree.SubElement(self.xml, 'NETWORK')
+        self.xml = etree.Element("BIF", attrib={"VERSION": "0.3"})
+        self.network = etree.SubElement(self.xml, "NETWORK")
         if self.model.name:
-            etree.SubElement(self.network, 'NAME').text = self.model.name
+            etree.SubElement(self.network, "NAME").text = self.model.name
         else:
-            etree.SubElement(self.network, 'NAME').text = "UNTITLED"
+            etree.SubElement(self.network, "NAME").text = "UNTITLED"
 
         self.variables = self.get_variables()
         self.states = self.get_states()
         self.properties = self.get_properties()
         self.definition = self.get_definition()
         self.tables = self.get_values()
 
@@ -289,15 +321,17 @@
          'hear-bark': <Element VARIABLE at 0x7fe28607de48>,
          'dog-out': <Element VARIABLE at 0x7fe28607ddc8>,
          'light-on': <Element VARIABLE at 0x7fe28607de88>}
         """
         variables = self.model.nodes()
         variable_tag = {}
         for var in sorted(variables):
-            variable_tag[var] = etree.SubElement(self.network, "VARIABLE", attrib={'TYPE': 'nature'})
+            variable_tag[var] = etree.SubElement(
+                self.network, "VARIABLE", attrib={"TYPE": "nature"}
+            )
             etree.SubElement(variable_tag[var], "NAME").text = var
         return variable_tag
 
     def get_states(self):
         """
         Add outcome to variables of XMLBIF
 
@@ -333,15 +367,19 @@
 
     def _make_valid_state_name(self, state_name):
         """Transform the input state_name into a valid state in XMLBIF.
         XMLBIF states must start with a letter an only contain letters,
         numbers and underscores.
         """
         s = str(state_name)
-        s_fixed = pp.CharsNotIn(pp.alphanums + "_").setParseAction(pp.replaceWith("_")).transformString(s)
+        s_fixed = (
+            pp.CharsNotIn(pp.alphanums + "_")
+            .setParseAction(pp.replaceWith("_"))
+            .transformString(s)
+        )
         if not s_fixed[0].isalpha():
             s_fixed = "state" + s_fixed
         return s_fixed
 
     def get_properties(self):
         """
         Add property to variables in XMLBIF
@@ -359,15 +397,15 @@
          'hear-bark': <Element PROPERTY at 0x7f7a2ffac188>,
          'bowel-problem': <Element PROPERTY at 0x7f7a2ffac0c8>,
          'dog-out': <Element PROPERTY at 0x7f7a2ffac108>}
         """
         variables = self.model.nodes()
         property_tag = {}
         for var in sorted(variables):
-            properties = self.model.node[var]
+            properties = self.model.nodes[var]
             property_tag[var] = etree.SubElement(self.variables[var], "PROPERTY")
             for prop, val in properties.items():
                 property_tag[var].text = str(prop) + " = " + str(val)
         return property_tag
 
     def get_definition(self):
         """
@@ -416,18 +454,20 @@
          'family-out': <Element TABLE at 0x7f240726f408>,
          'hear-bark': <Element TABLE at 0x7f240726f448>}
         """
         cpds = self.model.get_cpds()
         definition_tag = self.definition
         table_tag = {}
         for cpd in cpds:
-            table_tag[cpd.variable] = etree.SubElement(definition_tag[cpd.variable], "TABLE")
-            table_tag[cpd.variable].text = ''
+            table_tag[cpd.variable] = etree.SubElement(
+                definition_tag[cpd.variable], "TABLE"
+            )
+            table_tag[cpd.variable].text = ""
             for val in cpd.get_values().ravel(order="F"):
-                table_tag[cpd.variable].text += str(val) + ' '
+                table_tag[cpd.variable].text += str(val) + " "
 
         return table_tag
 
     def write_xmlbif(self, filename):
         """
         Write the xml data into the file.
 
@@ -436,9 +476,9 @@
         filename: Name of the file.
 
         Examples
         -------
         >>> writer = XMLBIFWriter(model)
         >>> writer.write_xmlbif(test_file)
         """
-        with open(filename, 'w') as fout:
+        with open(filename, "w") as fout:
             fout.write(self.__str__())
```

### Comparing `pgmpy-0.1.7/pgmpy/readwrite/BIF.py` & `pgmpy-0.1.9/pgmpy/readwrite/BIF.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,137 +1,188 @@
 import re
 import collections
 from string import Template
+from itertools import product
 
-import numpy
-from pyparsing import Word, alphanums, Suppress, Optional, CharsNotIn, Group, nums, ZeroOrMore, OneOrMore,\
-    cppStyleComment, printables
+import numpy as np
+from joblib import Parallel, delayed
+from pyparsing import (
+    Word,
+    alphanums,
+    Suppress,
+    Optional,
+    CharsNotIn,
+    Group,
+    nums,
+    ZeroOrMore,
+    OneOrMore,
+    cppStyleComment,
+    printables,
+)
 
 from pgmpy.models import BayesianModel
 from pgmpy.factors.discrete import TabularCPD
 from pgmpy.extern.six.moves import map, range
 
 
 class BIFReader(object):
 
     """
     Base class for reading network file in bif format
     """
 
-    def __init__(self, path=None, string=None):
+    def __init__(self, path=None, string=None, include_properties=False, n_jobs=-1):
         """
-        Initialisation of BifReader object
+        Initializes a BIFReader object.
 
         Parameters
-        ----------------
+        ----------
         path : file or str
-                File of bif data
+            File of bif data
+
         string : str
-                String of bif data
+            String of bif data
+
+        include_properties: boolean
+            If True, gets the properties tag from the file and stores in graph properties.
+
+        n_jobs: int (default: -1)
+            Number of jobs to run in parallel. `-1` means use all processors.
+
         Examples
-        -----------------
+        --------
         # dog-problem.bif file is present at
         # http://www.cs.cmu.edu/~javabayes/Examples/DogProblem/dog-problem.bif
         >>> from pgmpy.readwrite import BIFReader
         >>> reader = BIFReader("bif_test.bif")
         >>> reader = BIFReader("bif_test.bif")
         <pgmpy.readwrite.BIF.BIFReader object at 0x7f2375621cf8>
         """
         if path:
-            with open(path, 'r') as network:
+            with open(path, "r") as network:
                 self.network = network.read()
 
         elif string:
             self.network = string
 
         else:
             raise ValueError("Must specify either path or string")
 
+        self.n_jobs = n_jobs
+        self.include_properties = include_properties
+
         if '"' in self.network:
             # Replacing quotes by spaces to remove case sensitivity like:
             # "Dog-Problem" and Dog-problem
             # or "true""false" and "true" "false" and true false
-            self.network = self.network.replace('"', ' ')
+            self.network = self.network.replace('"', " ")
 
-        if '/*' in self.network or '//' in self.network:
-            self.network = cppStyleComment.suppress().transformString(self.network)  # removing comments from the file
-
-        self.name_expr, self.state_expr, self.property_expr = self.get_variable_grammar()
+        if "/*" in self.network or "//" in self.network:
+            self.network = cppStyleComment.suppress().transformString(
+                self.network
+            )  # removing comments from the file
+
+        self.name_expr, self.state_expr, self.property_expr = (
+            self.get_variable_grammar()
+        )
         self.probability_expr, self.cpd_expr = self.get_probability_grammar()
         self.network_name = self.get_network_name()
         self.variable_names = self.get_variables()
         self.variable_states = self.get_states()
-        self.variable_properties = self.get_property()
+        if self.include_properties:
+            self.variable_properties = self.get_property()
         self.variable_parents = self.get_parents()
         self.variable_cpds = self.get_values()
         self.variable_edges = self.get_edges()
 
     def get_variable_grammar(self):
         """
          A method that returns variable grammar
         """
         # Defining a expression for valid word
-        word_expr = Word(alphanums + '_' + '-')
-        word_expr2 = Word(initChars=printables, excludeChars=['{', '}', ',', ' '])
-        name_expr = Suppress('variable') + word_expr + Suppress('{')
+        word_expr = Word(alphanums + "_" + "-")
+        word_expr2 = Word(initChars=printables, excludeChars=["{", "}", ",", " "])
+        name_expr = Suppress("variable") + word_expr + Suppress("{")
         state_expr = ZeroOrMore(word_expr2 + Optional(Suppress(",")))
         # Defining a variable state expression
-        variable_state_expr = Suppress('type') + Suppress(word_expr) + Suppress('[') + Suppress(Word(nums)) + \
-            Suppress(']') + Suppress('{') + Group(state_expr) + Suppress('}') + Suppress(';')
+        variable_state_expr = (
+            Suppress("type")
+            + Suppress(word_expr)
+            + Suppress("[")
+            + Suppress(Word(nums))
+            + Suppress("]")
+            + Suppress("{")
+            + Group(state_expr)
+            + Suppress("}")
+            + Suppress(";")
+        )
         # variable states is of the form type description [args] { val1, val2 }; (comma may or may not be present)
 
-        property_expr = Suppress('property') + CharsNotIn(';') + Suppress(';')  # Creating a expr to find property
+        property_expr = (
+            Suppress("property") + CharsNotIn(";") + Suppress(";")
+        )  # Creating a expr to find property
 
         return name_expr, variable_state_expr, property_expr
 
     def get_probability_grammar(self):
         """
         A method that returns probability grammar
         """
         # Creating valid word expression for probability, it is of the format
         # wor1 | var2 , var3 or var1 var2 var3 or simply var
-        word_expr = Word(alphanums + '-' + '_') + Suppress(Optional("|")) + Suppress(Optional(","))
-        word_expr2 = Word(initChars=printables, excludeChars=[',', ')', ' ', '(']) + Suppress(Optional(","))
+        word_expr = (
+            Word(alphanums + "-" + "_")
+            + Suppress(Optional("|"))
+            + Suppress(Optional(","))
+        )
+        word_expr2 = Word(
+            initChars=printables, excludeChars=[",", ")", " ", "("]
+        ) + Suppress(Optional(","))
         # creating an expression for valid numbers, of the format
         # 1.00 or 1 or 1.00. 0.00 or 9.8e-5 etc
-        num_expr = Word(nums + '-' + '+' + 'e' + 'E' + '.') + Suppress(Optional(","))
-        probability_expr = Suppress('probability') + Suppress('(') + OneOrMore(word_expr) + Suppress(')')
-        optional_expr = Suppress('(') + Suppress(OneOrMore(word_expr2)) + Suppress(')')
-        probab_attributes = optional_expr | Suppress('table')
+        num_expr = Word(nums + "-" + "+" + "e" + "E" + ".") + Suppress(Optional(","))
+        probability_expr = (
+            Suppress("probability")
+            + Suppress("(")
+            + OneOrMore(word_expr)
+            + Suppress(")")
+        )
+        optional_expr = Suppress("(") + OneOrMore(word_expr2) + Suppress(")")
+        probab_attributes = optional_expr | Suppress("table")
         cpd_expr = probab_attributes + OneOrMore(num_expr)
 
         return probability_expr, cpd_expr
 
     def variable_block(self):
-        start = re.finditer('variable', self.network)
+        start = re.finditer("variable", self.network)
         for index in start:
-            end = self.network.find('}\n', index.start())
-            yield self.network[index.start():end]
+            end = self.network.find("}\n", index.start())
+            yield self.network[index.start() : end]
 
     def probability_block(self):
-        start = re.finditer('probability', self.network)
+        start = re.finditer("probability", self.network)
         for index in start:
-            end = self.network.find('}\n', index.start())
-            yield self.network[index.start():end]
+            end = self.network.find("}\n", index.start())
+            yield self.network[index.start() : end]
 
     def get_network_name(self):
         """
         Retruns the name of the network
 
         Example
         ---------------
         >>> from pgmpy.readwrite import BIFReader
         >>> reader = BIF.BifReader("bif_test.bif")
         >>> reader.network_name()
         'Dog-Problem'
         """
-        start = self.network.find('network')
-        end = self.network.find('}\n', start)
+        start = self.network.find("network")
+        end = self.network.find("}\n", start)
         # Creating a network attribute
-        network_attribute = Suppress('network') + Word(alphanums + '_' + '-') + '{'
+        network_attribute = Suppress("network") + Word(alphanums + "_" + "-") + "{"
         network_name = network_attribute.searchString(self.network[start:end])[0][0]
 
         return network_name
 
     def get_variables(self):
         """
         Returns list of variables of the network
@@ -207,18 +258,44 @@
         'dog-out': ['family-out', 'bowel-problem'],
         'family-out': [],
         'hear-bark': ['dog-out'],
         'light-on': ['family-out']}
         """
         variable_parents = {}
         for block in self.probability_block():
-            names = self.probability_expr.searchString(block.split('\n')[0])[0]
+            names = self.probability_expr.searchString(block.split("\n")[0])[0]
             variable_parents[names[0]] = names[1:]
         return variable_parents
 
+    def _get_values_from_block(self, block):
+        names = self.probability_expr.searchString(block)
+        var_name, parents = names[0][0], names[0][1:]
+        cpds = self.cpd_expr.searchString(block)
+        if "table" in block:
+            arr = np.array([float(j) for i in cpds for j in i])
+            arr = arr.reshape(
+                (
+                    len(self.variable_states[var_name]),
+                    arr.size // len(self.variable_states[var_name]),
+                )
+            )
+        else:
+            arr_length = np.prod([len(self.variable_states[var]) for var in parents])
+            arr = np.zeros((len(self.variable_states[var_name]), arr_length))
+            values_dict = {}
+            for prob_line in cpds:
+                states = prob_line[: len(parents)]
+                vals = [float(i) for i in prob_line[len(parents) :]]
+                values_dict[tuple(states)] = vals
+            for index, combination in enumerate(
+                product(*[self.variable_states[var] for var in parents])
+            ):
+                arr[:, index] = values_dict[combination]
+        return var_name, arr
+
     def get_values(self):
         """
         Returns the CPD of the variables present in the network
 
         Example
         --------
         >>> from pgmpy.readwrite import BIFReader
@@ -230,32 +307,22 @@
                             [0.01, 0.03, 0.1, 0.7]]),
         'family-out': np.array([[0.15],
                                 [0.85]]),
         'hear-bark': np.array([[0.7, 0.01],
                                 [0.3, 0.99]]),
         'light-on': np.array([[0.6, 0.05],
                             [0.4, 0.95]])}
-         """
+        """
+        cpd_values = Parallel(n_jobs=self.n_jobs)(
+            delayed(self._get_values_from_block)(block)
+            for block in self.probability_block()
+        )
         variable_cpds = {}
-        for block in self.probability_block():
-            name = self.probability_expr.searchString(block)[0][0]
-            cpds = self.cpd_expr.searchString(block)
-            arr = [float(j) for i in cpds for j in i]
-            if 'table' in block:
-                arr = numpy.array(arr)
-                arr = arr.reshape((len(self.variable_states[name]),
-                                   arr.size // len(self.variable_states[name])))
-            else:
-                length = len(self.variable_states[name])
-                reshape_arr = [[] for i in range(length)]
-                for i, val in enumerate(arr):
-                    reshape_arr[i % length].append(val)
-                arr = reshape_arr
-                arr = numpy.array(arr)
-            variable_cpds[name] = arr
+        for var_name, arr in cpd_values:
+            variable_cpds[var_name] = arr
 
         return variable_cpds
 
     def get_edges(self):
         """
         Returns the edges of the network
 
@@ -265,16 +332,19 @@
         >>> reader = BIFReader("bif_test.bif")
         >>> reader.get_edges()
         [['family-out', 'light-on'],
          ['family-out', 'dog-out'],
          ['bowel-problem', 'dog-out'],
          ['dog-out', 'hear-bark']]
         """
-        edges = [[value, key] for key in self.variable_parents.keys()
-                 for value in self.variable_parents[key]]
+        edges = [
+            [value, key]
+            for key in self.variable_parents.keys()
+            for value in self.variable_parents[key]
+        ]
         return edges
 
     def get_model(self):
         """
         Returns the fitted bayesian model
 
         Example
@@ -289,30 +359,48 @@
             model.add_nodes_from(self.variable_names)
             model.add_edges_from(self.variable_edges)
             model.name = self.network_name
 
             tabular_cpds = []
             for var in sorted(self.variable_cpds.keys()):
                 values = self.variable_cpds[var]
-                cpd = TabularCPD(var, len(self.variable_states[var]), values,
-                                 evidence=self.variable_parents[var],
-                                 evidence_card=[len(self.variable_states[evidence_var])
-                                                for evidence_var in self.variable_parents[var]])
+                sn = {
+                    p_var: self.variable_states[p_var]
+                    for p_var in self.variable_parents[var]
+                }
+                sn[var] = self.variable_states[var]
+                cpd = TabularCPD(
+                    var,
+                    len(self.variable_states[var]),
+                    values,
+                    evidence=self.variable_parents[var],
+                    evidence_card=[
+                        len(self.variable_states[evidence_var])
+                        for evidence_var in self.variable_parents[var]
+                    ],
+                    state_names=sn,
+                )
                 tabular_cpds.append(cpd)
 
             model.add_cpds(*tabular_cpds)
-            for node, properties in self.variable_properties.items():
-                for prop in properties:
-                    prop_name, prop_value = map(lambda t: t.strip(), prop.split('='))
-                    model.node[node][prop_name] = prop_value
+
+            if self.include_properties:
+                for node, properties in self.variable_properties.items():
+                    for prop in properties:
+                        prop_name, prop_value = map(
+                            lambda t: t.strip(), prop.split("=")
+                        )
+                        model.nodes[node][prop_name] = prop_value
 
             return model
 
         except AttributeError:
-            raise AttributeError('First get states of variables, edges, parents and network name')
+            raise AttributeError(
+                "First get states of variables, edges, parents and network name"
+            )
 
 
 class BIFWriter(object):
 
     """
     Base class for writing BIF network file format
     """
@@ -332,70 +420,86 @@
         >>> writer
         <writer_BIF.BIFWriter at 0x7f05e5ea27b8>
         """
         if not isinstance(model, BayesianModel):
             raise TypeError("model must be an instance of BayesianModel")
         self.model = model
         if not self.model.name:
-            self.network_name = 'unknown'
+            self.network_name = "unknown"
         else:
             self.network_name = self.model.name
         self.variable_states = self.get_states()
         self.property_tag = self.get_properties()
         self.variable_parents = self.get_parents()
         self.tables = self.get_cpds()
 
     def BIF_templates(self):
         """
         Create template for writing in BIF format
         """
-        network_template = Template('network $name {\n}\n')
+        network_template = Template("network $name {\n}\n")
         # property tag may or may not be present in model,and since no of properties
         # can be more than one , will replace them accoriding to format otherwise null
-        variable_template = Template("""variable $name {
+        variable_template = Template(
+            """variable $name {
     type discrete [ $no_of_states ] { $states };
-$properties}\n""")
-        property_template = Template('    property $prop ;\n')
+$properties}\n"""
+        )
+        property_template = Template("    property $prop ;\n")
         # $variable_ here is name of variable, used underscore for clarity
-        probability_template = Template("""probability ( $variable_$seprator_$parents ) {
+        probability_template = Template(
+            """probability ( $variable_$seprator_$parents ) {
     table $values ;
-}\n""")
-        return network_template, variable_template, property_template, probability_template
+}\n"""
+        )
+        return (
+            network_template,
+            variable_template,
+            property_template,
+            probability_template,
+        )
 
     def __str__(self):
         """
         Returns the BIF format as string
         """
-        network_template, variable_template, property_template, probability_template = self.BIF_templates()
-        network = ''
+        network_template, variable_template, property_template, probability_template = (
+            self.BIF_templates()
+        )
+        network = ""
         network += network_template.substitute(name=self.network_name)
         variables = self.model.nodes()
 
         for var in sorted(variables):
             no_of_states = str(len(self.variable_states[var]))
-            states = ', '.join(self.variable_states[var])
+            states = ", ".join(self.variable_states[var])
             if not self.property_tag[var]:
-                properties = ''
+                properties = ""
             else:
-                properties = ''
+                properties = ""
                 for prop_val in self.property_tag[var]:
                     properties += property_template.substitute(prop=prop_val)
-            network += variable_template.substitute(name=var, no_of_states=no_of_states,
-                                                    states=states, properties=properties)
+            network += variable_template.substitute(
+                name=var,
+                no_of_states=no_of_states,
+                states=states,
+                properties=properties,
+            )
 
         for var in sorted(variables):
             if not self.variable_parents[var]:
-                parents = ''
-                seprator = ''
+                parents = ""
+                seprator = ""
             else:
-                parents = ', '.join(self.variable_parents[var])
-                seprator = ' | '
-            cpd = ', '.join(map(str, self.tables[var]))
-            network += probability_template.substitute(variable_=var, seprator_=seprator,
-                                                       parents=parents, values=cpd)
+                parents = ", ".join(self.variable_parents[var])
+                seprator = " | "
+            cpd = ", ".join(map(str, self.tables[var]))
+            network += probability_template.substitute(
+                variable_=var, seprator_=seprator, parents=parents, values=cpd
+            )
 
         return network
 
     def get_variables(self):
         """
         Add variables to BIF
 
@@ -436,15 +540,15 @@
         """
         variable_states = {}
         cpds = self.model.get_cpds()
         for cpd in cpds:
             variable = cpd.variable
             variable_states[variable] = []
             for state in range(cpd.get_cardinality([variable])[variable]):
-                variable_states[variable].append(str(variable) + '_' + str(state))
+                variable_states[variable].append(str(variable) + "_" + str(state))
         return variable_states
 
     def get_properties(self):
         """
         Add property to variables in BIF
 
         Returns
@@ -462,15 +566,15 @@
          'family-out': ['position = (257, 99)'],
          'hear-bark': ['position = (296, 268)'],
          'light-on': ['position = (218, 195)']}
         """
         variables = self.model.nodes()
         property_tag = {}
         for variable in sorted(variables):
-            properties = self.model.node[variable]
+            properties = self.model.nodes[variable]
             properties = collections.OrderedDict(sorted(properties.items()))
             property_tag[variable] = []
             for prop, val in properties.items():
                 property_tag[variable].append(str(prop) + " = " + str(val))
         return property_tag
 
     def get_parents(self):
@@ -540,9 +644,9 @@
         -------
         >>> from pgmpy.readwrite import BIFReader, BIFWriter
         >>> model = BIFReader('dog-problem.bif').get_model()
         >>> writer = BIFWriter(model)
         >>> writer.write_bif(filname='test_file.bif')
         """
         writer = self.__str__()
-        with open(filename, 'w') as fout:
+        with open(filename, "w") as fout:
             fout.write(writer)
```

### Comparing `pgmpy-0.1.7/pgmpy/readwrite/ProbModelXML.py` & `pgmpy-0.1.9/pgmpy/readwrite/ProbModelXML.py`

 * *Files 12% similar despite different names*

```diff
@@ -103,14 +103,15 @@
                 numericValue=number/>
         </EvidenceCase>
     </Evidence>
 </ProbModelXML>
 """
 
 import warnings
+
 try:
     from lxml import etree
 except ImportError:
     try:
         import xml.etree.ElementTree as etree
     except ImportError:
         # import xml.etree.cElementTree as etree
@@ -127,15 +128,15 @@
 from pgmpy.extern import six
 from pgmpy.extern.six.moves import map
 
 # warnings.warn("Not Complete. Please use only for "
 #               "reading and writing Bayesian Models.")
 
 
-def generate_probmodelxml(model, encoding='utf-8', prettyprint=True):
+def generate_probmodelxml(model, encoding="utf-8", prettyprint=True):
     """
     Generate ProbModelXML lines for model.
 
     Parameters
     ----------
     model : Graph
         The Bayesian or Markov Model
@@ -153,15 +154,15 @@
     """
     writer = ProbModelXMLWriter(G, encoding=encoding, prettyprint=prettyprint)
     for line in str(writer).splitlines():
         yield line
 
 
 # @open_file(1, mode='wb')
-def write_probmodelxml(model, path, encoding='utf-8', prettyprint=True):
+def write_probmodelxml(model, path, encoding="utf-8", prettyprint=True):
     """
     Write model in ProbModelXML format to path.
 
     Parameters
     ----------
     model : A NetworkX graph
             Bayesian network or Markov network
@@ -174,16 +175,15 @@
             If True use line breaks and indenting in output XML.
 
     Examples
     --------
     >>> G = nx.path_graph(4)
     >>> pgmpy.readwrite.write_probmodelxml(G, "test.probmodelxml")
     """
-    writer = ProbModelXMLWriter(model, path, encoding=encoding,
-                                prettyprint=prettyprint)
+    writer = ProbModelXMLWriter(model, path, encoding=encoding, prettyprint=prettyprint)
     writer.dump(path)
 
 
 # @open_file(0, mode='rb')
 def read_probmodelxml(path):
     """
     Read model in ProbModelXML format from path.
@@ -250,48 +250,51 @@
     Examples
     --------
     >>> model_data = pgmpy.readwrite.get_model_data(model)
     >>> writer.get_model_data(model)
     """
     if not isinstance(model, BayesianModel):
         raise TypeError("Model must an instance of BayesianModel.")
-    model_data = {'probnet': {'type': 'BayesianNetwork', 'Variables': {}}}
+    model_data = {"probnet": {"type": "BayesianNetwork", "Variables": {}}}
 
     variables = model.nodes()
     for var in variables:
-        model_data['probnet']['Variables'][var] = model.node[var]
+        model_data["probnet"]["Variables"][var] = model.nodes[var]
 
-    model_data['probnet']['edges'] = {}
+    model_data["probnet"]["edges"] = {}
     edges = model.edges()
     for edge in edges:
-        model_data['probnet']['edges'][str(edge)] = model.edge[edge[0]][edge[1]]
+        model_data["probnet"]["edges"][str(edge)] = model.adj[edge[0]][edge[1]]
 
-    model_data['probnet']['Potentials'] = []
+    model_data["probnet"]["Potentials"] = []
     cpds = model.get_cpds()
     for cpd in cpds:
         potential_dict = {}
-        potential_dict['Variables'] = {}
+        potential_dict["Variables"] = {}
         evidence = cpd.variables[:0:-1]
         if evidence:
-            potential_dict['Variables'][cpd.variable] = evidence
+            potential_dict["Variables"][cpd.variable] = evidence
         else:
-            potential_dict['Variables'][cpd.variable] = []
-        potential_dict['type'] = "Table"
-        potential_dict['role'] = "conditionalProbability"
-        potential_dict['Values'] = " ".join([str(val) for val in cpd.values.ravel().astype(float)]) + " "
-        model_data['probnet']['Potentials'].append(potential_dict)
+            potential_dict["Variables"][cpd.variable] = []
+        potential_dict["type"] = "Table"
+        potential_dict["role"] = "conditionalProbability"
+        potential_dict["Values"] = (
+            " ".join([str(val) for val in cpd.values.ravel().astype(float)]) + " "
+        )
+        model_data["probnet"]["Potentials"].append(potential_dict)
 
     return model_data
 
 
 class ProbModelXMLWriter(object):
     """
     Class for writing models in ProbModelXML format.
     """
-    def __init__(self, model_data, encoding='utf-8', prettyprint=True):
+
+    def __init__(self, model_data, encoding="utf-8", prettyprint=True):
         """
         Initialize a ProbModelXMLWriter Object.
 
         Parameters
         ----------
         model : A BayesianModel or MarkovModel
             The model to write.
@@ -311,55 +314,63 @@
         # TODO: add policies, InferenceOptions, Evidence
         # TODO: add parsing of language and comments and additional properties
         self.data = model_data
         self.encoding = encoding
         self.prettyprint = prettyprint
 
         # Creating initial tags
-        self.xml = etree.Element("ProbModelXML", attrib={'formatVersion': '1.0'})
-        self.probnet = etree.SubElement(self.xml, 'ProbNet')
-        self.variables = etree.SubElement(self.probnet, 'Variables')
-        self.links = etree.SubElement(self.probnet, 'Links')
-        self.potentials = etree.SubElement(self.probnet, 'Potentials')
-        self.additional_constraints = etree.SubElement(self.probnet, 'AdditionalConstraints')
+        self.xml = etree.Element("ProbModelXML", attrib={"formatVersion": "1.0"})
+        self.probnet = etree.SubElement(self.xml, "ProbNet")
+        self.variables = etree.SubElement(self.probnet, "Variables")
+        self.links = etree.SubElement(self.probnet, "Links")
+        self.potentials = etree.SubElement(self.probnet, "Potentials")
+        self.additional_constraints = etree.SubElement(
+            self.probnet, "AdditionalConstraints"
+        )
 
         # adding information for probnet
-        self.probnet.attrib['type'] = self.data['probnet']['type']
+        self.probnet.attrib["type"] = self.data["probnet"]["type"]
         try:
-            etree.SubElement(self.probnet, 'Language').text = self.data['probnet']['Language']
+            etree.SubElement(self.probnet, "Language").text = self.data["probnet"][
+                "Language"
+            ]
         except KeyError:
             pass
         try:
-            etree.SubElement(self.probnet, 'Comment').text = self.data['probnet']['Comment']
+            etree.SubElement(self.probnet, "Comment").text = self.data["probnet"][
+                "Comment"
+            ]
         except KeyError:
             pass
         try:
-            self._add_additional_properties(self.xml, self.data['probnet']['AdditionalProperties'])
+            self._add_additional_properties(
+                self.xml, self.data["probnet"]["AdditionalProperties"]
+            )
         except KeyError:
-            etree.SubElement(self.probnet, 'AdditionalProperties')
+            etree.SubElement(self.probnet, "AdditionalProperties")
         try:
-            self._add_decision_criteria(self.data['probnet']['DecisionCriteria'])
+            self._add_decision_criteria(self.data["probnet"]["DecisionCriteria"])
         except KeyError:
-            etree.SubElement(self.probnet, 'DecisionCriteria')
+            etree.SubElement(self.probnet, "DecisionCriteria")
 
         # Add Additional Constraints
-        if 'AdditionalConstraints' in self.data['probnet']:
-            for constraint in sorted(self.data['probnet']['AdditionalConstraints']):
+        if "AdditionalConstraints" in self.data["probnet"]:
+            for constraint in sorted(self.data["probnet"]["AdditionalConstraints"]):
                 self._add_constraint(constraint)
 
         # Add variables
-        for variable in sorted(self.data['probnet']['Variables']):
+        for variable in sorted(self.data["probnet"]["Variables"]):
             self._add_variable(variable)
 
         # Add edges
-        for edge in sorted(self.data['probnet']['edges']):
+        for edge in sorted(self.data["probnet"]["edges"]):
             self._add_link(edge)
 
         # Add Potentials
-        for potential in self.data['probnet']['Potentials']:
+        for potential in self.data["probnet"]["Potentials"]:
             self._add_potential(potential, self.potentials)
 
     def __str__(self):
         """
         Return the XML as string.
         """
         if self.prettyprint:
@@ -367,83 +378,107 @@
         return etree.tostring(self.xml, encoding=self.encoding)
 
     @staticmethod
     def _add_additional_properties(position, properties_dict):
         """
         Sets AdditionalProperties of the ProbModelXML.
         """
-        add_prop = etree.SubElement(position, 'AdditionalProperties')
+        add_prop = etree.SubElement(position, "AdditionalProperties")
         for key, value in properties_dict.items():
-            etree.SubElement(add_prop, 'Property', attrib={'name': key, 'value': value})
+            etree.SubElement(add_prop, "Property", attrib={"name": key, "value": value})
 
     def _add_variable(self, variable):
         """
         Adds a node to the ProbModelXML.
         """
         # TODO: Add feature for accepting additional properties of states.
-        variable_data = self.data['probnet']['Variables'][variable]
-        variable_element = etree.SubElement(self.variables, 'Variable', attrib={'name': variable,
-                                                                                'type': variable_data['type'],
-                                                                                'role': variable_data['role']})
+        variable_data = self.data["probnet"]["Variables"][variable]
+        variable_element = etree.SubElement(
+            self.variables,
+            "Variable",
+            attrib={
+                "name": variable,
+                "type": variable_data["type"],
+                "role": variable_data["role"],
+            },
+        )
 
         try:
-            etree.SubElement(variable_element, 'Comment').text = variable_data['Comment']
+            etree.SubElement(variable_element, "Comment").text = variable_data[
+                "Comment"
+            ]
         except KeyError:
             pass
         try:
-            etree.SubElement(variable_element, 'Coordinates', variable_data['Coordinates'])
+            etree.SubElement(
+                variable_element, "Coordinates", variable_data["Coordinates"]
+            )
         except KeyError:
             pass
 
         try:
-            for key, value in sorted(variable_data['AdditionalProperties'].items()):
-                etree.SubElement(variable_element, 'Property', attrib={'name': key, 'value': value})
+            for key, value in sorted(variable_data["AdditionalProperties"].items()):
+                etree.SubElement(
+                    variable_element, "Property", attrib={"name": key, "value": value}
+                )
         except KeyError:
-            etree.SubElement(variable_element, 'AdditionalProperties')
-        states = etree.SubElement(variable_element, 'States')
-        for s in sorted(variable_data['States']):
-            state = etree.SubElement(states, 'State', attrib={'name': s})
+            etree.SubElement(variable_element, "AdditionalProperties")
+        states = etree.SubElement(variable_element, "States")
+        for s in sorted(variable_data["States"]):
+            state = etree.SubElement(states, "State", attrib={"name": s})
             try:
-                self._add_additional_properties(state, variable_data['States'][s]['AdditionalProperties'])
+                self._add_additional_properties(
+                    state, variable_data["States"][s]["AdditionalProperties"]
+                )
             except KeyError:
-                etree.SubElement(state, 'AdditionalProperties')
+                etree.SubElement(state, "AdditionalProperties")
 
     def _add_link(self, edge):
         """
         Adds an edge to the ProbModelXML.
         """
-        edge_data = self.data['probnet']['edges'][edge]
+        edge_data = self.data["probnet"]["edges"][edge]
         if isinstance(edge, six.string_types):
             edge = eval(edge)
-        link = etree.SubElement(self.links, 'Link', attrib={'var1': edge[0], 'var2': edge[1],
-                                                            'directed': edge_data['directed']})
+        link = etree.SubElement(
+            self.links,
+            "Link",
+            attrib={
+                "var1": edge[0],
+                "var2": edge[1],
+                "directed": edge_data["directed"],
+            },
+        )
         try:
-            etree.SubElement(link, 'Comment').text = edge_data['Comment']
+            etree.SubElement(link, "Comment").text = edge_data["Comment"]
         except KeyError:
             pass
         try:
-            etree.SubElement(link, 'Label').text = edge_data['Label']
+            etree.SubElement(link, "Label").text = edge_data["Label"]
         except KeyError:
             pass
         try:
-            self._add_additional_properties(link, edge_data['AdditionalProperties'])
+            self._add_additional_properties(link, edge_data["AdditionalProperties"])
         except KeyError:
-            etree.SubElement(link, 'AdditionalProperties')
+            etree.SubElement(link, "AdditionalProperties")
 
     def _add_constraint(self, constraint):
         """
         Adds constraint to the ProbModelXML.
         """
-        constraint_data = self.data['probnet']['AdditionalConstraints'][constraint]
+        constraint_data = self.data["probnet"]["AdditionalConstraints"][constraint]
         constraint_element = etree.SubElement(
-            self.additional_constraints, 'Constraint', attrib={'name': constraint})
+            self.additional_constraints, "Constraint", attrib={"name": constraint}
+        )
         for argument in sorted(constraint_data):
             name = argument
             value = constraint_data[name]
-            etree.SubElement(constraint_element, 'Argument', attrib={'name': name, 'value': value})
+            etree.SubElement(
+                constraint_element, "Argument", attrib={"name": name, "value": value}
+            )
 
     def _add_decision_criteria(self, criteria_dict):
         """
         Adds Decision Criteria to the ProbModelXML.
 
         Parameters
         ----------
@@ -452,17 +487,19 @@
             For example: {'effectiveness': {}, 'cost': {}}
 
         Examples
         -------
         >>> writer = ProbModelXMLWriter(model)
         >>> writer._add_decision_criteria(criteria_dict)
         """
-        decision_tag = etree.SubElement(self.xml, 'DecisionCriteria', attrib={})
+        decision_tag = etree.SubElement(self.xml, "DecisionCriteria", attrib={})
         for criteria in sorted(criteria_dict):
-            criteria_tag = etree.SubElement(decision_tag, 'Criterion', attrib={'name': criteria})
+            criteria_tag = etree.SubElement(
+                decision_tag, "Criterion", attrib={"name": criteria}
+            )
             self._add_additional_properties(criteria_tag, criteria_dict[criteria])
 
     def _add_potential(self, potential, parent_tag):
         """
         Adds Potentials to the ProbModelXML.
 
         Parameters
@@ -481,86 +518,124 @@
                          <Element Subpotentials at 0x7f315fc44e48>
 
         Examples
         --------
         >>> writer = ProbModelXMLWriter(model)
         >>> writer._add_potential(potential, parent_tag)
         """
-        potential_type = potential['type']
+        potential_type = potential["type"]
         try:
-            potential_tag = etree.SubElement(parent_tag, 'Potential', attrib={
-                'type': potential['type'], 'role': potential['role']})
+            potential_tag = etree.SubElement(
+                parent_tag,
+                "Potential",
+                attrib={"type": potential["type"], "role": potential["role"]},
+            )
         except KeyError:
-            potential_tag = etree.SubElement(parent_tag, 'Potential', attrib={
-                'type': potential['type']})
-        self._add_element(potential, 'Comment', potential_tag)
-        if 'AdditionalProperties' in potential:
-            self._add_additional_properties(potential_tag, potential['AdditionalProperties'])
+            potential_tag = etree.SubElement(
+                parent_tag, "Potential", attrib={"type": potential["type"]}
+            )
+        self._add_element(potential, "Comment", potential_tag)
+        if "AdditionalProperties" in potential:
+            self._add_additional_properties(
+                potential_tag, potential["AdditionalProperties"]
+            )
         if potential_type == "delta":
-            etree.SubElement(potential_tag, 'Variable', attrib={'name': potential['Variable']})
-            self._add_element(potential, 'State', potential_tag)
-            self._add_element(potential, 'StateIndex', potential_tag)
-            self._add_element(potential, 'NumericValue', potential_tag)
+            etree.SubElement(
+                potential_tag, "Variable", attrib={"name": potential["Variable"]}
+            )
+            self._add_element(potential, "State", potential_tag)
+            self._add_element(potential, "StateIndex", potential_tag)
+            self._add_element(potential, "NumericValue", potential_tag)
         else:
-            if 'UtilityVariable' in potential:
-                etree.SubElement(potential_tag, 'UtilityVariable', attrib={
-                    'name': potential['UtilityVariable']})
-            if 'Variables' in potential:
-                variable_tag = etree.SubElement(potential_tag, 'Variables')
-                for var in sorted(potential['Variables']):
-                    etree.SubElement(variable_tag, 'Variable', attrib={'name': var})
-                    for child in sorted(potential['Variables'][var]):
-                        etree.SubElement(variable_tag, 'Variable', attrib={'name': child})
-            self._add_element(potential, 'Values', potential_tag)
-            if 'UncertainValues' in potential:
-                value_tag = etree.SubElement(potential_tag, 'UncertainValues', attrib={})
-                for value in sorted(potential['UncertainValues']):
+            if "UtilityVariable" in potential:
+                etree.SubElement(
+                    potential_tag,
+                    "UtilityVariable",
+                    attrib={"name": potential["UtilityVariable"]},
+                )
+            if "Variables" in potential:
+                variable_tag = etree.SubElement(potential_tag, "Variables")
+                for var in sorted(potential["Variables"]):
+                    etree.SubElement(variable_tag, "Variable", attrib={"name": var})
+                    for child in sorted(potential["Variables"][var]):
+                        etree.SubElement(
+                            variable_tag, "Variable", attrib={"name": child}
+                        )
+            self._add_element(potential, "Values", potential_tag)
+            if "UncertainValues" in potential:
+                value_tag = etree.SubElement(
+                    potential_tag, "UncertainValues", attrib={}
+                )
+                for value in sorted(potential["UncertainValues"]):
                     try:
-                        etree.SubElement(value_tag, 'Value', attrib={
-                            'distribution': value['distribution'],
-                            'name': value['name']}).text = value['value']
+                        etree.SubElement(
+                            value_tag,
+                            "Value",
+                            attrib={
+                                "distribution": value["distribution"],
+                                "name": value["name"],
+                            },
+                        ).text = value["value"]
                     except KeyError:
-                        etree.SubElement(value_tag, 'Value', attrib={
-                            'distribution': value['distribution']}).text = value['value']
-            if 'TopVariable' in potential:
-                etree.SubElement(potential_tag, 'TopVariable', attrib={'name': potential['TopVariable']})
-            if 'Branches' in potential:
-                branches_tag = etree.SubElement(potential_tag, 'Branches')
-                for branch in potential['Branches']:
-                    branch_tag = etree.SubElement(branches_tag, 'Branch')
-                    if 'States' in branch:
-                        states_tag = etree.SubElement(branch_tag, 'States')
-                        for state in sorted(branch['States']):
-                            etree.SubElement(states_tag, 'State', attrib={'name': state['name']})
-                    if 'Potential' in branch:
-                        self._add_potential(branch['Potential'], branch_tag)
-                    self._add_element(potential, 'Label', potential_tag)
-                    self._add_element(potential, 'Reference', potential_tag)
-                    if 'Thresholds' in branch:
-                        thresholds_tag = etree.SubElement(branch_tag, 'Thresholds')
-                        for threshold in branch['Thresholds']:
+                        etree.SubElement(
+                            value_tag,
+                            "Value",
+                            attrib={"distribution": value["distribution"]},
+                        ).text = value["value"]
+            if "TopVariable" in potential:
+                etree.SubElement(
+                    potential_tag,
+                    "TopVariable",
+                    attrib={"name": potential["TopVariable"]},
+                )
+            if "Branches" in potential:
+                branches_tag = etree.SubElement(potential_tag, "Branches")
+                for branch in potential["Branches"]:
+                    branch_tag = etree.SubElement(branches_tag, "Branch")
+                    if "States" in branch:
+                        states_tag = etree.SubElement(branch_tag, "States")
+                        for state in sorted(branch["States"]):
+                            etree.SubElement(
+                                states_tag, "State", attrib={"name": state["name"]}
+                            )
+                    if "Potential" in branch:
+                        self._add_potential(branch["Potential"], branch_tag)
+                    self._add_element(potential, "Label", potential_tag)
+                    self._add_element(potential, "Reference", potential_tag)
+                    if "Thresholds" in branch:
+                        thresholds_tag = etree.SubElement(branch_tag, "Thresholds")
+                        for threshold in branch["Thresholds"]:
                             try:
-                                etree.SubElement(thresholds_tag, 'Threshold', attrib={
-                                    'value': threshold['value'], 'belongsTo': threshold['belongsTo']})
+                                etree.SubElement(
+                                    thresholds_tag,
+                                    "Threshold",
+                                    attrib={
+                                        "value": threshold["value"],
+                                        "belongsTo": threshold["belongsTo"],
+                                    },
+                                )
                             except KeyError:
-                                etree.SubElement(thresholds_tag, 'Threshold', attrib={
-                                    'value': threshold['value']})
-            self._add_element(potential, 'Model', potential_tag)
-            self._add_element(potential, 'Coefficients', potential_tag)
-            self._add_element(potential, 'CovarianceMatrix', potential_tag)
-            if 'Subpotentials' in potential:
-                subpotentials = etree.SubElement(potential_tag, 'Subpotentials')
-                for subpotential in potential['Subpotentials']:
+                                etree.SubElement(
+                                    thresholds_tag,
+                                    "Threshold",
+                                    attrib={"value": threshold["value"]},
+                                )
+            self._add_element(potential, "Model", potential_tag)
+            self._add_element(potential, "Coefficients", potential_tag)
+            self._add_element(potential, "CovarianceMatrix", potential_tag)
+            if "Subpotentials" in potential:
+                subpotentials = etree.SubElement(potential_tag, "Subpotentials")
+                for subpotential in potential["Subpotentials"]:
                     self._add_potential(subpotential, subpotentials)
-            if 'Potential' in potential:
-                self._add_potential(potential['Potential'], potential_tag)
-            if 'NumericVariables' in potential:
-                numvar_tag = etree.SubElement(potential_tag, 'NumericVariables')
-                for var in sorted(potential['NumericVariables']):
-                    etree.SubElement(numvar_tag, 'Variable', attrib={'name': var})
+            if "Potential" in potential:
+                self._add_potential(potential["Potential"], potential_tag)
+            if "NumericVariables" in potential:
+                numvar_tag = etree.SubElement(potential_tag, "NumericVariables")
+                for var in sorted(potential["NumericVariables"]):
+                    etree.SubElement(numvar_tag, "Variable", attrib={"name": var})
 
     @staticmethod
     def _add_element(potential, var, potential_tag):
         """
         Helper function to add variable tag to the potential_tag
 
         Parameters
@@ -600,22 +675,22 @@
         stream.write(header.encode(self.encoding))
         document.write(stream, encoding=self.encoding)
 
     def indent(self, elem, level=0):
         """
         Inplace prettyprint formatter.
         """
-        i = "\n" + level*"  "
+        i = "\n" + level * "  "
         if len(elem):
             if not elem.text or not elem.text.strip():
                 elem.text = i + "  "
             if not elem.tail or not elem.tail.strip():
                 elem.tail = i
             for elem in elem:
-                self.indent(elem, level+1)
+                self.indent(elem, level + 1)
             if not elem.tail or not elem.tail.strip():
                 elem.tail = i
         else:
             if level and (not elem.tail or not elem.tail.strip()):
                 elem.tail = i
 
     def write_file(self, filename):
@@ -627,23 +702,24 @@
         filename: Name of the file.
 
         Examples
         -------
         >>> writer = ProbModelXMLWriter(model)
         >>> writer.write_file(test_file)
         """
-        writer = self.__str__()[:-1].decode('utf-8')
-        with open(filename, 'w') as fout:
+        writer = self.__str__()[:-1].decode("utf-8")
+        with open(filename, "w") as fout:
             fout.write(writer)
 
 
 class ProbModelXMLReader(object):
     """
     Class for reading ProbModelXML format from files or strings.
     """
+
     # TODO: add methods to parse policies, inferenceoption, evidence etc.
     # TODO: add reading formatVersion
     def __init__(self, path=None, string=None):
         """
         Initialize an instance of ProbModelXMLReader class.
 
         Parameters
@@ -723,51 +799,51 @@
     def create_probnet(self):
         """
         Returns a BayesianModel or MarkovModel object depending on the
         type of ProbModelXML passed to ProbModelXMLReader class.
         """
         self.probnet = {}
         # Add general properties
-        probnet_elem = self.xml.find('ProbNet')
-        self.probnet['type'] = probnet_elem.attrib['type']
-        if probnet_elem.find('Comment') is not None:
-            self.add_comment(probnet_elem.find('Comment').text)
-        if probnet_elem.find('Language') is not None:
-            self.add_language(probnet_elem.find('Language').text)
-        if probnet_elem.find('AdditionalProperties') is not None:
-            self.probnet['AdditionalProperties'] = {}
-            for prop in probnet_elem.find('AdditionalProperties'):
-                self.add_additional_property(self.probnet['AdditionalProperties'], prop)
+        probnet_elem = self.xml.find("ProbNet")
+        self.probnet["type"] = probnet_elem.attrib["type"]
+        if probnet_elem.find("Comment") is not None:
+            self.add_comment(probnet_elem.find("Comment").text)
+        if probnet_elem.find("Language") is not None:
+            self.add_language(probnet_elem.find("Language").text)
+        if probnet_elem.find("AdditionalProperties") is not None:
+            self.probnet["AdditionalProperties"] = {}
+            for prop in probnet_elem.find("AdditionalProperties"):
+                self.add_additional_property(self.probnet["AdditionalProperties"], prop)
 
         # Add additional Constraints
-        self.probnet['AdditionalConstraints'] = {}
-        for constraint in probnet_elem.findall('AdditionalConstraints/Constraint'):
+        self.probnet["AdditionalConstraints"] = {}
+        for constraint in probnet_elem.findall("AdditionalConstraints/Constraint"):
             self.add_probnet_additionalconstraints(constraint)
 
         # Add Decision Criterion
-        self.probnet['DecisionCriteria'] = {}
-        for criterion in probnet_elem.findall('DecisionCriteria/Criterion'):
+        self.probnet["DecisionCriteria"] = {}
+        for criterion in probnet_elem.findall("DecisionCriteria/Criterion"):
             self.add_criterion(criterion)
 
         # Add nodes
-        self.probnet['Variables'] = {}
-        for variable in probnet_elem.find('Variables'):
+        self.probnet["Variables"] = {}
+        for variable in probnet_elem.find("Variables"):
             self.add_node(variable)
 
         # Add edges
-        self.probnet['edges'] = {}
-        for edge in probnet_elem.findall('Links/Link'):
+        self.probnet["edges"] = {}
+        for edge in probnet_elem.findall("Links/Link"):
             self.add_edge(edge)
 
         # Add CPD
-        self.probnet['Potentials'] = []
-        for potential in probnet_elem.findall('Potentials/Potential'):
+        self.probnet["Potentials"] = []
+        for potential in probnet_elem.findall("Potentials/Potential"):
             probnet_dict = {}
             self.add_potential(potential, probnet_dict)
-            self.probnet['Potentials'].append(probnet_dict)
+            self.probnet["Potentials"].append(probnet_dict)
 
     def add_probnet_additionalconstraints(self, constraint):
         """
         Adds Additional Constraints to the probnet dict.
 
         Parameters
         ----------
@@ -775,20 +851,22 @@
             etree Element consisting Constraint tag.
 
         Examples
         -------
         >>> reader = ProbModelXMLReader()
         >>> reader.add_additionalconstraints(constraint)
         """
-        constraint_name = constraint.attrib['name']
-        self.probnet['AdditionalConstraints'][constraint_name] = {}
-        for argument in constraint.findall('Argument'):
-            argument_name = argument.attrib['name']
-            argument_value = argument.attrib['value']
-            self.probnet['AdditionalConstraints'][constraint_name][argument_name] = argument_value
+        constraint_name = constraint.attrib["name"]
+        self.probnet["AdditionalConstraints"][constraint_name] = {}
+        for argument in constraint.findall("Argument"):
+            argument_name = argument.attrib["name"]
+            argument_value = argument.attrib["value"]
+            self.probnet["AdditionalConstraints"][constraint_name][
+                argument_name
+            ] = argument_value
 
     def add_criterion(self, criterion):
         """
         Adds Decision Criteria to the probnet dict.
 
         Parameters
         ----------
@@ -796,21 +874,23 @@
             etree Element consisting DecisionCritera tag.
 
         Examples
         -------
         >>> reader = ProbModelXMLReader()
         >>> reader.add_criterion(criterion)
         """
-        criterion_name = criterion.attrib['name']
-        self.probnet['DecisionCriteria'][criterion_name] = {}
-        if criterion.find('AdditionalProperties/Property') is not None:
-            for prop in criterion.findall('AdditionalProperties/Property'):
-                prop_name = prop.attrib['name']
-                prop_value = prop.attrib['value']
-                self.probnet['DecisionCriteria'][criterion_name]['AdditionalProperties'][prop_name] = prop_value
+        criterion_name = criterion.attrib["name"]
+        self.probnet["DecisionCriteria"][criterion_name] = {}
+        if criterion.find("AdditionalProperties/Property") is not None:
+            for prop in criterion.findall("AdditionalProperties/Property"):
+                prop_name = prop.attrib["name"]
+                prop_value = prop.attrib["value"]
+                self.probnet["DecisionCriteria"][criterion_name][
+                    "AdditionalProperties"
+                ][prop_name] = prop_value
 
     def add_comment(self, comment):
         """
         Adds Comment to the probnet dict.
 
         Parameters
         ----------
@@ -818,15 +898,15 @@
             String consisting of comment.
 
         Examples
         -------
         >>> reader = ProbModelXMLReader()
         >>> reader.add_comment(comment)
         """
-        self.probnet['Comment'] = comment
+        self.probnet["Comment"] = comment
 
     def add_language(self, language):
         """
         Adds Language to the probnet dict.
 
         Parameters
         ----------
@@ -834,19 +914,19 @@
             String consisting of language.
 
         Examples
         -------
         >>> reader = ProbModelXMLReader()
         >>> reader.add_language(language)
         """
-        self.probnet['Language'] = language
+        self.probnet["Language"] = language
 
     @staticmethod
     def add_additional_property(place, prop):
-        place[prop.attrib['name']] = prop.attrib['value']
+        place[prop.attrib["name"]] = prop.attrib["value"]
 
     def add_node(self, variable):
         """
         Adds Variables to the probnet dict.
 
         Parameters
         ----------
@@ -855,34 +935,42 @@
 
         Examples
         -------
         >>> reader = ProbModelXMLReader()
         >>> reader.add_node(variable)
         """
         # TODO: Do some checks with variable type and roles. Right now I don't know when they are to be used.
-        variable_name = variable.attrib['name']
-        self.probnet['Variables'][variable_name] = {}
-        self.probnet['Variables'][variable_name]['type'] = variable.attrib['type']
-        self.probnet['Variables'][variable_name]['role'] = variable.attrib['role']
-        if variable.find('Comment') is not None:
-            self.probnet['Variables'][variable_name]['Comment'] = variable.find('Comment').text
-        if variable.find('Coordinates') is not None:
-            self.probnet['Variables'][variable_name]['Coordinates'] = variable.find('Coordinates').attrib
-        if variable.find('AdditionalProperties/Property') is not None:
-            self.probnet['Variables'][variable_name]['AdditionalProperties'] = {}
-            for prop in variable.findall('AdditionalProperties/Property'):
-                self.probnet['Variables'][variable_name]['AdditionalProperties'][prop.attrib['name']] = \
-                    prop.attrib['value']
-        if variable.find('States/State') is None:
+        variable_name = variable.attrib["name"]
+        self.probnet["Variables"][variable_name] = {}
+        self.probnet["Variables"][variable_name]["type"] = variable.attrib["type"]
+        self.probnet["Variables"][variable_name]["role"] = variable.attrib["role"]
+        if variable.find("Comment") is not None:
+            self.probnet["Variables"][variable_name]["Comment"] = variable.find(
+                "Comment"
+            ).text
+        if variable.find("Coordinates") is not None:
+            self.probnet["Variables"][variable_name]["Coordinates"] = variable.find(
+                "Coordinates"
+            ).attrib
+        if variable.find("AdditionalProperties/Property") is not None:
+            self.probnet["Variables"][variable_name]["AdditionalProperties"] = {}
+            for prop in variable.findall("AdditionalProperties/Property"):
+                self.probnet["Variables"][variable_name]["AdditionalProperties"][
+                    prop.attrib["name"]
+                ] = prop.attrib["value"]
+        if variable.find("States/State") is None:
             warnings.warn("States not available for node: " + variable_name)
         else:
-            self.probnet['Variables'][variable_name]['States'] = {state.attrib['name']: {
-                prop.attrib['name']: prop.attrib['value'] for
-                prop in state.findall('AdditionalProperties/Property')} for state in variable.findall(
-                'States/State')}
+            self.probnet["Variables"][variable_name]["States"] = {
+                state.attrib["name"]: {
+                    prop.attrib["name"]: prop.attrib["value"]
+                    for prop in state.findall("AdditionalProperties/Property")
+                }
+                for state in variable.findall("States/State")
+            }
 
     def add_edge(self, edge):
         """
         Adds Edges to the probnet dict.
 
         Parameters
         ----------
@@ -890,27 +978,29 @@
             etree Element consisting Variable tag.
 
         Examples
         -------
         >>> reader = ProbModelXMLReader()
         >>> reader.add_edge(edge)
         """
-        var1 = edge.findall('Variable')[0].attrib['name']
-        var2 = edge.findall('Variable')[1].attrib['name']
-        self.probnet['edges'][(var1, var2)] = {}
-        self.probnet['edges'][(var1, var2)]['directed'] = edge.attrib['directed']
+        var1 = edge.findall("Variable")[0].attrib["name"]
+        var2 = edge.findall("Variable")[1].attrib["name"]
+        self.probnet["edges"][(var1, var2)] = {}
+        self.probnet["edges"][(var1, var2)]["directed"] = edge.attrib["directed"]
         # TODO: check for the case of undirected graphs if we need to add to both elements of the dic for a single edge.
-        if edge.find('Comment') is not None:
-            self.probnet['edges'][(var1, var2)]['Comment'] = edge.find('Comment').text
-        if edge.find('Label') is not None:
-            self.probnet['edges'][(var1, var2)]['Label'] = edge.find('Label').text
-        if edge.find('AdditionalProperties/Property') is not None:
-            self.probnet['edges'][(var1, var2)]['AdditionalProperties'] = {}
-            for prop in edge.findall('AdditionalProperties/Property'):
-                self.probnet['edges'][(var1, var2)]['AdditionalProperties'][prop.attrib['name']] = prop.attrib['value']
+        if edge.find("Comment") is not None:
+            self.probnet["edges"][(var1, var2)]["Comment"] = edge.find("Comment").text
+        if edge.find("Label") is not None:
+            self.probnet["edges"][(var1, var2)]["Label"] = edge.find("Label").text
+        if edge.find("AdditionalProperties/Property") is not None:
+            self.probnet["edges"][(var1, var2)]["AdditionalProperties"] = {}
+            for prop in edge.findall("AdditionalProperties/Property"):
+                self.probnet["edges"][(var1, var2)]["AdditionalProperties"][
+                    prop.attrib["name"]
+                ] = prop.attrib["value"]
 
     def add_potential(self, potential, potential_dict):
         """
         Adds Potential to the potential dict.
 
         Parameters
         ----------
@@ -920,143 +1010,175 @@
             Dictionary to parse Potential tag.
 
         Examples
         -------
         >>> reader = ProbModelXMLReader()
         >>> reader.add_potential(potential, potential_dict)
         """
-        potential_type = potential.attrib['type']
-        potential_dict['type'] = potential_type
+        potential_type = potential.attrib["type"]
+        potential_dict["type"] = potential_type
         try:
-            potential_dict['role'] = potential.attrib['role']
+            potential_dict["role"] = potential.attrib["role"]
         except KeyError:
             pass
-        if potential.find('Comment') is not None:
-            potential_dict['Comment'] = potential.find('Comment').text
-        for prop in potential.findall('AdditionalProperties/Property'):
-            potential_dict['AdditionalProperties'][prop.attrib['name']] = prop.attrib['value']
+        if potential.find("Comment") is not None:
+            potential_dict["Comment"] = potential.find("Comment").text
+        for prop in potential.findall("AdditionalProperties/Property"):
+            potential_dict["AdditionalProperties"][prop.attrib["name"]] = prop.attrib[
+                "value"
+            ]
         if potential_type == "delta":
-            potential_dict['Variable'] = potential.find('Variable').attrib['name']
-            if potential.find('State') is not None:
-                potential_dict['State'] = potential.find('State').text
-            if potential.find('StateIndex') is not None:
-                potential_dict['StateIndex'] = potential.find('StateIndex').text
-            if potential.find('NumericValue') is not None:
-                potential_dict['NumericValue'] = potential.find('NumericValue').text
+            potential_dict["Variable"] = potential.find("Variable").attrib["name"]
+            if potential.find("State") is not None:
+                potential_dict["State"] = potential.find("State").text
+            if potential.find("StateIndex") is not None:
+                potential_dict["StateIndex"] = potential.find("StateIndex").text
+            if potential.find("NumericValue") is not None:
+                potential_dict["NumericValue"] = potential.find("NumericValue").text
         else:
-            if potential.find('UtilityVariable') is not None:
-                potential_dict['UtilityVaribale'] = potential.find('UtilityVariable').attrib['name']
-            if len(potential.findall('Variables/Variable')):
-                potential_dict['Variables'] = {}
+            if potential.find("UtilityVariable") is not None:
+                potential_dict["UtilityVaribale"] = potential.find(
+                    "UtilityVariable"
+                ).attrib["name"]
+            if len(potential.findall("Variables/Variable")):
+                potential_dict["Variables"] = {}
                 var_list = []
-                for var in potential.findall('Variables/Variable'):
-                    var_list.append(var.attrib['name'])
-                potential_dict['Variables'][var_list[0]] = var_list[1:]
-            if potential.find('Values') is not None:
-                potential_dict['Values'] = potential.find('Values').text
-            if len(potential.findall('UncertainValues/Value')):
-                potential_dict['UncertainValues'] = []
-                for value in potential.findall('UncertainValues/Value'):
+                for var in potential.findall("Variables/Variable"):
+                    var_list.append(var.attrib["name"])
+                potential_dict["Variables"][var_list[0]] = var_list[1:]
+            if potential.find("Values") is not None:
+                potential_dict["Values"] = potential.find("Values").text
+            if len(potential.findall("UncertainValues/Value")):
+                potential_dict["UncertainValues"] = []
+                for value in potential.findall("UncertainValues/Value"):
                     try:
-                        potential_dict['UncertainValues'].append(
-                            {'distribution': value.attrib['distribution'], 'name': value.attrib['name'],
-                             'value': value.text})
+                        potential_dict["UncertainValues"].append(
+                            {
+                                "distribution": value.attrib["distribution"],
+                                "name": value.attrib["name"],
+                                "value": value.text,
+                            }
+                        )
                     except KeyError:
-                        potential_dict['UncertainValues'].append(
-                            {'distribution': value.attrib['distribution'], 'value': value.text})
-            if potential.find('TopVariable') is not None:
-                potential_dict['TopVariable'] = potential.find('TopVariable').attrib['name']
-
-            if len(potential.findall('Branches/Branch')):
-                potential_dict['Branches'] = []
-                for branch in potential.findall('Branches/Branch'):
+                        potential_dict["UncertainValues"].append(
+                            {
+                                "distribution": value.attrib["distribution"],
+                                "value": value.text,
+                            }
+                        )
+            if potential.find("TopVariable") is not None:
+                potential_dict["TopVariable"] = potential.find("TopVariable").attrib[
+                    "name"
+                ]
+
+            if len(potential.findall("Branches/Branch")):
+                potential_dict["Branches"] = []
+                for branch in potential.findall("Branches/Branch"):
                     branch_dict = {}
-                    if len(branch.findall('States/State')):
+                    if len(branch.findall("States/State")):
                         states = []
-                        for state in branch.findall('States/State'):
-                            states.append({'name': state.attrib['name']})
-                        branch_dict['States'] = states
-                    if branch.find('Potential') is not None:
+                        for state in branch.findall("States/State"):
+                            states.append({"name": state.attrib["name"]})
+                        branch_dict["States"] = states
+                    if branch.find("Potential") is not None:
                         branch_potential = {}
-                        self.add_potential(branch.find('Potential'), branch_potential)
-                        branch_dict['Potential'] = branch_potential
-                    if branch.find('Label') is not None:
-                        label = branch.find('Label').text
-                        branch_dict['Label'] = label
-                    if branch.find('Reference') is not None:
-                        reference = branch.find('Reference').text
-                        branch_dict['Reference'] = reference
-                    if len(branch.findall('Thresholds/Threshold')):
+                        self.add_potential(branch.find("Potential"), branch_potential)
+                        branch_dict["Potential"] = branch_potential
+                    if branch.find("Label") is not None:
+                        label = branch.find("Label").text
+                        branch_dict["Label"] = label
+                    if branch.find("Reference") is not None:
+                        reference = branch.find("Reference").text
+                        branch_dict["Reference"] = reference
+                    if len(branch.findall("Thresholds/Threshold")):
                         thresholds = []
-                        for threshold in branch.findall('Thresholds/Threshold'):
+                        for threshold in branch.findall("Thresholds/Threshold"):
                             try:
-                                thresholds.append({
-                                    'value': threshold.attrib['value'], 'belongsTo': threshold.attrib['belongsTo']})
+                                thresholds.append(
+                                    {
+                                        "value": threshold.attrib["value"],
+                                        "belongsTo": threshold.attrib["belongsTo"],
+                                    }
+                                )
                             except KeyError:
-                                thresholds.append({'value': threshold.attrib['value']})
-                        branch_dict['Thresholds'] = thresholds
-                    potential_dict['Branches'].append(branch_dict)
-
-            if potential.find('Model') is not None:
-                potential_dict['Model'] = potential.find('Model').text
-            if len(potential.findall('Subpotentials/Potential')):
-                potential_dict['Subpotentials'] = []
-                for subpotential in potential.findall('Subpotentials/Potential'):
+                                thresholds.append({"value": threshold.attrib["value"]})
+                        branch_dict["Thresholds"] = thresholds
+                    potential_dict["Branches"].append(branch_dict)
+
+            if potential.find("Model") is not None:
+                potential_dict["Model"] = potential.find("Model").text
+            if len(potential.findall("Subpotentials/Potential")):
+                potential_dict["Subpotentials"] = []
+                for subpotential in potential.findall("Subpotentials/Potential"):
                     subpotential_dict = {}
                     self.add_potential(subpotential, subpotential_dict)
-                    potential_dict['Subpotentials'].append(subpotential_dict)
-            if potential.find('Coefficients') is not None:
-                potential_dict['Coefficients'] = potential.find('Coefficients').text
-            if potential.find('CovarianceMatrix') is not None:
-                potential_dict['CovarianceMatrix'] = potential.find('CovarianceMatrix').text
-            if potential.find('Potential') is not None:
-                potential_dict['Potential'] = {}
-                self.add_potential(potential.find('Potential'), potential_dict['Potential'])
-            if len(potential.findall('NumericVariables/Variable')):
-                potential_dict['NumericVariables'] = []
-                for variable in potential.findall('NumericVariables/Variable'):
-                    potential_dict['NumericVariables'].append(variable.attrib['name'])
+                    potential_dict["Subpotentials"].append(subpotential_dict)
+            if potential.find("Coefficients") is not None:
+                potential_dict["Coefficients"] = potential.find("Coefficients").text
+            if potential.find("CovarianceMatrix") is not None:
+                potential_dict["CovarianceMatrix"] = potential.find(
+                    "CovarianceMatrix"
+                ).text
+            if potential.find("Potential") is not None:
+                potential_dict["Potential"] = {}
+                self.add_potential(
+                    potential.find("Potential"), potential_dict["Potential"]
+                )
+            if len(potential.findall("NumericVariables/Variable")):
+                potential_dict["NumericVariables"] = []
+                for variable in potential.findall("NumericVariables/Variable"):
+                    potential_dict["NumericVariables"].append(variable.attrib["name"])
 
     def get_model(self):
         """
         Returns the model instance of the ProbModel.
 
         Return
         ---------------
         model: an instance of BayesianModel.
 
         Examples
         -------
         >>> reader = ProbModelXMLReader()
         >>> reader.get_model()
         """
-        if self.probnet.get('type') == "BayesianNetwork":
+        if self.probnet.get("type") == "BayesianNetwork":
             model = BayesianModel()
-            model.add_nodes_from(self.probnet['Variables'].keys())
-            model.add_edges_from(self.probnet['edges'].keys())
+            model.add_nodes_from(self.probnet["Variables"].keys())
+            model.add_edges_from(self.probnet["edges"].keys())
 
             tabular_cpds = []
-            cpds = self.probnet['Potentials']
+            cpds = self.probnet["Potentials"]
             for cpd in cpds:
-                var = list(cpd['Variables'].keys())[0]
-                states = self.probnet['Variables'][var]['States']
-                evidence = cpd['Variables'][var]
-                evidence_card = [len(self.probnet['Variables'][evidence_var]['States'])
-                                 for evidence_var in evidence]
-                arr = list(map(float, cpd['Values'].split()))
+                var = list(cpd["Variables"].keys())[0]
+                states = self.probnet["Variables"][var]["States"]
+                evidence = cpd["Variables"][var]
+                evidence_card = [
+                    len(self.probnet["Variables"][evidence_var]["States"])
+                    for evidence_var in evidence
+                ]
+                arr = list(map(float, cpd["Values"].split()))
                 values = np.array(arr)
-                values = values.reshape((len(states), values.size//len(states)))
-                tabular_cpds.append(TabularCPD(var, len(states), values, evidence, evidence_card))
+                values = values.reshape((len(states), values.size // len(states)))
+                tabular_cpds.append(
+                    TabularCPD(var, len(states), values, evidence, evidence_card)
+                )
 
             model.add_cpds(*tabular_cpds)
 
             variables = model.nodes()
             for var in variables:
-                for prop_name, prop_value in self.probnet['Variables'][var].items():
-                    model.node[var][prop_name] = prop_value
+                for prop_name, prop_value in self.probnet["Variables"][var].items():
+                    model.nodes[var][prop_name] = prop_value
             edges = model.edges()
-            for edge in edges:
-                for prop_name, prop_value in self.probnet['edges'][edge].items():
-                    model.edge[edge[0]][edge[1]][prop_name] = prop_value
+
+            if nx.__version__.startswith("1"):
+                for edge in edges:
+                    for prop_name, prop_value in self.probnet["edges"][edge].items():
+                        model.edge[edge[0]][edge[1]][prop_name] = prop_value
+            else:
+                for edge in edges:
+                    for prop_name, prop_value in self.probnet["edges"][edge].items():
+                        model.adj[edge[0]][edge[1]][prop_name] = prop_value
             return model
         else:
             raise ValueError("Please specify only Bayesian Network.")
```

### Comparing `pgmpy-0.1.7/pgmpy/utils/mathext.py` & `pgmpy-0.1.9/pgmpy/utils/mathext.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 from collections import namedtuple
 
 import numpy as np
 from itertools import combinations, chain
 from pgmpy.extern.six.moves import map
 
 
-State = namedtuple('State', ['var', 'state'])
+State = namedtuple("State", ["var", "state"])
 
 
 def cartesian(arrays, out=None):
     """Generate a cartesian product of input arrays.
 
     Parameters
     ----------
@@ -82,21 +82,23 @@
     array(['v_1', 'v_1', 'v_0', 'v_1', 'v_2', 'v_0', 'v_1', 'v_1', 'v_1',
       'v_2'], dtype='<U3')
     """
     weights = np.array(weights)
     if weights.ndim == 1:
         return np.random.choice(values, size=size, p=weights)
     else:
-        return np.fromiter(map(lambda t: np.random.choice(values, p=t), weights), dtype='int')
+        return np.fromiter(
+            map(lambda t: np.random.choice(values, p=t), weights), dtype="int"
+        )
 
 
 def powerset(l):
     """
     Generates all subsets of list `l` (as tuples).
 
     Example
     -------
     >>> from pgmpy.utils.mathext import powerset
     >>> list(powerset([1,2,3]))
     [(), (1,), (2,), (3,), (1,2), (1,3), (2,3), (1,2,3)]
     """
-    return chain.from_iterable(combinations(l, r) for r in range(len(l)+1))
+    return chain.from_iterable(combinations(l, r) for r in range(len(l) + 1))
```

### Comparing `pgmpy-0.1.7/pgmpy/utils/check_functions.py` & `pgmpy-0.1.9/pgmpy/utils/check_functions.py`

 * *Files 9% similar despite different names*

```diff
@@ -19,8 +19,12 @@
 
 
 def _check_length_equal(param_1, param_2, name_param_1, name_param_2):
     """
     Raises an error when the length of given two arguments is not equal
     """
     if len(param_1) != len(param_2):
-        raise ValueError("Length of {} must be same as Length of {}".format(name_param_1, name_param_2))
+        raise ValueError(
+            "Length of {} must be same as Length of {}".format(
+                name_param_1, name_param_2
+            )
+        )
```

### Comparing `pgmpy-0.1.7/pgmpy/factors/continuous/discretize.py` & `pgmpy-0.1.9/pgmpy/factors/continuous/discretize.py`

 * *Files 3% similar despite different names*

```diff
@@ -87,16 +87,18 @@
         >>> chld.get_labels()
         ['x=-5.0', 'x=-4.5', 'x=-4.0', 'x=-3.5', 'x=-3.0', 'x=-2.5',
          'x=-2.0', 'x=-1.5', 'x=-1.0', 'x=-0.5', 'x=0.0', 'x=0.5', 'x=1.0',
          'x=1.5', 'x=2.0', 'x=2.5', 'x=3.0', 'x=3.5', 'x=4.0', 'x=4.5']
 
         """
         step = (self.high - self.low) / self.cardinality
-        labels = ['x={i}'.format(i=str(i)) for i in np.round(
-            np.arange(self.low, self.high, step), 3)]
+        labels = [
+            "x={i}".format(i=str(i))
+            for i in np.round(np.arange(self.low, self.high, step), 3)
+        ]
         return labels
 
 
 class RoundingDiscretizer(BaseDiscretizer):
     """
     This class uses the rounding method for discretizing the
     given continuous distribution.
@@ -126,19 +128,26 @@
      0.065590616803036905, 0.027834684208772664, 0.0092447094199902269]
     """
 
     def get_discrete_values(self):
         step = (self.high - self.low) / self.cardinality
 
         # for x=[low]
-        discrete_values = [self.factor.cdf(self.low + step/2) - self.factor.cdf(self.low)]
+        discrete_values = [
+            self.factor.cdf(self.low + step / 2) - self.factor.cdf(self.low)
+        ]
 
         # for x=[low+step, low+2*step, ........., high-step]
         points = np.linspace(self.low + step, self.high - step, self.cardinality - 1)
-        discrete_values.extend([self.factor.cdf(i + step/2) - self.factor.cdf(i - step/2) for i in points])
+        discrete_values.extend(
+            [
+                self.factor.cdf(i + step / 2) - self.factor.cdf(i - step / 2)
+                for i in points
+            ]
+        )
 
         return discrete_values
 
 
 class UnbiasedDiscretizer(BaseDiscretizer):
     """
     This class uses the unbiased method for discretizing the
@@ -180,28 +189,38 @@
     >>> exp_node.discretize(UnbiasedDiscretizer, low=0, high=5, cardinality=10)
     [0.39627368905806137, 0.4049838434034298, 0.13331784003148325,
      0.043887287876647259, 0.014447413395300212, 0.0047559685431339703,
      0.0015656350182896128, 0.00051540201980112557, 0.00016965346326140994,
      3.7867260839208328e-05]
 
     """
+
     def get_discrete_values(self):
         lev = self._lim_moment
         step = (self.high - self.low) / (self.cardinality - 1)
 
         # for x=[low]
-        discrete_values = [(lev(self.low) - lev(self.low + step)) / step +
-                           1 - self.factor.cdf(self.low)]
+        discrete_values = [
+            (lev(self.low) - lev(self.low + step)) / step
+            + 1
+            - self.factor.cdf(self.low)
+        ]
 
         # for x=[low+step, low+2*step, ........., high-step]
         points = np.linspace(self.low + step, self.high - step, self.cardinality - 2)
-        discrete_values.extend([(2 * lev(i) - lev(i - step) - lev(i + step)) / step for i in points])
+        discrete_values.extend(
+            [(2 * lev(i) - lev(i - step) - lev(i + step)) / step for i in points]
+        )
 
         # for x=[high]
-        discrete_values.append((lev(self.high) - lev(self.high - step)) / step - 1 + self.factor.cdf(self.high))
+        discrete_values.append(
+            (lev(self.high) - lev(self.high - step)) / step
+            - 1
+            + self.factor.cdf(self.high)
+        )
 
         return discrete_values
 
     def _lim_moment(self, u, order=1):
         """
         This method calculates the kth order limiting moment of
         the distribution. It is given by -
@@ -221,16 +240,21 @@
         ----------
         u: float
             The point at which the moment is to be calculated.
 
         order: int
             The order of the moment, default is first order.
         """
+
         def fun(x):
             return np.power(x, order) * self.factor.pdf(x)
-        return (integrate.quad(fun, -np.inf, u)[0] +
-                np.power(u, order)*(1 - self.factor.cdf(u)))
+
+        return integrate.quad(fun, -np.inf, u)[0] + np.power(u, order) * (
+            1 - self.factor.cdf(u)
+        )
 
     def get_labels(self):
-        labels = list('x={i}'.format(i=str(i)) for i in np.round
-                      (np.linspace(self.low, self.high, self.cardinality), 3))
+        labels = list(
+            "x={i}".format(i=str(i))
+            for i in np.round(np.linspace(self.low, self.high, self.cardinality), 3)
+        )
         return labels
```

### Comparing `pgmpy-0.1.7/pgmpy/factors/continuous/ContinuousFactor.py` & `pgmpy-0.1.9/pgmpy/factors/continuous/ContinuousFactor.py`

 * *Files 1% similar despite different names*

```diff
@@ -10,14 +10,15 @@
 
 
 class ContinuousFactor(BaseFactor):
     """
     Base class for factors representing various multivariate
     representations.
     """
+
     def __init__(self, variables, pdf, *args, **kwargs):
         """
         Parameters
         ----------
         variables: list or array-like
             The variables for wich the distribution is defined.
 
@@ -35,44 +36,50 @@
         >>> dirichlet_factor = ContinuousFactor(['x', 'y'], dirichlet_pdf)
         >>> dirichlet_factor.scope()
         ['x', 'y']
         >>> dirichlet_factor.assignment(5,6)
         226800.0
         """
         if not isinstance(variables, (list, tuple, np.ndarray)):
-            raise TypeError("variables: Expected type list or array-like, "
-                            "got type {var_type}".format(var_type=type(variables)))
+            raise TypeError(
+                "variables: Expected type list or array-like, "
+                "got type {var_type}".format(var_type=type(variables))
+            )
 
         if len(set(variables)) != len(variables):
             raise ValueError("Variable names cannot be same.")
 
         variables = list(variables)
 
         if isinstance(pdf, str):
-            if pdf == 'gaussian':
+            if pdf == "gaussian":
                 self.distribution = GaussianDistribution(
                     variables=variables,
-                    mean=kwargs['mean'],
-                    covariance=kwargs['covariance'])
+                    mean=kwargs["mean"],
+                    covariance=kwargs["covariance"],
+                )
             else:
-                raise NotImplementedError("{dist} distribution not supported.",
-                                          "Please use CustomDistribution".
-                                          format(dist=pdf))
+                raise NotImplementedError(
+                    "{dist} distribution not supported.",
+                    "Please use CustomDistribution".format(dist=pdf),
+                )
 
         elif isinstance(pdf, CustomDistribution):
             self.distribution = pdf
 
         elif callable(pdf):
             self.distribution = CustomDistribution(
-                variables=variables,
-                distribution=pdf)
+                variables=variables, distribution=pdf
+            )
 
         else:
-            raise ValueError("pdf: Expected type: str or function, ",
-                             "Got: {instance}".format(instance=type(variables)))
+            raise ValueError(
+                "pdf: Expected type: str or function, ",
+                "Got: {instance}".format(instance=type(variables)),
+            )
 
     @property
     def pdf(self):
         """
         Returns the pdf of the ContinuousFactor.
         """
         return self.distribution.pdf
@@ -289,15 +296,15 @@
         0.117099663049
         >>> std_normal.normalize()
         >>> std_normal.assignment(1, 1)
         0.0585498315243
 
         """
         phi = self if inplace else self.copy()
-        phi.distriution = phi.distribution.normalize(inplace=False)
+        phi.distribution.normalize(inplace=True)
 
         if not inplace:
             return phi
 
     def is_valid_cpd(self):
         return self.distribution.is_valid_cpd()
 
@@ -323,22 +330,26 @@
         -------
         ContinuousFactor or None:
                         if inplace=True (default) returns None
                         if inplace=False returns a new `DiscreteFactor` instance.
 
         """
         if not isinstance(other, ContinuousFactor):
-            raise TypeError("ContinuousFactor objects can only be multiplied ",
-                            "or divided with another ContinuousFactor object. ",
-                            "Got {other_type}, expected: ContinuousFactor.".format(
-                                other_type=type(other)))
+            raise TypeError(
+                "ContinuousFactor objects can only be multiplied ",
+                "or divided with another ContinuousFactor object. ",
+                "Got {other_type}, expected: ContinuousFactor.".format(
+                    other_type=type(other)
+                ),
+            )
 
         phi = self if inplace else self.copy()
         phi.distribution = phi.distribution._operate(
-            other=other.distribution, operation=operation, inplace=False)
+            other=other.distribution, operation=operation, inplace=False
+        )
 
         if not inplace:
             return phi
 
     def product(self, other, inplace=True):
         """
         Gives the ContinuousFactor product with the other factor.
@@ -367,30 +378,30 @@
         >>> sn3.assignment(0, 0)
         0.063493635934240983
 
         >>> sn3 = sn1 * sn2
         >>> sn3.assignment(0, 0)
         0.063493635934240983
         """
-        return self._operate(other, 'product', inplace)
+        return self._operate(other, "product", inplace)
 
     def divide(self, other, inplace=True):
         """
         Gives the ContinuousFactor divide with the other factor.
 
         Parameters
         ----------
         other: ContinuousFactor
-            The ContinuousFactor to be multiplied.
+            The ContinuousFactor to be divided.
 
         Returns
         -------
         ContinuousFactor or None:
                         if inplace=True (default) returns None
-                        if inplace=False returns a new `DiscreteFactor` instance.
+                        if inplace=False returns a new `ContinuousFactor` instance.
 
         Example
         -------
         >>> from pgmpy.factors.continuous import ContinuousFactor
         >>> from scipy.stats import multivariate_normal
         >>> sn_pdf1 = lambda x: multivariate_normal.pdf([x], [0], [[1]])
         >>> sn_pdf2 = lambda x1,x2: multivariate_normal.pdf([x1, x2], [0, 0], [[1, 0], [0, 1]])
@@ -404,15 +415,15 @@
         >>> sn4 = sn2 / sn1
         >>> sn4.assignment(0, 0)
         0.3989422804014327
         """
         if set(other.scope()) - set(self.scope()):
             raise ValueError("Scope of divisor should be a subset of dividend")
 
-        return self._operate(other, 'divide', inplace)
+        return self._operate(other, "divide", inplace)
 
     def __mul__(self, other):
         return self.product(other, inplace=False)
 
     def __rmul__(self, other):
         return self.__mul__(other)
```

### Comparing `pgmpy-0.1.7/pgmpy/factors/continuous/LinearGaussianCPD.py` & `pgmpy-0.1.9/pgmpy/factors/continuous/LinearGaussianCPD.py`

 * *Files 7% similar despite different names*

```diff
@@ -30,15 +30,17 @@
 
     $ p(Y |x) = \mathcal{N}(\beta_0 + \boldmath{β}.T * \boldmath{x} ; \sigma_2) $
 
 
     Reference: https://cedar.buffalo.edu/~srihari/CSE574/Chap8/Ch8-PGM-GaussianBNs/8.5%20GaussianBNs.pdf
     """
 
-    def __init__(self, variable, evidence_mean, evidence_variance, evidence=[], beta=None):
+    def __init__(
+        self, variable, evidence_mean, evidence_variance, evidence=[], beta=None
+    ):
         """
         Parameters
         ----------
 
         variable: any hashable python object
             The variable whose CPD is defined.
 
@@ -78,20 +80,21 @@
         if beta is not None:
             self.beta = beta
             self.beta_0 = beta[0]
             self.beta_vector = np.asarray(beta[1:])
 
             if len(evidence) != len(beta) - 1:
                 raise ValueError(
-                    "The number of variables in evidence must be one less than the length of the beta vector.")
+                    "The number of variables in evidence must be one less than the length of the beta vector."
+                )
 
         variables = [variable] + evidence
-        super(LinearGaussianCPD, self).__init__(variables, pdf='gaussian',
-                                                mean=self.mean,
-                                                covariance=self.variance)
+        super(LinearGaussianCPD, self).__init__(
+            variables, pdf="gaussian", mean=self.mean, covariance=self.variance
+        )
 
     def sum_of_product(self, xi, xj):
         prod_xixj = xi * xj
         return np.sum(prod_xixj)
 
     def maximum_likelihood_estimator(self, data, states):
         """ 
@@ -110,98 +113,101 @@
         beta, variance (tuple): Returns estimated betas and the variance.
         """
         x_df = pd.DataFrame(data, columns=states)
         x_len = len(self.evidence)
 
         sym_coefs = []
         for i in range(0, x_len):
-            sym_coefs.append('b' + str(i + 1) + '_coef')
+            sym_coefs.append("b" + str(i + 1) + "_coef")
 
         sum_x = x_df.sum()
-        x = [sum_x['(Y|X)']]
+        x = [sum_x["(Y|X)"]]
         coef_matrix = pd.DataFrame(columns=sym_coefs)
 
         # First we compute just the coefficients of beta_1 to beta_N.
         # Later we compute beta_0 and append it.
         for i in range(0, x_len):
-            x.append(self.sum_of_product(x_df['(Y|X)'], x_df[self.evidence[i]]))
+            x.append(self.sum_of_product(x_df["(Y|X)"], x_df[self.evidence[i]]))
             for j in range(0, x_len):
                 coef_matrix.loc[i, sym_coefs[j]] = self.sum_of_product(
-                    x_df[self.evidence[i]], x_df[self.evidence[j]])
+                    x_df[self.evidence[i]], x_df[self.evidence[j]]
+                )
 
-        coef_matrix.insert(0, 'b0_coef', sum_x[self.evidence].values)
+        coef_matrix.insert(0, "b0_coef", sum_x[self.evidence].values)
         row_1 = np.append([len(x_df)], sum_x[self.evidence].values)
         coef_matrix.loc[-1] = row_1
         coef_matrix.index = coef_matrix.index + 1  # shifting index
         coef_matrix.sort_index(inplace=True)
 
-        beta_coef_matrix = np.matrix(coef_matrix.values, dtype='float')
+        beta_coef_matrix = np.matrix(coef_matrix.values, dtype="float")
         coef_inv = np.linalg.inv(beta_coef_matrix)
         beta_est = np.array(np.matmul(coef_inv, np.transpose(x)))
         self.beta = beta_est[0]
 
         sigma_est = 0
         x_len_df = len(x_df)
         for i in range(0, x_len):
             for j in range(0, x_len):
-                sigma_est += self.beta[i + 1] * self.beta[j + 1] * (self.sum_of_product(
-                    x_df[self.evidence[i]], x_df[self.evidence[j]]) / x_len_df - np.mean(x_df[self.evidence[i]]) * np.mean(x_df[self.evidence[j]]))
+                sigma_est += (
+                    self.beta[i + 1]
+                    * self.beta[j + 1]
+                    * (
+                        self.sum_of_product(
+                            x_df[self.evidence[i]], x_df[self.evidence[j]]
+                        )
+                        / x_len_df
+                        - np.mean(x_df[self.evidence[i]])
+                        * np.mean(x_df[self.evidence[j]])
+                    )
+                )
 
         sigma_est = np.sqrt(
-            self.sum_of_product(
-                x_df['(Y|X)'],
-                x_df['(Y|X)']) /
-            x_len_df -
-            np.mean(
-                x_df['(Y|X)']) *
-            np.mean(
-                x_df['(Y|X)']) -
-            sigma_est)
+            self.sum_of_product(x_df["(Y|X)"], x_df["(Y|X)"]) / x_len_df
+            - np.mean(x_df["(Y|X)"]) * np.mean(x_df["(Y|X)"])
+            - sigma_est
+        )
         self.sigma_yx = sigma_est
         return self.beta, self.sigma_yx
 
-    def fit(
-            self,
-            data,
-            states,
-            estimator=None,
-            complete_samples_only=True,
-            **kwargs):
+    def fit(self, data, states, estimator=None, complete_samples_only=True, **kwargs):
         """
         Determine βs from data
 
         Parameters
         ----------
         data: pandas.DataFrame
             Dataframe containing samples from the conditional distribution, p(Y|X)
             estimator: 'MLE' or 'MAP'
 
         completely_samples_only: boolean (True or False)
             Are they downsampled or complete? Defaults to True
 
         """
-        if estimator == 'MLE':
+        if estimator == "MLE":
             mean, variance = self.maximum_likelihood_estimator(data, states)
-        elif estimator == 'MAP':
+        elif estimator == "MAP":
             raise NotImplementedError(
-                "fit method has not been implemented using Maximum A-Priori (MAP)")
+                "fit method has not been implemented using Maximum A-Priori (MAP)"
+            )
 
         return mean, variance
 
     @property
     def pdf(self):
-
         def _pdf(*args):
             # The first element of args is the value of the variable on which CPD is defined
             # and the rest of the elements give the mean values of the parent
             # variables.
-            mean = sum([arg * coeff for (arg, coeff)
-                        in zip(args[1:], self.beta_vector)]) + self.beta_0
+            mean = (
+                sum([arg * coeff for (arg, coeff) in zip(args[1:], self.beta_vector)])
+                + self.beta_0
+            )
             return multivariate_normal.pdf(
-                args[0], np.array(mean), np.array([[self.variance]]))
+                args[0], np.array(mean), np.array([[self.variance]])
+            )
 
         return _pdf
 
     def copy(self):
         """
         Returns a copy of the distribution.
 
@@ -215,30 +221,36 @@
         >>> cpd = LinearGaussianCPD('Y',  [0.2, -2, 3, 7], 9.6, ['X1', 'X2', 'X3'])
         >>> copy_cpd = cpd.copy()
         >>> copy_cpd.variable
         'Y'
         >>> copy_cpd.evidence
         ['X1', 'X2', 'X3']
         """
-        copy_cpd = LinearGaussianCPD(self.variable, self.beta, self.variance,
-                                     list(self.evidence))
+        copy_cpd = LinearGaussianCPD(
+            self.variable, self.beta, self.variance, list(self.evidence)
+        )
 
         return copy_cpd
 
     def __str__(self):
         if self.evidence and list(self.beta_vector):
             # P(Y| X1, X2, X3) = N(-2*X1_mu + 3*X2_mu + 7*X3_mu; 0.2)
             rep_str = "P({node} | {parents}) = N({mu} + {b_0}; {sigma})".format(
                 node=str(self.variable),
-                parents=', '.join([str(var) for var in self.evidence]),
-                mu=" + ".join(["{coeff}*{parent}".format(
-                    coeff=coeff, parent=parent) for coeff, parent in
-                    zip(self.beta_vector, self.evidence)]),
+                parents=", ".join([str(var) for var in self.evidence]),
+                mu=" + ".join(
+                    [
+                        "{coeff}*{parent}".format(coeff=coeff, parent=parent)
+                        for coeff, parent in zip(self.beta_vector, self.evidence)
+                    ]
+                ),
                 b_0=str(self.beta_0),
-                sigma=str(self.variance))
+                sigma=str(self.variance),
+            )
         else:
             # P(X) = N(1, 4)
             rep_str = "P({X}) = N({beta_0}; {variance})".format(
                 X=str(self.variable),
                 beta_0=str(self.beta_0),
-                variance=str(self.variance))
+                variance=str(self.variance),
+            )
         return rep_str
```

### Comparing `pgmpy-0.1.7/pgmpy/factors/FactorSet.py` & `pgmpy-0.1.9/pgmpy/factors/FactorSet.py`

 * *Files 0% similar despite different names*

```diff
@@ -11,14 +11,15 @@
 
     A factor set provides a compact representation of  higher dimensional factor
     :math:`\phi_1\cdot\phi_2\cdots\phi_n`
 
     For example the factor set corresponding to factor :math:`\phi_1\cdot\phi_2` would be the union of the factors
     :math:`\phi_1` and :math:`\phi_2` i.e. factor set :math:`\vec\phi = \phi_1 \cup \phi_2`.
     """
+
     def __init__(self, *factors_list):
         """
         Initialize the factor set class.
 
         Parameters
         ----------
         factors_list: Factor1, Factor2, ....
@@ -208,15 +209,17 @@
              <DiscreteFactor representing phi(x5:2, x6:2, x7:2) at 0x7f8e32b5b650>,
              <DiscreteFactor representing phi(x1:2, x2:3, x3:2) at 0x7f8e32b5b050>,
              <DiscreteFactor representing phi(x5:2, x7:2, x8:2) at 0x7f8e32b5b8d0>])
         """
         factor_set = self if inplace else self.copy()
         factor_set1 = factorset.copy()
 
-        factor_set.add_factors(*[phi.identity_factor() / phi for phi in factor_set1.factors])
+        factor_set.add_factors(
+            *[phi.identity_factor() / phi for phi in factor_set1.factors]
+        )
 
         if not inplace:
             return factor_set
 
     def marginalize(self, variables, inplace=True):
         """
         Marginalizes the factors present in the factor sets with respect to the given variables.
@@ -242,28 +245,33 @@
         >>> factor_set1 = FactorSet(phi1, phi2)
         >>> factor_set1.marginalize('x1')
         >>> print(factor_set1)
         set([<DiscreteFactor representing phi(x2:3, x3:2) at 0x7f8e32b4cc10>,
              <DiscreteFactor representing phi(x3:2, x4:2) at 0x7f8e32b4cf90>])
         """
         if isinstance(variables, six.string_types):
-            raise TypeError('Expected list or array-like type got type str')
+            raise TypeError("Expected list or array-like type got type str")
 
         factor_set = self if inplace else self.copy()
 
-        factors_to_be_marginalized = set(filter(lambda x: set(x.scope()).intersection(variables),
-                                                factor_set.factors))
+        factors_to_be_marginalized = set(
+            filter(lambda x: set(x.scope()).intersection(variables), factor_set.factors)
+        )
 
         for factor in factors_to_be_marginalized:
-            variables_to_be_marginalized = list(set(factor.scope()).intersection(variables))
+            variables_to_be_marginalized = list(
+                set(factor.scope()).intersection(variables)
+            )
             if inplace:
                 factor.marginalize(variables_to_be_marginalized, inplace=True)
             else:
                 factor_set.remove_factors(factor)
-                factor_set.add_factors(factor.marginalize(variables_to_be_marginalized, inplace=False))
+                factor_set.add_factors(
+                    factor.marginalize(variables_to_be_marginalized, inplace=False)
+                )
 
         if not inplace:
             return factor_set
 
     def __mul__(self, other):
         return self.product(other)
```

### Comparing `pgmpy-0.1.7/pgmpy/factors/base.py` & `pgmpy-0.1.9/pgmpy/factors/base.py`

 * *Files 5% similar despite different names*

```diff
@@ -3,14 +3,15 @@
 from pgmpy.extern.six.moves import reduce
 
 
 class BaseFactor(object):
     """
     Base class for Factors. Any Factor implementation should inherit this class.
     """
+
     def __init__(self, *args, **kwargs):
         pass
 
     @abstractmethod
     def is_valid_cpd(self):
         pass
 
@@ -59,16 +60,17 @@
             [[10, 30],
              [55, 77]]]])
     """
     if not all(isinstance(phi, BaseFactor) for phi in args):
         raise TypeError("Arguments must be factors")
     # Check if all of the arguments are of the same type
     elif len(set(map(type, args))) != 1:
-            raise NotImplementedError("All the args are expected to ",
-                                      "be instances of the same factor class.")
+        raise NotImplementedError(
+            "All the args are expected to be instances of the same factor class."
+        )
 
     return reduce(lambda phi1, phi2: phi1 * phi2, args)
 
 
 def factor_divide(phi1, phi2):
     """
     Returns `DiscreteFactor` representing `phi1 / phi2`.
@@ -106,11 +108,12 @@
             [ 5.        ,  2.75      ]]])
     """
     if not isinstance(phi1, BaseFactor) or not isinstance(phi2, BaseFactor):
         raise TypeError("phi1 and phi2 should be factors instances")
 
     # Check if all of the arguments are of the same type
     elif type(phi1) != type(phi2):
-        raise NotImplementedError("All the args are expected to be instances",
-                                  "of the same factor class.")
+        raise NotImplementedError(
+            "All the args are expected to be instances of the same factor class."
+        )
 
     return phi1.divide(phi2, inplace=False)
```

### Comparing `pgmpy-0.1.7/pgmpy/factors/distributions/CustomDistribution.py` & `pgmpy-0.1.9/pgmpy/factors/distributions/CustomDistribution.py`

 * *Files 3% similar despite different names*

```diff
@@ -28,15 +28,17 @@
         >>> dirichlet_dist = CustomDistribution(variables=['x', 'y'], distribution=dirichlet_pdf)
         >>> dirichlet_dist.variables
         ['x', 'y']
         """
         if not isinstance(variables, (list, tuple, np.ndarray)):
             raise TypeError(
                 "variables: Expected type: iterable, got: {type}".format(
-                    type=type(variables)))
+                    type=type(variables)
+                )
+            )
 
         if len(set(variables)) != len(variables):
             raise ValueError("Multiple variables can't have the same name")
 
         self._variables = list(variables)
         self._pdf = distribution
 
@@ -216,42 +218,47 @@
         >>> custom_dist.reduce([('y', 2)])
         >>> custom_dist.variables
         ['x', 'z']
         >>> custom_dist.assignment(1, 3)
         24.0
         """
         if not isinstance(values, (list, tuple, np.ndarray)):
-            raise TypeError("variables: Expected type: iterable, "
-                            "got: {var_type}".format(var_type=type(values)))
+            raise TypeError(
+                "variables: Expected type: iterable, "
+                "got: {var_type}".format(var_type=type(values))
+            )
 
         for var, value in values:
             if var not in self.variables:
                 raise ValueError("{var} not in scope.".format(var=var))
 
         phi = self if inplace else self.copy()
 
         var_to_remove = [var for var, value in values]
         var_to_keep = [var for var in self.variables if var not in var_to_remove]
 
-        reduced_var_index = [(self.variables.index(var), value) for var, value in values]
+        reduced_var_index = [
+            (self.variables.index(var), value) for var, value in values
+        ]
         pdf = self.pdf
 
         def reduced_pdf(*args, **kwargs):
             reduced_args = list(args)
             reduced_kwargs = kwargs.copy()
 
             if reduced_args:
                 for index, val in reduced_var_index:
                     reduced_args.insert(index, val)
             if reduced_kwargs:
                 for variable, val in values:
                     reduced_kwargs[variable] = val
             if reduced_args and reduced_kwargs:
-                reduced_args = [arg for arg in reduced_args if arg not in
-                                reduced_kwargs.values()]
+                reduced_args = [
+                    arg for arg in reduced_args if arg not in reduced_kwargs.values()
+                ]
 
             return pdf(*reduced_args, **reduced_kwargs)
 
         phi.variables = var_to_keep
         phi._pdf = reduced_pdf
 
         if not inplace:
@@ -294,16 +301,18 @@
         >>> normal_dist.assignment(1)
         0.24197072451914328
         """
         if len(variables) == 0:
             raise ValueError("Shouldn't be calling marginalize over no variable.")
 
         if not isinstance(variables, (list, tuple, np.ndarray)):
-            raise TypeError("variables: Expected type iterable, "
-                            "got: {var_type}".format(var_type=type(variables)))
+            raise TypeError(
+                "variables: Expected type iterable, "
+                "got: {var_type}".format(var_type=type(variables))
+            )
 
         for var in variables:
             if var not in self.variables:
                 raise ValueError("{var} not in scope.".format(var=var))
 
         phi = self if inplace else self.copy()
 
@@ -313,19 +322,26 @@
         pdf = phi._pdf
 
         # The arguments need to be reordered because integrate.nquad
         # integrates the first n-arguments of the function passed.
 
         def reordered_pdf(*args):
             # ordered_args restores the original order as it was in self.variables
-            ordered_args = [args[reordered_var_index.index(index_id)] for index_id in range(len(all_var))]
+            ordered_args = [
+                args[reordered_var_index.index(index_id)]
+                for index_id in range(len(all_var))
+            ]
             return pdf(*ordered_args)
 
         def marginalized_pdf(*args):
-            return integrate.nquad(reordered_pdf, [[-np.inf, np.inf] for i in range(len(variables))], args=args)[0]
+            return integrate.nquad(
+                reordered_pdf,
+                [[-np.inf, np.inf] for i in range(len(variables))],
+                args=args,
+            )[0]
 
         phi._pdf = marginalized_pdf
         phi.variables = var_to_keep
 
         if not inplace:
             return phi
 
@@ -367,15 +383,18 @@
 
         phi._pdf = lambda *args: pdf(*args) / pdf_mod
 
         if not inplace:
             return phi
 
     def is_valid_cpd(self):
-        return np.isclose(integrate.nquad(self.pdf, [[-np.inf, np.inf] for var in self.variables])[0], 1)
+        return np.isclose(
+            integrate.nquad(self.pdf, [[-np.inf, np.inf] for var in self.variables])[0],
+            1,
+        )
 
     def _operate(self, other, operation, inplace=True):
         """
         Gives the CustomDistribution operation (product or divide) with
         the other distribution.
 
         Parameters
@@ -395,32 +414,38 @@
         -------
         CustomDistribution or None:
                         if inplace=True (default) returns None
                         if inplace=False returns a new `CustomDistribution` instance.
 
         """
         if not isinstance(other, CustomDistribution):
-            raise TypeError("CustomDistribution objects can only be multiplied "
-                            "or divided with another CustomDistribution  "
-                            "object. Got {other_type}, expected: "
-                            "CustomDistribution.".format(other_type=type(other)))
+            raise TypeError(
+                "CustomDistribution objects can only be multiplied "
+                "or divided with another CustomDistribution  "
+                "object. Got {other_type}, expected: "
+                "CustomDistribution.".format(other_type=type(other))
+            )
 
         phi = self if inplace else self.copy()
         pdf = self.pdf
         self_var = [var for var in self.variables]
 
-        modified_pdf_var = self_var + [var for var in other.variables if var not in self_var]
+        modified_pdf_var = self_var + [
+            var for var in other.variables if var not in self_var
+        ]
 
         def modified_pdf(*args):
-            self_pdf_args = list(args[:len(self_var)])
-            other_pdf_args = [args[modified_pdf_var.index(var)] for var in other.variables]
+            self_pdf_args = list(args[: len(self_var)])
+            other_pdf_args = [
+                args[modified_pdf_var.index(var)] for var in other.variables
+            ]
 
-            if operation == 'product':
+            if operation == "product":
                 return pdf(*self_pdf_args) * other._pdf(*other_pdf_args)
-            if operation == 'divide':
+            if operation == "divide":
                 return pdf(*self_pdf_args) / other._pdf(*other_pdf_args)
 
         phi.variables = modified_pdf_var
         phi._pdf = modified_pdf
 
         if not inplace:
             return phi
@@ -456,15 +481,15 @@
         >>> sn3.assignment(0, 0)
         0.063493635934240983
 
         >>> sn3 = sn1 * sn2
         >>> sn3.assignment(0, 0)
         0.063493635934240983
         """
-        return self._operate(other, 'product', inplace)
+        return self._operate(other, "product", inplace)
 
     def divide(self, other, inplace=True):
         """
         Gives the CustomDistribution divide with the other factor.
 
         Parameters
         ----------
@@ -496,15 +521,15 @@
         >>> sn3 = sn2 / sn1
         >>> sn3.assignment(0, 0)
         0.3989422804014327
         """
         if set(other.variables) - set(self.variables):
             raise ValueError("Scope of divisor should be a subset of dividend")
 
-        return self._operate(other, 'divide', inplace)
+        return self._operate(other, "divide", inplace)
 
     def __mul__(self, other):
         return self.product(other, inplace=False)
 
     def __rmul__(self, other):
         return self.__mul__(other)
```

### Comparing `pgmpy-0.1.7/pgmpy/factors/distributions/GaussianDistribution.py` & `pgmpy-0.1.9/pgmpy/factors/distributions/GaussianDistribution.py`

 * *Files 4% similar despite different names*

```diff
@@ -12,14 +12,15 @@
     """
     In its most common representation, a multivariate Gaussian distribution
     over X1, X2, ..., Xn is characterized by an n-dimensional mean vector μ,
     and a symmetric n x n covariance matrix Σ.
 
     This is the base class for its representation.
     """
+
     def __init__(self, variables, mean, cov):
         """
         Parameters
         ----------
         variables: iterable of any hashable python object
             The variables for which the distribution is defined.
 
@@ -55,23 +56,26 @@
 
         self.variables = variables
         self.mean = np.asarray(np.reshape(mean, (no_of_var, 1)), dtype=float)
         self.covariance = np.asarray(cov, dtype=float)
         self._precision_matrix = None
 
         if len(mean) != no_of_var:
-            raise ValueError("Length of mean_vector must be equal to the",
-                             "number of variables.")
+            raise ValueError(
+                "Length of mean_vector must be equal to the", "number of variables."
+            )
 
         if self.covariance.shape != (no_of_var, no_of_var):
-            raise ValueError("The Covariance matrix should be a square matrix",
-                             " with order equal to the number of variables. ",
-                             "Got: {got_shape}, Expected: {exp_shape}".format
-                             (got_shape=self.covariance.shape,
-                              exp_shape=(no_of_var, no_of_var)))
+            raise ValueError(
+                "The Covariance matrix should be a square matrix",
+                " with order equal to the number of variables. ",
+                "Got: {got_shape}, Expected: {exp_shape}".format(
+                    got_shape=self.covariance.shape, exp_shape=(no_of_var, no_of_var)
+                ),
+            )
 
     @property
     def pdf(self):
         """
         Returns the probability density function(pdf).
 
         Returns
@@ -88,15 +92,16 @@
         ...                [-2, -5, 8]])
         >>> dist.pdf
         <function pgmpy.factors.distributions.GaussianDistribution.GaussianDistribution.pdf.<locals>.<lambda>>
         >>> dist.pdf([0, 0, 0])
         0.0014805631279234139
         """
         return lambda *args: multivariate_normal.pdf(
-            args, self.mean.reshape(1, len(self.variables))[0], self.covariance)
+            args, self.mean.reshape(1, len(self.variables))[0], self.covariance
+        )
 
     def assignment(self, *x):
         """
         Returns the probability value of the PDF at the given parameter values.
 
         Parameters
         ----------
@@ -194,22 +199,24 @@
         array([[ 1.],
                [-3.]]))
         >>> dis.covariance
         array([[4., 2.],
                [2., 5.]])
         """
         if not isinstance(variables, list):
-            raise TypeError("variables: Expected type list or array-like,"
-                            "got type {var_type}".format(
-                                var_type=type(variables)))
+            raise TypeError(
+                "variables: Expected type list or array-like,"
+                "got type {var_type}".format(var_type=type(variables))
+            )
 
         phi = self if inplace else self.copy()
 
-        index_to_keep = [self.variables.index(var) for var in self.variables
-                         if var not in variables]
+        index_to_keep = [
+            self.variables.index(var) for var in self.variables if var not in variables
+        ]
 
         phi.variables = [phi.variables[index] for index in index_to_keep]
         phi.mean = phi.mean[index_to_keep]
         phi.covariance = phi.covariance[np.ix_(index_to_keep, index_to_keep)]
         phi._precision_matrix = None
 
         if not inplace:
@@ -271,35 +278,43 @@
                [ 1.]])
         >>> dis.covariance
         array([[ 4., -4.],
                [-4.,  7.]])
 
         """
         if not isinstance(values, list):
-            raise TypeError("values: Expected type list or array-like, ",
-                            "got type {var_type}".format(
-                                var_type=type(values)))
+            raise TypeError(
+                "values: Expected type list or array-like, ",
+                "got type {var_type}".format(var_type=type(values)),
+            )
 
         phi = self if inplace else self.copy()
 
         var_to_reduce = [var for var, value in values]
 
         # index_to_keep -> j vector
-        index_to_keep = [self.variables.index(var) for var in self.variables
-                         if var not in var_to_reduce]
+        index_to_keep = [
+            self.variables.index(var)
+            for var in self.variables
+            if var not in var_to_reduce
+        ]
         # index_to_reduce -> i vector
         index_to_reduce = [self.variables.index(var) for var in var_to_reduce]
 
         mu_j = self.mean[index_to_keep]
         mu_i = self.mean[index_to_reduce]
-        x_i = np.array([value for var, value in values]).reshape(len(index_to_reduce), 1)
+        x_i = np.array([value for var, value in values]).reshape(
+            len(index_to_reduce), 1
+        )
 
         sig_i_j = self.covariance[np.ix_(index_to_reduce, index_to_keep)]
         sig_j_i = self.covariance[np.ix_(index_to_keep, index_to_reduce)]
-        sig_i_i_inv = np.linalg.inv(self.covariance[np.ix_(index_to_reduce, index_to_reduce)])
+        sig_i_i_inv = np.linalg.inv(
+            self.covariance[np.ix_(index_to_reduce, index_to_reduce)]
+        )
         sig_j_j = self.covariance[np.ix_(index_to_keep, index_to_keep)]
 
         phi.variables = [self.variables[index] for index in index_to_keep]
         phi.mean = mu_j + np.dot(np.dot(sig_j_i, sig_i_i_inv), x_i - mu_i)
         phi.covariance = sig_j_j - np.dot(np.dot(sig_j_i, sig_i_i_inv), sig_i_j)
         phi._precision_matrix = None
 
@@ -349,17 +364,17 @@
                 [ 2,  5, -5],
                 [-2, -5,  8]])
         >>> copy_dis.precision_matrix
         array([[ 0.3125    , -0.125     ,  0.        ],
                 [-0.125     ,  0.58333333,  0.33333333],
                 [ 0.        ,  0.33333333,  0.33333333]])
         """
-        copy_distribution = GaussianDistribution(variables=self.variables,
-                                                 mean=self.mean.copy(),
-                                                 cov=self.covariance.copy())
+        copy_distribution = GaussianDistribution(
+            variables=self.variables, mean=self.mean.copy(), cov=self.covariance.copy()
+        )
         if self._precision_matrix is not None:
             copy_distribution._precision_matrix = self._precision_matrix.copy()
 
         return copy_distribution
 
     def to_canonical_factor(self):
         u"""
@@ -409,16 +424,17 @@
         sigma = self.covariance
 
         K = self.precision_matrix
 
         h = np.dot(K, mu)
 
         g = -(0.5) * np.dot(mu.T, h)[0, 0] - np.log(
-            np.power(2 * np.pi, len(self.variables)/2) *
-            np.power(abs(np.linalg.det(sigma)), 0.5))
+            np.power(2 * np.pi, len(self.variables) / 2)
+            * np.power(abs(np.linalg.det(sigma)), 0.5)
+        )
 
         return CanonicalDistribution(self.variables, K, h, g)
 
     def _operate(self, other, operation, inplace=True):
         """
         Gives the CanonicalDistribution operation (product or divide) with
         the other factor.
@@ -453,16 +469,19 @@
                [-1. , -2.5,  4. ,  4.5]])
         >>> dis3.mean
         array([[ 1.6],
                [-1.5],
                [ 1.6],
                [ 3.5]])
         """
-        phi = self.to_canonical_factor()._operate(
-            other.to_canonical_factor(), operation, inplace=False).to_joint_gaussian()
+        phi = (
+            self.to_canonical_factor()
+            ._operate(other.to_canonical_factor(), operation, inplace=False)
+            .to_joint_gaussian()
+        )
 
         if not inplace:
             return phi
 
     def product(self, other, inplace=True):
         """
         TODO: Make it work when using `*` instead of product.
@@ -499,15 +518,15 @@
                [-1. , -2.5,  4. ,  4.5]])
         >>> dis3.mean
         array([[ 1.6],
                [-1.5],
                [ 1.6],
                [ 3.5]])
         """
-        return self._operate(other, operation='product', inplace=inplace)
+        return self._operate(other, operation="product", inplace=inplace)
 
     def divide(self, other, inplace=True):
         """
         Returns the division of two gaussian distributions.
 
         Parameters
         ----------
@@ -539,33 +558,37 @@
                [-1. , -2.5,  4. ,  4.5]])
         >>> dis3.mean
         array([[ 1.6],
                [-1.5],
                [ 1.6],
                [ 3.5]])
         """
-        return self._operate(other, operation='divide', inplace=inplace)
+        return self._operate(other, operation="divide", inplace=inplace)
 
     def __repr__(self):
         return "GaussianDistribution representing N({var}) at {address}".format(
-            var=self.variables, address=hex(id(self)))
+            var=self.variables, address=hex(id(self))
+        )
 
     def __mul__(self, other):
         return self.product(other, inplace=False)
 
     def __rmul__(self, other):
         return self.__mul__(other)
 
     def __truediv__(self, other):
         return self.divide(other, inplace=False)
 
     __div__ = __truediv__
 
     def __eq__(self, other):
-        if not (isinstance(self, GaussianDistribution) and isinstance(self, GaussianDistribution)):
+        if not (
+            isinstance(self, GaussianDistribution)
+            and isinstance(self, GaussianDistribution)
+        ):
             return False
 
         elif set(self.scope()) != set(other.scope()):
             return False
 
         else:
             # Computing transform_index to be able to easily have variables in same order.
```

### Comparing `pgmpy-0.1.7/pgmpy/factors/distributions/base.py` & `pgmpy-0.1.9/pgmpy/factors/distributions/base.py`

 * *Files 0% similar despite different names*

```diff
@@ -38,8 +38,9 @@
     def product(self, other, inplace=True):
         pass
 
     @abstractmethod
     def divide(self, other, inplace=True):
         pass
     """
+
     pass
```

### Comparing `pgmpy-0.1.7/pgmpy/factors/distributions/CanonicalDistribution.py` & `pgmpy-0.1.9/pgmpy/factors/distributions/CanonicalDistribution.py`

 * *Files 3% similar despite different names*

```diff
@@ -24,14 +24,15 @@
 
     Reference
     ---------
     Probabilistic Graphical Models, Principles and Techniques,
     Daphne Koller and Nir Friedman, Section 14.2, Chapter 14.
 
     """
+
     def __init__(self, variables, K, h, g):
         """
         Parameters
         ----------
         variables: list or array-like
         The variables for wich the distribution is defined.
 
@@ -66,34 +67,41 @@
         >>> phi.g
         -3
 
         """
         no_of_var = len(variables)
 
         if len(h) != no_of_var:
-            raise ValueError("Length of h parameter vector must be equal to "
-                             "the number of variables.")
+            raise ValueError(
+                "Length of h parameter vector must be equal to "
+                "the number of variables."
+            )
 
         self.variables = variables
         self.h = np.asarray(np.reshape(h, (no_of_var, 1)), dtype=float)
         self.g = g
         self.K = np.asarray(K, dtype=float)
 
         if self.K.shape != (no_of_var, no_of_var):
-            raise ValueError("The K matrix should be a square matrix with "
-                             "order equal to the number of variables. Got: "
-                             "{got_shape}, Expected: {exp_shape}".format(
-                                 got_shape=self.K.shape,
-                                 exp_shape=(no_of_var, no_of_var)))
+            raise ValueError(
+                "The K matrix should be a square matrix with "
+                "order equal to the number of variables. Got: "
+                "{got_shape}, Expected: {exp_shape}".format(
+                    got_shape=self.K.shape, exp_shape=(no_of_var, no_of_var)
+                )
+            )
 
     @property
     def pdf(self):
         def fun(*args):
             x = np.array(args)
-            return np.exp(self.g + np.dot(x, self.h)[0] - 0.5 * np.dot(x.T, np.dot(self.K, x)))
+            return np.exp(
+                self.g + np.dot(x, self.h)[0] - 0.5 * np.dot(x.T, np.dot(self.K, x))
+            )
+
         return fun
 
     def assignment(self, *x):
         """
         Returns the probability value of the PDF at the given parameter values.
         Parameters
         ----------
@@ -153,16 +161,17 @@
         array([[1],
                [-1]])
 
         >>> phi2.g
         -3
 
         """
-        copy_factor = CanonicalDistribution(self.variables, self.K.copy(),
-                                            self.h.copy(), self.g)
+        copy_factor = CanonicalDistribution(
+            self.variables, self.K.copy(), self.h.copy(), self.g
+        )
 
         return copy_factor
 
     def to_joint_gaussian(self):
         """
         Return an equivalent Joint Gaussian Distribution.
 
@@ -255,27 +264,32 @@
         array([[ 1. ],
                [ 4.5]])
 
         >>> phi.g
         -2.375
         """
         if not isinstance(values, (list, tuple, np.ndarray)):
-            raise TypeError("variables: Expected type list or array-like, "
-                            "got type {var_type}".format(var_type=type(values)))
+            raise TypeError(
+                "variables: Expected type list or array-like, "
+                "got type {var_type}".format(var_type=type(values))
+            )
 
         if not all([var in self.variables for var, value in values]):
             raise ValueError("Variable not in scope.")
 
         phi = self if inplace else self.copy()
 
         var_to_reduce = [var for var, value in values]
 
         # index_to_keep -> j vector
-        index_to_keep = [self.variables.index(var) for var in self.variables
-                         if var not in var_to_reduce]
+        index_to_keep = [
+            self.variables.index(var)
+            for var in self.variables
+            if var not in var_to_reduce
+        ]
         # index_to_reduce -> i vector
         index_to_reduce = [self.variables.index(var) for var in var_to_reduce]
 
         K_i_i = self.K[np.ix_(index_to_keep, index_to_keep)]
         K_i_j = self.K[np.ix_(index_to_keep, index_to_reduce)]
         K_j_j = self.K[np.ix_(index_to_reduce, index_to_reduce)]
         h_i = self.h[index_to_keep]
@@ -283,15 +297,17 @@
 
         # The values for the reduced variables.
         y = np.array([value for var, value in values]).reshape(len(index_to_reduce), 1)
 
         phi.variables = [self.variables[index] for index in index_to_keep]
         phi.K = K_i_i
         phi.h = h_i - np.dot(K_i_j, y)
-        phi.g = self.g + (np.dot(h_j.T, y) - (0.5 * np.dot(np.dot(y.T, K_j_j), y)))[0][0]
+        phi.g = (
+            self.g + (np.dot(h_j.T, y) - (0.5 * np.dot(np.dot(y.T, K_j_j), y)))[0][0]
+        )
 
         if not inplace:
             return phi
 
     def marginalize(self, variables, inplace=True):
         u"""
         Modifies the factor with marginalized values.
@@ -354,25 +370,28 @@
         array([[ 1. ],
                 [ 3.5]])
 
         >>> phi.g
         0.22579135
         """
         if not isinstance(variables, (list, tuple, np.ndarray)):
-            raise TypeError("variables: Expected type list or array-like, "
-                            "got type {var_type}".format(var_type=type(variables)))
+            raise TypeError(
+                "variables: Expected type list or array-like, "
+                "got type {var_type}".format(var_type=type(variables))
+            )
 
         if not all([var in self.variables for var in variables]):
             raise ValueError("Variable not in scope.")
 
         phi = self if inplace else self.copy()
 
         # index_to_keep -> i vector
-        index_to_keep = [self.variables.index(var) for var in self.variables
-                         if var not in variables]
+        index_to_keep = [
+            self.variables.index(var) for var in self.variables if var not in variables
+        ]
         # index_to_marginalize -> j vector
         index_to_marginalize = [self.variables.index(var) for var in variables]
 
         K_i_i = self.K[np.ix_(index_to_keep, index_to_keep)]
         K_i_j = self.K[np.ix_(index_to_keep, index_to_marginalize)]
         K_j_i = self.K[np.ix_(index_to_marginalize, index_to_keep)]
         K_j_j = self.K[np.ix_(index_to_marginalize, index_to_marginalize)]
@@ -380,16 +399,23 @@
         h_i = self.h[index_to_keep]
         h_j = self.h[index_to_marginalize]
 
         phi.variables = [self.variables[index] for index in index_to_keep]
 
         phi.K = K_i_i - np.dot(np.dot(K_i_j, K_j_j_inv), K_j_i)
         phi.h = h_i - np.dot(np.dot(K_i_j, K_j_j_inv), h_j)
-        phi.g = self.g + 0.5 * (len(variables) * np.log(2 * np.pi) -
-                                np.log(abs(np.linalg.det(K_j_j))) + np.dot(np.dot(h_j.T, K_j_j), h_j))[0][0]
+        phi.g = (
+            self.g
+            + 0.5
+            * (
+                len(variables) * np.log(2 * np.pi)
+                - np.log(abs(np.linalg.det(K_j_j)))
+                + np.dot(np.dot(h_j.T, K_j_j), h_j)
+            )[0][0]
+        )
 
         if not inplace:
             return phi
 
     def _operate(self, other, operation, inplace=True):
         """
         Gives the CanonicalDistribution operation (product or divide) with
@@ -456,19 +482,22 @@
         -3
 
         """
         if not isinstance(other, CanonicalDistribution):
             raise TypeError(
                 "CanonicalDistribution object can only be multiplied or divided "
                 "with an another CanonicalDistribution object. Got {other_type}, "
-                "expected CanonicalDistribution.".format(other_type=type(other)))
+                "expected CanonicalDistribution.".format(other_type=type(other))
+            )
 
         phi = self if inplace else self.copy()
 
-        all_vars = self.variables + [var for var in other.variables if var not in self.variables]
+        all_vars = self.variables + [
+            var for var in other.variables if var not in self.variables
+        ]
         no_of_var = len(all_vars)
 
         self_var_index = [all_vars.index(var) for var in self.variables]
         other_var_index = [all_vars.index(var) for var in other.variables]
 
         def _extend_K_scope(K, index):
             ext_K = np.zeros([no_of_var, no_of_var])
@@ -478,22 +507,30 @@
         def _extend_h_scope(h, index):
             ext_h = np.zeros(no_of_var).reshape(no_of_var, 1)
             ext_h[index] = h
             return ext_h
 
         phi.variables = all_vars
 
-        if operation == 'product':
-            phi.K = _extend_K_scope(self.K, self_var_index) + _extend_K_scope(other.K, other_var_index)
-            phi.h = _extend_h_scope(self.h, self_var_index) + _extend_h_scope(other.h, other_var_index)
+        if operation == "product":
+            phi.K = _extend_K_scope(self.K, self_var_index) + _extend_K_scope(
+                other.K, other_var_index
+            )
+            phi.h = _extend_h_scope(self.h, self_var_index) + _extend_h_scope(
+                other.h, other_var_index
+            )
             phi.g = self.g + other.g
 
         else:
-            phi.K = _extend_K_scope(self.K, self_var_index) - _extend_K_scope(other.K, other_var_index)
-            phi.h = _extend_h_scope(self.h, self_var_index) - _extend_h_scope(other.h, other_var_index)
+            phi.K = _extend_K_scope(self.K, self_var_index) - _extend_K_scope(
+                other.K, other_var_index
+            )
+            phi.h = _extend_h_scope(self.h, self_var_index) - _extend_h_scope(
+                other.h, other_var_index
+            )
             phi.g = self.g - other.g
 
         if not inplace:
             return phi
 
     def product(self, other, inplace=True):
         """
@@ -529,15 +566,15 @@
                [-1. , -2.5,  4. ,  4.5]])
         >>> dis3.mean
         array([[ 1.6],
                [-1.5],
                [ 1.6],
                [ 3.5]])
         """
-        return self._operate(other, operation='product', inplace=inplace)
+        return self._operate(other, operation="product", inplace=inplace)
 
     def divide(self, other, inplace=True):
         """
         Returns the division of two gaussian distributions.
 
         Parameters
         ----------
@@ -569,15 +606,15 @@
                [-1. , -2.5,  4. ,  4.5]])
         >>> dis3.mean
         array([[ 1.6],
                [-1.5],
                [ 1.6],
                [ 3.5]])
         """
-        return self._operate(other, operation='divide', inplace=inplace)
+        return self._operate(other, operation="divide", inplace=inplace)
 
     def __mul__(self, other):
         return self.product(other, inplace=False)
 
     def __rmul__(self, other):
         return self.__mul__(other)
```

### Comparing `pgmpy-0.1.7/pgmpy/factors/discrete/CPD.py` & `pgmpy-0.1.9/pgmpy/factors/discrete/CPD.py`

 * *Files 3% similar despite different names*

```diff
@@ -8,16 +8,14 @@
 
 import numpy as np
 
 from pgmpy.factors.discrete import DiscreteFactor
 from pgmpy.extern import tabulate
 from pgmpy.extern import six
 from pgmpy.extern.six.moves import range, zip
-from pgmpy.utils import StateNameInit
-from pgmpy.utils import StateNameDecorator
 
 
 class TabularCPD(DiscreteFactor):
     """
     Defines the conditional probability distribution table (cpd table)
 
     Examples
@@ -95,17 +93,24 @@
     Public Methods
     --------------
     get_values()
     marginalize([variables_list])
     normalize()
     reduce([values_list])
     """
-    @StateNameInit()
-    def __init__(self, variable, variable_card, values,
-                 evidence=None, evidence_card=None):
+
+    def __init__(
+        self,
+        variable,
+        variable_card,
+        values,
+        evidence=None,
+        evidence_card=None,
+        state_names={},
+    ):
 
         self.variable = variable
         self.variable_card = None
 
         variables = [variable]
 
         if not isinstance(variable_card, numbers.Integral):
@@ -119,36 +124,44 @@
             cardinality.extend(evidence_card)
 
         if evidence is not None:
             if isinstance(evidence, six.string_types):
                 raise TypeError("Evidence must be list, tuple or array of strings.")
             variables.extend(evidence)
             if not len(evidence_card) == len(evidence):
-                raise ValueError("Length of evidence_card doesn't match length of evidence")
+                raise ValueError(
+                    "Length of evidence_card doesn't match length of evidence"
+                )
 
         values = np.array(values)
         if values.ndim != 2:
             raise TypeError("Values must be a 2D list/array")
 
-        super(TabularCPD, self).__init__(variables, cardinality, values.flatten('C'),
-                                         state_names=self.state_names)
+        super(TabularCPD, self).__init__(
+            variables, cardinality, values.flatten("C"), state_names=state_names
+        )
 
     def __repr__(self):
-        var_str = '<TabularCPD representing P({var}:{card}'.format(
-            var=self.variable, card=self.variable_card)
+        var_str = "<TabularCPD representing P({var}:{card}".format(
+            var=self.variable, card=self.variable_card
+        )
 
         evidence = self.variables[1:]
         evidence_card = self.cardinality[1:]
         if evidence:
-            evidence_str = ' | ' + ', '.join(['{var}:{card}'.format(var=var, card=card)
-                                              for var, card in zip(evidence, evidence_card)])
+            evidence_str = " | " + ", ".join(
+                [
+                    "{var}:{card}".format(var=var, card=card)
+                    for var, card in zip(evidence, evidence_card)
+                ]
+            )
         else:
-            evidence_str = ''
+            evidence_str = ""
 
-        return var_str + evidence_str + ') at {address}>'.format(address=hex(id(self)))
+        return var_str + evidence_str + ") at {address}>".format(address=hex(id(self)))
 
     def get_values(self):
         """
         Returns the cpd
 
         Examples
         --------
@@ -159,15 +172,17 @@
         ...                  evidence='evi1', evidence_card=2)
         >>> cpd.get_values()
         array([[ 0.1,  0.1],
                [ 0.1,  0.1],
                [ 0.8,  0.8]])
         """
         if self.variable in self.variables:
-            return self.values.reshape(self.cardinality[0], np.prod(self.cardinality[1:]))
+            return self.values.reshape(
+                self.cardinality[0], np.prod(self.cardinality[1:])
+            )
         else:
             return self.values.reshape(1, np.prod(self.cardinality))
 
     def __str__(self):
         return self._make_table_str(tablefmt="grid")
 
     def _str(self, phi_or_p="p", tablefmt="fancy_grid"):
@@ -179,34 +194,49 @@
 
         evidence = self.variables[1:]
         evidence_card = self.cardinality[1:]
         if evidence:
             col_indexes = np.array(list(product(*[range(i) for i in evidence_card])))
             if self.state_names and print_state_names:
                 for i in range(len(evidence_card)):
-                    column_header = [str(evidence[i])] + ['{var}({state})'.format(
-                        var=evidence[i],
-                        state=self.state_names[evidence[i]][d])
-                        for d in col_indexes.T[i]]
+                    column_header = [str(evidence[i])] + [
+                        "{var}({state})".format(
+                            var=evidence[i], state=self.state_names[evidence[i]][d]
+                        )
+                        for d in col_indexes.T[i]
+                    ]
                     headers_list.append(column_header)
             else:
                 for i in range(len(evidence_card)):
-                    column_header = [str(evidence[i])] + ['{s}_{d}'.format(
-                        s=evidence[i], d=d) for d in col_indexes.T[i]]
+                    column_header = [str(evidence[i])] + [
+                        "{s}_{d}".format(s=evidence[i], d=d) for d in col_indexes.T[i]
+                    ]
                     headers_list.append(column_header)
 
         # Build row headers
         if self.state_names and print_state_names:
-            variable_array = [['{var}({state})'.format
-                               (var=self.variable, state=self.state_names[self.variable][i])
-                               for i in range(self.variable_card)]]
+            variable_array = [
+                [
+                    "{var}({state})".format(
+                        var=self.variable, state=self.state_names[self.variable][i]
+                    )
+                    for i in range(self.variable_card)
+                ]
+            ]
         else:
-            variable_array = [['{s}_{d}'.format(s=self.variable, d=i) for i in range(self.variable_card)]]
+            variable_array = [
+                [
+                    "{s}_{d}".format(s=self.variable, d=i)
+                    for i in range(self.variable_card)
+                ]
+            ]
         # Stack with data
-        labeled_rows = np.hstack((np.array(variable_array).T, self.get_values())).tolist()
+        labeled_rows = np.hstack(
+            (np.array(variable_array).T, self.get_values())
+        ).tolist()
         # No support for multi-headers in tabulate
         cdf_str = tabulate(headers_list + labeled_rows, tablefmt=tablefmt)
         return cdf_str
 
     def copy(self):
         """
         Returns a copy of the TabularCPD object.
@@ -229,16 +259,22 @@
                 [ 0.6,  0.2]],
 
                [[ 0.3,  0.4],
                 [ 0.4,  0.8]]])
         """
         evidence = self.variables[1:] if len(self.variables) > 1 else None
         evidence_card = self.cardinality[1:] if len(self.variables) > 1 else None
-        return TabularCPD(self.variable, self.variable_card, self.get_values(),
-                          evidence, evidence_card)
+        return TabularCPD(
+            self.variable,
+            self.variable_card,
+            self.get_values(),
+            evidence,
+            evidence_card,
+            state_names=self.state_names,
+        )
 
     def normalize(self, inplace=True):
         """
         Normalizes the cpd table.
 
         Parameters
         ----------
@@ -284,25 +320,26 @@
         ...                        ['intel', 'diff'], [2, 2])
         >>> cpd_table.marginalize(['diff'])
         >>> cpd_table.get_values()
         array([[ 0.65,  0.4 ],
                 [ 0.35,  0.6 ]])
         """
         if self.variable in variables:
-            raise ValueError("Marginalization not allowed on the variable on which CPD is defined")
+            raise ValueError(
+                "Marginalization not allowed on the variable on which CPD is defined"
+            )
 
         tabular_cpd = self if inplace else self.copy()
 
         super(TabularCPD, tabular_cpd).marginalize(variables)
         tabular_cpd.normalize()
 
         if not inplace:
             return tabular_cpd
 
-    @StateNameDecorator(argument='values', return_val=None)
     def reduce(self, values, inplace=True):
         """
         Reduces the cpd table to the context of given variable values.
 
         Parameters
         ----------
         values: list, array-like
@@ -320,15 +357,17 @@
         ...                        ['intel', 'diff'], [2, 2])
         >>> cpd_table.reduce([('diff', 0)])
         >>> cpd_table.get_values()
         array([[ 0.7,  0.6],
                [ 0.3,  0.4]])
         """
         if self.variable in (value[0] for value in values):
-            raise ValueError("Reduce not allowed on the variable on which CPD is defined")
+            raise ValueError(
+                "Reduce not allowed on the variable on which CPD is defined"
+            )
 
         tabular_cpd = self if inplace else self.copy()
 
         super(TabularCPD, tabular_cpd).reduce(values)
         tabular_cpd.normalize()
 
         if not inplace:
@@ -345,15 +384,20 @@
         ...                               [0.1, 0.1],
         ...                               [0.8, 0.8]],
         ...                  evidence='evi1', evidence_card=2)
         >>> factor = cpd.to_factor()
         >>> factor
         <DiscreteFactor representing phi(grade:3, evi1:2) at 0x7f847a4f2d68>
         """
-        return DiscreteFactor(self.variables, self.cardinality, self.values)
+        return DiscreteFactor(
+            variables=self.variables,
+            cardinality=self.cardinality,
+            values=self.values,
+            state_names=self.state_names,
+        )
 
     def reorder_parents(self, new_order, inplace=True):
         """
         Returns a new cpd table according to provided order.
 
         Parameters
         ----------
@@ -436,32 +480,42 @@
         >>> cpd.cardinality
         array([3, 3, 2])
         >>> cpd.variable
         'grade'
         >>> cpd.variable_card
         3
         """
-        if (len(self.variables) <= 1 or (set(new_order) - set(self.variables)) or
-                (set(self.variables[1:]) - set(new_order))):
+        if (
+            len(self.variables) <= 1
+            or (set(new_order) - set(self.variables))
+            or (set(self.variables[1:]) - set(new_order))
+        ):
             raise ValueError("New order either has missing or extra arguments")
         else:
             if new_order != self.variables[1:]:
                 evidence = self.variables[1:]
                 evidence_card = self.cardinality[1:]
                 card_map = dict(zip(evidence, evidence_card))
                 old_pos_map = dict(zip(evidence, range(len(evidence))))
                 trans_ord = [0] + [(old_pos_map[letter] + 1) for letter in new_order]
                 new_values = np.transpose(self.values, trans_ord)
 
                 if inplace:
                     variables = [self.variables[0]] + new_order
-                    cardinality = [self.variable_card] + [card_map[var] for var in new_order]
-                    super(TabularCPD, self).__init__(variables, cardinality, new_values.flatten('C'))
+                    cardinality = [self.variable_card] + [
+                        card_map[var] for var in new_order
+                    ]
+                    super(TabularCPD, self).__init__(
+                        variables, cardinality, new_values.flatten("C")
+                    )
                     return self.get_values()
                 else:
-                    return new_values.reshape(self.cardinality[0], np.prod([card_map[var] for var in new_order]))
+                    return new_values.reshape(
+                        self.cardinality[0],
+                        np.prod([card_map[var] for var in new_order]),
+                    )
             else:
                 warn("Same ordering provided as current")
                 return self.get_values()
 
     def get_evidence(self):
         return self.variables[:0:-1]
```

### Comparing `pgmpy-0.1.7/pgmpy/factors/discrete/DiscreteFactor.py` & `pgmpy-0.1.9/pgmpy/factors/discrete/DiscreteFactor.py`

 * *Files 3% similar despite different names*

```diff
@@ -2,38 +2,37 @@
 
 from itertools import product
 from collections import namedtuple
 
 import numpy as np
 
 from pgmpy.factors.base import BaseFactor
+from pgmpy.utils import StateNameMixin
 from pgmpy.extern import tabulate
 from pgmpy.extern import six
 from pgmpy.extern.six.moves import map, range, reduce, zip
-from pgmpy.utils import StateNameInit, StateNameDecorator
 
-State = namedtuple('State', ['var', 'state'])
+State = namedtuple("State", ["var", "state"])
 
 
-class DiscreteFactor(BaseFactor):
+class DiscreteFactor(BaseFactor, StateNameMixin):
     """
     Base class for DiscreteFactor.
 
     Public Methods
     --------------
     assignment(index)
     get_cardinality(variable)
     marginalize([variable_list])
     normalize()
     product(*DiscreteFactor)
     reduce([variable_values_list])
     """
 
-    @StateNameInit()
-    def __init__(self, variables, cardinality, values):
+    def __init__(self, variables, cardinality, values, state_names={}):
         """
         Initialize a factor class.
 
         Defined above, we have the following mapping from variable
         assignments to the index of the row vector in the value field:
 
         +-----+-----+-----+-------------------+
@@ -94,27 +93,37 @@
         """
         if isinstance(variables, six.string_types):
             raise TypeError("Variables: Expected type list or array like, got string")
 
         values = np.array(values, dtype=float)
 
         if len(cardinality) != len(variables):
-            raise ValueError("Number of elements in cardinality must be equal to number of variables")
+            raise ValueError(
+                "Number of elements in cardinality must be equal to number of variables"
+            )
 
         if values.size != np.product(cardinality):
-            raise ValueError("Values array must be of size: {size}".format(
-                size=np.product(cardinality)))
+            raise ValueError(
+                "Values array must be of size: {size}".format(
+                    size=np.product(cardinality)
+                )
+            )
 
         if len(set(variables)) != len(variables):
             raise ValueError("Variable names cannot be same")
 
         self.variables = list(variables)
         self.cardinality = np.array(cardinality, dtype=int)
         self.values = values.reshape(self.cardinality)
 
+        # Set the state names
+        super(DiscreteFactor, self).store_state_names(
+            variables, cardinality, state_names
+        )
+
     def scope(self):
         """
         Returns the scope of the factor.
 
         Returns
         -------
         list: List of variable names in the scope of the factor.
@@ -154,15 +163,14 @@
             raise TypeError("variables: Expected type list or array-like, got type str")
 
         if not all([var in self.variables for var in variables]):
             raise ValueError("Variable not in scope")
 
         return {var: self.cardinality[self.variables.index(var)] for var in variables}
 
-    @StateNameDecorator(argument=None, return_val=True)
     def assignment(self, index):
         """
         Returns a list of assignments for the corresponding index.
 
         Parameters
         ----------
         index: list, array-like
@@ -190,15 +198,21 @@
         rev_card = self.cardinality[::-1]
         for i, card in enumerate(rev_card):
             assignments[:, i] = index % card
             index = index // card
 
         assignments = assignments[:, ::-1]
 
-        return [[(key, val) for key, val in zip(self.variables, values)] for values in assignments]
+        return [
+            [
+                (key, self.get_state_names(key, val))
+                for key, val in zip(self.variables, values)
+            ]
+            for values in assignments
+        ]
 
     def identity_factor(self):
         """
         Returns the identity factor.
 
         Def: The identity factor of a factor has the same scope and cardinality as the original factor,
              but the values for all the assignments is 1. When the identity factor is multiplied with
@@ -220,15 +234,20 @@
                 [ 1.,  1.],
                 [ 1.,  1.]],
 
                [[ 1.,  1.],
                 [ 1.,  1.],
                 [ 1.,  1.]]])
         """
-        return DiscreteFactor(self.variables, self.cardinality, np.ones(self.values.size))
+        return DiscreteFactor(
+            variables=self.variables,
+            cardinality=self.cardinality,
+            values=np.ones(self.values.size),
+            state_names=self.state_names,
+        )
 
     def marginalize(self, variables, inplace=True):
         """
         Modifies the factor with marginalized values.
 
         Parameters
         ----------
@@ -375,15 +394,14 @@
         phi = self if inplace else self.copy()
 
         phi.values = phi.values / phi.values.sum()
 
         if not inplace:
             return phi
 
-    @StateNameDecorator(argument='values', return_val=None)
     def reduce(self, values, inplace=True):
         """
         Reduces the factor to the context of given variable values.
 
         Parameters
         ----------
         values: list, array-like
@@ -409,29 +427,34 @@
         array([2])
         >>> phi.values
         array([0., 1.])
         """
         if isinstance(values, six.string_types):
             raise TypeError("values: Expected type list or array-like, got type str")
 
-        if (any(isinstance(value, six.string_types) for value in values) or
-                not all(isinstance(state, (int, np.integer)) for var, state in values)):
-            raise TypeError("values: must contain tuples or array-like elements of the form "
-                            "(hashable object, type int)")
+        if not all([isinstance(state_tuple, tuple) for state_tuple in values]):
+            raise TypeError(
+                "values: Expected type list of tuples, get type {type}", type(values[0])
+            )
 
         phi = self if inplace else self.copy()
+        values = [
+            (var, self.get_state_no(var, state_name)) for var, state_name in values
+        ]
 
         var_index_to_del = []
         slice_ = [slice(None)] * len(self.variables)
         for var, state in values:
             var_index = phi.variables.index(var)
             slice_[var_index] = state
             var_index_to_del.append(var_index)
 
-        var_index_to_keep = sorted(set(range(len(phi.variables))) - set(var_index_to_del))
+        var_index_to_keep = sorted(
+            set(range(len(phi.variables))) - set(var_index_to_del)
+        )
         # set difference is not gaurenteed to maintain ordering
         phi.variables = [phi.variables[index] for index in var_index_to_keep]
         phi.cardinality = phi.cardinality[var_index_to_keep]
         phi.values = phi.values[tuple(slice_)]
 
         if not inplace:
             return phi
@@ -491,36 +514,40 @@
             phi1 = phi1.copy()
 
             # modifying phi to add new variables
             extra_vars = set(phi1.variables) - set(phi.variables)
             if extra_vars:
                 slice_ = [slice(None)] * len(phi.variables)
                 slice_.extend([np.newaxis] * len(extra_vars))
-                phi.values = phi.values[slice_]
+                phi.values = phi.values[tuple(slice_)]
 
                 phi.variables.extend(extra_vars)
 
                 new_var_card = phi1.get_cardinality(extra_vars)
-                phi.cardinality = np.append(phi.cardinality, [new_var_card[var] for var in extra_vars])
+                phi.cardinality = np.append(
+                    phi.cardinality, [new_var_card[var] for var in extra_vars]
+                )
 
             # modifying phi1 to add new variables
             extra_vars = set(phi.variables) - set(phi1.variables)
             if extra_vars:
                 slice_ = [slice(None)] * len(phi1.variables)
                 slice_.extend([np.newaxis] * len(extra_vars))
-                phi1.values = phi1.values[slice_]
+                phi1.values = phi1.values[tuple(slice_)]
 
                 phi1.variables.extend(extra_vars)
                 # No need to modify cardinality as we don't need it.
 
             # rearranging the axes of phi1 to match phi
             for axis in range(phi.values.ndim):
                 exchange_index = phi1.variables.index(phi.variables[axis])
-                phi1.variables[axis], phi1.variables[exchange_index] = phi1.variables[exchange_index], \
-                    phi1.variables[axis]
+                phi1.variables[axis], phi1.variables[exchange_index] = (
+                    phi1.variables[exchange_index],
+                    phi1.variables[axis],
+                )
                 phi1.values = phi1.values.swapaxes(axis, exchange_index)
 
             phi.values = phi.values + phi1.values
 
         if not inplace:
             return phi
 
@@ -579,39 +606,44 @@
             phi1 = phi1.copy()
 
             # modifying phi to add new variables
             extra_vars = set(phi1.variables) - set(phi.variables)
             if extra_vars:
                 slice_ = [slice(None)] * len(phi.variables)
                 slice_.extend([np.newaxis] * len(extra_vars))
-                phi.values = phi.values[slice_]
+                phi.values = phi.values[tuple(slice_)]
 
                 phi.variables.extend(extra_vars)
 
                 new_var_card = phi1.get_cardinality(extra_vars)
-                phi.cardinality = np.append(phi.cardinality, [new_var_card[var] for var in extra_vars])
+                phi.cardinality = np.append(
+                    phi.cardinality, [new_var_card[var] for var in extra_vars]
+                )
 
             # modifying phi1 to add new variables
             extra_vars = set(phi.variables) - set(phi1.variables)
             if extra_vars:
                 slice_ = [slice(None)] * len(phi1.variables)
                 slice_.extend([np.newaxis] * len(extra_vars))
-                phi1.values = phi1.values[slice_]
+                phi1.values = phi1.values[tuple(slice_)]
 
                 phi1.variables.extend(extra_vars)
                 # No need to modify cardinality as we don't need it.
 
             # rearranging the axes of phi1 to match phi
             for axis in range(phi.values.ndim):
                 exchange_index = phi1.variables.index(phi.variables[axis])
-                phi1.variables[axis], phi1.variables[exchange_index] = phi1.variables[exchange_index], \
-                    phi1.variables[axis]
+                phi1.variables[axis], phi1.variables[exchange_index] = (
+                    phi1.variables[exchange_index],
+                    phi1.variables[axis],
+                )
                 phi1.values = phi1.values.swapaxes(axis, exchange_index)
 
             phi.values = phi.values * phi1.values
+            phi.add_state_names(phi1)
 
         if not inplace:
             return phi
 
     def divide(self, phi1, inplace=True):
         """
         DiscreteFactor division by `phi1`.
@@ -656,22 +688,25 @@
             raise ValueError("Scope of divisor should be a subset of dividend")
 
         # Adding extra variables in phi1.
         extra_vars = set(phi.variables) - set(phi1.variables)
         if extra_vars:
             slice_ = [slice(None)] * len(phi1.variables)
             slice_.extend([np.newaxis] * len(extra_vars))
-            phi1.values = phi1.values[slice_]
+            phi1.values = phi1.values[tuple(slice_)]
 
             phi1.variables.extend(extra_vars)
 
         # Rearranging the axes of phi1 to match phi
         for axis in range(phi.values.ndim):
             exchange_index = phi1.variables.index(phi.variables[axis])
-            phi1.variables[axis], phi1.variables[exchange_index] = phi1.variables[exchange_index], phi1.variables[axis]
+            phi1.variables[axis], phi1.variables[exchange_index] = (
+                phi1.variables[exchange_index],
+                phi1.variables[axis],
+            )
             phi1.values = phi1.values.swapaxes(axis, exchange_index)
 
         phi.values = phi.values / phi1.values
 
         # If factor division 0/0 = 0 but is undefined for x/0. In pgmpy we are using
         # np.inf to represent x/0 cases.
         phi.values[np.isnan(phi.values)] = 0
@@ -704,63 +739,84 @@
 
                [[ 9, 10, 11],
                 [12, 13, 14],
                 [15, 16, 17]]])
         """
         # not creating a new copy of self.values and self.cardinality
         # because __init__ methods does that.
-        return DiscreteFactor(self.scope(), self.cardinality, self.values)
+        return DiscreteFactor(
+            self.scope(), self.cardinality, self.values, state_names=self.state_names
+        )
 
     def is_valid_cpd(self):
-        return np.allclose(self.to_factor().marginalize(self.scope()[:1], inplace=False).values.flatten('C'),
-                           np.ones(np.product(self.cardinality[:0:-1])),
-                           atol=0.01)
+        return np.allclose(
+            self.to_factor()
+            .marginalize(self.scope()[:1], inplace=False)
+            .values.flatten("C"),
+            np.ones(np.product(self.cardinality[:0:-1])),
+            atol=0.01,
+        )
 
     def __str__(self):
-        return self._str(phi_or_p='phi', tablefmt='grid')
+        return self._str(phi_or_p="phi", tablefmt="grid")
 
     def _str(self, phi_or_p="phi", tablefmt="grid", print_state_names=True):
         """
         Generate the string from `__str__` method.
 
         Parameters
         ----------
         phi_or_p: 'phi' | 'p'
                 'phi': When used for Factors.
                   'p': When used for CPDs.
         print_state_names: boolean
                 If True, the user defined state names are displayed.
         """
         string_header = list(map(lambda x: six.text_type(x), self.scope()))
-        string_header.append('{phi_or_p}({variables})'.format(phi_or_p=phi_or_p,
-                                                              variables=','.join(string_header)))
+        string_header.append(
+            "{phi_or_p}({variables})".format(
+                phi_or_p=phi_or_p, variables=",".join(string_header)
+            )
+        )
 
         value_index = 0
         factor_table = []
         for prob in product(*[range(card) for card in self.cardinality]):
             if self.state_names and print_state_names:
-                prob_list = ["{var}({state})".format(
-                    var=list(self.variables)[i], state=self.state_names[list(
-                        self.variables)[i]][prob[i]])
-                             for i in range(len(self.variables))]
+                prob_list = [
+                    "{var}({state})".format(
+                        var=list(self.variables)[i],
+                        state=self.state_names[list(self.variables)[i]][prob[i]],
+                    )
+                    for i in range(len(self.variables))
+                ]
             else:
-                prob_list = ["{s}_{d}".format(s=list(self.variables)[i], d=prob[i])
-                             for i in range(len(self.variables))]
+                prob_list = [
+                    "{s}_{d}".format(s=list(self.variables)[i], d=prob[i])
+                    for i in range(len(self.variables))
+                ]
 
             prob_list.append(self.values.ravel()[value_index])
             factor_table.append(prob_list)
             value_index += 1
 
-        return tabulate(factor_table, headers=string_header, tablefmt=tablefmt, floatfmt=".4f")
+        return tabulate(
+            factor_table, headers=string_header, tablefmt=tablefmt, floatfmt=".4f"
+        )
 
     def __repr__(self):
-        var_card = ", ".join(['{var}:{card}'.format(var=var, card=card)
-                              for var, card in zip(self.variables, self.cardinality)])
+        var_card = ", ".join(
+            [
+                "{var}:{card}".format(var=var, card=card)
+                for var, card in zip(self.variables, self.cardinality)
+            ]
+        )
         return "<DiscreteFactor representing phi({var_card}) at {address}>".format(
-            address=hex(id(self)), var_card=var_card)
+            address=hex(id(self)), var_card=var_card
+        )
 
     def __mul__(self, other):
         return self.product(other, inplace=False)
 
     def __rmul__(self, other):
         return self.__mul__(other)
 
@@ -782,18 +838,22 @@
         elif set(self.scope()) != set(other.scope()):
             return False
 
         else:
             phi = other.copy()
             for axis in range(self.values.ndim):
                 exchange_index = phi.variables.index(self.variables[axis])
-                phi.variables[axis], phi.variables[exchange_index] = (phi.variables[exchange_index],
-                                                                      phi.variables[axis])
-                phi.cardinality[axis], phi.cardinality[exchange_index] = (phi.cardinality[exchange_index],
-                                                                          phi.cardinality[axis])
+                phi.variables[axis], phi.variables[exchange_index] = (
+                    phi.variables[exchange_index],
+                    phi.variables[axis],
+                )
+                phi.cardinality[axis], phi.cardinality[exchange_index] = (
+                    phi.cardinality[exchange_index],
+                    phi.cardinality[axis],
+                )
                 phi.values = phi.values.swapaxes(axis, exchange_index)
 
             if phi.values.shape != self.values.shape:
                 return False
             elif not np.allclose(phi.values, self.values):
                 return False
             elif not all(self.cardinality == phi.cardinality):
@@ -806,13 +866,17 @@
 
     def __hash__(self):
         variable_hashes = [hash(variable) for variable in self.variables]
         sorted_var_hashes = sorted(variable_hashes)
         phi = self.copy()
         for axis in range(phi.values.ndim):
             exchange_index = variable_hashes.index(sorted_var_hashes[axis])
-            variable_hashes[axis], variable_hashes[exchange_index] = (variable_hashes[exchange_index],
-                                                                      variable_hashes[axis])
-            phi.cardinality[axis], phi.cardinality[exchange_index] = (phi.cardinality[exchange_index],
-                                                                      phi.cardinality[axis])
+            variable_hashes[axis], variable_hashes[exchange_index] = (
+                variable_hashes[exchange_index],
+                variable_hashes[axis],
+            )
+            phi.cardinality[axis], phi.cardinality[exchange_index] = (
+                phi.cardinality[exchange_index],
+                phi.cardinality[axis],
+            )
             phi.values = phi.values.swapaxes(axis, exchange_index)
         return hash(str(sorted_var_hashes) + str(phi.values) + str(phi.cardinality))
```

### Comparing `pgmpy-0.1.7/pgmpy/factors/discrete/JointProbabilityDistribution.py` & `pgmpy-0.1.9/pgmpy/factors/discrete/JointProbabilityDistribution.py`

 * *Files 6% similar despite different names*

```diff
@@ -78,29 +78,36 @@
         x1_0  x2_1  x3_1         0.1250
         x1_1  x2_0  x3_0         0.1250
         x1_1  x2_0  x3_1         0.1250
         x1_1  x2_1  x3_0         0.1250
         x1_1  x2_1  x3_1         0.1250
        """
         if np.isclose(np.sum(values), 1):
-            super(JointProbabilityDistribution, self).__init__(variables, cardinality, values)
+            super(JointProbabilityDistribution, self).__init__(
+                variables, cardinality, values
+            )
         else:
             raise ValueError("The probability values doesn't sum to 1.")
 
     def __repr__(self):
-        var_card = ", ".join(['{var}:{card}'.format(var=var, card=card)
-                              for var, card in zip(self.variables, self.cardinality)])
-        return "<Joint Distribution representing P({var_card}) at {address}>".format(address=hex(id(self)),
-                                                                                     var_card=var_card)
+        var_card = ", ".join(
+            [
+                "{var}:{card}".format(var=var, card=card)
+                for var, card in zip(self.variables, self.cardinality)
+            ]
+        )
+        return "<Joint Distribution representing P({var_card}) at {address}>".format(
+            address=hex(id(self)), var_card=var_card
+        )
 
     def __str__(self):
         if six.PY2:
-            return self._str(phi_or_p='P', tablefmt='pqsl')
+            return self._str(phi_or_p="P", tablefmt="pqsl")
         else:
-            return self._str(phi_or_p='P')
+            return self._str(phi_or_p="P")
 
     def marginal_distribution(self, variables, inplace=True):
         """
         Returns the marginal distribution over variables.
 
         Parameters
         ----------
@@ -123,20 +130,29 @@
         x1_0  x2_0      0.1502
         x1_0  x2_1      0.1626
         x1_0  x2_2      0.1197
         x1_1  x2_0      0.2339
         x1_1  x2_1      0.1996
         x1_1  x2_2      0.1340
         """
-        return self.marginalize(list(set(list(self.variables)) -
-                                     set(variables if isinstance(
-                                         variables, (list, set, dict, tuple)) else [variables])),
-                                inplace=inplace)
-
-    def check_independence(self, event1, event2, event3=None, condition_random_variable=False):
+        return self.marginalize(
+            list(
+                set(list(self.variables))
+                - set(
+                    variables
+                    if isinstance(variables, (list, set, dict, tuple))
+                    else [variables]
+                )
+            ),
+            inplace=inplace,
+        )
+
+    def check_independence(
+        self, event1, event2, event3=None, condition_random_variable=False
+    ):
         """
         Check if the Joint Probability Distribution satisfies the given independence condition.
 
         Parameters
         ----------
         event1: list
             random variable whose independence is to be checked.
@@ -165,45 +181,55 @@
         False
         >>> # Conditioning over random variable G
         >>> prob.check_independence(['I'], ['D'], ('G',), condition_random_variable=True)
         False
         """
         JPD = self.copy()
         if isinstance(event1, six.string_types):
-            raise TypeError('Event 1 should be a list or array-like structure')
+            raise TypeError("Event 1 should be a list or array-like structure")
 
         if isinstance(event2, six.string_types):
-            raise TypeError('Event 2 should be a list or array-like structure')
+            raise TypeError("Event 2 should be a list or array-like structure")
 
         if event3:
             if isinstance(event3, six.string_types):
-                raise TypeError('Event 3 cannot of type string')
+                raise TypeError("Event 3 cannot of type string")
 
             elif condition_random_variable:
                 if not all(isinstance(var, six.string_types) for var in event3):
-                    raise TypeError('Event3 should be a 1d list of strings')
+                    raise TypeError("Event3 should be a 1d list of strings")
                 event3 = list(event3)
                 # Using the definition of conditional independence
                 # If P(X,Y|Z) = P(X|Z)*P(Y|Z)
                 # This can be expanded to P(X,Y,Z)*P(Z) == P(X,Z)*P(Y,Z)
                 phi_z = JPD.marginal_distribution(event3, inplace=False).to_factor()
                 for variable_pair in itertools.product(event1, event2):
-                    phi_xyz = JPD.marginal_distribution(event3 + list(variable_pair), inplace=False).to_factor()
-                    phi_xz = JPD.marginal_distribution(event3 + [variable_pair[0]], inplace=False).to_factor()
-                    phi_yz = JPD.marginal_distribution(event3 + [variable_pair[1]], inplace=False).to_factor()
+                    phi_xyz = JPD.marginal_distribution(
+                        event3 + list(variable_pair), inplace=False
+                    ).to_factor()
+                    phi_xz = JPD.marginal_distribution(
+                        event3 + [variable_pair[0]], inplace=False
+                    ).to_factor()
+                    phi_yz = JPD.marginal_distribution(
+                        event3 + [variable_pair[1]], inplace=False
+                    ).to_factor()
                     if phi_xyz * phi_z != phi_xz * phi_yz:
                         return False
                 return True
             else:
                 JPD.conditional_distribution(event3)
 
         for variable_pair in itertools.product(event1, event2):
-            if (JPD.marginal_distribution(variable_pair, inplace=False) !=
-                    JPD.marginal_distribution(variable_pair[0], inplace=False) *
-                    JPD.marginal_distribution(variable_pair[1], inplace=False)):
+            if JPD.marginal_distribution(
+                variable_pair, inplace=False
+            ) != JPD.marginal_distribution(
+                variable_pair[0], inplace=False
+            ) * JPD.marginal_distribution(
+                variable_pair[1], inplace=False
+            ):
                 return False
         return True
 
     def get_independencies(self, condition=None):
         """
         Returns the independent variables in the joint probability distribution.
         Returns marginally independent variables if condition=None.
@@ -225,17 +251,21 @@
         (x2 _|_ x3)
         """
         JPD = self.copy()
         if condition:
             JPD.conditional_distribution(condition)
         independencies = Independencies()
         for variable_pair in itertools.combinations(list(JPD.variables), 2):
-            if (JPD.marginal_distribution(variable_pair, inplace=False) ==
-                    JPD.marginal_distribution(variable_pair[0], inplace=False) *
-                    JPD.marginal_distribution(variable_pair[1], inplace=False)):
+            if JPD.marginal_distribution(
+                variable_pair, inplace=False
+            ) == JPD.marginal_distribution(
+                variable_pair[0], inplace=False
+            ) * JPD.marginal_distribution(
+                variable_pair[1], inplace=False
+            ):
                 independencies.add_assertions(variable_pair)
         return independencies
 
     def conditional_distribution(self, values, inplace=True):
         """
         Returns Conditional Probability Distribution after setting values to 1.
 
@@ -315,17 +345,20 @@
                 for i in itertools.combinations(u, r):
                     yield i
 
         G = BayesianModel()
         for variable_index in range(len(order)):
             u = order[:variable_index]
             for subset in get_subsets(u):
-                if (len(subset) < len(u) and
-                        self.check_independence([order[variable_index]], set(u) - set(subset), subset, True)):
-                    G.add_edges_from([(variable, order[variable_index]) for variable in subset])
+                if len(subset) < len(u) and self.check_independence(
+                    [order[variable_index]], set(u) - set(subset), subset, True
+                ):
+                    G.add_edges_from(
+                        [(variable, order[variable_index]) for variable in subset]
+                    )
         return G
 
     def is_imap(self, model):
         """
         Checks whether the given BayesianModel is Imap of JointProbabilityDistribution
 
         Parameters
@@ -355,14 +388,15 @@
         >>> val = [0.01, 0.01, 0.08, 0.006, 0.006, 0.048, 0.004, 0.004, 0.032,
                    0.04, 0.04, 0.32, 0.024, 0.024, 0.192, 0.016, 0.016, 0.128]
         >>> JPD = JointProbabilityDistribution(['diff', 'intel', 'grade'], [2, 3, 3], val)
         >>> JPD.is_imap(bm)
         True
         """
         from pgmpy.models import BayesianModel
+
         if not isinstance(model, BayesianModel):
             raise TypeError("model must be an instance of BayesianModel")
         factors = [cpd.to_factor() for cpd in model.get_cpds()]
         factor_prod = six.moves.reduce(mul, factors)
         JPD_fact = DiscreteFactor(self.variables, self.cardinality, self.values)
         if JPD_fact == factor_prod:
             return True
```

### Comparing `pgmpy-0.1.7/pgmpy/models/MarkovModel.py` & `pgmpy-0.1.9/pgmpy/models/MarkovModel.py`

 * *Files 3% similar despite different names*

```diff
@@ -105,15 +105,15 @@
         >>> G.add_nodes_from(['Alice', 'Bob', 'Charles'])
         >>> G.add_edge('Alice', 'Bob')
         """
         # check that there is no self loop.
         if u != v:
             super(MarkovModel, self).add_edge(u, v, **kwargs)
         else:
-            raise ValueError('Self loops are not allowed')
+            raise ValueError("Self loops are not allowed")
 
     def add_factors(self, *factors):
         """
         Associate a factor to the graph.
         See factors class for the order of potential values
 
         Parameters
@@ -134,17 +134,17 @@
         ...                        ('Charles', 'Debbie'), ('Debbie', 'Alice')])
         >>> factor = DiscreteFactor(['Alice', 'Bob'], cardinality=[3, 2],
         ...                 values=np.random.rand(6))
         >>> student.add_factors(factor)
         """
         for factor in factors:
             if set(factor.variables) - set(factor.variables).intersection(
-                    set(self.nodes())):
-                raise ValueError("Factors defined on variable not in the model",
-                                 factor)
+                set(self.nodes())
+            ):
+                raise ValueError("Factors defined on variable not in the model", factor)
 
             self.factors.append(factor)
 
     def get_factors(self, node=None):
         """
         Returns all the factors containing the node. If node is not specified
         returns all the factors that have been added till now to the graph.
@@ -168,15 +168,15 @@
         [<DiscreteFactor representing phi(Alice:2, Bob:2) at 0x7f8a0e9bf630>,
          <DiscreteFactor representing phi(Bob:2, Charles:3) at 0x7f8a0e9bf5f8>]
         >>> student.get_factors('Alice')
         [<DiscreteFactor representing phi(Alice:2, Bob:2) at 0x7f8a0e9bf630>]
         """
         if node:
             if node not in self.nodes():
-                raise ValueError('Node not present in the Undirected Graph')
+                raise ValueError("Node not present in the Undirected Graph")
             node_factors = []
             for factor in self.factors:
                 if node in factor.scope():
                     node_factors.append(factor)
             return node_factors
         else:
             return self.factors
@@ -228,16 +228,16 @@
             for factor in self.factors:
                 for variable, cardinality in zip(factor.scope(), factor.cardinality):
                     if node == variable:
                         return cardinality
         else:
             cardinalities = defaultdict(int)
             for factor in self.factors:
-                    for variable, cardinality in zip(factor.scope(), factor.cardinality):
-                        cardinalities[variable] = cardinality
+                for variable, cardinality in zip(factor.scope(), factor.cardinality):
+                    cardinalities[variable] = cardinality
             return cardinalities
 
     def check_model(self):
         """
         Check the model for various errors. This method checks for the following
         errors -
 
@@ -250,17 +250,20 @@
             True if all the checks are passed
         """
         cardinalities = self.get_cardinality()
         for factor in self.factors:
             for variable, cardinality in zip(factor.scope(), factor.cardinality):
                 if cardinalities[variable] != cardinality:
                     raise ValueError(
-                        'Cardinality of variable {var} not matching among factors'.format(var=variable))
+                        "Cardinality of variable {var} not matching among factors".format(
+                            var=variable
+                        )
+                    )
                 if len(self.nodes()) != len(cardinalities):
-                    raise ValueError('Factors for all the variables not defined')
+                    raise ValueError("Factors for all the variables not defined")
             for var1, var2 in itertools.combinations(factor.variables, 2):
                 if var2 not in self.neighbors(var1):
                     raise ValueError("DiscreteFactor inconsistent with the model.")
         return True
 
     def to_factor_graph(self):
         """
@@ -279,29 +282,30 @@
         >>> student = MarkovModel([('Alice', 'Bob'), ('Bob', 'Charles')])
         >>> factor1 = DiscreteFactor(['Alice', 'Bob'], [3, 2], np.random.rand(6))
         >>> factor2 = DiscreteFactor(['Bob', 'Charles'], [2, 2], np.random.rand(4))
         >>> student.add_factors(factor1, factor2)
         >>> factor_graph = student.to_factor_graph()
         """
         from pgmpy.models import FactorGraph
+
         factor_graph = FactorGraph()
 
         if not self.factors:
-            raise ValueError('Factors not associated with the random variables.')
+            raise ValueError("Factors not associated with the random variables.")
 
         factor_graph.add_nodes_from(self.nodes())
         for factor in self.factors:
             scope = factor.scope()
-            factor_node = 'phi_' + '_'.join(scope)
+            factor_node = "phi_" + "_".join(scope)
             factor_graph.add_edges_from(itertools.product(scope, [factor_node]))
             factor_graph.add_factors(factor)
 
         return factor_graph
 
-    def triangulate(self, heuristic='H6', order=None, inplace=False):
+    def triangulate(self, heuristic="H6", order=None, inplace=False):
         """
         Triangulate the graph.
 
         If order of deletion is given heuristic algorithm will not be used.
 
         Parameters
         ----------
@@ -373,35 +377,38 @@
         def _find_size_of_clique(clique, cardinalities):
             """
             Computes the size of a clique.
 
             Size of a clique is defined as product of cardinalities of all the
             nodes present in the clique.
             """
-            return list(map(lambda x: np.prod([cardinalities[node] for node in x]),
-                            clique))
+            return list(
+                map(lambda x: np.prod([cardinalities[node] for node in x]), clique)
+            )
 
         def _get_cliques_dict(node):
             """
             Returns a dictionary in the form of {node: cliques_formed} of the
             node along with its neighboring nodes.
 
             clique_dict_removed would be containing the cliques created
             after deletion of the node
             clique_dict_node would be containing the cliques created before
             deletion of the node
             """
             graph_working_copy = nx.Graph(graph_copy.edges())
-            neighbors = graph_working_copy.neighbors(node)
+            neighbors = list(graph_working_copy.neighbors(node))
             graph_working_copy.add_edges_from(itertools.combinations(neighbors, 2))
-            clique_dict = nx.cliques_containing_node(graph_working_copy,
-                                                     nodes=([node] + neighbors))
+            clique_dict = nx.cliques_containing_node(
+                graph_working_copy, nodes=([node] + neighbors)
+            )
             graph_working_copy.remove_node(node)
-            clique_dict_removed = nx.cliques_containing_node(graph_working_copy,
-                                                             nodes=neighbors)
+            clique_dict_removed = nx.cliques_containing_node(
+                graph_working_copy, nodes=neighbors
+            )
             return clique_dict, clique_dict_removed
 
         if not order:
             order = []
 
             cardinalities = self.get_cardinality()
             for index in range(self.number_of_nodes()):
@@ -414,39 +421,38 @@
                 # C represents the sum of size of the cliques created by the
                 # node and its adjacent node
                 C = {}
                 for node in set(graph_copy.nodes()) - set(order):
                     clique_dict, clique_dict_removed = _get_cliques_dict(node)
                     S[node] = _find_size_of_clique(
                         _find_common_cliques(list(clique_dict_removed.values())),
-                        cardinalities
+                        cardinalities,
                     )[0]
                     common_clique_size = _find_size_of_clique(
-                        _find_common_cliques(list(clique_dict.values())),
-                        cardinalities
+                        _find_common_cliques(list(clique_dict.values())), cardinalities
                     )
                     M[node] = np.max(common_clique_size)
                     C[node] = np.sum(common_clique_size)
 
-                if heuristic == 'H1':
+                if heuristic == "H1":
                     node_to_delete = min(S, key=S.get)
 
-                elif heuristic == 'H2':
+                elif heuristic == "H2":
                     S_by_E = {key: S[key] / cardinalities[key] for key in S}
                     node_to_delete = min(S_by_E, key=S_by_E.get)
 
-                elif heuristic == 'H3':
+                elif heuristic == "H3":
                     S_minus_M = {key: S[key] - M[key] for key in S}
                     node_to_delete = min(S_minus_M, key=S_minus_M.get)
 
-                elif heuristic == 'H4':
+                elif heuristic == "H4":
                     S_minus_C = {key: S[key] - C[key] for key in S}
                     node_to_delete = min(S_minus_C, key=S_minus_C.get)
 
-                elif heuristic == 'H5':
+                elif heuristic == "H5":
                     S_by_M = {key: S[key] / M[key] for key in S}
                     node_to_delete = min(S_by_M, key=S_by_M.get)
 
                 else:
                     S_by_C = {key: S[key] / C[key] for key in S}
                     node_to_delete = min(S_by_C, key=S_by_C.get)
 
@@ -511,26 +517,27 @@
 
         # Else if the number of cliques is more than 1 then create a complete
         # graph with all the cliques as nodes and weight of the edges being
         # the length of sepset between two cliques
         elif len(cliques) >= 2:
             complete_graph = UndirectedGraph()
             edges = list(itertools.combinations(cliques, 2))
-            weights = list(map(lambda x: len(set(x[0]).intersection(set(x[1]))),
-                           edges))
+            weights = list(map(lambda x: len(set(x[0]).intersection(set(x[1]))), edges))
             for edge, weight in zip(edges, weights):
                 complete_graph.add_edge(*edge, weight=-weight)
 
             # Create clique trees by minimum (or maximum) spanning tree method
-            clique_trees = JunctionTree(nx.minimum_spanning_tree(complete_graph).edges())
+            clique_trees = JunctionTree(
+                nx.minimum_spanning_tree(complete_graph).edges()
+            )
 
         # Check whether the factors are defined for all the random variables or not
         all_vars = itertools.chain(*[factor.scope() for factor in self.factors])
         if set(all_vars) != set(self.nodes()):
-            ValueError('DiscreteFactor for all the random variables not specified')
+            ValueError("DiscreteFactor for all the random variables not specified")
 
         # Dictionary stating whether the factor is used to create clique
         # potential or not
         # If false, then it is not used to create any clique potential
         is_used = {factor: False for factor in self.factors}
 
         for node in clique_trees.nodes():
@@ -541,26 +548,30 @@
                 # then use it in creating clique potential
                 if not is_used[factor] and set(factor.scope()).issubset(node):
                     clique_factors.append(factor)
                     is_used[factor] = True
 
             # To compute clique potential, initially set it as unity factor
             var_card = [self.get_cardinality()[x] for x in node]
-            clique_potential = DiscreteFactor(node, var_card, np.ones(np.product(var_card)))
+            clique_potential = DiscreteFactor(
+                node, var_card, np.ones(np.product(var_card))
+            )
             # multiply it with the factors associated with the variables present
             # in the clique (or node)
             # Checking if there's clique_factors, to handle the case when clique_factors
             # is empty, otherwise factor_product with throw an error [ref #889]
             if clique_factors:
                 clique_potential *= factor_product(*clique_factors)
             clique_trees.add_factors(clique_potential)
 
         if not all(is_used.values()):
-            raise ValueError('All the factors were not used to create Junction Tree.'
-                             'Extra factors are defined.')
+            raise ValueError(
+                "All the factors were not used to create Junction Tree."
+                "Extra factors are defined."
+            )
 
         return clique_trees
 
     def markov_blanket(self, node):
         """
         Returns a markov blanket for a random variable.
 
@@ -605,15 +616,17 @@
         local_independencies = Independencies()
 
         all_vars = set(self.nodes())
         for node in self.nodes():
             markov_blanket = set(self.markov_blanket(node))
             rest = all_vars - set([node]) - markov_blanket
             try:
-                local_independencies.add_assertions([node, list(rest), list(markov_blanket)])
+                local_independencies.add_assertions(
+                    [node, list(rest), list(markov_blanket)]
+                )
             except ValueError:
                 pass
 
         local_independencies.reduce()
 
         if latex:
             return local_independencies.latex_string()
@@ -650,15 +663,15 @@
         # Create a junction tree from the markov model.
         # Creation of clique tree involves triangulation, finding maximal cliques
         # and creating a tree from these cliques
         junction_tree = self.to_junction_tree()
 
         # create an ordering of the nodes based on the ordering of the clique
         # in which it appeared first
-        root_node = junction_tree.nodes()[0]
+        root_node = next(iter(junction_tree.nodes()))
         bfs_edges = nx.bfs_edges(junction_tree, root_node)
         for node in root_node:
             var_clique_dict[node] = root_node
             var_order.append(node)
         for edge in bfs_edges:
             clique_node = edge[1]
             for node in clique_node:
@@ -667,15 +680,16 @@
                     var_order.append(node)
 
         # create a bayesian model by adding edges from parent of node to node as
         # par(x_i) = (var(c_k) - x_i) \cap {x_1, ..., x_{i-1}}
         for node_index in range(len(var_order)):
             node = var_order[node_index]
             node_parents = (set(var_clique_dict[node]) - set([node])).intersection(
-                set(var_order[:node_index]))
+                set(var_order[:node_index])
+            )
             bm.add_edges_from([(parent, node) for parent in node_parents])
             # TODO : Convert factor into CPDs
         return bm
 
     def get_partition_function(self):
         """
         Returns the partition function for a given undirected graph.
@@ -699,18 +713,19 @@
         >>> phi = [DiscreteFactor(edge, [2, 2], np.random.rand(4)) for edge in G.edges()]
         >>> G.add_factors(*phi)
         >>> G.get_partition_function()
         """
         self.check_model()
 
         factor = self.factors[0]
-        factor = factor_product(factor, *[self.factors[i] for i in
-                                          range(1, len(self.factors))])
+        factor = factor_product(
+            factor, *[self.factors[i] for i in range(1, len(self.factors))]
+        )
         if set(factor.scope()) != set(self.nodes()):
-            raise ValueError('DiscreteFactor for all the random variables not defined.')
+            raise ValueError("DiscreteFactor for all the random variables not defined.")
 
         return np.sum(factor.values)
 
     def copy(self):
         """
         Returns a copy of this Markov Model.
```

### Comparing `pgmpy-0.1.7/pgmpy/models/BayesianModel.py` & `pgmpy-0.1.9/pgmpy/models/BayesianModel.py`

 * *Files 15% similar despite different names*

```diff
@@ -4,25 +4,31 @@
 from collections import defaultdict
 import logging
 from operator import mul
 
 import networkx as nx
 import numpy as np
 import pandas as pd
+from tqdm import tqdm
+from joblib import Parallel, delayed
 
-from pgmpy.base import DirectedGraph
-from pgmpy.factors.discrete import TabularCPD, JointProbabilityDistribution, DiscreteFactor
+from pgmpy.base import DAG
+from pgmpy.factors.discrete import (
+    TabularCPD,
+    JointProbabilityDistribution,
+    DiscreteFactor,
+)
 from pgmpy.factors.continuous import ContinuousFactor
 from pgmpy.independencies import Independencies
 from pgmpy.extern import six
 from pgmpy.extern.six.moves import range, reduce
 from pgmpy.models.MarkovModel import MarkovModel
 
 
-class BayesianModel(DirectedGraph):
+class BayesianModel(DAG):
     """
     Base class for bayesian model.
 
     A models stores nodes and edges with conditional probability
     distribution (cpd) and other attributes.
 
     models hold directed edges.  Self loops are not allowed neither
@@ -108,18 +114,20 @@
         --------
         >>> from pgmpy.models import BayesianModel/home/abinash/software_packages/numpy-1.7.1
         >>> G = BayesianModel()
         >>> G.add_nodes_from(['grade', 'intel'])
         >>> G.add_edge('grade', 'intel')
         """
         if u == v:
-            raise ValueError('Self loops are not allowed.')
+            raise ValueError("Self loops are not allowed.")
         if u in self.nodes() and v in self.nodes() and nx.has_path(self, v, u):
             raise ValueError(
-                'Loops are not allowed. Adding the edge from (%s->%s) forms a loop.' % (u, v))
+                "Loops are not allowed. Adding the edge from (%s->%s) forms a loop."
+                % (u, v)
+            )
         else:
             super(BayesianModel, self).add_edge(u, v, **kwargs)
 
     def remove_node(self, node):
         """
         Remove node from the model.
 
@@ -236,23 +244,24 @@
         |gradeB| 0.1  | 0.1  |   0.1   |  0.1 |  0.1 |   0.1 |
         +------+------+------+---------+------+------+-------+
         |gradeC| 0.8  | 0.8  |   0.8   |  0.8 |  0.8 |   0.8 |
         +------+------+------+---------+------+------+-------+
         """
         for cpd in cpds:
             if not isinstance(cpd, (TabularCPD, ContinuousFactor)):
-                raise ValueError('Only TabularCPD or ContinuousFactor can be added.')
+                raise ValueError("Only TabularCPD or ContinuousFactor can be added.")
 
-            if set(cpd.scope()) - set(cpd.scope()).intersection(
-                    set(self.nodes())):
-                raise ValueError('CPD defined on variable not in the model', cpd)
+            if set(cpd.scope()) - set(cpd.scope()).intersection(set(self.nodes())):
+                raise ValueError("CPD defined on variable not in the model", cpd)
 
             for prev_cpd_index in range(len(self.cpds)):
                 if self.cpds[prev_cpd_index].variable == cpd.variable:
-                    logging.warning("Replacing existing CPD for {var}".format(var=cpd.variable))
+                    logging.warning(
+                        "Replacing existing CPD for {var}".format(var=cpd.variable)
+                    )
                     self.cpds[prev_cpd_index] = cpd
                     break
             else:
                 self.cpds.append(cpd)
 
     def get_cpds(self, node=None):
         """
@@ -276,17 +285,17 @@
         >>> student = BayesianModel([('diff', 'grade'), ('intel', 'grade')])
         >>> cpd = TabularCPD('grade', 2, [[0.1, 0.9, 0.2, 0.7],
         ...                               [0.9, 0.1, 0.8, 0.3]],
         ...                  ['intel', 'diff'], [2, 2])
         >>> student.add_cpds(cpd)
         >>> student.get_cpds()
         """
-        if node:
+        if node is not None:
             if node not in self.nodes():
-                raise ValueError('Node not present in the Directed Graph')
+                raise ValueError("Node not present in the Directed Graph")
             for cpd in self.cpds:
                 if cpd.variable == node:
                     return cpd
             else:
                 return None
         else:
             return self.cpds
@@ -374,243 +383,30 @@
         check: boolean
             True if all the checks are passed
         """
         for node in self.nodes():
             cpd = self.get_cpds(node=node)
 
             if cpd is None:
-                raise ValueError('No CPD associated with {}'.format(node))
+                raise ValueError("No CPD associated with {}".format(node))
             elif isinstance(cpd, (TabularCPD, ContinuousFactor)):
                 evidence = cpd.get_evidence()
                 parents = self.get_parents(node)
                 if set(evidence if evidence else []) != set(parents if parents else []):
-                    raise ValueError("CPD associated with {node} doesn't have "
-                                     "proper parents associated with it.".format(node=node))
+                    raise ValueError(
+                        "CPD associated with {node} doesn't have "
+                        "proper parents associated with it.".format(node=node)
+                    )
                 if not cpd.is_valid_cpd():
-                    raise ValueError("Sum or integral of conditional probabilites for node {node}"
-                                     " is not equal to 1.".format(node=node))
+                    raise ValueError(
+                        "Sum or integral of conditional probabilites for node {node}"
+                        " is not equal to 1.".format(node=node)
+                    )
         return True
 
-    def _get_ancestors_of(self, obs_nodes_list):
-        """
-        Returns a dictionary of all ancestors of all the observed nodes including the
-        node itself.
-
-        Parameters
-        ----------
-        obs_nodes_list: string, list-type
-            name of all the observed nodes
-
-        Examples
-        --------
-        >>> from pgmpy.models import BayesianModel
-        >>> model = BayesianModel([('D', 'G'), ('I', 'G'), ('G', 'L'),
-        ...                        ('I', 'L')])
-        >>> model._get_ancestors_of('G')
-        {'D', 'G', 'I'}
-        >>> model._get_ancestors_of(['G', 'I'])
-        {'D', 'G', 'I'}
-        """
-        if not isinstance(obs_nodes_list, (list, tuple)):
-            obs_nodes_list = [obs_nodes_list]
-
-        for node in obs_nodes_list:
-            if node not in self.nodes():
-                raise ValueError('Node {s} not in not in graph'.format(s=node))
-
-        ancestors_list = set()
-        nodes_list = set(obs_nodes_list)
-        while nodes_list:
-            node = nodes_list.pop()
-            if node not in ancestors_list:
-                nodes_list.update(self.predecessors(node))
-            ancestors_list.add(node)
-        return ancestors_list
-
-    def active_trail_nodes(self, variables, observed=None):
-        """
-        Returns a dictionary with the given variables as keys and all the nodes reachable
-        from that respective variable as values.
-
-        Parameters
-        ----------
-
-        variables: str or array like
-            variables whose active trails are to be found.
-
-        observed : List of nodes (optional)
-            If given the active trails would be computed assuming these nodes to be observed.
-
-        Examples
-        --------
-        >>> from pgmpy.models import BayesianModel
-        >>> student = BayesianModel()
-        >>> student.add_nodes_from(['diff', 'intel', 'grades'])
-        >>> student.add_edges_from([('diff', 'grades'), ('intel', 'grades')])
-        >>> student.active_trail_nodes('diff')
-        {'diff': {'diff', 'grades'}}
-        >>> student.active_trail_nodes(['diff', 'intel'], observed='grades')
-        {'diff': {'diff', 'intel'}, 'intel': {'diff', 'intel'}}
-
-        References
-        ----------
-        Details of the algorithm can be found in 'Probabilistic Graphical Model
-        Principles and Techniques' - Koller and Friedman
-        Page 75 Algorithm 3.1
-        """
-        if observed:
-            observed_list = observed if isinstance(observed, (list, tuple)) else [observed]
-        else:
-            observed_list = []
-        ancestors_list = self._get_ancestors_of(observed_list)
-
-        # Direction of flow of information
-        # up ->  from parent to child
-        # down -> from child to parent
-
-        active_trails = {}
-        for start in variables if isinstance(variables, (list, tuple)) else [variables]:
-            visit_list = set()
-            visit_list.add((start, 'up'))
-            traversed_list = set()
-            active_nodes = set()
-            while visit_list:
-                node, direction = visit_list.pop()
-                if (node, direction) not in traversed_list:
-                    if node not in observed_list:
-                        active_nodes.add(node)
-                    traversed_list.add((node, direction))
-                    if direction == 'up' and node not in observed_list:
-                        for parent in self.predecessors(node):
-                            visit_list.add((parent, 'up'))
-                        for child in self.successors(node):
-                            visit_list.add((child, 'down'))
-                    elif direction == 'down':
-                        if node not in observed_list:
-                            for child in self.successors(node):
-                                visit_list.add((child, 'down'))
-                        if node in ancestors_list:
-                            for parent in self.predecessors(node):
-                                visit_list.add((parent, 'up'))
-            active_trails[start] = active_nodes
-        return active_trails
-
-    def local_independencies(self, variables):
-        """
-        Returns an instance of Independencies containing the local independencies
-        of each of the variables.
-
-        Parameters
-        ----------
-        variables: str or array like
-            variables whose local independencies are to be found.
-
-        Examples
-        --------
-        >>> from pgmpy.models import BayesianModel
-        >>> student = BayesianModel()
-        >>> student.add_edges_from([('diff', 'grade'), ('intel', 'grade'),
-        >>>                         ('grade', 'letter'), ('intel', 'SAT')])
-        >>> ind = student.local_independencies('grade')
-        >>> ind
-        (grade _|_ SAT | diff, intel)
-        """
-        def dfs(node):
-            """
-            Returns the descendents of node.
-
-            Since Bayesian Networks are acyclic, this is a very simple dfs
-            which does not remember which nodes it has visited.
-            """
-            descendents = []
-            visit = [node]
-            while visit:
-                n = visit.pop()
-                neighbors = self.neighbors(n)
-                visit.extend(neighbors)
-                descendents.extend(neighbors)
-            return descendents
-
-        independencies = Independencies()
-        for variable in variables if isinstance(variables, (list, tuple)) else [variables]:
-            non_descendents = set(self.nodes()) - {variable} - set(dfs(variable))
-            parents = set(self.get_parents(variable))
-            if non_descendents - parents:
-                independencies.add_assertions([variable, non_descendents - parents, parents])
-        return independencies
-
-    def is_active_trail(self, start, end, observed=None):
-        """
-        Returns True if there is any active trail between start and end node
-
-        Parameters
-        ----------
-        start : Graph Node
-
-        end : Graph Node
-
-        observed : List of nodes (optional)
-            If given the active trail would be computed assuming these nodes to be observed.
-
-        additional_observed : List of nodes (optional)
-            If given the active trail would be computed assuming these nodes to be observed along with
-            the nodes marked as observed in the model.
-
-        Examples
-        --------
-        >>> from pgmpy.models import BayesianModel
-        >>> student = BayesianModel()
-        >>> student.add_nodes_from(['diff', 'intel', 'grades', 'letter', 'sat'])
-        >>> student.add_edges_from([('diff', 'grades'), ('intel', 'grades'), ('grades', 'letter'),
-        ...                         ('intel', 'sat')])
-        >>> student.is_active_trail('diff', 'intel')
-        False
-        >>> student.is_active_trail('grades', 'sat')
-        True
-        """
-        if end in self.active_trail_nodes(start, observed)[start]:
-            return True
-        else:
-            return False
-
-    def get_independencies(self, latex=False):
-        """
-        Computes independencies in the Bayesian Network, by checking d-seperation.
-
-        Parameters
-        ----------
-        latex: boolean
-            If latex=True then latex string of the independence assertion
-            would be created.
-
-        Examples
-        --------
-        >>> from pgmpy.models import BayesianModel
-        >>> chain = BayesianModel([('X', 'Y'), ('Y', 'Z')])
-        >>> chain.get_independencies()
-        (X _|_ Z | Y)
-        (Z _|_ X | Y)
-        """
-        independencies = Independencies()
-        for start in (self.nodes()):
-            rest = set(self.nodes()) - {start}
-            for r in range(len(rest)):
-                for observed in itertools.combinations(rest, r):
-                    d_seperated_variables = rest - set(observed) - set(
-                        self.active_trail_nodes(start, observed=observed)[start])
-                    if d_seperated_variables:
-                        independencies.add_assertions([start, d_seperated_variables, observed])
-
-        independencies.reduce()
-
-        if not latex:
-            return independencies
-        else:
-            return independencies.latex_string()
-
     def to_markov_model(self):
         """
         Converts bayesian model to markov model. The markov model created would
         be the moral graph of the bayesian model.
 
         Examples
         --------
@@ -666,15 +462,17 @@
         ...                         evidence=['grade'], evidence_card=[3])
         >>> G.add_cpds(diff_cpd, intel_cpd, grade_cpd, sat_cpd, letter_cpd)
         >>> jt = G.to_junction_tree()
         """
         mm = self.to_markov_model()
         return mm.to_junction_tree()
 
-    def fit(self, data, estimator=None, state_names=[], complete_samples_only=True, **kwargs):
+    def fit(
+        self, data, estimator=None, state_names=[], complete_samples_only=True, **kwargs
+    ):
         """
         Estimates the CPD for each variable based on a given data set.
 
         Parameters
         ----------
         data: pandas DataFrame object
             DataFrame object with column names identical to the variable names of the network.
@@ -708,28 +506,36 @@
         >>> model.fit(data)
         >>> model.get_cpds()
         [<TabularCPD representing P(A:2) at 0x7fb98a7d50f0>,
         <TabularCPD representing P(B:2) at 0x7fb98a7d5588>,
         <TabularCPD representing P(C:2 | A:2, B:2) at 0x7fb98a7b1f98>]
         """
 
-        from pgmpy.estimators import MaximumLikelihoodEstimator, BayesianEstimator, BaseEstimator
+        from pgmpy.estimators import (
+            MaximumLikelihoodEstimator,
+            BayesianEstimator,
+            BaseEstimator,
+        )
 
         if estimator is None:
             estimator = MaximumLikelihoodEstimator
         else:
             if not issubclass(estimator, BaseEstimator):
                 raise TypeError("Estimator object should be a valid pgmpy estimator.")
 
-        _estimator = estimator(self, data, state_names=state_names,
-                               complete_samples_only=complete_samples_only)
+        _estimator = estimator(
+            self,
+            data,
+            state_names=state_names,
+            complete_samples_only=complete_samples_only,
+        )
         cpds_list = _estimator.get_parameters(**kwargs)
         self.add_cpds(*cpds_list)
 
-    def predict(self, data):
+    def predict(self, data, n_jobs=-1):
         """
         Predicts states of all the missing variables.
 
         Parameters
         ----------
         data : pandas DataFrame object
             A DataFrame object with column names same as the variables in the model.
@@ -768,24 +574,35 @@
 
         if set(data.columns) == set(self.nodes()):
             raise ValueError("No variable missing in data. Nothing to predict")
 
         elif set(data.columns) - set(self.nodes()):
             raise ValueError("Data has variables which are not in the model")
 
-        missing_variables = set(self.nodes()) - set(data.columns)
-        pred_values = defaultdict(list)
+        data_unique = data.drop_duplicates()
+        missing_variables = set(self.nodes()) - set(data_unique.columns)
+        #         pred_values = defaultdict(list)
+        pred_values = []
 
         # Send state_names dict from one of the estimated CPDs to the inference class.
-        model_inference = VariableElimination(self, state_names=self.get_cpds()[0].state_names)
-        for index, data_point in data.iterrows():
-            states_dict = model_inference.map_query(variables=missing_variables, evidence=data_point.to_dict())
-            for k, v in states_dict.items():
-                pred_values[k].append(v)
-        return pd.DataFrame(pred_values, index=data.index)
+        model_inference = VariableElimination(self)
+        pred_values = Parallel(n_jobs=n_jobs)(
+            delayed(model_inference.map_query)(
+                variables=missing_variables,
+                evidence=data_point.to_dict(),
+                show_progress=False,
+            )
+            for index, data_point in tqdm(
+                data_unique.iterrows(), total=data_unique.shape[0]
+            )
+        )
+
+        df_results = pd.DataFrame(pred_values, index=data_unique.index)
+        data_with_results = pd.concat([data_unique, df_results], axis=1)
+        return data.merge(data_with_results, how="left").loc[:, missing_variables]
 
     def predict_probability(self, data):
         """
         Predicts probabilities of all states of the missing variables.
 
         Parameters
         ----------
@@ -838,88 +655,34 @@
             raise ValueError("Data has variables which are not in the model")
 
         missing_variables = set(self.nodes()) - set(data.columns)
         pred_values = defaultdict(list)
 
         model_inference = VariableElimination(self)
         for index, data_point in data.iterrows():
-            states_dict = model_inference.query(variables=missing_variables, evidence=data_point.to_dict())
+            full_distribution = model_inference.query(
+                variables=missing_variables,
+                evidence=data_point.to_dict(),
+                show_progress=False,
+            )
+            states_dict = {}
+            for var in missing_variables:
+                states_dict[var] = full_distribution.marginalize(
+                    missing_variables - {var}, inplace=False
+                )
             for k, v in states_dict.items():
                 for l in range(len(v.values)):
                     state = self.get_cpds(k).state_names[k][l]
-                    pred_values[k + '_' + str(state)].append(v.values[l])
+                    pred_values[k + "_" + str(state)].append(v.values[l])
         return pd.DataFrame(pred_values, index=data.index)
 
     def get_factorized_product(self, latex=False):
         # TODO: refer to IMap class for explanation why this is not implemented.
         pass
 
-    def get_immoralities(self):
-        """
-        Finds all the immoralities in the model
-        A v-structure X -> Z <- Y is an immorality if there is no direct edge between X and Y .
-
-        Returns
-        -------
-        set: A set of all the immoralities in the model
-
-        Examples
-        ---------
-        >>> from pgmpy.models import BayesianModel
-        >>> student = BayesianModel()
-        >>> student.add_edges_from([('diff', 'grade'), ('intel', 'grade'),
-        ...                         ('intel', 'SAT'), ('grade', 'letter')])
-        >>> student.get_immoralities()
-        {('diff','intel')}
-        """
-        immoralities = set()
-        for node in self.nodes():
-            for parents in itertools.combinations(self.predecessors(node), 2):
-                if not self.has_edge(parents[0], parents[1]) and not self.has_edge(parents[1], parents[0]):
-                    immoralities.add(tuple(sorted(parents)))
-        return immoralities
-
-    def is_iequivalent(self, model):
-        """
-        Checks whether the given model is I-equivalent
-
-        Two graphs G1 and G2 are said to be I-equivalent if they have same skeleton
-        and have same set of immoralities.
-
-        Note: For same skeleton different names of nodes can work but for immoralities
-        names of nodes must be same
-
-        Parameters
-        ----------
-        model : A Bayesian model object, for which you want to check I-equivalence
-
-        Returns
-        --------
-        boolean : True if both are I-equivalent, False otherwise
-
-        Examples
-        --------
-        >>> from pgmpy.models import BayesianModel
-        >>> G = BayesianModel()
-        >>> G.add_edges_from([('V', 'W'), ('W', 'X'),
-        ...                   ('X', 'Y'), ('Z', 'Y')])
-        >>> G1 = BayesianModel()
-        >>> G1.add_edges_from([('W', 'V'), ('X', 'W'),
-        ...                    ('X', 'Y'), ('Z', 'Y')])
-        >>> G.is_iequivalent(G1)
-        True
-
-        """
-        if not isinstance(model, BayesianModel):
-            raise TypeError('model must be an instance of Bayesian Model')
-        skeleton = nx.algorithms.isomorphism.GraphMatcher(self.to_undirected(), model.to_undirected())
-        if skeleton.is_isomorphic() and self.get_immoralities() == model.get_immoralities():
-            return True
-        return False
-
     def is_imap(self, JPD):
         """
         Checks whether the bayesian model is Imap of given JointProbabilityDistribution
 
         Parameters
         -----------
         JPD : An instance of JointProbabilityDistribution Class, for which you want to
@@ -995,35 +758,35 @@
         model_copy.add_nodes_from(self.nodes())
         model_copy.add_edges_from(self.edges())
         if self.cpds:
             model_copy.add_cpds(*[cpd.copy() for cpd in self.cpds])
         return model_copy
 
     def get_markov_blanket(self, node):
-        """                                                       
+        """
         Returns a markov blanket for a random variable. In the case
-        of Bayesian Networks, the markov blanket is the set of 
-        node's parents, its children and its children's other parents. 
-        
+        of Bayesian Networks, the markov blanket is the set of
+        node's parents, its children and its children's other parents.
+
         Returns
         -------
         list(blanket_nodes): List of nodes contained in Markov Blanket
 
         Parameters
         ----------
         node: string, int or any hashable python object.
               The node whose markov blanket would be returned.
-                                                                  
-        Examples                                                  
-        --------                                                  
+
+        Examples
+        --------
         >>> from pgmpy.models import BayesianModel
         >>> from pgmpy.factors.discrete import TabularCPD
-        >>> G = BayesianModel([('x', 'y'), ('z', 'y'), ('y', 'w'), ('y', 'v'), ('u', 'w'), 
+        >>> G = BayesianModel([('x', 'y'), ('z', 'y'), ('y', 'w'), ('y', 'v'), ('u', 'w'),
                                ('s', 'v'), ('w', 't'), ('w', 'm'), ('v', 'n'), ('v', 'q')])
-        >>> bayes_model.get_markov_blanket('y') 
+        >>> bayes_model.get_markov_blanket('y')
         ['s', 'w', 'x', 'u', 'z', 'v']
         """
         children = self.get_children(node)
         parents = self.get_parents(node)
         blanket_nodes = children + parents
         for child_node in children:
             blanket_nodes.extend(self.get_parents(child_node))
```

### Comparing `pgmpy-0.1.7/pgmpy/models/DynamicBayesianNetwork.py` & `pgmpy-0.1.9/pgmpy/models/DynamicBayesianNetwork.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,18 +1,18 @@
 from itertools import combinations
 from collections import defaultdict
 
 import numpy as np
 import networkx as nx
 
 from pgmpy.factors.discrete import TabularCPD
-from pgmpy.base import DirectedGraph
+from pgmpy.base import DAG
 
 
-class DynamicBayesianNetwork(DirectedGraph):
+class DynamicBayesianNetwork(DAG):
     def __init__(self, ebunch=None):
         """
         Base class for Dynamic Bayesian Network
 
         This is a time variant model of the static Bayesian model, where each
         time-slice has some static nodes and is then replicated over a certain
         time period.
@@ -113,28 +113,34 @@
         >>> from pgmpy.models import DynamicBayesianNetwork as DBN
         >>> dbn = DBN()
         >>> dbn.add_nodes_from(['A', 'B', 'C'])
         """
         for node in nodes:
             self.add_node(node)
 
-    def nodes(self):
+    def _nodes(self):
         """
         Returns the list of nodes present in the network
 
         Examples
         --------
         >>> from pgmpy.models import DynamicBayesianNetwork as DBN
         >>> dbn = DBN()
         >>> dbn.add_nodes_from(['A', 'B', 'C'])
-        >>> dbn.nodes()
+        >>> sorted(dbn._nodes())
         ['B', 'A', 'C']
         """
-        return list(set([node for node, timeslice in
-                         super(DynamicBayesianNetwork, self).nodes()]))
+        return list(
+            set(
+                [
+                    node
+                    for node, timeslice in super(DynamicBayesianNetwork, self).nodes()
+                ]
+            )
+        )
 
     def add_edge(self, start, end, **kwargs):
         """
         Add an edge between two nodes.
 
         The nodes will be automatically added if they are not present in the network.
 
@@ -154,47 +160,58 @@
 
         Examples
         --------
         >>> from pgmpy.models import DynamicBayesianNetwork as DBN
         >>> model = DBN()
         >>> model.add_nodes_from(['D', 'I'])
         >>> model.add_edge(('D',0), ('I',0))
-        >>> model.edges()
-        [(('D', 1), ('I', 1)), (('D', 0), ('I', 0))]
+        >>> sorted(model.edges())
+        [(('D', 0), ('I', 0)), (('D', 1), ('I', 1))]
         """
         try:
             if len(start) != 2 or len(end) != 2:
-                raise ValueError('Nodes must be of type (node, time_slice).')
+                raise ValueError("Nodes must be of type (node, time_slice).")
             elif not isinstance(start[1], int) or not isinstance(end[1], int):
-                raise ValueError('Nodes must be of type (node, time_slice).')
+                raise ValueError("Nodes must be of type (node, time_slice).")
             elif start[1] == end[1]:
                 start = (start[0], 0)
                 end = (end[0], 0)
             elif start[1] == end[1] - 1:
                 start = (start[0], 0)
                 end = (end[0], 1)
             elif start[1] > end[1]:
-                raise NotImplementedError('Edges in backward direction are not allowed.')
+                raise NotImplementedError(
+                    "Edges in backward direction are not allowed."
+                )
             elif start[1] != end[1]:
-                raise ValueError("Edges over multiple time slices is not currently supported")
+                raise ValueError(
+                    "Edges over multiple time slices is not currently supported"
+                )
         except TypeError:
-            raise ValueError('Nodes must be of type (node, time_slice).')
+            raise ValueError("Nodes must be of type (node, time_slice).")
 
         if start == end:
-            raise ValueError('Self Loops are not allowed')
-        elif start in super(DynamicBayesianNetwork, self).nodes() and end \
-                in super(DynamicBayesianNetwork, self).nodes() and \
-                nx.has_path(self, end, start):
-            raise ValueError('Loops are not allowed. Adding the edge from ({start} --> {end}) forms a loop.'.format(
-                start=str(start), end=str(end)))
+            raise ValueError("Self Loops are not allowed")
+        elif (
+            start in super(DynamicBayesianNetwork, self).nodes()
+            and end in super(DynamicBayesianNetwork, self).nodes()
+            and nx.has_path(self, end, start)
+        ):
+            raise ValueError(
+                "Loops are not allowed. Adding the edge from ({start} --> {end}) forms a loop.".format(
+                    start=str(start), end=str(end)
+                )
+            )
 
         super(DynamicBayesianNetwork, self).add_edge(start, end, **kwargs)
 
         if start[1] == end[1]:
-            super(DynamicBayesianNetwork, self).add_edge((start[0], 1 - start[1]), (end[0], 1 - end[1]))
+            super(DynamicBayesianNetwork, self).add_edge(
+                (start[0], 1 - start[1]), (end[0], 1 - end[1])
+            )
         else:
             super(DynamicBayesianNetwork, self).add_node((end[0], 1 - end[1]))
 
     def add_edges_from(self, ebunch, **kwargs):
         """
         Add all the edges in ebunch.
 
@@ -242,17 +259,23 @@
         ...                     (('G', 0), ('L', 0)), (('D', 0), ('D', 1)),
         ...                     (('I', 0), ('I', 1)), (('G', 0), ('G', 1)),
         ...                     (('G', 0), ('L', 1)), (('L', 0), ('L', 1))])
         >>> dbn.get_intra_edges()
         [(('D', 0), ('G', 0)), (('G', 0), ('L', 0)), (('I', 0), ('G', 0))
         """
         if not isinstance(time_slice, int) or time_slice < 0:
-            raise ValueError("The timeslice should be a positive value greater than or equal to zero")
-
-        return [tuple((x[0], time_slice) for x in edge) for edge in self.edges() if edge[0][1] == edge[1][1] == 0]
+            raise ValueError(
+                "The timeslice should be a positive value greater than or equal to zero"
+            )
+
+        return [
+            tuple((x[0], time_slice) for x in edge)
+            for edge in self.edges()
+            if edge[0][1] == edge[1][1] == 0
+        ]
 
     def get_inter_edges(self):
         """
         Returns the inter-slice edges present in the 2-TBN.
 
         Examples
         --------
@@ -286,15 +309,17 @@
         >>> dbn = DBN()
         >>> dbn.add_nodes_from(['D', 'G', 'I', 'S', 'L'])
         >>> dbn.add_edges_from([(('D',0),('G',0)),(('I',0),('G',0)),(('G',0),('L',0)),(('D',0),('D',1))])
         >>> dbn.get_interface_nodes()
         [('D', 0)]
         """
         if not isinstance(time_slice, int) or time_slice < 0:
-            raise ValueError("The timeslice should be a positive value greater than or equal to zero")
+            raise ValueError(
+                "The timeslice should be a positive value greater than or equal to zero"
+            )
 
         return [(edge[0][0], time_slice) for edge in self.get_inter_edges()]
 
     def get_slice_nodes(self, time_slice=0):
         """
         Returns the nodes present in a particular timeslice
 
@@ -308,17 +333,19 @@
         >>> from pgmpy.models import DynamicBayesianNetwork as DBN
         >>> dbn = DBN()
         >>> dbn.add_nodes_from(['D', 'G', 'I', 'S', 'L'])
         >>> dbn.add_edges_from([(('D', 0),('G', 0)),(('I', 0),('G', 0)),(('G', 0),('L', 0)),(('D', 0),('D', 1))])
         >>> dbn.get_slice_nodes()
         """
         if not isinstance(time_slice, int) or time_slice < 0:
-            raise ValueError("The timeslice should be a positive value greater than or equal to zero")
+            raise ValueError(
+                "The timeslice should be a positive value greater than or equal to zero"
+            )
 
-        return [(node, time_slice) for node in self.nodes()]
+        return [(node, time_slice) for node in self._nodes()]
 
     def add_cpds(self, *cpds):
         """
         This method adds the cpds to the dynamic bayesian network.
         Note that while adding variables and the evidence in cpd,
         they have to be of the following form
         (node_name, time_slice)
@@ -359,19 +386,20 @@
          <TabularCPD representing P(('D', 1):2 | ('D', 0):2) at 0x7ff810b9c2e8>,
          <TabularCPD representing P(('D', 0):2) at 0x7ff7f27e6f98>,
          <TabularCPD representing P(('I', 0):2) at 0x7ff7f27e6ba8>,
          <TabularCPD representing P(('I', 1):2 | ('I', 0):2) at 0x7ff7f27e6668>]
         """
         for cpd in cpds:
             if not isinstance(cpd, TabularCPD):
-                raise ValueError('cpd should be an instance of TabularCPD')
+                raise ValueError("cpd should be an instance of TabularCPD")
 
-            if set(cpd.variables) - set(cpd.variables).intersection(set(
-                    super(DynamicBayesianNetwork, self).nodes())):
-                raise ValueError('CPD defined on variable not in the model', cpd)
+            if set(cpd.variables) - set(cpd.variables).intersection(
+                set(super(DynamicBayesianNetwork, self).nodes())
+            ):
+                raise ValueError("CPD defined on variable not in the model", cpd)
 
         self.cpds.extend(cpds)
 
     def get_cpds(self, node=None, time_slice=0):
         """
         Returns the CPDs that have been associated with the network.
 
@@ -397,21 +425,26 @@
         ...                                      [0.3,0.7,0.02,0.2]], [('I', 0),('D', 0)],[2,2])
         >>> dbn.add_cpds(grade_cpd)
         >>> dbn.get_cpds()
         """
         # TODO: fix bugs in this
         if node:
             if node not in super(DynamicBayesianNetwork, self).nodes():
-                raise ValueError('Node not present in the model.')
+                raise ValueError("Node not present in the model.")
             else:
                 for cpd in self.cpds:
                     if cpd.variable == node:
                         return cpd
         else:
-            return [cpd for cpd in self.cpds if set(list(cpd.variables)).issubset(self.get_slice_nodes(time_slice))]
+            return_cpds = []
+            for var in self.get_slice_nodes(time_slice=time_slice):
+                cpd = self.get_cpds(node=var)
+                if cpd:
+                    return_cpds.append(cpd)
+            return return_cpds
 
     def remove_cpds(self, *cpds):
         """
         Removes the cpds that are provided in the argument.
 
         Parameters
         ----------
@@ -457,21 +490,29 @@
         for node in super(DynamicBayesianNetwork, self).nodes():
             cpd = self.get_cpds(node=node)
             if isinstance(cpd, TabularCPD):
                 evidence = cpd.variables[:0:-1]
                 evidence_card = cpd.cardinality[:0:-1]
                 parents = self.get_parents(node)
                 if set(evidence) != set(parents if parents else []):
-                    raise ValueError("CPD associated with {node} doesn't have "
-                                     "proper parents associated with it.".format(node=node))
-                if not np.allclose(cpd.to_factor().marginalize([node], inplace=False).values.flatten('C'),
-                                   np.ones(np.product(evidence_card)),
-                                   atol=0.01):
-                    raise ValueError('Sum of probabilities of states for node {node}'
-                                     ' is not equal to 1'.format(node=node))
+                    raise ValueError(
+                        "CPD associated with {node} doesn't have "
+                        "proper parents associated with it.".format(node=node)
+                    )
+                if not np.allclose(
+                    cpd.to_factor()
+                    .marginalize([node], inplace=False)
+                    .values.flatten("C"),
+                    np.ones(np.product(evidence_card)),
+                    atol=0.01,
+                ):
+                    raise ValueError(
+                        "Sum of probabilities of states for node {node}"
+                        " is not equal to 1".format(node=node)
+                    )
         return True
 
     def initialize_initial_state(self):
         """
         This method will automatically re-adjust the cpds and the edges added to the bayesian network.
         If an edge that is added as an intra time slice edge in the 0th timeslice, this method will
         automatically add it in the 1st timeslice. It will also add the cpds. However, to call this
@@ -506,23 +547,39 @@
         for cpd in self.cpds:
             temp_var = (cpd.variable[0], 1 - cpd.variable[1])
             parents = self.get_parents(temp_var)
             if not any(x.variable == temp_var for x in self.cpds):
                 if all(x[1] == parents[0][1] for x in parents):
                     if parents:
                         evidence_card = cpd.cardinality[:0:-1]
-                        new_cpd = TabularCPD(temp_var, cpd.variable_card,
-                                             cpd.values.reshape(cpd.variable_card, np.prod(evidence_card)),
-                                             parents, evidence_card)
+                        new_cpd = TabularCPD(
+                            temp_var,
+                            cpd.variable_card,
+                            cpd.values.reshape(
+                                cpd.variable_card, np.prod(evidence_card)
+                            ),
+                            parents,
+                            evidence_card,
+                        )
                     else:
                         if cpd.get_evidence():
-                            initial_cpd = cpd.marginalize(cpd.get_evidence(), inplace=False)
-                            new_cpd = TabularCPD(temp_var, cpd.variable_card, np.reshape(initial_cpd.values, (-1, 2)))
+                            initial_cpd = cpd.marginalize(
+                                cpd.get_evidence(), inplace=False
+                            )
+                            new_cpd = TabularCPD(
+                                temp_var,
+                                cpd.variable_card,
+                                np.reshape(initial_cpd.values, (-1, 2)),
+                            )
                         else:
-                            new_cpd = TabularCPD(temp_var, cpd.variable_card, np.reshape(cpd.values, (-1, 2)))
+                            new_cpd = TabularCPD(
+                                temp_var,
+                                cpd.variable_card,
+                                np.reshape(cpd.values, (-1, 2)),
+                            )
                     self.add_cpds(new_cpd)
             self.check_model()
 
     def moralize(self):
         """
         Removes all the immoralities in the Network and creates a moral
         graph (UndirectedGraph).
@@ -542,16 +599,15 @@
         (('D', 1), ('G', 1)),
         (('I', 0), ('D', 0)),
         (('G', 1), ('I', 1))]
         """
         moral_graph = self.to_undirected()
 
         for node in super(DynamicBayesianNetwork, self).nodes():
-            moral_graph.add_edges_from(combinations(
-                self.get_parents(node), 2))
+            moral_graph.add_edges_from(combinations(self.get_parents(node), 2))
 
         return moral_graph
 
     def copy(self):
         """
         Returns a copy of the dynamic bayesian network.
 
@@ -579,12 +635,12 @@
         (('D', 1), ('G', 1)),
         (('D', 0), ('G', 0)),
         (('D', 0), ('D', 1))]
         >> dbn_copy.get_cpds()
         [<TabularCPD representing P(('G', 0):3 | ('I', 0):2, ('D', 0):2) at 0x7f13961a3320>]
         """
         dbn = DynamicBayesianNetwork()
-        dbn.add_nodes_from(self.nodes())
+        dbn.add_nodes_from(self._nodes())
         dbn.add_edges_from(self.edges())
         cpd_copy = [cpd.copy() for cpd in self.get_cpds()]
         dbn.add_cpds(*cpd_copy)
         return dbn
```

### Comparing `pgmpy-0.1.7/pgmpy/models/ClusterGraph.py` & `pgmpy-0.1.9/pgmpy/models/ClusterGraph.py`

 * *Files 2% similar despite different names*

```diff
@@ -50,14 +50,15 @@
     >>> G.add_edge(('a', 'b', 'c'), ('a', 'b'))
 
     or a list of edges
 
     >>> G.add_edges_from([(('a', 'b', 'c'), ('a', 'b')),
     ...                   (('a', 'b', 'c'), ('a', 'c'))])
     """
+
     def __init__(self, ebunch=None):
         super(ClusterGraph, self).__init__()
         if ebunch:
             self.add_edges_from(ebunch)
         self.factors = []
 
     def add_node(self, node, **kwargs):
@@ -73,15 +74,17 @@
         Examples
         --------
         >>> from pgmpy.models import ClusterGraph
         >>> G = ClusterGraph()
         >>> G.add_node(('a', 'b', 'c'))
         """
         if not isinstance(node, (list, set, tuple)):
-            raise TypeError('Node can only be a list, set or tuple of nodes forming a clique')
+            raise TypeError(
+                "Node can only be a list, set or tuple of nodes forming a clique"
+            )
 
         node = tuple(node)
         super(ClusterGraph, self).add_node(node, **kwargs)
 
     def add_nodes_from(self, nodes, **kwargs):
         """
         Add multiple nodes to the cluster graph.
@@ -116,15 +119,15 @@
         >>> G.add_nodes_from([('a', 'b', 'c'), ('a', 'b'), ('a', 'c')])
         >>> G.add_edges_from([(('a', 'b', 'c'), ('a', 'b')),
         ...                   (('a', 'b', 'c'), ('a', 'c'))])
         """
         set_u = set(u)
         set_v = set(v)
         if set_u.isdisjoint(set_v):
-            raise ValueError('No sepset found between these two edges.')
+            raise ValueError("No sepset found between these two edges.")
 
         super(ClusterGraph, self).add_edge(u, v)
 
     def add_factors(self, *factors):
         """
         Associate a factor to the graph.
         See factors class for the order of potential values
@@ -149,16 +152,17 @@
         ...                 values=np.random.rand(6))
         >>> student.add_factors(factor)
         """
         for factor in factors:
             factor_scope = set(factor.scope())
             nodes = [set(node) for node in self.nodes()]
             if factor_scope not in nodes:
-                raise ValueError('Factors defined on clusters of variable not'
-                                 'present in model')
+                raise ValueError(
+                    "Factors defined on clusters of variable not" "present in model"
+                )
 
             self.factors.append(factor)
 
     def get_factors(self, node=None):
         """
         Return the factors that have been added till now to the graph.
 
@@ -182,15 +186,15 @@
         """
         if node is None:
             return self.factors
         else:
             nodes = [set(n) for n in self.nodes()]
 
             if set(node) not in nodes:
-                raise ValueError('Node not present in Cluster Graph')
+                raise ValueError("Node not present in Cluster Graph")
 
             factors = filter(lambda x: set(x.scope()) == set(node), self.factors)
             return next(factors)
 
     def remove_factors(self, *factors):
         """
         Removes the given factors from the added factors.
@@ -277,15 +281,17 @@
         >>> phi2 = DiscreteFactor(['a', 'b'], [2, 2], np.random.rand(4))
         >>> phi3 = DiscreteFactor(['a', 'c'], [2, 2], np.random.rand(4))
         >>> G.add_factors(phi1, phi2, phi3)
         >>> G.get_partition_function()
         """
         if self.check_model():
             factor = self.factors[0]
-            factor = factor_product(factor, *[self.factors[i] for i in range(1, len(self.factors))])
+            factor = factor_product(
+                factor, *[self.factors[i] for i in range(1, len(self.factors))]
+            )
             return np.sum(factor.values)
 
     def check_model(self):
         """
         Check the model for various errors. This method checks for the following
         errors.
 
@@ -301,25 +307,30 @@
         -------
         check: boolean
             True if all the checks are passed
         """
         for clique in self.nodes():
             factors = filter(lambda x: set(x.scope()) == set(clique), self.factors)
             if not any(factors):
-                raise ValueError('Factors for all the cliques or clusters not defined.')
+                raise ValueError("Factors for all the cliques or clusters not defined.")
 
         cardinalities = self.get_cardinality()
-        if len(set((x for clique in self.nodes() for x in clique))) != len(cardinalities):
-            raise ValueError('Factors for all the variables not defined.')
+        if len(set((x for clique in self.nodes() for x in clique))) != len(
+            cardinalities
+        ):
+            raise ValueError("Factors for all the variables not defined.")
 
         for factor in self.factors:
             for variable, cardinality in zip(factor.scope(), factor.cardinality):
-                if (cardinalities[variable] != cardinality):
+                if cardinalities[variable] != cardinality:
                     raise ValueError(
-                        'Cardinality of variable {var} not matching among factors'.format(var=variable))
+                        "Cardinality of variable {var} not matching among factors".format(
+                            var=variable
+                        )
+                    )
 
         return True
 
     def copy(self):
         """
         Returns a copy of ClusterGraph.
```

### Comparing `pgmpy-0.1.7/pgmpy/models/LinearGaussianBayesianNetwork.py` & `pgmpy-0.1.9/pgmpy/models/LinearGaussianBayesianNetwork.py`

 * *Files 4% similar despite different names*

```diff
@@ -45,23 +45,24 @@
         P(x1) = N(1; 4)
         P(x2| x1) = N(0.5*x1_mu); -5)
         P(x3| x2) = N(-1*x2_mu); 4)
 
         """
         for cpd in cpds:
             if not isinstance(cpd, LinearGaussianCPD):
-                raise ValueError('Only LinearGaussianCPD can be added.')
+                raise ValueError("Only LinearGaussianCPD can be added.")
 
-            if set(cpd.variables) - set(cpd.variables).intersection(
-                    set(self.nodes())):
-                raise ValueError('CPD defined on variable not in the model', cpd)
+            if set(cpd.variables) - set(cpd.variables).intersection(set(self.nodes())):
+                raise ValueError("CPD defined on variable not in the model", cpd)
 
             for prev_cpd_index in range(len(self.cpds)):
                 if self.cpds[prev_cpd_index].variable == cpd.variable:
-                    logging.warning("Replacing existing CPD for {var}".format(var=cpd.variable))
+                    logging.warning(
+                        "Replacing existing CPD for {var}".format(var=cpd.variable)
+                    )
                     self.cpds[prev_cpd_index] = cpd
                     break
             else:
                 self.cpds.append(cpd)
 
     def get_cpds(self, node=None):
         """
@@ -166,29 +167,49 @@
         """
         variables = nx.topological_sort(self)
         mean = np.zeros(len(variables))
         covariance = np.zeros((len(variables), len(variables)))
 
         for node_idx in range(len(variables)):
             cpd = self.get_cpds(variables[node_idx])
-            mean[node_idx] = sum([coeff * mean[variables.index(parent)] for
-                                  coeff, parent in zip(cpd.beta_vector, cpd.evidence)]) + cpd.beta_0
-            covariance[node_idx, node_idx] = sum(
-                [coeff * coeff * covariance[variables.index(parent), variables.index(parent)]
-                 for coeff, parent in zip(cpd.beta_vector, cpd.evidence)]) + cpd.variance
+            mean[node_idx] = (
+                sum(
+                    [
+                        coeff * mean[variables.index(parent)]
+                        for coeff, parent in zip(cpd.beta_vector, cpd.evidence)
+                    ]
+                )
+                + cpd.beta_0
+            )
+            covariance[node_idx, node_idx] = (
+                sum(
+                    [
+                        coeff
+                        * coeff
+                        * covariance[variables.index(parent), variables.index(parent)]
+                        for coeff, parent in zip(cpd.beta_vector, cpd.evidence)
+                    ]
+                )
+                + cpd.variance
+            )
 
         for node_i_idx in range(len(variables)):
             for node_j_idx in range(len(variables)):
                 if covariance[node_j_idx, node_i_idx] != 0:
-                    covariance[node_i_idx, node_j_idx] = covariance[node_j_idx, node_i_idx]
+                    covariance[node_i_idx, node_j_idx] = covariance[
+                        node_j_idx, node_i_idx
+                    ]
                 else:
                     cpd_j = self.get_cpds(variables[node_j_idx])
                     covariance[node_i_idx, node_j_idx] = sum(
-                        [coeff * covariance[node_i_idx, variables.index(parent)]
-                         for coeff, parent in zip(cpd_j.beta_vector, cpd_j.evidence)])
+                        [
+                            coeff * covariance[node_i_idx, variables.index(parent)]
+                            for coeff, parent in zip(cpd_j.beta_vector, cpd_j.evidence)
+                        ]
+                    )
 
         return GaussianDistribution(variables, mean, covariance)
 
     def check_model(self):
         """
         Checks the model for various errors. This method checks for the following
         error -
@@ -202,41 +223,53 @@
 
         """
         for node in self.nodes():
             cpd = self.get_cpds(node=node)
 
             if isinstance(cpd, LinearGaussianCPD):
                 if set(cpd.evidence) != set(self.get_parents(node)):
-                    raise ValueError("CPD associated with %s doesn't have "
-                                     "proper parents associated with it." % node)
+                    raise ValueError(
+                        "CPD associated with %s doesn't have "
+                        "proper parents associated with it." % node
+                    )
         return True
 
     def get_cardinality(self, node):
         """
         Cardinality is not defined for continuous variables.
         """
         raise ValueError("Cardinality is not defined for continuous variables.")
 
-    def fit(self, data, estimator=None, state_names=[], complete_samples_only=True, **kwargs):
+    def fit(
+        self, data, estimator=None, state_names=[], complete_samples_only=True, **kwargs
+    ):
         """
         For now, fit method has not been implemented for LinearGaussianBayesianNetwork.
         """
-        
-        raise NotImplementedError("fit method has not been implemented for LinearGaussianBayesianNetwork.")
+
+        raise NotImplementedError(
+            "fit method has not been implemented for LinearGaussianBayesianNetwork."
+        )
 
     def predict(self, data):
         """
         For now, predict method has not been implemented for LinearGaussianBayesianNetwork.
         """
-        raise NotImplementedError("predict method has not been implemented for LinearGaussianBayesianNetwork.")
+        raise NotImplementedError(
+            "predict method has not been implemented for LinearGaussianBayesianNetwork."
+        )
 
     def to_markov_model(self):
         """
         For now, to_markov_model method has not been implemented for LinearGaussianBayesianNetwork.
         """
-        raise NotImplementedError("to_markov_model method has not been implemented for LinearGaussianBayesianNetwork.")
+        raise NotImplementedError(
+            "to_markov_model method has not been implemented for LinearGaussianBayesianNetwork."
+        )
 
     def is_imap(self, JPD):
         """
         For now, is_imap method has not been implemented for LinearGaussianBayesianNetwork.
         """
-        raise NotImplementedError("is_imap method has not been implemented for LinearGaussianBayesianNetwork.")
+        raise NotImplementedError(
+            "is_imap method has not been implemented for LinearGaussianBayesianNetwork."
+        )
```

### Comparing `pgmpy-0.1.7/pgmpy/models/NaiveBayes.py` & `pgmpy-0.1.9/pgmpy/models/NaiveBayes.py`

 * *Files 1% similar despite different names*

```diff
@@ -159,16 +159,20 @@
         >>> ind
         (b _|_ d, c | a)
         """
         independencies = Independencies()
         for variable in [variables] if isinstance(variables, str) else variables:
             if variable != self.parent_node:
                 independencies.add_assertions(
-                    [variable, list(
-                        set(self.children_nodes) - set(variable)), self.parent_node])
+                    [
+                        variable,
+                        list(set(self.children_nodes) - set(variable)),
+                        self.parent_node,
+                    ]
+                )
         return independencies
 
     def fit(self, data, parent_node=None, estimator=None):
         """
         Computes the CPD for each node from a given data in the form of a pandas dataframe.
         If a variable from the data is not present in the model, it adds that node into the model.
 
@@ -205,12 +209,16 @@
         """
         if not parent_node:
             if not self.parent_node:
                 raise ValueError("parent node must be specified for the model")
             else:
                 parent_node = self.parent_node
         if parent_node not in data.columns:
-            raise ValueError("parent node: {node} is not present in the given data".format(node=parent_node))
+            raise ValueError(
+                "parent node: {node} is not present in the given data".format(
+                    node=parent_node
+                )
+            )
         for child_node in data.columns:
             if child_node != parent_node:
                 self.add_edge(parent_node, child_node)
         super(NaiveBayes, self).fit(data, estimator)
```

### Comparing `pgmpy-0.1.7/pgmpy/models/MarkovChain.py` & `pgmpy-0.1.9/pgmpy/models/MarkovChain.py`

 * *Files 2% similar despite different names*

```diff
@@ -52,14 +52,15 @@
        intel  diff
     0      0     2
     1      1     0
     2      0     1
     3      1     0
     4      0     2
     """
+
     def __init__(self, variables=None, card=None, start_state=None):
         """
         Parameters:
         -----------
         variables: array-like iterable object
             A list of variables of the model.
 
@@ -69,18 +70,20 @@
         start_state: array-like iterable object
             List of tuples representing the starting states of the variables.
         """
         if variables is None:
             variables = []
         if card is None:
             card = []
-        if not hasattr(variables, '__iter__') or isinstance(variables, six.string_types):
-            raise ValueError('variables must be a non-string iterable.')
-        if not hasattr(card, '__iter__') or isinstance(card, six.string_types):
-            raise ValueError('card must be a non-string iterable.')
+        if not hasattr(variables, "__iter__") or isinstance(
+            variables, six.string_types
+        ):
+            raise ValueError("variables must be a non-string iterable.")
+        if not hasattr(card, "__iter__") or isinstance(card, six.string_types):
+            raise ValueError("card must be a non-string iterable.")
         self.variables = variables
         self.cardinalities = {v: c for v, c in zip(variables, card)}
         self.transition_models = {var: {} for var in variables}
         if start_state is None or self._check_state(start_state):
             self.state = start_state
 
     def set_start_state(self, start_state):
@@ -97,36 +100,43 @@
         ---------
         >>> from pgmpy.models import MarkovChain as MC
         >>> from pgmpy.factors.discrete import State
         >>> model = MC(['a', 'b'], [2, 2])
         >>> model.set_start_state([State('a', 0), State('b', 1)])
         """
         if start_state is not None:
-            if not hasattr(start_state, '__iter__') or isinstance(start_state, six.string_types):
-                raise ValueError('start_state must be a non-string iterable.')
+            if not hasattr(start_state, "__iter__") or isinstance(
+                start_state, six.string_types
+            ):
+                raise ValueError("start_state must be a non-string iterable.")
             # Must be an array-like iterable. Reorder according to self.variables.
             state_dict = {var: st for var, st in start_state}
             start_state = [State(var, state_dict[var]) for var in self.variables]
         if start_state is None or self._check_state(start_state):
             self.state = start_state
 
     def _check_state(self, state):
         """
         Checks if a list representing the state of the variables is valid.
         """
-        if not hasattr(state, '__iter__') or isinstance(state, six.string_types):
-            raise ValueError('Start state must be a non-string iterable object.')
+        if not hasattr(state, "__iter__") or isinstance(state, six.string_types):
+            raise ValueError("Start state must be a non-string iterable object.")
         state_vars = {s.var for s in state}
         if not state_vars == set(self.variables):
-            raise ValueError('Start state must represent a complete assignment to all variables.'
-                             'Expected variables in state: {svar}, Got: {mvar}.'.format(svar=state_vars,
-                                                                                        mvar=set(self.variables)))
+            raise ValueError(
+                "Start state must represent a complete assignment to all variables."
+                "Expected variables in state: {svar}, Got: {mvar}.".format(
+                    svar=state_vars, mvar=set(self.variables)
+                )
+            )
         for var, val in state:
             if val >= self.cardinalities[var]:
-                raise ValueError('Assignment {val} to {var} invalid.'.format(val=val, var=var))
+                raise ValueError(
+                    "Assignment {val} to {var} invalid.".format(val=val, var=var)
+                )
         return True
 
     def add_variable(self, variable, card=0):
         """
         Add a variable to the model.
 
         Parameters:
@@ -141,15 +151,15 @@
         >>> from pgmpy.models import MarkovChain as MC
         >>> model = MC()
         >>> model.add_variable('x', 4)
         """
         if variable not in self.variables:
             self.variables.append(variable)
         else:
-            warn('Variable {var} already exists.'.format(var=variable))
+            warn("Variable {var} already exists.".format(var=variable))
         self.cardinalities[variable] = card
         self.transition_models[variable] = {}
 
     def add_variables_from(self, variables, cards):
         """
         Add several variables to the model at once.
 
@@ -196,44 +206,64 @@
         """
         if isinstance(transition_model, list):
             transition_model = np.array(transition_model)
 
         # check if the transition model is valid
         if not isinstance(transition_model, dict):
             if not isinstance(transition_model, np.ndarray):
-                raise ValueError('Transition model must be a dict or numpy array')
+                raise ValueError("Transition model must be a dict or numpy array")
             elif len(transition_model.shape) != 2:
-                raise ValueError('Transition model must be 2d array.given {t}'.format(t=transition_model.shape))
+                raise ValueError(
+                    "Transition model must be 2d array.given {t}".format(
+                        t=transition_model.shape
+                    )
+                )
             elif transition_model.shape[0] != transition_model.shape[1]:
-                raise ValueError('Dimension mismatch {d1}!={d2}'.format(d1=transition_model.shape[0],
-                                 d2=transition_model.shape[1]))
+                raise ValueError(
+                    "Dimension mismatch {d1}!={d2}".format(
+                        d1=transition_model.shape[0], d2=transition_model.shape[1]
+                    )
+                )
             else:
                 # convert the matrix to dict
                 size = transition_model.shape[0]
-                transition_model = dict((i, dict((j, float(transition_model[i][j]))
-                                         for j in range(0, size))) for i in range(0, size))
+                transition_model = dict(
+                    (
+                        i,
+                        dict(
+                            (j, float(transition_model[i][j])) for j in range(0, size)
+                        ),
+                    )
+                    for i in range(0, size)
+                )
 
         exp_states = set(range(self.cardinalities[variable]))
         tm_states = set(transition_model.keys())
         if not exp_states == tm_states:
-            raise ValueError('Transitions must be defined for all states of variable {v}. '
-                             'Expected states: {es}, Got: {ts}.'.format(v=variable, es=exp_states, ts=tm_states))
+            raise ValueError(
+                "Transitions must be defined for all states of variable {v}. "
+                "Expected states: {es}, Got: {ts}.".format(
+                    v=variable, es=exp_states, ts=tm_states
+                )
+            )
 
         for _, transition in transition_model.items():
             if not isinstance(transition, dict):
-                raise ValueError('Each transition must be a dict.')
+                raise ValueError("Each transition must be a dict.")
             prob_sum = 0
 
             for _, prob in transition.items():
                 if prob < 0 or prob > 1:
-                    raise ValueError('Transitions must represent valid probability weights.')
+                    raise ValueError(
+                        "Transitions must represent valid probability weights."
+                    )
                 prob_sum += prob
 
             if not np.allclose(prob_sum, 1):
-                raise ValueError('Transition probabilities must sum to 1.')
+                raise ValueError("Transition probabilities must sum to 1.")
 
         self.transition_models[variable] = transition_model
 
     def sample(self, start_state=None, size=1):
         """
         Sample from the Markov Chain.
 
@@ -279,15 +309,17 @@
         var_states = defaultdict(dict)
         var_values = defaultdict(dict)
         samples = defaultdict(dict)
         for var in self.transition_models.keys():
             for st in self.transition_models[var]:
                 var_states[var][st] = list(self.transition_models[var][st].keys())
                 var_values[var][st] = list(self.transition_models[var][st].values())
-                samples[var][st] = sample_discrete(var_states[var][st], var_values[var][st], size=size)
+                samples[var][st] = sample_discrete(
+                    var_states[var][st], var_values[var][st], size=size
+                )
 
         for i in range(size - 1):
             for j, (var, st) in enumerate(self.state):
                 next_st = samples[var][st][i]
                 self.state[j] = State(var, next_st)
             sampled.loc[i + 1] = [st for var, st in self.state]
 
@@ -359,16 +391,18 @@
             # else use previously-set state
         else:
             self.set_start_state(start_state)
         # sampled.loc[0] = [self.state[var] for var in self.variables]
 
         for i in range(size):
             for j, (var, st) in enumerate(self.state):
-                next_st = sample_discrete(list(self.transition_models[var][st].keys()),
-                                          list(self.transition_models[var][st].values()))[0]
+                next_st = sample_discrete(
+                    list(self.transition_models[var][st].keys()),
+                    list(self.transition_models[var][st].values()),
+                )[0]
                 self.state[j] = State(var, next_st)
             yield self.state[:]
 
     def is_stationarity(self, tolerance=0.2, sample=None):
         """
         Checks if the given markov chain is stationary and checks the steady state
         probablity values for the state are consistent.
@@ -399,25 +433,34 @@
         >>> model.is_stationarity()
         True
         """
         keys = self.transition_models.keys()
         return_val = True
         for k in keys:
             # convert dict to numpy matrix
-            transition_mat = np.array([np.array(list(self.transition_models[k][i].values()))
-                                       for i in self.transition_models[k].keys()], dtype=np.float)
+            transition_mat = np.array(
+                [
+                    np.array(list(self.transition_models[k][i].values()))
+                    for i in self.transition_models[k].keys()
+                ],
+                dtype=np.float,
+            )
             S, U = eig(transition_mat.T)
-            stationary = np.array(U[:, np.where(np.abs(S - 1.) < 1e-8)[0][0]].flat)
+            stationary = np.array(U[:, np.where(np.abs(S - 1.0) < 1e-8)[0][0]].flat)
             stationary = (stationary / np.sum(stationary)).real
 
             probabilites = []
             window_size = 10000 if sample is None else len(sample)
             for i in range(0, transition_mat.shape[0]):
-                probabilites.extend(self.prob_from_sample([State(k, i)], window_size=window_size))
-            if any(np.abs(i) > tolerance for i in np.subtract(probabilites, stationary)):
+                probabilites.extend(
+                    self.prob_from_sample([State(k, i)], window_size=window_size)
+                )
+            if any(
+                np.abs(i) > tolerance for i in np.subtract(probabilites, stationary)
+            ):
                 return_val = return_val and False
             else:
                 return_val = return_val and True
 
         return return_val
 
     def random_state(self):
@@ -431,15 +474,18 @@
         Examples:
         ---------
         >>> from pgmpy.models import MarkovChain as MC
         >>> model = MC(['intel', 'diff'], [2, 3])
         >>> model.random_state()
         [State('diff', 2), State('intel', 1)]
         """
-        return [State(var, np.random.randint(self.cardinalities[var])) for var in self.variables]
+        return [
+            State(var, np.random.randint(self.cardinalities[var]))
+            for var in self.variables
+        ]
 
     def copy(self):
         """
         Returns a copy of Markov Chain Model.
 
         Return Type:
         ------------
@@ -457,13 +503,16 @@
         >>> model.add_transition_model('diff', diff_tm)
         >>> model.set_start_state([State('intel', 0), State('diff', 2)])
         >>> model_copy = model.copy()
         >>> model_copy.transition_models
         >>> {'diff': {0: {0: 0.1, 1: 0.5, 2: 0.4}, 1: {0: 0.2, 1: 0.2, 2: 0.6}, 2: {0: 0.7, 1: 0.15, 2: 0.15}},
              'intel': {0: {0: 0.25, 1: 0.75}, 1: {0: 0.5, 1: 0.5}}}
         """
-        markovchain_copy = MarkovChain(variables=list(self.cardinalities.keys()),
-                                       card=list(self.cardinalities.values()), start_state=self.state)
+        markovchain_copy = MarkovChain(
+            variables=list(self.cardinalities.keys()),
+            card=list(self.cardinalities.values()),
+            start_state=self.state,
+        )
         if self.transition_models:
             markovchain_copy.transition_models = self.transition_models.copy()
 
         return markovchain_copy
```

### Comparing `pgmpy-0.1.7/pgmpy/models/FactorGraph.py` & `pgmpy-0.1.9/pgmpy/models/FactorGraph.py`

 * *Files 4% similar despite different names*

```diff
@@ -86,15 +86,15 @@
         >>> phi1 = DiscreteFactor(['a', 'b'], [2, 2], np.random.rand(4))
         >>> G.add_nodes_from([phi1, phi2])
         >>> G.add_edge('a', phi1)
         """
         if u != v:
             super(FactorGraph, self).add_edge(u, v, **kwargs)
         else:
-            raise ValueError('Self loops are not allowed')
+            raise ValueError("Self loops are not allowed")
 
     def add_factors(self, *factors):
         """
         Associate a factor to the graph.
         See factors class for the order of potential values.
 
         Parameters
@@ -113,17 +113,17 @@
         >>> phi2 = DiscreteFactor(['b', 'c'], [2, 2], np.random.rand(4))
         >>> G.add_factors(phi1, phi2)
         >>> G.add_edges_from([('a', phi1), ('b', phi1),
         ...                   ('b', phi2), ('c', phi2)])
         """
         for factor in factors:
             if set(factor.variables) - set(factor.variables).intersection(
-                    set(self.nodes())):
-                raise ValueError("Factors defined on variable not in the model",
-                                 factor)
+                set(self.nodes())
+            ):
+                raise ValueError("Factors defined on variable not in the model", factor)
 
             self.factors.append(factor)
 
     def remove_factors(self, *factors):
         """
         Removes the given factors from the added factors.
 
@@ -200,33 +200,40 @@
         * Check if cardinality information for all the variables is availble or not.
         * Check if cardinality of random variable remains same across all the
           factors.
         """
         variable_nodes = set([x for factor in self.factors for x in factor.scope()])
         factor_nodes = set(self.nodes()) - variable_nodes
 
-        if not all(isinstance(factor_node, DiscreteFactor) for factor_node in factor_nodes):
-            raise ValueError('Factors not associated for all the random variables')
-
-        if (not (bipartite.is_bipartite(self)) or
-            not (bipartite.is_bipartite_node_set(self, variable_nodes) or
-                 bipartite.is_bipartite_node_set(self, variable_nodes))):
-            raise ValueError('Edges can only be between variables and factors')
+        if not all(
+            isinstance(factor_node, DiscreteFactor) for factor_node in factor_nodes
+        ):
+            raise ValueError("Factors not associated for all the random variables")
+
+        if not (bipartite.is_bipartite(self)) or not (
+            bipartite.is_bipartite_node_set(self, variable_nodes)
+            or bipartite.is_bipartite_node_set(self, variable_nodes)
+        ):
+            raise ValueError("Edges can only be between variables and factors")
 
         if len(factor_nodes) != len(self.factors):
-            raise ValueError('Factors not associated with all the factor nodes.')
+            raise ValueError("Factors not associated with all the factor nodes.")
 
         cardinalities = self.get_cardinality()
         if len(variable_nodes) != len(cardinalities):
-            raise ValueError('Factors for all the variables not defined')
+            raise ValueError("Factors for all the variables not defined")
 
         for factor in self.factors:
             for variable, cardinality in zip(factor.scope(), factor.cardinality):
-                if (cardinalities[variable] != cardinality):
-                    raise ValueError('Cardinality of variable {var} not matching among factors'.format(var=variable))
+                if cardinalities[variable] != cardinality:
+                    raise ValueError(
+                        "Cardinality of variable {var} not matching among factors".format(
+                            var=variable
+                        )
+                    )
 
         return True
 
     def get_variable_nodes(self):
         """
         Returns variable nodes present in the graph.
 
@@ -304,15 +311,15 @@
         >>> mm = G.to_markov_model()
         """
         mm = MarkovModel()
 
         variable_nodes = self.get_variable_nodes()
 
         if len(set(self.nodes()) - set(variable_nodes)) != len(self.factors):
-            raise ValueError('Factors not associated with all the factor nodes.')
+            raise ValueError("Factors not associated with all the factor nodes.")
 
         mm.add_nodes_from(variable_nodes)
         for factor in self.factors:
             scope = factor.scope()
             mm.add_edges_from(itertools.combinations(scope, 2))
             mm.add_factors(factor)
 
@@ -367,18 +374,22 @@
         >>> G.get_factors(node=phi1)
         """
         if node is None:
             return self.factors
         else:
             factor_nodes = self.get_factor_nodes()
             if node not in factor_nodes:
-                raise ValueError('Factors are not associated with the '
-                                 'corresponding node.')
-            factors = list(filter(lambda x: set(x.scope()) == set(self.neighbors(node)),
-                                  self.factors))
+                raise ValueError(
+                    "Factors are not associated with the " "corresponding node."
+                )
+            factors = list(
+                filter(
+                    lambda x: set(x.scope()) == set(self.neighbors(node)), self.factors
+                )
+            )
             return factors[0]
 
     def get_partition_function(self):
         """
         Returns the partition function for a given undirected graph.
 
         A partition function is defined as
@@ -400,18 +411,19 @@
         >>> G.add_nodes_from([phi1, phi2])
         >>> G.add_edges_from([('a', phi1), ('b', phi1),
         ...                   ('b', phi2), ('c', phi2)])
         >>> G.get_factors()
         >>> G.get_partition_function()
         """
         factor = self.factors[0]
-        factor = factor_product(factor, *[self.factors[i] for i in
-                                          range(1, len(self.factors))])
+        factor = factor_product(
+            factor, *[self.factors[i] for i in range(1, len(self.factors))]
+        )
         if set(factor.scope()) != set(self.get_variable_nodes()):
-            raise ValueError('DiscreteFactor for all the random variables not defined.')
+            raise ValueError("DiscreteFactor for all the random variables not defined.")
 
         return np.sum(factor.values)
 
     def copy(self):
         """
         Returns a copy of the model.
```

### Comparing `pgmpy-0.1.7/pgmpy/models/NoisyOrModel.py` & `pgmpy-0.1.9/pgmpy/models/NoisyOrModel.py`

 * *Files 2% similar despite different names*

```diff
@@ -14,14 +14,15 @@
 
     This is an implementation of generalized Noisy-Or models and
     is not limited to Boolean variables and also any arbitrary
     function can be used instead of the boolean OR function.
 
     Reference: http://xenon.stanford.edu/~srinivas/research/6-UAI93-Srinivas-Generalization-of-Noisy-Or.pdf
     """
+
     def __init__(self, variables, cardinality, inhibitor_probability):
         # TODO: Accept values of each state so that it could be
         # put into F to compute the final state values of the output
         """
         Init method for NoisyOrModel.
 
         Parameters
@@ -74,19 +75,27 @@
         """
         if len(variables) == 1:
             if not isinstance(inhibitor_probability[0], (list, tuple)):
                 inhibitor_probability = [inhibitor_probability]
 
         if len(variables) != len(cardinality):
             raise ValueError("Size of variables and cardinality should be same")
-        elif any(cardinal != len(prob_array) for prob_array, cardinal in zip(inhibitor_probability, cardinality)) or \
-                len(cardinality) != len(inhibitor_probability):
-            raise ValueError("Size of variables and inhibitor_probability should be same")
-        elif not all(0 <= item <= 1 for item in chain.from_iterable(inhibitor_probability)):
-            raise ValueError("Probability values should be between 0 and 1(both inclusive).")
+        elif any(
+            cardinal != len(prob_array)
+            for prob_array, cardinal in zip(inhibitor_probability, cardinality)
+        ) or len(cardinality) != len(inhibitor_probability):
+            raise ValueError(
+                "Size of variables and inhibitor_probability should be same"
+            )
+        elif not all(
+            0 <= item <= 1 for item in chain.from_iterable(inhibitor_probability)
+        ):
+            raise ValueError(
+                "Probability values should be between 0 and 1(both inclusive)."
+            )
         else:
             self.variables = np.concatenate((self.variables, variables))
             self.cardinality = np.concatenate((self.cardinality, cardinality))
             self.inhibitor_probability.extend(inhibitor_probability)
 
     def del_variables(self, variables):
         """
@@ -101,20 +110,29 @@
         --------
         >>> from pgmpy.models import NoisyOrModel
         >>> model = NoisyOrModel(['x1', 'x2', 'x3'], [2, 3, 2], [[0.6, 0.4],
         ...                                                      [0.2, 0.4, 0.7],
         ...                                                      [0.1, 0. 4]])
         >>> model.del_variables(['x1'])
         """
-        variables = [variables] if isinstance(variables, six.string_types) else set(variables)
-        indices = [index for index, variable in enumerate(self.variables) if variable in variables]
+        variables = (
+            [variables] if isinstance(variables, six.string_types) else set(variables)
+        )
+        indices = [
+            index
+            for index, variable in enumerate(self.variables)
+            if variable in variables
+        ]
         self.variables = np.delete(self.variables, indices, 0)
         self.cardinality = np.delete(self.cardinality, indices, 0)
-        self.inhibitor_probability = [prob_array for index, prob_array in enumerate(self.inhibitor_probability)
-                                      if index not in indices]
+        self.inhibitor_probability = [
+            prob_array
+            for index, prob_array in enumerate(self.inhibitor_probability)
+            if index not in indices
+        ]
 
     #
     # def out_prob(self, func):
     #     """
     #     Compute the conditional probability of output variable
     #     given all other variables [P(X|U)] where X is the output
     #     variable and U is the set of input variables.
```

### Comparing `pgmpy-0.1.7/pgmpy/models/JunctionTree.py` & `pgmpy-0.1.9/pgmpy/models/JunctionTree.py`

 * *Files 2% similar despite different names*

```diff
@@ -67,16 +67,18 @@
         >>> from pgmpy.models import JunctionTree
         >>> G = JunctionTree()
         >>> G.add_nodes_from([('a', 'b', 'c'), ('a', 'b'), ('a', 'c')])
         >>> G.add_edges_from([(('a', 'b', 'c'), ('a', 'b')),
         ...                   (('a', 'b', 'c'), ('a', 'c'))])
         """
         if u in self.nodes() and v in self.nodes() and nx.has_path(self, u, v):
-            raise ValueError('Addition of edge between {u} and {v} forms a cycle breaking the '
-                             'properties of Junction Tree'.format(u=str(u), v=str(v)))
+            raise ValueError(
+                "Addition of edge between {u} and {v} forms a cycle breaking the "
+                "properties of Junction Tree".format(u=str(u), v=str(v))
+            )
 
         super(JunctionTree, self).add_edge(u, v, **kwargs)
 
     def check_model(self):
         """
         Check the model for various errors. This method checks for the following
         errors. In the same time also updates the cardinalities of all the random
@@ -88,15 +90,15 @@
 
         Returns
         -------
         check: boolean
             True if all the checks are passed
         """
         if not nx.is_connected(self):
-            raise ValueError('The Junction Tree defined is not fully connected.')
+            raise ValueError("The Junction Tree defined is not fully connected.")
 
         return super(JunctionTree, self).check_model()
 
     def copy(self):
         """
         Returns a copy of JunctionTree.
```

### Comparing `pgmpy-0.1.7/pgmpy/inference/ExactInference.py` & `pgmpy-0.1.9/pgmpy/inference/ExactInference.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,136 +1,270 @@
 #!/usr/bin/env python3
 import copy
 import itertools
 
 import networkx as nx
 import numpy as np
+from tqdm import tqdm
 from pgmpy.extern.six.moves import filter, range
 
 from pgmpy.extern.six import string_types
 from pgmpy.factors import factor_product
 from pgmpy.inference import Inference
-from pgmpy.models import JunctionTree
-from pgmpy.utils import StateNameDecorator
+from pgmpy.inference.EliminationOrder import (
+    WeightedMinFill,
+    MinNeighbors,
+    MinFill,
+    MinWeight,
+)
+from pgmpy.models import JunctionTree, BayesianModel
 
 
 class VariableElimination(Inference):
+    def _get_working_factors(self, evidence):
+        """
+        Uses the evidence given to the query methods to modify the factors before running
+        the variable elimination algorithm.
+
+        Parameters
+        ----------
+        evidence: dict
+            Dict of the form {variable: state}
+
+        Returns
+        -------
+        dict: Modified working factors.
+        """
+        working_factors = {
+            node: {factor for factor in self.factors[node]} for node in self.factors
+        }
+
+        # Dealing with evidence. Reducing factors over it before VE is run.
+        if evidence:
+            for evidence_var in evidence:
+                for factor in working_factors[evidence_var]:
+                    factor_reduced = factor.reduce(
+                        [(evidence_var, evidence[evidence_var])], inplace=False
+                    )
+                    for var in factor_reduced.scope():
+                        working_factors[var].remove(factor)
+                        working_factors[var].add(factor_reduced)
+                del working_factors[evidence_var]
+        return working_factors
+
+    def _get_elimination_order(
+        self, variables, evidence, elimination_order, show_progress=True
+    ):
+        """
+        Deals with all elimination order parameters given to _variable_elimination method
+        and returns a list of variables that are to be eliminated
+
+        Parameters
+        ----------
+        elimination_order: str or list
+
+        Returns
+        -------
+        list: A list of variables names in the order they need to be eliminated.
+        """
+        to_eliminate = (
+            set(self.variables)
+            - set(variables)
+            - set(evidence.keys() if evidence else [])
+        )
+
+        # Step 1: If elimination_order is a list, verify it's correct and return.
+        if hasattr(elimination_order, "__iter__") and (
+            not isinstance(elimination_order, str)
+        ):
+            if any(
+                var in elimination_order
+                for var in set(variables).union(
+                    set(evidence.keys() if evidence else [])
+                )
+            ):
+                raise ValueError(
+                    "Elimination order contains variables which are in"
+                    " variables or evidence args"
+                )
+            else:
+                return elimination_order
 
-    @StateNameDecorator(argument='evidence', return_val=None)
-    def _variable_elimination(self, variables, operation, evidence=None, elimination_order=None):
+        # Step 2: If elimination order is None or a Markov model, return a random order.
+        elif (elimination_order is None) or (not isinstance(self.model, BayesianModel)):
+            return to_eliminate
+
+        # Step 3: If elimination order is a str, compute the order using the specified heuristic.
+        elif isinstance(elimination_order, str) and isinstance(
+            self.model, BayesianModel
+        ):
+            heuristic_dict = {
+                "weightedminfill": WeightedMinFill,
+                "minneighbors": MinNeighbors,
+                "minweight": MinWeight,
+                "minfill": MinFill,
+            }
+            elimination_order = heuristic_dict[elimination_order.lower()](
+                self.model
+            ).get_elimination_order(nodes=to_eliminate, show_progress=show_progress)
+            return elimination_order
+
+    def _variable_elimination(
+        self,
+        variables,
+        operation,
+        evidence=None,
+        elimination_order="MinFill",
+        joint=True,
+        show_progress=True,
+    ):
         """
         Implementation of a generalized variable elimination.
 
         Parameters
         ----------
         variables: list, array-like
             variables that are not to be eliminated.
+
         operation: str ('marginalize' | 'maximize')
             The operation to do for eliminating the variable.
+
         evidence: dict
             a dict key, value pair as {var: state_of_var_observed}
             None if no evidence
-        elimination_order: list, array-like
-            list of variables representing the order in which they
-            are to be eliminated. If None order is computed automatically.
+
+        elimination_order: str or list (array-like)
+            If str: Heuristic to use to find the elimination order.
+            If array-like: The elimination order to use.
+            If None: A random elimination order is used.
         """
+        # Step 1: Deal with the input arguments.
         if isinstance(variables, string_types):
             raise TypeError("variables must be a list of strings")
         if isinstance(evidence, string_types):
             raise TypeError("evidence must be a list of strings")
 
         # Dealing with the case when variables is not provided.
         if not variables:
             all_factors = []
             for factor_li in self.factors.values():
                 all_factors.extend(factor_li)
-            return set(all_factors)
+            if joint:
+                return factor_product(*set(all_factors))
+            else:
+                return set(all_factors)
 
+        # Step 2: Prepare data structures to run the algorithm.
         eliminated_variables = set()
-        working_factors = {node: {factor for factor in self.factors[node]}
-                           for node in self.factors}
-
-        # Dealing with evidence. Reducing factors over it before VE is run.
-        if evidence:
-            for evidence_var in evidence:
-                for factor in working_factors[evidence_var]:
-                    factor_reduced = factor.reduce([(evidence_var, evidence[evidence_var])], inplace=False)
-                    for var in factor_reduced.scope():
-                        working_factors[var].remove(factor)
-                        working_factors[var].add(factor_reduced)
-                del working_factors[evidence_var]
-
-        # TODO: Modify it to find the optimal elimination order
-        if not elimination_order:
-            elimination_order = list(set(self.variables) -
-                                     set(variables) -
-                                     set(evidence.keys() if evidence else []))
-
-        elif any(var in elimination_order for var in
-                 set(variables).union(set(evidence.keys() if evidence else []))):
-            raise ValueError("Elimination order contains variables which are in"
-                             " variables or evidence args")
+        # Get working factors and elimination order
+        working_factors = self._get_working_factors(evidence)
+        elimination_order = self._get_elimination_order(
+            variables, evidence, elimination_order, show_progress=show_progress
+        )
+
+        # Step 3: Run variable elimination
+        if show_progress:
+            pbar = tqdm(elimination_order)
+        else:
+            pbar = elimination_order
 
-        for var in elimination_order:
+        for var in pbar:
+            if show_progress:
+                pbar.set_description("Eliminating: {var}".format(var=var))
             # Removing all the factors containing the variables which are
             # eliminated (as all the factors should be considered only once)
-            factors = [factor for factor in working_factors[var]
-                       if not set(factor.variables).intersection(eliminated_variables)]
+            factors = [
+                factor
+                for factor in working_factors[var]
+                if not set(factor.variables).intersection(eliminated_variables)
+            ]
             phi = factor_product(*factors)
             phi = getattr(phi, operation)([var], inplace=False)
             del working_factors[var]
             for variable in phi.variables:
                 working_factors[variable].add(phi)
             eliminated_variables.add(var)
 
+        # Step 4: Prepare variables to be returned.
         final_distribution = set()
         for node in working_factors:
             factors = working_factors[node]
             for factor in factors:
                 if not set(factor.variables).intersection(eliminated_variables):
                     final_distribution.add(factor)
 
-        query_var_factor = {}
-        for query_var in variables:
-            phi = factor_product(*final_distribution)
-            query_var_factor[query_var] = phi.marginalize(list(set(variables) -
-                                                               set([query_var])),
-                                                          inplace=False).normalize(inplace=False)
-        return query_var_factor
-
-    def query(self, variables, evidence=None, elimination_order=None):
+        if joint:
+            if isinstance(self.model, BayesianModel):
+                return factor_product(*final_distribution).normalize(inplace=False)
+            else:
+                return factor_product(*final_distribution)
+        else:
+            query_var_factor = {}
+            for query_var in variables:
+                phi = factor_product(*final_distribution)
+                query_var_factor[query_var] = phi.marginalize(
+                    list(set(variables) - set([query_var])), inplace=False
+                ).normalize(inplace=False)
+            return query_var_factor
+
+    def query(
+        self,
+        variables,
+        evidence=None,
+        elimination_order="MinFill",
+        joint=True,
+        show_progress=True,
+    ):
         """
         Parameters
         ----------
         variables: list
             list of variables for which you want to compute the probability
+
         evidence: dict
             a dict key, value pair as {var: state_of_var_observed}
             None if no evidence
+
         elimination_order: list
             order of variable eliminations (if nothing is provided) order is
             computed automatically
 
+        joint: boolean (default: True)
+            If True, returns a Joint Distribution over `variables`.
+            If False, returns a dict of distributions over each of the `variables`.
+
         Examples
         --------
         >>> from pgmpy.inference import VariableElimination
         >>> from pgmpy.models import BayesianModel
         >>> import numpy as np
         >>> import pandas as pd
         >>> values = pd.DataFrame(np.random.randint(low=0, high=2, size=(1000, 5)),
         ...                       columns=['A', 'B', 'C', 'D', 'E'])
         >>> model = BayesianModel([('A', 'B'), ('C', 'B'), ('C', 'D'), ('B', 'E')])
         >>> model.fit(values)
         >>> inference = VariableElimination(model)
         >>> phi_query = inference.query(['A', 'B'])
         """
-        return self._variable_elimination(variables, 'marginalize',
-                                          evidence=evidence, elimination_order=elimination_order)
-
-    def max_marginal(self, variables=None, evidence=None, elimination_order=None):
+        return self._variable_elimination(
+            variables=variables,
+            operation="marginalize",
+            evidence=evidence,
+            elimination_order=elimination_order,
+            joint=joint,
+            show_progress=show_progress,
+        )
+
+    def max_marginal(
+        self,
+        variables=None,
+        evidence=None,
+        elimination_order="MinFill",
+        show_progress=True,
+    ):
         """
         Computes the max-marginal over the variables given the evidence.
 
         Parameters
         ----------
         variables: list
             list of variables over which we want to compute the max-marginal.
@@ -152,26 +286,31 @@
         >>> model = BayesianModel([('A', 'B'), ('C', 'B'), ('C', 'D'), ('B', 'E')])
         >>> model.fit(values)
         >>> inference = VariableElimination(model)
         >>> phi_query = inference.max_marginal(['A', 'B'])
         """
         if not variables:
             variables = []
-        final_distribution = self._variable_elimination(variables, 'maximize',
-                                                        evidence=evidence,
-                                                        elimination_order=elimination_order)
-
-        # To handle the case when no argument is passed then
-        # _variable_elimination returns a dict.
-        if isinstance(final_distribution, dict):
-            final_distribution = final_distribution.values()
-        return np.max(factor_product(*final_distribution).values)
-
-    @StateNameDecorator(argument=None, return_val=True)
-    def map_query(self, variables=None, evidence=None, elimination_order=None):
+        final_distribution = self._variable_elimination(
+            variables=variables,
+            operation="maximize",
+            evidence=evidence,
+            elimination_order=elimination_order,
+            show_progress=show_progress,
+        )
+
+        return np.max(final_distribution.values)
+
+    def map_query(
+        self,
+        variables=None,
+        evidence=None,
+        elimination_order="MinFill",
+        show_progress=True,
+    ):
         """
         Computes the MAP Query over the variables given the evidence.
 
         Note: When multiple variables are passed, it returns the map_query for each
         of them individually.
 
         Parameters
@@ -195,24 +334,24 @@
         ...                       columns=['A', 'B', 'C', 'D', 'E'])
         >>> model = BayesianModel([('A', 'B'), ('C', 'B'), ('C', 'D'), ('B', 'E')])
         >>> model.fit(values)
         >>> inference = VariableElimination(model)
         >>> phi_query = inference.map_query(['A', 'B'])
         """
         # TODO:Check the note in docstring. Change that behavior to return the joint MAP
-        final_distribution = self._variable_elimination(variables, 'marginalize',
-                                                        evidence=evidence,
-                                                        elimination_order=elimination_order)
-        # To handle the case when no argument is passed then
-        # _variable_elimination returns a dict.
-        if isinstance(final_distribution, dict):
-            final_distribution = final_distribution.values()
-        distribution = factor_product(*final_distribution)
-        argmax = np.argmax(distribution.values)
-        assignment = distribution.assignment([argmax])[0]
+        final_distribution = self._variable_elimination(
+            variables=variables,
+            operation="marginalize",
+            evidence=evidence,
+            elimination_order=elimination_order,
+            joint=True,
+            show_progress=show_progress,
+        )
+        argmax = np.argmax(final_distribution.values)
+        assignment = final_distribution.assignment([argmax])[0]
 
         map_query_results = {}
         for var_assignment in assignment:
             var, value = var_assignment
             map_query_results[var] = value
 
         if not variables:
@@ -244,41 +383,49 @@
         >>> model.fit(values)
         >>> inference = VariableElimination(model)
         >>> inference.induced_graph(['C', 'D', 'A', 'B', 'E'])
         <networkx.classes.graph.Graph at 0x7f34ac8c5160>
         """
         # If the elimination order does not contain the same variables as the model
         if set(elimination_order) != set(self.variables):
-            raise ValueError("Set of variables in elimination order"
-                             " different from variables in model")
+            raise ValueError(
+                "Set of variables in elimination order"
+                " different from variables in model"
+            )
 
         eliminated_variables = set()
-        working_factors = {node: [factor.scope() for factor in self.factors[node]]
-                           for node in self.factors}
+        working_factors = {
+            node: [factor.scope() for factor in self.factors[node]]
+            for node in self.factors
+        }
 
         # The set of cliques that should be in the induced graph
         cliques = set()
         for factors in working_factors.values():
             for factor in factors:
                 cliques.add(tuple(factor))
 
         # Removing all the factors containing the variables which are
         # eliminated (as all the factors should be considered only once)
         for var in elimination_order:
-            factors = [factor for factor in working_factors[var]
-                       if not set(factor).intersection(eliminated_variables)]
+            factors = [
+                factor
+                for factor in working_factors[var]
+                if not set(factor).intersection(eliminated_variables)
+            ]
             phi = set(itertools.chain(*factors)).difference({var})
             cliques.add(tuple(phi))
             del working_factors[var]
             for variable in phi:
                 working_factors[variable].append(list(phi))
             eliminated_variables.add(var)
 
-        edges_comb = [itertools.combinations(c, 2)
-                      for c in filter(lambda x: len(x) > 1, cliques)]
+        edges_comb = [
+            itertools.combinations(c, 2) for c in filter(lambda x: len(x) > 1, cliques)
+        ]
         return nx.Graph(itertools.chain(*edges_comb))
 
     def induced_width(self, elimination_order):
         """
         Returns the width (integer) of the induced graph formed by running Variable Elimination on the network.
         The width is the defined as the number of nodes in the largest clique in the graph minus 1.
 
@@ -367,20 +514,24 @@
         neighboring ones.
         """
         sepset = frozenset(sending_clique).intersection(frozenset(recieving_clique))
         sepset_key = frozenset((sending_clique, recieving_clique))
 
         # \sigma_{i \rightarrow j} = \sum_{C_i - S_{i, j}} \beta_i
         # marginalize the clique over the sepset
-        sigma = getattr(self.clique_beliefs[sending_clique], operation)(list(frozenset(sending_clique) - sepset),
-                                                                        inplace=False)
+        sigma = getattr(self.clique_beliefs[sending_clique], operation)(
+            list(frozenset(sending_clique) - sepset), inplace=False
+        )
 
         # \beta_j = \beta_j * \frac{\sigma_{i \rightarrow j}}{\mu_{i, j}}
-        self.clique_beliefs[recieving_clique] *= (sigma / self.sepset_beliefs[sepset_key]
-                                                  if self.sepset_beliefs[sepset_key] else sigma)
+        self.clique_beliefs[recieving_clique] *= (
+            sigma / self.sepset_beliefs[sepset_key]
+            if self.sepset_beliefs[sepset_key]
+            else sigma
+        )
 
         # \mu_{i, j} = \sigma_{i \rightarrow j}
         self.sepset_beliefs[sepset_key] = sigma
 
     def _is_converged(self, operation):
         """
         Checks whether the calibration has converged or not. At convergence
@@ -404,23 +555,31 @@
         # If no clique belief, then the clique tree is not calibrated
         if not self.clique_beliefs:
             return False
 
         for edge in self.junction_tree.edges():
             sepset = frozenset(edge[0]).intersection(frozenset(edge[1]))
             sepset_key = frozenset(edge)
-            if (edge[0] not in self.clique_beliefs or edge[1] not in self.clique_beliefs or
-                    sepset_key not in self.sepset_beliefs):
+            if (
+                edge[0] not in self.clique_beliefs
+                or edge[1] not in self.clique_beliefs
+                or sepset_key not in self.sepset_beliefs
+            ):
                 return False
 
-            marginal_1 = getattr(self.clique_beliefs[edge[0]], operation)(list(frozenset(edge[0]) - sepset),
-                                                                          inplace=False)
-            marginal_2 = getattr(self.clique_beliefs[edge[1]], operation)(list(frozenset(edge[1]) - sepset),
-                                                                          inplace=False)
-            if marginal_1 != marginal_2 or marginal_1 != self.sepset_beliefs[sepset_key]:
+            marginal_1 = getattr(self.clique_beliefs[edge[0]], operation)(
+                list(frozenset(edge[0]) - sepset), inplace=False
+            )
+            marginal_2 = getattr(self.clique_beliefs[edge[1]], operation)(
+                list(frozenset(edge[1]) - sepset), inplace=False
+            )
+            if (
+                marginal_1 != marginal_2
+                or marginal_1 != self.sepset_beliefs[sepset_key]
+            ):
                 return False
         return True
 
     def _calibrate_junction_tree(self, operation):
         """
         Generalized calibration of junction tree or clique using belief propagation. This method can be used for both
         calibrating as well as max-calibrating.
@@ -434,26 +593,32 @@
         Reference
         ---------
         Algorithm 10.3 Calibration using belief propagation in clique tree
         Probabilistic Graphical Models: Principles and Techniques
         Daphne Koller and Nir Friedman.
         """
         # Initialize clique beliefs as well as sepset beliefs
-        self.clique_beliefs = {clique: self.junction_tree.get_factors(clique)
-                               for clique in self.junction_tree.nodes()}
-        self.sepset_beliefs = {frozenset(edge): None for edge in self.junction_tree.edges()}
+        self.clique_beliefs = {
+            clique: self.junction_tree.get_factors(clique)
+            for clique in self.junction_tree.nodes()
+        }
+        self.sepset_beliefs = {
+            frozenset(edge): None for edge in self.junction_tree.edges()
+        }
 
         for clique in self.junction_tree.nodes():
             if not self._is_converged(operation=operation):
                 neighbors = self.junction_tree.neighbors(clique)
                 # update root's belief using nieighbor clique's beliefs
                 # upward pass
                 for neighbor_clique in neighbors:
                     self._update_beliefs(neighbor_clique, clique, operation=operation)
-                bfs_edges = nx.algorithms.breadth_first_search.bfs_edges(self.junction_tree, clique)
+                bfs_edges = nx.algorithms.breadth_first_search.bfs_edges(
+                    self.junction_tree, clique
+                )
                 # update the beliefs of all the nodes starting from the root to leaves using root's belief
                 # downward pass
                 for edge in bfs_edges:
                     self._update_beliefs(edge[0], edge[1], operation=operation)
             else:
                 break
 
@@ -484,15 +649,15 @@
         ...                         [[0.1, 0.4, 0.8],
         ...                          [0.9, 0.6, 0.2]],
         ...                         evidence=['grade'], evidence_card=[3])
         >>> G.add_cpds(diff_cpd, intel_cpd, grade_cpd, sat_cpd, letter_cpd)
         >>> bp = BeliefPropagation(G)
         >>> bp.calibrate()
         """
-        self._calibrate_junction_tree(operation='marginalize')
+        self._calibrate_junction_tree(operation="marginalize")
 
     def max_calibrate(self):
         """
         Max-calibration of the junction tree using belief propagation.
 
         Examples
         --------
@@ -517,17 +682,17 @@
         ...                         [[0.1, 0.4, 0.8],
         ...                          [0.9, 0.6, 0.2]],
         ...                         evidence=['grade'], evidence_card=[3])
         >>> G.add_cpds(diff_cpd, intel_cpd, grade_cpd, sat_cpd, letter_cpd)
         >>> bp = BeliefPropagation(G)
         >>> bp.max_calibrate()
         """
-        self._calibrate_junction_tree(operation='maximize')
+        self._calibrate_junction_tree(operation="maximize")
 
-    def _query(self, variables, operation, evidence=None):
+    def _query(self, variables, operation, evidence=None, joint=True):
         """
         This is a generalized query method that can be used for both query and map query.
 
         Parameters
         ----------
         variables: list
             list of variables for which you want to compute the probability
@@ -566,75 +731,95 @@
         else:
             query_variables = list(variables)
         query_variables.extend(evidence.keys() if evidence else [])
 
         # Find a tree T' such that query_variables are a subset of scope(T')
         nodes_with_query_variables = set()
         for var in query_variables:
-            nodes_with_query_variables.update(filter(lambda x: var in x, self.junction_tree.nodes()))
+            nodes_with_query_variables.update(
+                filter(lambda x: var in x, self.junction_tree.nodes())
+            )
         subtree_nodes = nodes_with_query_variables
 
         # Conversion of set to tuple just for indexing
         nodes_with_query_variables = tuple(nodes_with_query_variables)
         # As junction tree is a tree, that means that there would be only path between any two nodes in the tree
         # thus we can just take the path between any two nodes; no matter there order is
         for i in range(len(nodes_with_query_variables) - 1):
-            subtree_nodes.update(nx.shortest_path(self.junction_tree, nodes_with_query_variables[i],
-                                                  nodes_with_query_variables[i + 1]))
+            subtree_nodes.update(
+                nx.shortest_path(
+                    self.junction_tree,
+                    nodes_with_query_variables[i],
+                    nodes_with_query_variables[i + 1],
+                )
+            )
         subtree_undirected_graph = self.junction_tree.subgraph(subtree_nodes)
         # Converting subtree into a junction tree
         if len(subtree_nodes) == 1:
             subtree = JunctionTree()
             subtree.add_node(subtree_nodes.pop())
         else:
             subtree = JunctionTree(subtree_undirected_graph.edges())
 
         # Selecting a node is root node. Root node would be having only one neighbor
         if len(subtree.nodes()) == 1:
-            root_node = subtree.nodes()[0]
+            root_node = list(subtree.nodes())[0]
         else:
-            root_node = tuple(filter(lambda x: len(subtree.neighbors(x)) == 1, subtree.nodes()))[0]
+            root_node = tuple(
+                filter(lambda x: len(list(subtree.neighbors(x))) == 1, subtree.nodes())
+            )[0]
         clique_potential_list = [self.clique_beliefs[root_node]]
 
         # For other nodes in the subtree compute the clique potentials as follows
         # As all the nodes are nothing but tuples so simple set(root_node) won't work at it would update the set with'
         # all the elements of the tuple; instead use set([root_node]) as it would include only the tuple not the
         # internal elements within it.
         parent_nodes = set([root_node])
         nodes_traversed = set()
         while parent_nodes:
             parent_node = parent_nodes.pop()
             for child_node in set(subtree.neighbors(parent_node)) - nodes_traversed:
-                clique_potential_list.append(self.clique_beliefs[child_node] /
-                                             self.sepset_beliefs[frozenset([parent_node, child_node])])
+                clique_potential_list.append(
+                    self.clique_beliefs[child_node]
+                    / self.sepset_beliefs[frozenset([parent_node, child_node])]
+                )
                 parent_nodes.update([child_node])
             nodes_traversed.update([parent_node])
 
         # Add factors to the corresponding junction tree
         subtree.add_factors(*clique_potential_list)
 
         # Sum product variable elimination on the subtree
         variable_elimination = VariableElimination(subtree)
-        if operation == 'marginalize':
-            return variable_elimination.query(variables=variables, evidence=evidence)
-        elif operation == 'maximize':
-            return variable_elimination.map_query(variables=variables, evidence=evidence)
+        if operation == "marginalize":
+            return variable_elimination.query(
+                variables=variables, evidence=evidence, joint=joint
+            )
+        elif operation == "maximize":
+            return variable_elimination.map_query(
+                variables=variables, evidence=evidence
+            )
 
-    def query(self, variables, evidence=None):
+    def query(self, variables, evidence=None, joint=True):
         """
         Query method using belief propagation.
 
         Parameters
         ----------
         variables: list
             list of variables for which you want to compute the probability
+
         evidence: dict
             a dict key, value pair as {var: state_of_var_observed}
             None if no evidence
 
+        joint: boolean
+            If True, returns a Joint Distribution over `variables`.
+            If False, returns a dict of distributions over each of the `variables`.
+
         Examples
         --------
         >>> from pgmpy.factors.discrete import TabularCPD
         >>> from pgmpy.models import BayesianModel
         >>> from pgmpy.inference import BeliefPropagation
         >>> bayesian_model = BayesianModel([('A', 'J'), ('R', 'J'), ('J', 'Q'),
         ...                                 ('J', 'L'), ('G', 'L')])
@@ -654,15 +839,17 @@
         ...                    ['G', 'J'], [2, 2])
         >>> cpd_g = TabularCPD('G', 2, [[0.6], [0.4]])
         >>> bayesian_model.add_cpds(cpd_a, cpd_r, cpd_j, cpd_q, cpd_l, cpd_g)
         >>> belief_propagation = BeliefPropagation(bayesian_model)
         >>> belief_propagation.query(variables=['J', 'Q'],
         ...                          evidence={'A': 0, 'R': 0, 'G': 0, 'L': 1})
         """
-        return self._query(variables=variables, operation='marginalize', evidence=evidence)
+        return self._query(
+            variables=variables, operation="marginalize", evidence=evidence, joint=joint
+        )
 
     def map_query(self, variables=None, evidence=None):
         """
         MAP Query method using belief propagation.
 
         Note: When multiple variables are passed, it returns the map_query for each
         of them individually.
@@ -702,23 +889,22 @@
         >>> belief_propagation.map_query(variables=['J', 'Q'],
         ...                              evidence={'A': 0, 'R': 0, 'G': 0, 'L': 1})
         """
         # TODO:Check the note in docstring. Change that behavior to return the joint MAP
         if not variables:
             variables = set(self.variables)
 
-        final_distribution = self._query(variables=variables, operation='marginalize', evidence=evidence)
+        final_distribution = self._query(
+            variables=variables, operation="marginalize", evidence=evidence
+        )
 
         # To handle the case when no argument is passed then
         # _variable_elimination returns a dict.
-        if isinstance(final_distribution, dict):
-            final_distribution = final_distribution.values()
-        distribution = factor_product(*final_distribution)
-        argmax = np.argmax(distribution.values)
-        assignment = distribution.assignment([argmax])[0]
+        argmax = np.argmax(final_distribution.values)
+        assignment = final_distribution.assignment([argmax])[0]
 
         map_query_results = {}
         for var_assignment in assignment:
             var, value = var_assignment
             map_query_results[var] = value
 
         if not variables:
```

### Comparing `pgmpy-0.1.7/pgmpy/inference/mplp.py` & `pgmpy-0.1.9/pgmpy/inference/mplp.py`

 * *Files 6% similar despite different names*

```diff
@@ -43,26 +43,29 @@
     ...                             values=np.array([0.0043227, 231.34, 231.34, 0.0043227]))
     >>> factor_d_e = DiscreteFactor(['E', 'F'], cardinality=[2, 2],
     ...                             values=np.array([31.228, 0.032023, 0.032023, 31.228]))
     >>> student.add_factors(factor_a, factor_b, factor_c, factor_d, factor_e, factor_f, factor_a_b,
     ...                     factor_b_c, factor_c_d, factor_d_e)
     >>> mplp = Mplp(student)
     """
+
     def __init__(self, model):
         if not isinstance(model, MarkovModel):
-            raise TypeError('Only MarkovModel is supported')
+            raise TypeError("Only MarkovModel is supported")
 
         super(Mplp, self).__init__(model)
         self.model = model
 
         # S = \{c \cap c^{'} : c, c^{'} \in C, c \cap c^{'} \neq \emptyset\}
         self.intersection_set_variables = set()
         # We generate the Intersections of all the pairwise edges taken one at a time to form S
         for edge_pair in it.combinations(model.edges(), 2):
-            self.intersection_set_variables.add(frozenset(edge_pair[0]) & frozenset(edge_pair[1]))
+            self.intersection_set_variables.add(
+                frozenset(edge_pair[0]) & frozenset(edge_pair[1])
+            )
 
         # The corresponding optimization problem = \min_{\delta}{dual_lp(\delta)} where:
         # dual_lp(\delta) = \sum_{i \in V}{max_{x_i}(Objective[nodes])} + \sum_{f /in F}{max_{x_f}(Objective[factors])
         # Objective[nodes] = \theta_i(x_i) + \sum_{f \mid i \in f}{\delta_{fi}(x_i)}
         # Objective[factors] = \theta_f(x_f) - \sum_{i \in f}{\delta_{fi}(x_i)}
         # In a way Objective stores the corresponding optimization problem for all the nodes and the factors.
 
@@ -70,18 +73,22 @@
         self.objective = {}
         self.cluster_set = {}
         for factor in model.get_factors():
             scope = frozenset(factor.scope())
             self.objective[scope] = factor
             # For every factor consisting of more that a single node, we initialize a cluster.
             if len(scope) > 1:
-                self.cluster_set[scope] = self.Cluster(self.intersection_set_variables, factor)
+                self.cluster_set[scope] = self.Cluster(
+                    self.intersection_set_variables, factor
+                )
 
         # dual_lp(\delta) is the dual linear program
-        self.dual_lp = sum([np.amax(self.objective[obj].values) for obj in self.objective])
+        self.dual_lp = sum(
+            [np.amax(self.objective[obj].values) for obj in self.objective]
+        )
 
         # Best integral value of the primal objective is stored here
         self.best_int_objective = 0
 
         # Assignment of the nodes that results in the "maximum" integral value of the primal objective
         self.best_assignment = {}
         # Results of the "maximum" integral value of the primal objective.
@@ -106,44 +113,54 @@
         intersection_set_variables: set containing frozensets.
                                     collection of intersection of all pairs of cluster variables.
                         For eg: \{\{C_1 \cap C_2\}, \{C_2 \cap C_3\}, \{C_3 \cap C_1\} \} for clusters C_1, C_2 & C_3.
 
         cluster_potential: DiscreteFactor
                            Each cluster has a initial probability distribution provided beforehand.
         """
+
         def __init__(self, intersection_set_variables, cluster_potential):
             """
             Initialization of the current cluster
             """
 
             # The variables with which the cluster is made of.
             self.cluster_variables = frozenset(cluster_potential.scope())
 
             # The cluster potentials must be specified before only.
             self.cluster_potential = copy.deepcopy(cluster_potential)
 
             # Generate intersection sets for this cluster; S(c)
-            self.intersection_sets_for_cluster_c = [intersect.intersection(self.cluster_variables)
-                                                    for intersect in intersection_set_variables
-                                                    if intersect.intersection(self.cluster_variables)]
+            self.intersection_sets_for_cluster_c = [
+                intersect.intersection(self.cluster_variables)
+                for intersect in intersection_set_variables
+                if intersect.intersection(self.cluster_variables)
+            ]
 
             # Initialize messages from this cluster to its respective intersection sets
             # \lambda_{c \rightarrow \s} = 0
             self.message_from_cluster = {}
             for intersection in self.intersection_sets_for_cluster_c:
                 # Present variable. It can be a node or an edge too. (that is ['A'] or ['A', 'C'] too)
                 present_variables = list(intersection)
 
                 # Present variables cardinality
-                present_variables_card = cluster_potential.get_cardinality(present_variables)
-                present_variables_card = [present_variables_card[var] for var in present_variables]
+                present_variables_card = cluster_potential.get_cardinality(
+                    present_variables
+                )
+                present_variables_card = [
+                    present_variables_card[var] for var in present_variables
+                ]
 
                 # We need to create a new factor whose messages are blank
-                self.message_from_cluster[intersection] = \
-                    DiscreteFactor(present_variables, present_variables_card, np.zeros(np.prod(present_variables_card)))
+                self.message_from_cluster[intersection] = DiscreteFactor(
+                    present_variables,
+                    present_variables_card,
+                    np.zeros(np.prod(present_variables_card)),
+                )
 
     def _update_message(self, sending_cluster):
 
         """
         This is the message-update method.
 
         Parameters
@@ -170,62 +187,85 @@
         for current_intersect in sending_cluster.intersection_sets_for_cluster_c:
             objective_cluster += self.objective[current_intersect]
 
         updated_results = []
         objective = []
         for current_intersect in sending_cluster.intersection_sets_for_cluster_c:
             # Step. 2) Maximize step.1 result wrt variables present in the cluster but not in the current intersect.
-            phi = objective_cluster.maximize(list(sending_cluster.cluster_variables - current_intersect),
-                                             inplace=False)
+            phi = objective_cluster.maximize(
+                list(sending_cluster.cluster_variables - current_intersect),
+                inplace=False,
+            )
 
             # Step. 3) Multiply 1/{\| f \|}
             intersection_length = len(sending_cluster.intersection_sets_for_cluster_c)
-            phi *= (1 / intersection_length)
+            phi *= 1 / intersection_length
             objective.append(phi)
 
             # Step. 4) Subtract \delta_i^{-f}
             # These are the messages not emanating from the sending cluster but going into the current intersect.
             # which is = Objective[current_intersect_node] - messages from the cluster to the current intersect node.
-            updated_results.append(phi + -1 * (self.objective[current_intersect] + -1 * sending_cluster.
-                                               message_from_cluster[current_intersect]))
+            updated_results.append(
+                phi
+                + -1
+                * (
+                    self.objective[current_intersect]
+                    + -1 * sending_cluster.message_from_cluster[current_intersect]
+                )
+            )
 
         # This loop is primarily for simultaneous updating:
         # 1. This cluster's message to each of the intersects.
         # 2. The value of the Objective for intersection_nodes.
         index = -1
         cluster_potential = copy.deepcopy(sending_cluster.cluster_potential)
         for current_intersect in sending_cluster.intersection_sets_for_cluster_c:
             index += 1
-            sending_cluster.message_from_cluster[current_intersect] = updated_results[index]
+            sending_cluster.message_from_cluster[current_intersect] = updated_results[
+                index
+            ]
             self.objective[current_intersect] = objective[index]
             cluster_potential += (-1) * updated_results[index]
 
         # Here we update the Objective for the current factor.
         self.objective[sending_cluster.cluster_variables] = cluster_potential
 
     def _local_decode(self):
         """
         Finds the index of the maximum values for all the single node dual objectives.
 
         Reference:
         code presented by Sontag in 2012 here: http://cs.nyu.edu/~dsontag/code/README_v2.html
         """
         # The current assignment of the single node factors is stored in the form of a dictionary
-        decoded_result_assignment = {node: np.argmax(self.objective[node].values)
-                                     for node in self.objective if len(node) == 1}
+        decoded_result_assignment = {
+            node: np.argmax(self.objective[node].values)
+            for node in self.objective
+            if len(node) == 1
+        }
         # Use the original cluster_potentials of each factor to find the primal integral value.
         # 1. For single node factors
-        integer_value = sum([self.factors[variable][0].values[decoded_result_assignment[frozenset([variable])]]
-                             for variable in self.variables])
+        integer_value = sum(
+            [
+                self.factors[variable][0].values[
+                    decoded_result_assignment[frozenset([variable])]
+                ]
+                for variable in self.variables
+            ]
+        )
         # 2. For clusters
         for cluster_key in self.cluster_set:
             cluster = self.cluster_set[cluster_key]
-            index = [tuple([variable, decoded_result_assignment[frozenset([variable])]])
-                     for variable in cluster.cluster_variables]
-            integer_value += cluster.cluster_potential.reduce(index, inplace=False).values
+            index = [
+                tuple([variable, decoded_result_assignment[frozenset([variable])]])
+                for variable in cluster.cluster_variables
+            ]
+            integer_value += cluster.cluster_potential.reduce(
+                index, inplace=False
+            ).values
 
         # Check if this is the best assignment till now
         if self.best_int_objective < integer_value:
             self.best_int_objective = integer_value
             self.best_assignment = decoded_result_assignment
 
     def _is_converged(self, dual_threshold=None, integrality_gap_threshold=None):
@@ -245,27 +285,32 @@
                                    is satisfactory.
 
         References
         ----------
         code presented by Sontag in 2012 here: http://cs.nyu.edu/~dsontag/code/README_v2.html
         """
         # Find the new objective after the message updates
-        new_dual_lp = sum([np.amax(self.objective[obj].values) for obj in self.objective])
+        new_dual_lp = sum(
+            [np.amax(self.objective[obj].values) for obj in self.objective]
+        )
 
         # Update the dual_gap as the difference between the dual objective of the previous and the current iteration.
         self.dual_gap = abs(self.dual_lp - new_dual_lp)
 
         # Update the integrality_gap as the difference between our best result vs the dual objective of the lp.
         self.integrality_gap = abs(self.dual_lp - self.best_int_objective)
 
         # As the decrement of the dual_lp gets very low, we assume that we might have stuck in a local minima.
         if dual_threshold and self.dual_gap < dual_threshold:
             return True
         # Check the threshold for the integrality gap
-        elif integrality_gap_threshold and self.integrality_gap < integrality_gap_threshold:
+        elif (
+            integrality_gap_threshold
+            and self.integrality_gap < integrality_gap_threshold
+        ):
             return True
         else:
             self.dual_lp = new_dual_lp
             return False
 
     def find_triangles(self):
         """
@@ -299,17 +344,23 @@
                         The list of variables forming the triangles to be updated. It is of the form of
                         [['var_5', 'var_8', 'var_7'], ['var_4', 'var_5', 'var_7']]
 
         """
         new_intersection_set = []
         for triangle_vars in triangles_list:
             cardinalities = [self.cardinality[variable] for variable in triangle_vars]
-            current_intersection_set = [frozenset(intersect) for intersect in it.combinations(triangle_vars, 2)]
-            current_factor = DiscreteFactor(triangle_vars, cardinalities, np.zeros(np.prod(cardinalities)))
-            self.cluster_set[frozenset(triangle_vars)] = self.Cluster(current_intersection_set, current_factor)
+            current_intersection_set = [
+                frozenset(intersect) for intersect in it.combinations(triangle_vars, 2)
+            ]
+            current_factor = DiscreteFactor(
+                triangle_vars, cardinalities, np.zeros(np.prod(cardinalities))
+            )
+            self.cluster_set[frozenset(triangle_vars)] = self.Cluster(
+                current_intersection_set, current_factor
+            )
             # add new factors
             self.model.factors.append(current_factor)
             # add new intersection sets
             new_intersection_set.extend(current_intersection_set)
             # add new factors in objective
             self.objective[frozenset(triangle_vars)] = current_factor
 
@@ -325,18 +376,25 @@
 
         Return: {frozenset({'var_8', 'var_5', 'var_7'}): 5.024, frozenset({'var_5', 'var_4', 'var_7'}): 10.23}
         """
         triplet_scores = {}
         for triplet in triangles_list:
 
             # Find the intersection sets of the current triplet
-            triplet_intersections = [intersect for intersect in it.combinations(triplet, 2)]
+            triplet_intersections = [
+                intersect for intersect in it.combinations(triplet, 2)
+            ]
 
             # Independent maximization
-            ind_max = sum([np.amax(self.objective[frozenset(intersect)].values) for intersect in triplet_intersections])
+            ind_max = sum(
+                [
+                    np.amax(self.objective[frozenset(intersect)].values)
+                    for intersect in triplet_intersections
+                ]
+            )
 
             # Joint maximization
             joint_max = self.objective[frozenset(triplet_intersections[0])]
             for intersect in triplet_intersections[1:]:
                 joint_max += self.objective[frozenset(intersect)]
             joint_max = np.amax(joint_max.values)
             # score = Independent maximization solution - Joint maximization solution
@@ -359,15 +417,18 @@
             # scope is greater than 1
             for factor in self.model.get_factors():
                 if len(factor.scope()) > 1:
                     self._update_message(self.cluster_set[frozenset(factor.scope())])
             # Find an integral solution by locally maximizing the single node beliefs
             self._local_decode()
             # If mplp converges to a global/local optima, we break.
-            if self._is_converged(self.dual_threshold, self.integrality_gap_threshold) and niter >= 16:
+            if (
+                self._is_converged(self.dual_threshold, self.integrality_gap_threshold)
+                and niter >= 16
+            ):
                 break
 
     def _tighten_triplet(self, max_iterations, later_iter, max_triplets, prolong):
         """
         This method finds all the triplets that are eligible and adds them iteratively in the bunch of max_triplets
 
         Parameters
@@ -388,26 +449,28 @@
         # Find all the triplets that are possible in the present model
         triangles = self.find_triangles()
         # Evaluate scores for each of the triplets found above
         triplet_scores = self._get_triplet_scores(triangles)
         # Arrange the keys on the basis of increasing order of the values of the dict. triplet_scores
         sorted_scores = sorted(triplet_scores, key=triplet_scores.get)
         for niter in range(max_iterations):
-            if self._is_converged(integrality_gap_threshold=self.integrality_gap_threshold):
+            if self._is_converged(
+                integrality_gap_threshold=self.integrality_gap_threshold
+            ):
                 break
             # add triplets that are yet not added.
             add_triplets = []
-            for triplet_number in (range(len(sorted_scores))):
+            for triplet_number in range(len(sorted_scores)):
                 # At once, we can add atmost 5 triplets
                 if triplet_number >= max_triplets:
                     break
                 add_triplets.append(sorted_scores.pop())
             # Break from the tighten triplets loop if there are no triplets to add if the prolong is set to False
             if not add_triplets and prolong is False:
-                    break
+                break
             # Update the eligible triplets to tighten the relaxation
             self._update_triangles(add_triplets)
             # Run MPLP for a maximum of later_iter times.
             self._run_mplp(later_iter)
 
     def get_integrality_gap(self):
         """
@@ -432,16 +495,25 @@
         """
 
         return self.integrality_gap
 
     def query(self):
         raise NotImplementedError("map_query() is the only query method available.")
 
-    def map_query(self, init_iter=1000, later_iter=20, dual_threshold=0.0002, integrality_gap_threshold=0.0002,
-                  tighten_triplet=True, max_triplets=5, max_iterations=100, prolong=False):
+    def map_query(
+        self,
+        init_iter=1000,
+        later_iter=20,
+        dual_threshold=0.0002,
+        integrality_gap_threshold=0.0002,
+        tighten_triplet=True,
+        max_triplets=5,
+        max_iterations=100,
+        prolong=False,
+    ):
         """
         MAP query method using Max Product LP method.
         This returns the best assignment of the nodes in the form of a dictionary.
 
         Parameters
         ----------
         init_iter: integer
@@ -510,10 +582,15 @@
         self.integrality_gap_threshold = integrality_gap_threshold
         # Run MPLP initially for a maximum of init_iter times.
         self._run_mplp(init_iter)
         # If triplets are to be used for the tightening, we proceed as follows
         if tighten_triplet:
             self._tighten_triplet(max_iterations, later_iter, max_triplets, prolong)
         # Get the best result from the best assignment
-        self.best_decoded_result = {factor.scope()[0]: factor.values[self.best_assignment[frozenset(factor.scope())]]
-                                    for factor in self.model.factors if len(factor.scope()) == 1}
+        self.best_decoded_result = {
+            factor.scope()[0]: factor.values[
+                self.best_assignment[frozenset(factor.scope())]
+            ]
+            for factor in self.model.factors
+            if len(factor.scope()) == 1
+        }
         return self.best_decoded_result
```

### Comparing `pgmpy-0.1.7/pgmpy/inference/dbn_inference.py` & `pgmpy-0.1.9/pgmpy/inference/dbn_inference.py`

 * *Files 2% similar despite different names*

```diff
@@ -70,22 +70,30 @@
         start_markov_model = self.start_bayesian_model.to_markov_model()
         one_and_half_markov_model = self.one_and_half_model.to_markov_model()
 
         combinations_slice_0 = tee(combinations(self.interface_nodes_0, 2), 2)
         combinations_slice_1 = combinations(self.interface_nodes_1, 2)
 
         start_markov_model.add_edges_from(combinations_slice_0[0])
-        one_and_half_markov_model.add_edges_from(chain(combinations_slice_0[1], combinations_slice_1))
+        one_and_half_markov_model.add_edges_from(
+            chain(combinations_slice_0[1], combinations_slice_1)
+        )
 
         self.one_and_half_junction_tree = one_and_half_markov_model.to_junction_tree()
         self.start_junction_tree = start_markov_model.to_junction_tree()
 
-        self.start_interface_clique = self._get_clique(self.start_junction_tree, self.interface_nodes_0)
-        self.in_clique = self._get_clique(self.one_and_half_junction_tree, self.interface_nodes_0)
-        self.out_clique = self._get_clique(self.one_and_half_junction_tree, self.interface_nodes_1)
+        self.start_interface_clique = self._get_clique(
+            self.start_junction_tree, self.interface_nodes_0
+        )
+        self.in_clique = self._get_clique(
+            self.one_and_half_junction_tree, self.interface_nodes_0
+        )
+        self.out_clique = self._get_clique(
+            self.one_and_half_junction_tree, self.interface_nodes_1
+        )
 
     def _shift_nodes(self, nodes, time_slice):
         """
         Shifting the nodes to a certain required timeslice.
 
         Parameters:
         ----------
@@ -108,15 +116,17 @@
         junction_tree: Junction tree
             from which the nodes are to be extracted.
 
         nodes: iterable container
             A container of nodes (list, dict, set, etc.).
         """
 
-        return [clique for clique in junction_tree.nodes() if set(nodes).issubset(clique)][0]
+        return [
+            clique for clique in junction_tree.nodes() if set(nodes).issubset(clique)
+        ][0]
 
     def _get_evidence(self, evidence_dict, time_slice, shift):
         """
         Getting the evidence belonging to a particular timeslice.
 
         Parameters:
         ----------
@@ -127,15 +137,19 @@
         time: int
             the evidence corresponding to the time slice
 
         shift: int
             shifting the evidence corresponding to the given time slice.
         """
         if evidence_dict:
-            return {(node[0], shift): evidence_dict[node] for node in evidence_dict if node[1] == time_slice}
+            return {
+                (node[0], shift): evidence_dict[node]
+                for node in evidence_dict
+                if node[1] == time_slice
+            }
 
     def _marginalize_factor(self, nodes, factor):
         """
         Marginalizing the factor selectively for a set of variables.
 
         Parameters:
         ----------
@@ -262,53 +276,69 @@
         start_bp = BeliefPropagation(self.start_junction_tree)
         mid_bp = BeliefPropagation(self.one_and_half_junction_tree)
         evidence_0 = self._get_evidence(evidence, 0, 0)
         interface_nodes_dict = {}
         potential_dict = {}
 
         if evidence:
-            interface_nodes_dict = {k: v for k, v in evidence_0.items() if k in self.interface_nodes_0}
+            interface_nodes_dict = {
+                k: v for k, v in evidence_0.items() if k in self.interface_nodes_0
+            }
         initial_factor = self._get_factor(start_bp, evidence_0)
-        marginalized_factor = self._marginalize_factor(self.interface_nodes_0, initial_factor)
+        marginalized_factor = self._marginalize_factor(
+            self.interface_nodes_0, initial_factor
+        )
         potential_dict[0] = marginalized_factor
         self._update_belief(mid_bp, self.in_clique, marginalized_factor)
 
         if variable_dict[0]:
-            factor_values = start_bp.query(variable_dict[0], evidence=evidence_0)
+            factor_values = start_bp.query(
+                variable_dict[0], evidence=evidence_0, joint=False
+            )
         else:
             factor_values = {}
 
         for time_slice in range(1, time_range + 1):
             evidence_time = self._get_evidence(evidence, time_slice, 1)
             if interface_nodes_dict:
                 evidence_time.update(interface_nodes_dict)
 
             if variable_dict[time_slice]:
                 variable_time = self._shift_nodes(variable_dict[time_slice], 1)
-                new_values = mid_bp.query(variable_time, evidence=evidence_time)
+                new_values = mid_bp.query(
+                    variable_time, evidence=evidence_time, joint=False
+                )
                 changed_values = {}
                 for key in new_values.keys():
                     new_key = (key[0], time_slice)
-                    new_factor = DiscreteFactor([new_key], new_values[key].cardinality, new_values[key].values)
+                    new_factor = DiscreteFactor(
+                        [new_key], new_values[key].cardinality, new_values[key].values
+                    )
                     changed_values[new_key] = new_factor
                 factor_values.update(changed_values)
 
             clique_phi = self._get_factor(mid_bp, evidence_time)
-            out_clique_phi = self._marginalize_factor(self.interface_nodes_1, clique_phi)
+            out_clique_phi = self._marginalize_factor(
+                self.interface_nodes_1, clique_phi
+            )
             new_factor = self._shift_factor(out_clique_phi, 0)
             potential_dict[time_slice] = new_factor
             mid_bp = BeliefPropagation(self.one_and_half_junction_tree)
             self._update_belief(mid_bp, self.in_clique, new_factor)
 
             if evidence_time:
-                interface_nodes_dict = {(k[0], 0): v for k, v in evidence_time.items() if k in self.interface_nodes_1}
+                interface_nodes_dict = {
+                    (k[0], 0): v
+                    for k, v in evidence_time.items()
+                    if k in self.interface_nodes_1
+                }
             else:
                 interface_nodes_dict = {}
 
-        if args == 'potential':
+        if args == "potential":
             return potential_dict
 
         return factor_values
 
     def backward_inference(self, variables, evidence=None):
         """
         Backward inference method using belief propagation.
@@ -353,52 +383,64 @@
             variable_dict[var[1]].append(var)
         time_range = max(variable_dict)
         interface_nodes_dict = {}
         if evidence:
             evid_time_range = max([time_slice for var, time_slice in evidence.keys()])
             time_range = max(time_range, evid_time_range)
         end_bp = BeliefPropagation(self.start_junction_tree)
-        potential_dict = self.forward_inference(variables, evidence, 'potential')
+        potential_dict = self.forward_inference(variables, evidence, "potential")
         update_factor = self._shift_factor(potential_dict[time_range], 1)
         factor_values = {}
 
         for time_slice in range(time_range, 0, -1):
             evidence_time = self._get_evidence(evidence, time_slice, 1)
             evidence_prev_time = self._get_evidence(evidence, time_slice - 1, 0)
             if evidence_prev_time:
-                interface_nodes_dict = {k: v for k, v in evidence_prev_time.items() if k in self.interface_nodes_0}
+                interface_nodes_dict = {
+                    k: v
+                    for k, v in evidence_prev_time.items()
+                    if k in self.interface_nodes_0
+                }
             if evidence_time:
                 evidence_time.update(interface_nodes_dict)
             mid_bp = BeliefPropagation(self.one_and_half_junction_tree)
             self._update_belief(mid_bp, self.in_clique, potential_dict[time_slice - 1])
             forward_factor = self._shift_factor(potential_dict[time_slice], 1)
             self._update_belief(mid_bp, self.out_clique, forward_factor, update_factor)
 
             if variable_dict[time_slice]:
                 variable_time = self._shift_nodes(variable_dict[time_slice], 1)
-                new_values = mid_bp.query(variable_time, evidence=evidence_time)
+                new_values = mid_bp.query(
+                    variable_time, evidence=evidence_time, joint=False
+                )
                 changed_values = {}
                 for key in new_values.keys():
                     new_key = (key[0], time_slice)
-                    new_factor = DiscreteFactor([new_key], new_values[key].cardinality, new_values[key].values)
+                    new_factor = DiscreteFactor(
+                        [new_key], new_values[key].cardinality, new_values[key].values
+                    )
                     changed_values[new_key] = new_factor
                 factor_values.update(changed_values)
 
             clique_phi = self._get_factor(mid_bp, evidence_time)
             in_clique_phi = self._marginalize_factor(self.interface_nodes_0, clique_phi)
             update_factor = self._shift_factor(in_clique_phi, 1)
 
         out_clique_phi = self._shift_factor(update_factor, 0)
-        self._update_belief(end_bp, self.start_interface_clique, potential_dict[0], out_clique_phi)
+        self._update_belief(
+            end_bp, self.start_interface_clique, potential_dict[0], out_clique_phi
+        )
         evidence_0 = self._get_evidence(evidence, 0, 0)
         if variable_dict[0]:
-            factor_values.update(end_bp.query(variable_dict[0], evidence_0))
+            factor_values.update(
+                end_bp.query(variable_dict[0], evidence_0, joint=False)
+            )
         return factor_values
 
-    def query(self, variables, evidence=None, args='exact'):
+    def query(self, variables, evidence=None, args="exact"):
         """
         Query method for Dynamic Bayesian Network using Interface Algorithm.
 
         Parameters:
         ----------
         variables: list
             list of variables for which you want to compute the probability
@@ -430,9 +472,9 @@
         ...                      evidence_card=[2])
         >>> dbnet.add_cpds(z_start_cpd, z_trans_cpd, x_i_cpd, y_i_cpd)
         >>> dbnet.initialize_initial_state()
         >>> dbn_inf = DBNInference(dbnet)
         >>> dbn_inf.query([('X', 0)], {('Y', 0):0, ('Y', 1):1, ('Y', 2):1})[('X', 0)].values
         array([ 0.66594382,  0.33405618])
         """
-        if args == 'exact':
+        if args == "exact":
             return self.backward_inference(variables, evidence)
```

### Comparing `pgmpy-0.1.7/pgmpy/inference/base.py` & `pgmpy-0.1.9/pgmpy/inference/base.py`

 * *Files 5% similar despite different names*

```diff
@@ -4,15 +4,14 @@
 from itertools import chain
 
 from pgmpy.models import BayesianModel
 from pgmpy.models import MarkovModel
 from pgmpy.models import FactorGraph
 from pgmpy.models import JunctionTree
 from pgmpy.models import DynamicBayesianNetwork
-from pgmpy.utils import StateNameInit
 from pgmpy.factors.discrete import TabularCPD
 
 
 class Inference(object):
     """
     Base class for all inference algorithms.
 
@@ -51,15 +50,14 @@
     >>> factor_b_c = DiscreteFactor(['Bob', 'Charles'], cardinality=[2, 2], value=np.random.rand(4))
     >>> factor_c_d = DiscreteFactor(['Charles', 'Debbie'], cardinality=[2, 2], value=np.random.rand(4))
     >>> factor_d_a = DiscreteFactor(['Debbie', 'Alice'], cardinality=[2, 2], value=np.random.rand(4))
     >>> student.add_factors(factor_a_b, factor_b_c, factor_c_d, factor_d_a)
     >>> model = Inference(student)
     """
 
-    @StateNameInit()
     def __init__(self, model):
         self.model = model
         model.check_model()
 
         if isinstance(model, JunctionTree):
             self.variables = set(chain(*model.nodes()))
         else:
@@ -85,9 +83,13 @@
                     self.factors[var].append(factor)
 
         elif isinstance(model, DynamicBayesianNetwork):
             self.start_bayesian_model = BayesianModel(model.get_intra_edges(0))
             self.start_bayesian_model.add_cpds(*model.get_cpds(time_slice=0))
             cpd_inter = [model.get_cpds(node) for node in model.get_interface_nodes(1)]
             self.interface_nodes = model.get_interface_nodes(0)
-            self.one_and_half_model = BayesianModel(model.get_inter_edges() + model.get_intra_edges(1))
-            self.one_and_half_model.add_cpds(*(model.get_cpds(time_slice=1) + cpd_inter))
+            self.one_and_half_model = BayesianModel(
+                model.get_inter_edges() + model.get_intra_edges(1)
+            )
+            self.one_and_half_model.add_cpds(
+                *(model.get_cpds(time_slice=1) + cpd_inter)
+            )
```

### Comparing `pgmpy-0.1.7/pgmpy/inference/EliminationOrder.py` & `pgmpy-0.1.9/pgmpy/inference/EliminationOrder.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,31 +1,33 @@
 from abc import abstractmethod
 from itertools import combinations
+from tqdm import tqdm
 
 import numpy as np
 
 from pgmpy.models import BayesianModel
 
 
 class BaseEliminationOrder:
     """
     Base class for finding elimination orders.
     """
+
     def __init__(self, model):
         """
         Init method for the base class of Elimination Orders.
 
         Parameters
         ----------
         model: BayesianModel instance
             The model on which we want to compute the elimination orders.
         """
         if not isinstance(model, BayesianModel):
             raise ValueError("Model should be a BayesianModel instance")
-        self.bayesian_model = model
+        self.bayesian_model = model.copy()
         self.moralized_model = self.bayesian_model.moralize()
 
     @abstractmethod
     def cost(self, node):
         """
         The cost function to compute the cost of elimination of each node.
         This method is just a dummy and returns 0 for all the nodes. Actual cost functions
@@ -34,15 +36,15 @@
         Parameters
         ----------
         node: string, any hashable python object.
             The node whose cost is to be computed.
         """
         return 0
 
-    def get_elimination_order(self, nodes=None):
+    def get_elimination_order(self, nodes=None, show_progress=True):
         """
         Returns the optimal elimination order based on the cost function.
         The node having the least cost is removed first.
 
         Parameters
         ----------
         nodes: list, tuple, set (array-like)
@@ -76,23 +78,33 @@
         >>> WeightedMinFill(model).get_elimination_order(['c', 'd', 'g', 'l', 's'])
         ['c', 's', 'l', 'd', 'g']
         >>> WeightedMinFill(model).get_elimination_order(['c', 'd', 'g', 'l', 's'])
         ['c', 's', 'l', 'd', 'g']
         >>> WeightedMinFill(model).get_elimination_order(['c', 'd', 'g', 'l', 's'])
         ['c', 's', 'l', 'd', 'g']
         """
-        if not nodes:
+        if nodes is None:
             nodes = self.bayesian_model.nodes()
+        nodes = set(nodes)
 
         ordering = []
+        if show_progress:
+            pbar = tqdm(total=len(nodes))
+            pbar.set_description("Finding Elimination Order: ")
+
         while nodes:
             scores = {node: self.cost(node) for node in nodes}
             min_score_node = min(scores, key=scores.get)
             ordering.append(min_score_node)
             nodes.remove(min_score_node)
+            self.bayesian_model.remove_node(min_score_node)
+            self.moralized_model.remove_node(min_score_node)
+
+            if show_progress:
+                pbar.update(1)
         return ordering
 
     def fill_in_edges(self, node):
         """
         Return edges needed to be added to the graph if a node is removed.
 
         Parameters
@@ -108,35 +120,44 @@
         """
         Cost function for WeightedMinFill.
         The cost of eliminating a node is the sum of weights of the edges that need to
         be added to the graph due to its elimination, where a weight of an edge is the
         product of the weights, domain cardinality, of its constituent vertices.
         """
         edges = combinations(self.moralized_model.neighbors(node), 2)
-        return sum([self.bayesian_model.get_cardinality(edge[0]) *
-                    self.bayesian_model.get_cardinality(edge[1]) for edge in edges])
+        return sum(
+            [
+                self.bayesian_model.get_cardinality(edge[0])
+                * self.bayesian_model.get_cardinality(edge[1])
+                for edge in edges
+            ]
+        )
 
 
-class MinNeighbours(BaseEliminationOrder):
+class MinNeighbors(BaseEliminationOrder):
     def cost(self, node):
         """
         The cost of a eliminating a node is the number of neighbors it has in the
         current graph.
         """
-        return len(self.moralized_model.neighbors(node))
+        return len(list(self.moralized_model.neighbors(node)))
 
 
 class MinWeight(BaseEliminationOrder):
     def cost(self, node):
         """
         The cost of a eliminating a node is the product of weights, domain cardinality,
         of its neighbors.
         """
-        return np.prod([self.bayesian_model.get_cardinality(neig_node) for neig_node in
-                        self.moralized_model.neighbors(node)])
+        return np.prod(
+            [
+                self.bayesian_model.get_cardinality(neig_node)
+                for neig_node in self.moralized_model.neighbors(node)
+            ]
+        )
 
 
 class MinFill(BaseEliminationOrder):
     def cost(self, node):
         """
         The cost of a eliminating a node is the number of edges that need to be added
         (fill in edges) to the graph due to its elimination
```

### Comparing `pgmpy-0.1.7/pgmpy/tests/test_base/test_DirectedGraph.py` & `pgmpy-0.1.9/pgmpy/tests/test_models/test_JunctionTree.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,139 +1,171 @@
-#!/usr/bin/env python3
-
-import unittest
-
-from pgmpy.base import DirectedGraph
-import pgmpy.tests.help_functions as hf
-
-
-class TestDirectedGraphCreation(unittest.TestCase):
-    def setUp(self):
-        self.graph = DirectedGraph()
-
-    def test_class_init_without_data(self):
-        self.assertIsInstance(self.graph, DirectedGraph)
-
-    def test_class_init_with_data_string(self):
-        self.graph = DirectedGraph([('a', 'b'), ('b', 'c')])
-        self.assertListEqual(sorted(self.graph.nodes()), ['a', 'b', 'c'])
-        self.assertListEqual(hf.recursive_sorted(self.graph.edges()),
-                             [['a', 'b'], ['b', 'c']])
-
-    def test_add_node_string(self):
-        self.graph.add_node('a')
-        self.assertListEqual(self.graph.nodes(), ['a'])
-
-    def test_add_node_nonstring(self):
-        self.graph.add_node(1)
-
-    def test_add_nodes_from_string(self):
-        self.graph.add_nodes_from(['a', 'b', 'c', 'd'])
-        self.assertListEqual(sorted(self.graph.nodes()), ['a', 'b', 'c', 'd'])
-
-    def test_add_nodes_from_non_string(self):
-        self.graph.add_nodes_from([1, 2, 3, 4])
-
-    def test_add_node_weight(self):
-        self.graph.add_node('weighted_a', 0.3)
-        self.assertEqual(self.graph.node['weighted_a']['weight'], 0.3)
-
-    def test_add_nodes_from_weight(self):
-        self.graph.add_nodes_from(['weighted_b', 'weighted_c'], [0.5, 0.6])
-        self.assertEqual(self.graph.node['weighted_b']['weight'], 0.5)
-        self.assertEqual(self.graph.node['weighted_c']['weight'], 0.6)
-
-        self.graph.add_nodes_from(['e', 'f'])
-        self.assertEqual(self.graph.node['e']['weight'], None)
-        self.assertEqual(self.graph.node['f']['weight'], None)
-
-    def test_add_edge_string(self):
-        self.graph.add_edge('d', 'e')
-        self.assertListEqual(sorted(self.graph.nodes()), ['d', 'e'])
-        self.assertListEqual(self.graph.edges(), [('d', 'e')])
-        self.graph.add_nodes_from(['a', 'b', 'c'])
-        self.graph.add_edge('a', 'b')
-        self.assertListEqual(hf.recursive_sorted(self.graph.edges()),
-                             [['a', 'b'], ['d', 'e']])
-
-    def test_add_edge_nonstring(self):
-        self.graph.add_edge(1, 2)
-
-    def test_add_edges_from_string(self):
-        self.graph.add_edges_from([('a', 'b'), ('b', 'c')])
-        self.assertListEqual(sorted(self.graph.nodes()), ['a', 'b', 'c'])
-        self.assertListEqual(hf.recursive_sorted(self.graph.edges()),
-                             [['a', 'b'], ['b', 'c']])
-        self.graph.add_nodes_from(['d', 'e', 'f'])
-        self.graph.add_edges_from([('d', 'e'), ('e', 'f')])
-        self.assertListEqual(sorted(self.graph.nodes()),
-                             ['a', 'b', 'c', 'd', 'e', 'f'])
-        self.assertListEqual(hf.recursive_sorted(self.graph.edges()),
-                             hf.recursive_sorted([('a', 'b'), ('b', 'c'),
-                                                  ('d', 'e'), ('e', 'f')]))
-
-    def test_add_edges_from_nonstring(self):
-        self.graph.add_edges_from([(1, 2), (2, 3)])
-
-    def test_add_edge_weight(self):
-        self.graph.add_edge('a', 'b', weight=0.3)
-        self.assertEqual(self.graph.edge['a']['b']['weight'], 0.3)
-
-    def test_add_edges_from_weight(self):
-        self.graph.add_edges_from([('b', 'c'), ('c', 'd')], weights=[0.5, 0.6])
-        self.assertEqual(self.graph.edge['b']['c']['weight'], 0.5)
-        self.assertEqual(self.graph.edge['c']['d']['weight'], 0.6)
-
-        self.graph.add_edges_from([('e', 'f')])
-        self.assertEqual(self.graph.edge['e']['f']['weight'], None)
-
-    def test_update_node_parents_bm_constructor(self):
-        self.graph = DirectedGraph([('a', 'b'), ('b', 'c')])
-        self.assertListEqual(self.graph.predecessors('a'), [])
-        self.assertListEqual(self.graph.predecessors('b'), ['a'])
-        self.assertListEqual(self.graph.predecessors('c'), ['b'])
-
-    def test_update_node_parents(self):
-        self.graph.add_nodes_from(['a', 'b', 'c'])
-        self.graph.add_edges_from([('a', 'b'), ('b', 'c')])
-        self.assertListEqual(self.graph.predecessors('a'), [])
-        self.assertListEqual(self.graph.predecessors('b'), ['a'])
-        self.assertListEqual(self.graph.predecessors('c'), ['b'])
-
-    def test_get_leaves(self):
-        self.graph.add_edges_from([('A', 'B'), ('B', 'C'), ('B', 'D'),
-                                   ('D', 'E'), ('D', 'F'), ('A', 'G')])
-        self.assertEqual(sorted(self.graph.get_leaves()),
-                         sorted(['C', 'G', 'E', 'F']))
-
-    def test_get_roots(self):
-        self.graph.add_edges_from([('A', 'B'), ('B', 'C'), ('B', 'D'),
-                                   ('D', 'E'), ('D', 'F'), ('A', 'G')])
-        self.assertEqual(['A'], self.graph.get_roots())
-        self.graph.add_edge('H', 'G')
-        self.assertEqual(sorted(['A', 'H']), sorted(self.graph.get_roots()))
-
-    def tearDown(self):
-        del self.graph
-
-
-class TestDirectedGraphMoralization(unittest.TestCase):
-    def setUp(self):
-        self.graph = DirectedGraph()
-        self.graph.add_edges_from([('diff', 'grade'), ('intel', 'grade')])
-
-    def test_get_parents(self):
-        self.assertListEqual(sorted(self.graph.get_parents('grade')),
-                             ['diff', 'intel'])
-
-    def test_moralize(self):
-        moral_graph = self.graph.moralize()
-        self.assertListEqual(hf.recursive_sorted(moral_graph.edges()),
-                             [['diff', 'grade'], ['diff', 'intel'],
-                              ['grade', 'intel']])
-
-    def test_get_children(self):
-        self.assertListEqual(sorted(self.graph.get_children('diff')),
-                             ['grade'])
-
-    def tearDown(self):
-        del self.graph
+import unittest
+import numpy as np
+
+from pgmpy.factors.discrete import DiscreteFactor
+from pgmpy.models import JunctionTree
+from pgmpy.tests import help_functions as hf
+
+
+class TestJunctionTreeCreation(unittest.TestCase):
+    def setUp(self):
+        self.graph = JunctionTree()
+
+    def test_add_single_node(self):
+        self.graph.add_node(("a", "b"))
+        self.assertListEqual(list(self.graph.nodes()), [("a", "b")])
+
+    def test_add_single_node_raises_error(self):
+        self.assertRaises(TypeError, self.graph.add_node, "a")
+
+    def test_add_multiple_nodes(self):
+        self.graph.add_nodes_from([("a", "b"), ("b", "c")])
+        self.assertListEqual(
+            hf.recursive_sorted(self.graph.nodes()), [["a", "b"], ["b", "c"]]
+        )
+
+    def test_add_single_edge(self):
+        self.graph.add_edge(("a", "b"), ("b", "c"))
+        self.assertListEqual(
+            hf.recursive_sorted(self.graph.nodes()), [["a", "b"], ["b", "c"]]
+        )
+        self.assertListEqual(
+            sorted([node for edge in self.graph.edges() for node in edge]),
+            [("a", "b"), ("b", "c")],
+        )
+
+    def test_add_single_edge_raises_error(self):
+        self.assertRaises(ValueError, self.graph.add_edge, ("a", "b"), ("c", "d"))
+
+    def test_add_cyclic_path_raises_error(self):
+        self.graph.add_edge(("a", "b"), ("b", "c"))
+        self.graph.add_edge(("b", "c"), ("c", "d"))
+        self.assertRaises(ValueError, self.graph.add_edge, ("c", "d"), ("a", "b"))
+
+    def tearDown(self):
+        del self.graph
+
+
+class TestJunctionTreeMethods(unittest.TestCase):
+    def setUp(self):
+        self.factor1 = DiscreteFactor(["a", "b"], [2, 2], np.random.rand(4))
+        self.factor2 = DiscreteFactor(["b", "c"], [2, 2], np.random.rand(4))
+        self.factor3 = DiscreteFactor(["d", "e"], [2, 2], np.random.rand(4))
+        self.factor4 = DiscreteFactor(["e", "f"], [2, 2], np.random.rand(4))
+        self.factor5 = DiscreteFactor(["a", "b", "e"], [2, 2, 2], np.random.rand(8))
+
+        self.graph1 = JunctionTree()
+        self.graph1.add_edge(("a", "b"), ("b", "c"))
+        self.graph1.add_factors(self.factor1, self.factor2)
+
+        self.graph2 = JunctionTree()
+        self.graph2.add_nodes_from([("a", "b"), ("b", "c"), ("d", "e")])
+        self.graph2.add_edge(("a", "b"), ("b", "c"))
+        self.graph2.add_factors(self.factor1, self.factor2, self.factor3)
+
+        self.graph3 = JunctionTree()
+        self.graph3.add_edges_from([(("a", "b"), ("b", "c")), (("d", "e"), ("e", "f"))])
+        self.graph3.add_factors(self.factor1, self.factor2, self.factor3, self.factor4)
+
+        self.graph4 = JunctionTree()
+        self.graph4.add_edges_from(
+            [
+                (("a", "b", "e"), ("b", "c")),
+                (("a", "b", "e"), ("e", "f")),
+                (("d", "e"), ("e", "f")),
+            ]
+        )
+        self.graph4.add_factors(self.factor5, self.factor2, self.factor3, self.factor4)
+
+    def test_check_model(self):
+        self.assertRaises(ValueError, self.graph2.check_model)
+        self.assertRaises(ValueError, self.graph3.check_model)
+        self.assertTrue(self.graph1.check_model())
+        self.assertTrue(self.graph4.check_model())
+
+    def tearDown(self):
+        del self.factor1
+        del self.factor2
+        del self.factor3
+        del self.factor4
+        del self.factor5
+
+        del self.graph1
+        del self.graph2
+        del self.graph3
+        del self.graph4
+
+
+class TestJunctionTreeCopy(unittest.TestCase):
+    def setUp(self):
+        self.graph = JunctionTree()
+
+    def test_copy_with_nodes(self):
+        self.graph.add_nodes_from([("a", "b", "c"), ("a", "b"), ("a", "c")])
+        self.graph.add_edges_from(
+            [(("a", "b", "c"), ("a", "b")), (("a", "b", "c"), ("a", "c"))]
+        )
+        graph_copy = self.graph.copy()
+
+        self.graph.remove_edge(("a", "b", "c"), ("a", "c"))
+        self.assertFalse(self.graph.has_edge(("a", "b", "c"), ("a", "c")))
+        self.assertTrue(graph_copy.has_edge(("a", "b", "c"), ("a", "c")))
+
+        self.graph.remove_node(("a", "c"))
+        self.assertFalse(self.graph.has_node(("a", "c")))
+        self.assertTrue(graph_copy.has_node(("a", "c")))
+
+        self.graph.add_node(("c", "d"))
+        self.assertTrue(self.graph.has_node(("c", "d")))
+        self.assertFalse(graph_copy.has_node(("c", "d")))
+
+    def test_copy_with_factors(self):
+        self.graph.add_edges_from([[("a", "b"), ("b", "c")]])
+        phi1 = DiscreteFactor(["a", "b"], [2, 2], np.random.rand(4))
+        phi2 = DiscreteFactor(["b", "c"], [2, 2], np.random.rand(4))
+        self.graph.add_factors(phi1, phi2)
+        graph_copy = self.graph.copy()
+
+        self.assertIsInstance(graph_copy, JunctionTree)
+        self.assertIsNot(self.graph, graph_copy)
+        self.assertEqual(
+            hf.recursive_sorted(self.graph.nodes()),
+            hf.recursive_sorted(graph_copy.nodes()),
+        )
+        self.assertEqual(
+            hf.recursive_sorted(self.graph.edges()),
+            hf.recursive_sorted(graph_copy.edges()),
+        )
+        self.assertTrue(graph_copy.check_model())
+        self.assertEqual(self.graph.get_factors(), graph_copy.get_factors())
+
+        self.graph.remove_factors(phi1, phi2)
+        self.assertTrue(
+            phi1 not in self.graph.factors and phi2 not in self.graph.factors
+        )
+        self.assertTrue(phi1 in graph_copy.factors and phi2 in graph_copy.factors)
+
+        self.graph.add_factors(phi1, phi2)
+        self.graph.factors[0] = DiscreteFactor(["a", "b"], [2, 2], np.random.rand(4))
+        self.assertNotEqual(self.graph.get_factors()[0], graph_copy.get_factors()[0])
+        self.assertNotEqual(self.graph.factors, graph_copy.factors)
+
+    def test_copy_with_factorchanges(self):
+        self.graph.add_edges_from([[("a", "b"), ("b", "c")]])
+        phi1 = DiscreteFactor(["a", "b"], [2, 2], np.random.rand(4))
+        phi2 = DiscreteFactor(["b", "c"], [2, 2], np.random.rand(4))
+        self.graph.add_factors(phi1, phi2)
+        graph_copy = self.graph.copy()
+
+        self.graph.factors[0].reduce([("a", 0)])
+        self.assertNotEqual(
+            self.graph.factors[0].scope(), graph_copy.factors[0].scope()
+        )
+        self.assertNotEqual(self.graph, graph_copy)
+        self.graph.factors[1].marginalize(["b"])
+        self.assertNotEqual(
+            self.graph.factors[1].scope(), graph_copy.factors[1].scope()
+        )
+        self.assertNotEqual(self.graph, graph_copy)
+
+    def tearDown(self):
+        del self.graph
```

### Comparing `pgmpy-0.1.7/pgmpy/tests/test_base/test_UndirectedGraph.py` & `pgmpy-0.1.9/pgmpy/tests/test_base/test_UndirectedGraph.py`

 * *Files 20% similar despite different names*

```diff
@@ -9,93 +9,102 @@
     def setUp(self):
         self.graph = UndirectedGraph()
 
     def test_class_init_without_data(self):
         self.assertIsInstance(self.graph, UndirectedGraph)
 
     def test_class_init_with_data_string(self):
-        self.G = UndirectedGraph([('a', 'b'), ('b', 'c')])
-        self.assertListEqual(sorted(self.G.nodes()), ['a', 'b', 'c'])
-        self.assertListEqual(hf.recursive_sorted(self.G.edges()),
-                             [['a', 'b'], ['b', 'c']])
+        self.G = UndirectedGraph([("a", "b"), ("b", "c")])
+        self.assertListEqual(sorted(self.G.nodes()), ["a", "b", "c"])
+        self.assertListEqual(
+            hf.recursive_sorted(self.G.edges()), [["a", "b"], ["b", "c"]]
+        )
 
     def test_add_node_string(self):
-        self.graph.add_node('a')
-        self.assertListEqual(self.graph.nodes(), ['a'])
+        self.graph.add_node("a")
+        self.assertListEqual(list(self.graph.nodes()), ["a"])
 
     def test_add_node_nonstring(self):
         self.graph.add_node(1)
-        self.assertListEqual(self.graph.nodes(), [1])
+        self.assertListEqual(list(self.graph.nodes()), [1])
 
     def test_add_nodes_from_string(self):
-        self.graph.add_nodes_from(['a', 'b', 'c', 'd'])
-        self.assertListEqual(sorted(self.graph.nodes()),
-                             ['a', 'b', 'c', 'd'])
+        self.graph.add_nodes_from(["a", "b", "c", "d"])
+        self.assertListEqual(sorted(self.graph.nodes()), ["a", "b", "c", "d"])
 
     def test_add_node_with_weight(self):
-        self.graph.add_node('a')
-        self.graph.add_node('weight_a', weight=0.3)
-        self.assertEqual(self.graph.node['weight_a']['weight'], 0.3)
-        self.assertEqual(self.graph.node['a']['weight'], None)
+        self.graph.add_node("a")
+        self.graph.add_node("weight_a", weight=0.3)
+        self.assertEqual(self.graph.nodes["weight_a"]["weight"], 0.3)
+        self.assertEqual(self.graph.nodes["a"]["weight"], None)
 
     def test_add_nodes_from_with_weight(self):
         self.graph.add_node(1)
-        self.graph.add_nodes_from(['weight_b', 'weight_c'], weights=[0.3, 0.5])
-        self.assertEqual(self.graph.node['weight_b']['weight'], 0.3)
-        self.assertEqual(self.graph.node['weight_c']['weight'], 0.5)
-        self.assertEqual(self.graph.node[1]['weight'], None)
+        self.graph.add_nodes_from(["weight_b", "weight_c"], weights=[0.3, 0.5])
+        self.assertEqual(self.graph.nodes["weight_b"]["weight"], 0.3)
+        self.assertEqual(self.graph.nodes["weight_c"]["weight"], 0.5)
+        self.assertEqual(self.graph.nodes[1]["weight"], None)
 
     def test_add_nodes_from_non_string(self):
         self.graph.add_nodes_from([1, 2, 3, 4])
 
     def test_add_edge_string(self):
-        self.graph.add_edge('d', 'e')
-        self.assertListEqual(sorted(self.graph.nodes()), ['d', 'e'])
-        self.assertListEqual(hf.recursive_sorted(self.graph.edges()),
-                             [['d', 'e']])
-        self.graph.add_nodes_from(['a', 'b', 'c'])
-        self.graph.add_edge('a', 'b')
-        self.assertListEqual(hf.recursive_sorted(self.graph.edges()),
-                             [['a', 'b'], ['d', 'e']])
+        self.graph.add_edge("d", "e")
+        self.assertListEqual(sorted(self.graph.nodes()), ["d", "e"])
+        self.assertListEqual(hf.recursive_sorted(self.graph.edges()), [["d", "e"]])
+        self.graph.add_nodes_from(["a", "b", "c"])
+        self.graph.add_edge("a", "b")
+        self.assertListEqual(
+            hf.recursive_sorted(self.graph.edges()), [["a", "b"], ["d", "e"]]
+        )
 
     def test_add_edge_nonstring(self):
         self.graph.add_edge(1, 2)
 
     def test_add_edges_from_string(self):
-        self.graph.add_edges_from([('a', 'b'), ('b', 'c')])
-        self.assertListEqual(sorted(self.graph.nodes()), ['a', 'b', 'c'])
-        self.assertListEqual(hf.recursive_sorted(self.graph.edges()),
-                             [['a', 'b'], ['b', 'c']])
-        self.graph.add_nodes_from(['d', 'e', 'f'])
-        self.graph.add_edges_from([('d', 'e'), ('e', 'f')])
-        self.assertListEqual(sorted(self.graph.nodes()),
-                             ['a', 'b', 'c', 'd', 'e', 'f'])
-        self.assertListEqual(hf.recursive_sorted(self.graph.edges()),
-                             hf.recursive_sorted([('a', 'b'), ('b', 'c'),
-                                                  ('d', 'e'), ('e', 'f')]))
+        self.graph.add_edges_from([("a", "b"), ("b", "c")])
+        self.assertListEqual(sorted(self.graph.nodes()), ["a", "b", "c"])
+        self.assertListEqual(
+            hf.recursive_sorted(self.graph.edges()), [["a", "b"], ["b", "c"]]
+        )
+        self.graph.add_nodes_from(["d", "e", "f"])
+        self.graph.add_edges_from([("d", "e"), ("e", "f")])
+        self.assertListEqual(sorted(self.graph.nodes()), ["a", "b", "c", "d", "e", "f"])
+        self.assertListEqual(
+            hf.recursive_sorted(self.graph.edges()),
+            hf.recursive_sorted([("a", "b"), ("b", "c"), ("d", "e"), ("e", "f")]),
+        )
 
     def test_add_edges_from_nonstring(self):
         self.graph.add_edges_from([(1, 2), (2, 3)])
 
     def test_number_of_neighbors(self):
-        self.graph.add_edges_from([('a', 'b'), ('b', 'c')])
-        self.assertEqual(len(self.graph.neighbors('b')), 2)
+        self.graph.add_edges_from([("a", "b"), ("b", "c")])
+        self.assertEqual(len(list(self.graph.neighbors("b"))), 2)
 
     def tearDown(self):
         del self.graph
 
 
 class TestUndirectedGraphMethods(unittest.TestCase):
     def test_is_clique(self):
-        G = UndirectedGraph([('A', 'B'), ('C', 'B'), ('B', 'D'),
-                             ('B', 'E'), ('D', 'E'), ('E', 'F'),
-                             ('D', 'F'), ('B', 'F')])
-        self.assertFalse(G.is_clique(nodes=['A', 'B', 'C', 'D']))
-        self.assertTrue(G.is_clique(nodes=['B', 'D', 'E', 'F']))
-        self.assertTrue(G.is_clique(nodes=['D', 'E', 'B']))
+        G = UndirectedGraph(
+            [
+                ("A", "B"),
+                ("C", "B"),
+                ("B", "D"),
+                ("B", "E"),
+                ("D", "E"),
+                ("E", "F"),
+                ("D", "F"),
+                ("B", "F"),
+            ]
+        )
+        self.assertFalse(G.is_clique(nodes=["A", "B", "C", "D"]))
+        self.assertTrue(G.is_clique(nodes=["B", "D", "E", "F"]))
+        self.assertTrue(G.is_clique(nodes=["D", "E", "B"]))
 
     def test_is_triangulated(self):
-        G = UndirectedGraph([('A', 'B'), ('A', 'C'),
-                             ('B', 'D'), ('C', 'D')])
+        G = UndirectedGraph([("A", "B"), ("A", "C"), ("B", "D"), ("C", "D")])
         self.assertFalse(G.is_triangulated())
-        G.add_edge('A', 'D')
+        G.add_edge("A", "D")
         self.assertTrue(G.is_triangulated())
```

### Comparing `pgmpy-0.1.7/pgmpy/tests/test_independencies/test_Independencies.py` & `pgmpy-0.1.9/pgmpy/tests/test_independencies/test_Independencies.py`

 * *Files 24% similar despite different names*

```diff
@@ -4,60 +4,64 @@
 
 
 class TestIndependenceAssertion(unittest.TestCase):
     def setUp(self):
         self.assertion = IndependenceAssertion()
 
     def test_return_list_if_str(self):
-        self.assertListEqual(self.assertion._return_list_if_str('U'), ['U'])
-        self.assertListEqual(self.assertion._return_list_if_str(['U', 'V']), ['U', 'V'])
+        self.assertListEqual(self.assertion._return_list_if_str("U"), ["U"])
+        self.assertListEqual(self.assertion._return_list_if_str(["U", "V"]), ["U", "V"])
 
     def test_get_assertion(self):
-        self.assertTupleEqual(IndependenceAssertion('U', 'V', 'Z').get_assertion(), ({'U'}, {'V'}, {'Z'}))
-        self.assertTupleEqual(IndependenceAssertion('U', 'V').get_assertion(), ({'U'}, {'V'}, set()))
+        self.assertTupleEqual(
+            IndependenceAssertion("U", "V", "Z").get_assertion(), ({"U"}, {"V"}, {"Z"})
+        )
+        self.assertTupleEqual(
+            IndependenceAssertion("U", "V").get_assertion(), ({"U"}, {"V"}, set())
+        )
 
     def test_init(self):
-        self.assertion1 = IndependenceAssertion('U', 'V', 'Z')
-        self.assertSetEqual(self.assertion1.event1, {'U'})
-        self.assertSetEqual(self.assertion1.event2, {'V'})
-        self.assertSetEqual(self.assertion1.event3, {'Z'})
-        self.assertion1 = IndependenceAssertion(['U', 'V'], ['Y', 'Z'], ['A', 'B'])
-        self.assertSetEqual(self.assertion1.event1, {'U', 'V'})
-        self.assertSetEqual(self.assertion1.event2, {'Y', 'Z'})
-        self.assertSetEqual(self.assertion1.event3, {'A', 'B'})
+        self.assertion1 = IndependenceAssertion("U", "V", "Z")
+        self.assertSetEqual(self.assertion1.event1, {"U"})
+        self.assertSetEqual(self.assertion1.event2, {"V"})
+        self.assertSetEqual(self.assertion1.event3, {"Z"})
+        self.assertion1 = IndependenceAssertion(["U", "V"], ["Y", "Z"], ["A", "B"])
+        self.assertSetEqual(self.assertion1.event1, {"U", "V"})
+        self.assertSetEqual(self.assertion1.event2, {"Y", "Z"})
+        self.assertSetEqual(self.assertion1.event3, {"A", "B"})
 
     def test_init_exceptions(self):
-        self.assertRaises(ValueError, IndependenceAssertion, event2=['U'], event3='V')
-        self.assertRaises(ValueError, IndependenceAssertion, event2=['U'])
-        self.assertRaises(ValueError, IndependenceAssertion, event3=['Z'])
-        self.assertRaises(ValueError, IndependenceAssertion, event1=['U'])
-        self.assertRaises(ValueError, IndependenceAssertion, event1=['U'], event3=['Z'])
+        self.assertRaises(ValueError, IndependenceAssertion, event2=["U"], event3="V")
+        self.assertRaises(ValueError, IndependenceAssertion, event2=["U"])
+        self.assertRaises(ValueError, IndependenceAssertion, event3=["Z"])
+        self.assertRaises(ValueError, IndependenceAssertion, event1=["U"])
+        self.assertRaises(ValueError, IndependenceAssertion, event1=["U"], event3=["Z"])
 
     def tearDown(self):
         del self.assertion
 
 
 class TestIndependeciesAssertionEq(unittest.TestCase):
     def setUp(self):
-        self.i1 = IndependenceAssertion('a', 'b', 'c')
-        self.i2 = IndependenceAssertion('a', 'b')
-        self.i3 = IndependenceAssertion('a', ['b', 'c', 'd'])
-        self.i4 = IndependenceAssertion('a', ['b', 'c', 'd'], 'e')
-        self.i5 = IndependenceAssertion('a', ['d', 'c', 'b'], 'e')
-        self.i6 = IndependenceAssertion('a', ['d', 'c'], ['e', 'b'])
-        self.i7 = IndependenceAssertion('a', ['c', 'd'], ['b', 'e'])
-        self.i8 = IndependenceAssertion('a', ['f', 'd'], ['b', 'e'])
-        self.i9 = IndependenceAssertion('a', ['d', 'k', 'b'], 'e')
-        self.i10 = IndependenceAssertion(['k', 'b', 'd'], 'a', 'e')
+        self.i1 = IndependenceAssertion("a", "b", "c")
+        self.i2 = IndependenceAssertion("a", "b")
+        self.i3 = IndependenceAssertion("a", ["b", "c", "d"])
+        self.i4 = IndependenceAssertion("a", ["b", "c", "d"], "e")
+        self.i5 = IndependenceAssertion("a", ["d", "c", "b"], "e")
+        self.i6 = IndependenceAssertion("a", ["d", "c"], ["e", "b"])
+        self.i7 = IndependenceAssertion("a", ["c", "d"], ["b", "e"])
+        self.i8 = IndependenceAssertion("a", ["f", "d"], ["b", "e"])
+        self.i9 = IndependenceAssertion("a", ["d", "k", "b"], "e")
+        self.i10 = IndependenceAssertion(["k", "b", "d"], "a", "e")
 
     def test_eq1(self):
-        self.assertFalse(self.i1 == 'a')
+        self.assertFalse(self.i1 == "a")
         self.assertFalse(self.i2 == 1)
-        self.assertFalse(self.i4 == [2, 'a'])
-        self.assertFalse(self.i6 == 'c')
+        self.assertFalse(self.i4 == [2, "a"])
+        self.assertFalse(self.i6 == "c")
 
     def test_eq2(self):
         self.assertFalse(self.i1 == self.i2)
         self.assertFalse(self.i1 == self.i3)
         self.assertFalse(self.i2 == self.i4)
         self.assertFalse(self.i3 == self.i6)
 
@@ -82,86 +86,122 @@
         del self.i9
         del self.i10
 
 
 class TestIndependencies(unittest.TestCase):
     def setUp(self):
         self.Independencies = Independencies()
-        self.Independencies3 = Independencies(['a', ['b', 'c', 'd'], ['e', 'f', 'g']],
-                                              ['c', ['d', 'e', 'f'], ['g', 'h']])
-        self.Independencies4 = Independencies([['f', 'd', 'e'], 'c', ['h', 'g']],
-                                              [['b', 'c', 'd'], 'a', ['f', 'g', 'e']])
-        self.Independencies5 = Independencies(['a', ['b', 'c', 'd'], ['e', 'f', 'g']],
-                                              ['c', ['d', 'e', 'f'], 'g'])
+        self.Independencies3 = Independencies(
+            ["a", ["b", "c", "d"], ["e", "f", "g"]], ["c", ["d", "e", "f"], ["g", "h"]]
+        )
+        self.Independencies4 = Independencies(
+            [["f", "d", "e"], "c", ["h", "g"]], [["b", "c", "d"], "a", ["f", "g", "e"]]
+        )
+        self.Independencies5 = Independencies(
+            ["a", ["b", "c", "d"], ["e", "f", "g"]], ["c", ["d", "e", "f"], "g"]
+        )
 
     def test_init(self):
-        self.Independencies1 = Independencies(['X', 'Y', 'Z'])
-        self.assertEqual(self.Independencies1, Independencies(['X', 'Y', 'Z']))
+        self.Independencies1 = Independencies(["X", "Y", "Z"])
+        self.assertEqual(self.Independencies1, Independencies(["X", "Y", "Z"]))
         self.Independencies2 = Independencies()
         self.assertEqual(self.Independencies2, Independencies())
 
     def test_add_assertions(self):
-        self.Independencies1 = Independencies(['X', 'Y', 'Z'])
-        self.assertEqual(self.Independencies1, Independencies(['X', 'Y', 'Z']))
-        self.Independencies2 = Independencies(['A', 'B', 'C'], ['D', 'E', 'F'])
-        self.assertEqual(self.Independencies2, Independencies(['A', 'B', 'C'], ['D', 'E', 'F']))
+        self.Independencies1 = Independencies(["X", "Y", "Z"])
+        self.assertEqual(self.Independencies1, Independencies(["X", "Y", "Z"]))
+        self.Independencies2 = Independencies(["A", "B", "C"], ["D", "E", "F"])
+        self.assertEqual(
+            self.Independencies2, Independencies(["A", "B", "C"], ["D", "E", "F"])
+        )
 
     def test_get_assertions(self):
-        self.Independencies1 = Independencies(['X', 'Y', 'Z'])
-        self.assertEqual(self.Independencies1.independencies, self.Independencies1.get_assertions())
-        self.Independencies2 = Independencies(['A', 'B', 'C'], ['D', 'E', 'F'])
-        self.assertEqual(self.Independencies2.independencies, self.Independencies2.get_assertions())
+        self.Independencies1 = Independencies(["X", "Y", "Z"])
+        self.assertEqual(
+            self.Independencies1.independencies, self.Independencies1.get_assertions()
+        )
+        self.Independencies2 = Independencies(["A", "B", "C"], ["D", "E", "F"])
+        self.assertEqual(
+            self.Independencies2.independencies, self.Independencies2.get_assertions()
+        )
 
     def test_closure(self):
-        ind1 = Independencies(('A', ['B', 'C'], 'D'))
-        self.assertEqual(ind1.closure(), Independencies(('A', ['B', 'C'], 'D'),
-                                                        ('A', 'B', ['C', 'D']),
-                                                        ('A', 'C', ['B', 'D']),
-                                                        ('A', 'B', 'D'),
-                                                        ('A', 'C', 'D')))
-        ind2 = Independencies(('W', ['X', 'Y', 'Z']))
-        self.assertEqual(ind2.closure(),
-                         Independencies(
-                             ('W', 'Y'), ('W', 'Y', 'X'), ('W', 'Y', 'Z'), ('W', 'Y', ['X', 'Z']),
-                             ('W', ['Y', 'X']), ('W', 'X', ['Y', 'Z']), ('W', ['X', 'Z'], 'Y'),
-                             ('W', 'X'), ('W', ['X', 'Z']), ('W', ['Y', 'Z'], 'X'),
-                             ('W', ['Y', 'X', 'Z']), ('W', 'X', 'Z'), ('W', ['Y', 'Z']),
-                             ('W', 'Z', 'X'), ('W', 'Z'), ('W', ['Y', 'X'], 'Z'), ('W', 'X', 'Y'),
-                             ('W', 'Z', ['Y', 'X']), ('W', 'Z', 'Y')))
-        ind3 = Independencies(('c', 'a', ['b', 'e', 'd']), (['e', 'c'], 'b', ['a', 'd']), (['b', 'd'], 'e', 'a'),
-                              ('e', ['b', 'd'], 'c'), ('e', ['b', 'c'], 'd'), (['e', 'c'], 'a', 'b'))
+        ind1 = Independencies(("A", ["B", "C"], "D"))
+        self.assertEqual(
+            ind1.closure(),
+            Independencies(
+                ("A", ["B", "C"], "D"),
+                ("A", "B", ["C", "D"]),
+                ("A", "C", ["B", "D"]),
+                ("A", "B", "D"),
+                ("A", "C", "D"),
+            ),
+        )
+        ind2 = Independencies(("W", ["X", "Y", "Z"]))
+        self.assertEqual(
+            ind2.closure(),
+            Independencies(
+                ("W", "Y"),
+                ("W", "Y", "X"),
+                ("W", "Y", "Z"),
+                ("W", "Y", ["X", "Z"]),
+                ("W", ["Y", "X"]),
+                ("W", "X", ["Y", "Z"]),
+                ("W", ["X", "Z"], "Y"),
+                ("W", "X"),
+                ("W", ["X", "Z"]),
+                ("W", ["Y", "Z"], "X"),
+                ("W", ["Y", "X", "Z"]),
+                ("W", "X", "Z"),
+                ("W", ["Y", "Z"]),
+                ("W", "Z", "X"),
+                ("W", "Z"),
+                ("W", ["Y", "X"], "Z"),
+                ("W", "X", "Y"),
+                ("W", "Z", ["Y", "X"]),
+                ("W", "Z", "Y"),
+            ),
+        )
+        ind3 = Independencies(
+            ("c", "a", ["b", "e", "d"]),
+            (["e", "c"], "b", ["a", "d"]),
+            (["b", "d"], "e", "a"),
+            ("e", ["b", "d"], "c"),
+            ("e", ["b", "c"], "d"),
+            (["e", "c"], "a", "b"),
+        )
         self.assertEqual(len(ind3.closure().get_assertions()), 78)
 
     def test_entails(self):
-        ind1 = Independencies([['A', 'B'], ['C', 'D'], 'E'])
-        ind2 = Independencies(['A', 'C', 'E'])
+        ind1 = Independencies([["A", "B"], ["C", "D"], "E"])
+        ind2 = Independencies(["A", "C", "E"])
         self.assertTrue(ind1.entails(ind2))
         self.assertFalse(ind2.entails(ind1))
-        ind3 = Independencies(('W', ['X', 'Y', 'Z']))
+        ind3 = Independencies(("W", ["X", "Y", "Z"]))
         self.assertTrue(ind3.entails(ind3.closure()))
         self.assertTrue(ind3.closure().entails(ind3))
 
     def test_is_equivalent(self):
-        ind1 = Independencies(['X', ['Y', 'W'], 'Z'])
-        ind2 = Independencies(['X', 'Y', 'Z'], ['X', 'W', 'Z'])
-        ind3 = Independencies(['X', 'Y', 'Z'], ['X', 'W', 'Z'], ['X', 'Y', ['W', 'Z']])
+        ind1 = Independencies(["X", ["Y", "W"], "Z"])
+        ind2 = Independencies(["X", "Y", "Z"], ["X", "W", "Z"])
+        ind3 = Independencies(["X", "Y", "Z"], ["X", "W", "Z"], ["X", "Y", ["W", "Z"]])
         self.assertFalse(ind1.is_equivalent(ind2))
         self.assertTrue(ind1.is_equivalent(ind3))
 
     def test_eq(self):
         self.assertTrue(self.Independencies3 == self.Independencies4)
         self.assertFalse(self.Independencies3 != self.Independencies4)
         self.assertTrue(self.Independencies3 != self.Independencies5)
         self.assertFalse(self.Independencies4 == self.Independencies5)
-        self.assertFalse(Independencies() == Independencies(['A', 'B', 'C']))
-        self.assertFalse(Independencies(['A', 'B', 'C']) == Independencies())
+        self.assertFalse(Independencies() == Independencies(["A", "B", "C"]))
+        self.assertFalse(Independencies(["A", "B", "C"]) == Independencies())
         self.assertTrue(Independencies() == Independencies())
 
     def tearDown(self):
         del self.Independencies
         del self.Independencies3
         del self.Independencies4
         del self.Independencies5
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     unittest.main()
```

### Comparing `pgmpy-0.1.7/pgmpy/tests/test_inference/test_Mplp.py` & `pgmpy-0.1.9/pgmpy/tests/test_inference/test_Mplp.py`

 * *Files 19% similar despite different names*

```diff
@@ -5,15 +5,17 @@
 from pgmpy.inference.mplp import Mplp
 from pgmpy.readwrite import UAIReader
 
 
 class TestMplp(unittest.TestCase):
     def setUp(self):
 
-        reader_file = UAIReader('pgmpy/tests/test_readwrite/testdata/grid4x4_with_triplets.uai')
+        reader_file = UAIReader(
+            "pgmpy/tests/test_readwrite/testdata/grid4x4_with_triplets.uai"
+        )
         self.markov_model = reader_file.get_model()
 
         for factor in self.markov_model.factors:
             factor.values = np.log(factor.values)
         self.mplp = Mplp(self.markov_model)
 
 
@@ -21,67 +23,93 @@
 
     # Query when tighten triplet is OFF
     def test_query_tighten_triplet_off(self):
         query_result = self.mplp.map_query(tighten_triplet=False)
 
         # Results from the Sontag code for a mplp run without tightening is:
         expected_result = {
-            'P': -0.06353, 'N': 0.71691, 'O': 0.43431, 'L': -0.69140,
-            'M': -0.89940, 'J': 0.49731, 'K': 0.91856, 'H': 0.96819,
-            'I': 0.68913, 'F': 0.41164, 'G': 0.73371, 'D': -0.57461,
-            'E': 0.75254, 'B': 0.06297, 'C': -0.11423, 'A': 0.60557}
-
-        self.assertAlmostEqual(expected_result['A'], query_result['var_0'], places=4)
-        self.assertAlmostEqual(expected_result['B'], query_result['var_1'], places=4)
-        self.assertAlmostEqual(expected_result['C'], query_result['var_2'], places=4)
-        self.assertAlmostEqual(expected_result['D'], query_result['var_3'], places=4)
-        self.assertAlmostEqual(expected_result['E'], query_result['var_4'], places=4)
-        self.assertAlmostEqual(expected_result['F'], query_result['var_5'], places=4)
-        self.assertAlmostEqual(expected_result['G'], query_result['var_6'], places=4)
-        self.assertAlmostEqual(expected_result['H'], query_result['var_7'], places=4)
-        self.assertAlmostEqual(expected_result['I'], query_result['var_8'], places=4)
-        self.assertAlmostEqual(expected_result['J'], query_result['var_9'], places=4)
-        self.assertAlmostEqual(expected_result['K'], query_result['var_10'], places=4)
-        self.assertAlmostEqual(expected_result['L'], query_result['var_11'], places=4)
-        self.assertAlmostEqual(expected_result['M'], query_result['var_12'], places=4)
-        self.assertAlmostEqual(expected_result['N'], query_result['var_13'], places=4)
-        self.assertAlmostEqual(expected_result['O'], query_result['var_14'], places=4)
-        self.assertAlmostEqual(expected_result['P'], query_result['var_15'], places=4)
+            "P": -0.06353,
+            "N": 0.71691,
+            "O": 0.43431,
+            "L": -0.69140,
+            "M": -0.89940,
+            "J": 0.49731,
+            "K": 0.91856,
+            "H": 0.96819,
+            "I": 0.68913,
+            "F": 0.41164,
+            "G": 0.73371,
+            "D": -0.57461,
+            "E": 0.75254,
+            "B": 0.06297,
+            "C": -0.11423,
+            "A": 0.60557,
+        }
+
+        self.assertAlmostEqual(expected_result["A"], query_result["var_0"], places=4)
+        self.assertAlmostEqual(expected_result["B"], query_result["var_1"], places=4)
+        self.assertAlmostEqual(expected_result["C"], query_result["var_2"], places=4)
+        self.assertAlmostEqual(expected_result["D"], query_result["var_3"], places=4)
+        self.assertAlmostEqual(expected_result["E"], query_result["var_4"], places=4)
+        self.assertAlmostEqual(expected_result["F"], query_result["var_5"], places=4)
+        self.assertAlmostEqual(expected_result["G"], query_result["var_6"], places=4)
+        self.assertAlmostEqual(expected_result["H"], query_result["var_7"], places=4)
+        self.assertAlmostEqual(expected_result["I"], query_result["var_8"], places=4)
+        self.assertAlmostEqual(expected_result["J"], query_result["var_9"], places=4)
+        self.assertAlmostEqual(expected_result["K"], query_result["var_10"], places=4)
+        self.assertAlmostEqual(expected_result["L"], query_result["var_11"], places=4)
+        self.assertAlmostEqual(expected_result["M"], query_result["var_12"], places=4)
+        self.assertAlmostEqual(expected_result["N"], query_result["var_13"], places=4)
+        self.assertAlmostEqual(expected_result["O"], query_result["var_14"], places=4)
+        self.assertAlmostEqual(expected_result["P"], query_result["var_15"], places=4)
 
         # The final Integrality gap after solving for the present case
         int_gap = self.mplp.get_integrality_gap()
         self.assertAlmostEqual(64.59, int_gap, places=1)
 
 
 class TightenTripletOn(TestMplp):
 
     # Query when tighten triplet is ON
     def test_query_tighten_triplet_on(self):
         query_result = self.mplp.map_query(tighten_triplet=True)
         # Results from the Sontag code for a mplp run with tightening is:
         expected_result = {
-            'P': 0.06353, 'C': 0.11422, 'B': -0.06300, 'A': 0.60557,
-            'G': -0.73374, 'F': 0.41164, 'E': 0.75254, 'D': -0.57461,
-            'K': -0.91856, 'J': 0.49731, 'I': 0.68913, 'H': 0.96819,
-            'O': 0.43431, 'N': 0.71691, 'M': -0.89940, 'L': 0.69139}
-
-        self.assertAlmostEqual(expected_result['A'], query_result['var_0'], places=4)
-        self.assertAlmostEqual(expected_result['B'], query_result['var_1'], places=4)
-        self.assertAlmostEqual(expected_result['C'], query_result['var_2'], places=4)
-        self.assertAlmostEqual(expected_result['D'], query_result['var_3'], places=4)
-        self.assertAlmostEqual(expected_result['E'], query_result['var_4'], places=4)
-        self.assertAlmostEqual(expected_result['F'], query_result['var_5'], places=4)
-        self.assertAlmostEqual(expected_result['G'], query_result['var_6'], places=4)
-        self.assertAlmostEqual(expected_result['H'], query_result['var_7'], places=4)
-        self.assertAlmostEqual(expected_result['I'], query_result['var_8'], places=4)
-        self.assertAlmostEqual(expected_result['J'], query_result['var_9'], places=4)
-        self.assertAlmostEqual(expected_result['K'], query_result['var_10'], places=4)
-        self.assertAlmostEqual(expected_result['L'], query_result['var_11'], places=4)
-        self.assertAlmostEqual(expected_result['M'], query_result['var_12'], places=4)
-        self.assertAlmostEqual(expected_result['N'], query_result['var_13'], places=4)
-        self.assertAlmostEqual(expected_result['O'], query_result['var_14'], places=4)
-        self.assertAlmostEqual(expected_result['P'], query_result['var_15'], places=4)
+            "P": 0.06353,
+            "C": 0.11422,
+            "B": -0.06300,
+            "A": 0.60557,
+            "G": -0.73374,
+            "F": 0.41164,
+            "E": 0.75254,
+            "D": -0.57461,
+            "K": -0.91856,
+            "J": 0.49731,
+            "I": 0.68913,
+            "H": 0.96819,
+            "O": 0.43431,
+            "N": 0.71691,
+            "M": -0.89940,
+            "L": 0.69139,
+        }
+
+        self.assertAlmostEqual(expected_result["A"], query_result["var_0"], places=4)
+        self.assertAlmostEqual(expected_result["B"], query_result["var_1"], places=4)
+        self.assertAlmostEqual(expected_result["C"], query_result["var_2"], places=4)
+        self.assertAlmostEqual(expected_result["D"], query_result["var_3"], places=4)
+        self.assertAlmostEqual(expected_result["E"], query_result["var_4"], places=4)
+        self.assertAlmostEqual(expected_result["F"], query_result["var_5"], places=4)
+        self.assertAlmostEqual(expected_result["G"], query_result["var_6"], places=4)
+        self.assertAlmostEqual(expected_result["H"], query_result["var_7"], places=4)
+        self.assertAlmostEqual(expected_result["I"], query_result["var_8"], places=4)
+        self.assertAlmostEqual(expected_result["J"], query_result["var_9"], places=4)
+        self.assertAlmostEqual(expected_result["K"], query_result["var_10"], places=4)
+        self.assertAlmostEqual(expected_result["L"], query_result["var_11"], places=4)
+        self.assertAlmostEqual(expected_result["M"], query_result["var_12"], places=4)
+        self.assertAlmostEqual(expected_result["N"], query_result["var_13"], places=4)
+        self.assertAlmostEqual(expected_result["O"], query_result["var_14"], places=4)
+        self.assertAlmostEqual(expected_result["P"], query_result["var_15"], places=4)
 
         # The final Integrality gap after solving for the present case
         int_gap = self.mplp.get_integrality_gap()
         # Since the ties are broken arbitrary, we have 2 possible solutions howsoever trivial in difference
         self.assertIn(round(int_gap, 2), (7.98, 8.07))
```

### Comparing `pgmpy-0.1.7/pgmpy/tests/test_factors/test_continuous/test_Canonical_Factor.py` & `pgmpy-0.1.9/pgmpy/tests/test_factors/test_continuous/test_Canonical_Factor.py`

 * *Files 14% similar despite different names*

```diff
@@ -5,65 +5,105 @@
 
 from pgmpy.factors.distributions import GaussianDistribution as JGD
 from pgmpy.factors.continuous import CanonicalDistribution
 
 
 class TestCanonicalFactor(unittest.TestCase):
     def test_class_init(self):
-        phi = CanonicalDistribution(['x1', ('y', 'z'), 'x3'],
-                                    np.array([[1.1, -1, 0], [-1, 4, -2], [0, -2, 4]]),
-                                    np.array([[1], [4.7], [-1]]), -2)
-        self.assertEqual(phi.variables, ['x1', ('y', 'z'), 'x3'])
-        np_test.assert_array_equal(phi.K, np.array([[1.1, -1, 0], [-1, 4, -2], [0, -2, 4]], dtype=float))
+        phi = CanonicalDistribution(
+            ["x1", ("y", "z"), "x3"],
+            np.array([[1.1, -1, 0], [-1, 4, -2], [0, -2, 4]]),
+            np.array([[1], [4.7], [-1]]),
+            -2,
+        )
+        self.assertEqual(phi.variables, ["x1", ("y", "z"), "x3"])
+        np_test.assert_array_equal(
+            phi.K, np.array([[1.1, -1, 0], [-1, 4, -2], [0, -2, 4]], dtype=float)
+        )
         np_test.assert_array_equal(phi.h, np.array([[1], [4.7], [-1]], dtype=float))
         self.assertEqual(phi.g, -2)
 
-        phi = CanonicalDistribution(['x1', ('y', 'z'), 'x3'],
-                                    np.array([[1.1, -1, 0], [-1, 4, -2], [0, -2, 4]]),
-                                    np.array([1, 4.7, -1]), -2)
-        self.assertEqual(phi.variables, ['x1', ('y', 'z'), 'x3'])
-        np_test.assert_array_equal(phi.K, np.array([[1.1, -1, 0], [-1, 4, -2], [0, -2, 4]], dtype=float))
+        phi = CanonicalDistribution(
+            ["x1", ("y", "z"), "x3"],
+            np.array([[1.1, -1, 0], [-1, 4, -2], [0, -2, 4]]),
+            np.array([1, 4.7, -1]),
+            -2,
+        )
+        self.assertEqual(phi.variables, ["x1", ("y", "z"), "x3"])
+        np_test.assert_array_equal(
+            phi.K, np.array([[1.1, -1, 0], [-1, 4, -2], [0, -2, 4]], dtype=float)
+        )
         np_test.assert_array_equal(phi.h, np.array([[1], [4.7], [-1]], dtype=float))
         self.assertEqual(phi.g, -2)
 
-        phi = CanonicalDistribution(['x'], [[1]], [0], 1)
-        self.assertEqual(phi.variables, ['x'])
+        phi = CanonicalDistribution(["x"], [[1]], [0], 1)
+        self.assertEqual(phi.variables, ["x"])
         np_test.assert_array_equal(phi.K, np.array([[1]], dtype=float))
         np_test.assert_array_equal(phi.h, np.array([[0]], dtype=float))
         self.assertEqual(phi.g, 1)
 
     def test_class_init_valueerror(self):
-        self.assertRaises(ValueError, CanonicalDistribution, ['x1', 'x2', 'x3'],
-                          np.array([[1.1, -1, 0], [-1, 4, -2], [0, -2, 4]]),
-                          np.array([1, 2]), 7)
-        self.assertRaises(ValueError, CanonicalDistribution, ['x1', 'x2', 'x3'],
-                          np.array([[1.1, -1, 0], [-1, 4], [0, -2, 4]]),
-                          np.array([1, 2, 3]), 7)
-        self.assertRaises(ValueError, CanonicalDistribution, ['x1', 'x2', 'x3'],
-                          np.array([[1.1, -1, 0], [0, -2, 4]]),
-                          np.array([1, 2, 3]), 7)
-        self.assertRaises(ValueError, CanonicalDistribution, ['x1', 'x3'],
-                          np.array([[1.1, -1, 0], [-1, 4, -2], [0, -2, 4]]),
-                          np.array([1, 2, 3]), 7)
+        self.assertRaises(
+            ValueError,
+            CanonicalDistribution,
+            ["x1", "x2", "x3"],
+            np.array([[1.1, -1, 0], [-1, 4, -2], [0, -2, 4]]),
+            np.array([1, 2]),
+            7,
+        )
+        self.assertRaises(
+            ValueError,
+            CanonicalDistribution,
+            ["x1", "x2", "x3"],
+            np.array([[1.1, -1, 0], [-1, 4], [0, -2, 4]]),
+            np.array([1, 2, 3]),
+            7,
+        )
+        self.assertRaises(
+            ValueError,
+            CanonicalDistribution,
+            ["x1", "x2", "x3"],
+            np.array([[1.1, -1, 0], [0, -2, 4]]),
+            np.array([1, 2, 3]),
+            7,
+        )
+        self.assertRaises(
+            ValueError,
+            CanonicalDistribution,
+            ["x1", "x3"],
+            np.array([[1.1, -1, 0], [-1, 4, -2], [0, -2, 4]]),
+            np.array([1, 2, 3]),
+            7,
+        )
 
 
 class TestJGDMethods(unittest.TestCase):
     def setUp(self):
-        self.phi1 = CanonicalDistribution(['x1', 'x2', 'x3'],
-                                          np.array([[1.1, -1, 0], [-1, 4, -2], [0, -2, 4]]),
-                                          np.array([[1], [4.7], [-1]]), -2)
-        self.phi2 = CanonicalDistribution(['x'], [[1]], [0], 1)
+        self.phi1 = CanonicalDistribution(
+            ["x1", "x2", "x3"],
+            np.array([[1.1, -1, 0], [-1, 4, -2], [0, -2, 4]]),
+            np.array([[1], [4.7], [-1]]),
+            -2,
+        )
+        self.phi2 = CanonicalDistribution(["x"], [[1]], [0], 1)
         self.phi3 = self.phi1.copy()
 
-        self.gauss_phi1 = JGD(['x1', 'x2', 'x3'],
-                              np.array([[3.13043478], [2.44347826], [0.97173913]]),
-                              np.array([[1.30434783, 0.43478261, 0.2173913],
-                                        [0.43478261, 0.47826087, 0.23913043],
-                                        [0.2173913, 0.23913043, 0.36956522]], dtype=float))
-        self.gauss_phi2 = JGD(['x'], np.array([0]), np.array([[1]]))
+        self.gauss_phi1 = JGD(
+            ["x1", "x2", "x3"],
+            np.array([[3.13043478], [2.44347826], [0.97173913]]),
+            np.array(
+                [
+                    [1.30434783, 0.43478261, 0.2173913],
+                    [0.43478261, 0.47826087, 0.23913043],
+                    [0.2173913, 0.23913043, 0.36956522],
+                ],
+                dtype=float,
+            ),
+        )
+        self.gauss_phi2 = JGD(["x"], np.array([0]), np.array([[1]]))
 
     def test_assignment(self):
         np_test.assert_almost_equal(self.phi1.assignment(1, 2, 3), 0.0007848640)
         np_test.assert_almost_equal(self.phi2.assignment(1.2), 1.323129812337)
 
     def test_to_joint_gaussian(self):
         jgd1 = self.phi1.to_joint_gaussian()
@@ -74,114 +114,128 @@
         np_test.assert_almost_equal(jgd1.mean, self.gauss_phi1.mean)
 
         self.assertEqual(jgd2.variables, self.gauss_phi2.variables)
         np_test.assert_almost_equal(jgd2.covariance, self.gauss_phi2.covariance)
         np_test.assert_almost_equal(jgd2.mean, self.gauss_phi2.mean)
 
     def test_reduce(self):
-        phi = self.phi1.reduce([('x1', 7)], inplace=False)
-        self.assertEqual(phi.variables, ['x2', 'x3'])
-        np_test.assert_almost_equal(phi.K, np.array([[4.0, -2.0], [-2.0,  4.0]]))
+        phi = self.phi1.reduce([("x1", 7)], inplace=False)
+        self.assertEqual(phi.variables, ["x2", "x3"])
+        np_test.assert_almost_equal(phi.K, np.array([[4.0, -2.0], [-2.0, 4.0]]))
         np_test.assert_almost_equal(phi.h, np.array([[11.7], [-1.0]]))
         np_test.assert_almost_equal(phi.g, -21.95)
 
-        phi = self.phi1.reduce([('x1', 4), ('x2', 1.23)], inplace=False)
-        self.assertEqual(phi.variables, ['x3'])
+        phi = self.phi1.reduce([("x1", 4), ("x2", 1.23)], inplace=False)
+        self.assertEqual(phi.variables, ["x3"])
         np_test.assert_almost_equal(phi.K, np.array([[4.0]]))
         np_test.assert_almost_equal(phi.h, np.array([[1.46]]))
         np_test.assert_almost_equal(phi.g, 0.8752)
 
-        self.phi1.reduce([('x1', 7)])
-        self.assertEqual(self.phi1.variables, ['x2', 'x3'])
-        np_test.assert_almost_equal(self.phi1.K, np.array([[4.0, -2.0], [-2.0,  4.0]]))
+        self.phi1.reduce([("x1", 7)])
+        self.assertEqual(self.phi1.variables, ["x2", "x3"])
+        np_test.assert_almost_equal(self.phi1.K, np.array([[4.0, -2.0], [-2.0, 4.0]]))
         np_test.assert_almost_equal(self.phi1.h, np.array([[11.7], [-1.0]]))
         np_test.assert_almost_equal(self.phi1.g, -21.95)
 
         self.phi1 = self.phi3.copy()
-        self.phi1.reduce([('x1', 4), ('x2', 1.23)])
-        self.assertEqual(self.phi1.variables, ['x3'])
+        self.phi1.reduce([("x1", 4), ("x2", 1.23)])
+        self.assertEqual(self.phi1.variables, ["x3"])
         np_test.assert_almost_equal(self.phi1.K, np.array([[4.0]]))
         np_test.assert_almost_equal(self.phi1.h, np.array([[1.46]]))
         np_test.assert_almost_equal(self.phi1.g, 0.8752)
 
         self.phi1 = self.phi3.copy()
-        self.phi1.reduce([('x2', 1.23), ('x1', 4)])
-        self.assertEqual(self.phi1.variables, ['x3'])
+        self.phi1.reduce([("x2", 1.23), ("x1", 4)])
+        self.assertEqual(self.phi1.variables, ["x3"])
         np_test.assert_almost_equal(self.phi1.K, np.array([[4.0]]))
         np_test.assert_almost_equal(self.phi1.h, np.array([[1.46]]))
         np_test.assert_almost_equal(self.phi1.g, 0.8752)
 
     def test_marginalize(self):
-        phi = self.phi1.marginalize(['x1'], inplace=False)
-        self.assertEqual(phi.variables, ['x2', 'x3'])
-        np_test.assert_almost_equal(phi.K, np.array([[3.090909, -2.0], [-2.0,  4.0]]))
+        phi = self.phi1.marginalize(["x1"], inplace=False)
+        self.assertEqual(phi.variables, ["x2", "x3"])
+        np_test.assert_almost_equal(phi.K, np.array([[3.090909, -2.0], [-2.0, 4.0]]))
         np_test.assert_almost_equal(phi.h, np.array([[5.6090909], [-1.0]]))
         np_test.assert_almost_equal(phi.g, -0.5787165566)
 
-        phi = self.phi1.marginalize(['x1', 'x2'], inplace=False)
-        self.assertEqual(phi.variables, ['x3'])
+        phi = self.phi1.marginalize(["x1", "x2"], inplace=False)
+        self.assertEqual(phi.variables, ["x3"])
         np_test.assert_almost_equal(phi.K, np.array([[2.70588235]]))
         np_test.assert_almost_equal(phi.h, np.array([[2.62941176]]))
         np_test.assert_almost_equal(phi.g, 39.25598935059)
 
-        self.phi1.marginalize(['x1'])
-        self.assertEqual(self.phi1.variables, ['x2', 'x3'])
-        np_test.assert_almost_equal(self.phi1.K, np.array([[3.090909, -2.0], [-2.0,  4.0]]))
+        self.phi1.marginalize(["x1"])
+        self.assertEqual(self.phi1.variables, ["x2", "x3"])
+        np_test.assert_almost_equal(
+            self.phi1.K, np.array([[3.090909, -2.0], [-2.0, 4.0]])
+        )
         np_test.assert_almost_equal(self.phi1.h, np.array([[5.6090909], [-1.0]]))
         np_test.assert_almost_equal(self.phi1.g, -0.5787165566)
 
         self.phi1 = self.phi3
-        self.phi1.marginalize(['x1', 'x2'])
-        self.assertEqual(self.phi1.variables, ['x3'])
+        self.phi1.marginalize(["x1", "x2"])
+        self.assertEqual(self.phi1.variables, ["x3"])
         np_test.assert_almost_equal(self.phi1.K, np.array([[2.70588235]]))
         np_test.assert_almost_equal(self.phi1.h, np.array([[2.62941176]]))
         np_test.assert_almost_equal(self.phi1.g, 39.25598935059)
 
         self.phi1 = self.phi3
 
     def test_operate(self):
-        phi1 = self.phi1 * CanonicalDistribution(['x2', 'x4'], [[1, 2], [3, 4]], [0, 4.56], -6.78)
-        phi2 = self.phi1 / CanonicalDistribution(['x2', 'x3'], [[1, 2], [3, 4]], [0, 4.56], -6.78)
-
-        self.assertEqual(phi1.variables, ['x1', 'x2', 'x3', 'x4'])
-        np_test.assert_almost_equal(phi1.K, np.array([[1.1, -1.0,  0.0,  0.0],
-                                                      [-1.0,  5.0, -2.0,  2.0],
-                                                      [0.0, -2.0,  4.0,  0.0],
-                                                      [0.0,  3.0,  0.0,  4.0]]))
-        np_test.assert_almost_equal(phi1.h, np.array([[1.0],
-                                                      [4.7],
-                                                      [-1.0],
-                                                      [4.56]]))
+        phi1 = self.phi1 * CanonicalDistribution(
+            ["x2", "x4"], [[1, 2], [3, 4]], [0, 4.56], -6.78
+        )
+        phi2 = self.phi1 / CanonicalDistribution(
+            ["x2", "x3"], [[1, 2], [3, 4]], [0, 4.56], -6.78
+        )
+
+        self.assertEqual(phi1.variables, ["x1", "x2", "x3", "x4"])
+        np_test.assert_almost_equal(
+            phi1.K,
+            np.array(
+                [
+                    [1.1, -1.0, 0.0, 0.0],
+                    [-1.0, 5.0, -2.0, 2.0],
+                    [0.0, -2.0, 4.0, 0.0],
+                    [0.0, 3.0, 0.0, 4.0],
+                ]
+            ),
+        )
+        np_test.assert_almost_equal(phi1.h, np.array([[1.0], [4.7], [-1.0], [4.56]]))
         np_test.assert_almost_equal(phi1.g, -8.78)
 
-        self.assertEqual(phi2.variables, ['x1', 'x2', 'x3'])
-        np_test.assert_almost_equal(phi2.K, np.array([[1.1, -1.0,  0.0],
-                                                      [-1.0,  3.0, -4.0],
-                                                      [0.0, -5.0,  0.0]]))
-        np_test.assert_almost_equal(phi2.h, np.array([[1.0],
-                                                      [4.7],
-                                                      [-5.56]]))
+        self.assertEqual(phi2.variables, ["x1", "x2", "x3"])
+        np_test.assert_almost_equal(
+            phi2.K, np.array([[1.1, -1.0, 0.0], [-1.0, 3.0, -4.0], [0.0, -5.0, 0.0]])
+        )
+        np_test.assert_almost_equal(phi2.h, np.array([[1.0], [4.7], [-5.56]]))
         np_test.assert_almost_equal(phi2.g, 4.78)
 
     def test_copy(self):
         copy_phi1 = self.phi1.copy()
         self.assertEqual(copy_phi1.variables, self.phi1.variables)
         np_test.assert_array_equal(copy_phi1.K, self.phi1.K)
         np_test.assert_array_equal(copy_phi1.h, self.phi1.h)
         np_test.assert_array_equal(copy_phi1.g, self.phi1.g)
 
-        copy_phi1.marginalize(['x1'])
-        self.assertEqual(self.phi1.variables, ['x1', 'x2', 'x3'])
-        np_test.assert_array_equal(self.phi1.K, np.array([[1.1, -1, 0], [-1, 4, -2], [0, -2, 4]], dtype=float))
-        np_test.assert_array_equal(self.phi1.h, np.array([[1], [4.7], [-1]], dtype=float))
+        copy_phi1.marginalize(["x1"])
+        self.assertEqual(self.phi1.variables, ["x1", "x2", "x3"])
+        np_test.assert_array_equal(
+            self.phi1.K, np.array([[1.1, -1, 0], [-1, 4, -2], [0, -2, 4]], dtype=float)
+        )
+        np_test.assert_array_equal(
+            self.phi1.h, np.array([[1], [4.7], [-1]], dtype=float)
+        )
         self.assertEqual(self.phi1.g, -2)
 
-        self.phi1.marginalize(['x2'])
-        self.assertEqual(copy_phi1.variables, ['x2', 'x3'])
-        np_test.assert_almost_equal(copy_phi1.K, np.array([[3.090909, -2.0], [-2.0,  4.0]]))
+        self.phi1.marginalize(["x2"])
+        self.assertEqual(copy_phi1.variables, ["x2", "x3"])
+        np_test.assert_almost_equal(
+            copy_phi1.K, np.array([[3.090909, -2.0], [-2.0, 4.0]])
+        )
         np_test.assert_almost_equal(copy_phi1.h, np.array([[5.6090909], [-1.0]]))
         np_test.assert_almost_equal(copy_phi1.g, -0.5787165566)
 
         self.phi1 = self.phi3
 
     def tearDown(self):
         del self.phi1
```

### Comparing `pgmpy-0.1.7/pgmpy/tests/test_factors/test_continuous/test_Linear_Gaussain_CPD.py` & `pgmpy-0.1.9/pgmpy/tests/test_factors/test_continuous/test_Linear_Gaussain_CPD.py`

 * *Files 17% similar despite different names*

```diff
@@ -3,90 +3,92 @@
 import pandas as pd
 import numpy as np
 
 from pgmpy.factors.continuous import LinearGaussianCPD
 
 
 class TestLGCPD(unittest.TestCase):
-   # @unittest.skip("TODO")
+    # @unittest.skip("TODO")
     def test_class_init(self):
         mu = np.array([7, 13])
-        sigma = np.array([[4 , 3],
-                               [3 , 6]])
+        sigma = np.array([[4, 3], [3, 6]])
 
-        cpd1 = LinearGaussianCPD('Y', evidence_mean = mu, evidence_variance=sigma, 
-                                 evidence=['X1', 'X2'])
-        self.assertEqual(cpd1.variable, 'Y')
-        self.assertEqual(cpd1.evidence, ['X1', 'X2'])
+        cpd1 = LinearGaussianCPD(
+            "Y", evidence_mean=mu, evidence_variance=sigma, evidence=["X1", "X2"]
+        )
+        self.assertEqual(cpd1.variable, "Y")
+        self.assertEqual(cpd1.evidence, ["X1", "X2"])
 
     def test_maximum_likelihood_estimator(self):
         # Obtain the X and Y which are jointly gaussian from the distribution
         # beta = [2, 0.7, 0.3]
         sigma_c = 4
-        
-        x_df = pd.read_csv('pgmpy/tests/test_factors/test_continuous/gbn_values_1.csv')
-        
+
+        x_df = pd.read_csv("pgmpy/tests/test_factors/test_continuous/gbn_values_1.csv")
+
         mu = np.array([7, 13])
-        sigma = np.array([[4 , 3],
-                          [3 , 6]])
+        sigma = np.array([[4, 3], [3, 6]])
 
-        cpd1 = LinearGaussianCPD('Y', evidence_mean = mu, evidence_variance=sigma, 
-                                 evidence=['X1', 'X2'])
-        mean, variance = cpd1.fit(x_df, states=['(Y|X)', 'X1', 'X2'], estimator='MLE')
-        np_test.assert_allclose(mean, [ 2.361152,  0.693147,  0.276383], rtol=1e-03)
+        cpd1 = LinearGaussianCPD(
+            "Y", evidence_mean=mu, evidence_variance=sigma, evidence=["X1", "X2"]
+        )
+        mean, variance = cpd1.fit(x_df, states=["(Y|X)", "X1", "X2"], estimator="MLE")
+        np_test.assert_allclose(mean, [2.361152, 0.693147, 0.276383], rtol=1e-03)
         np_test.assert_allclose(variance, sigma_c, rtol=1e-1)
-        
-        
+
     @unittest.skip("TODO")
     def test_pdf(self):
-        cpd1 = LinearGaussianCPD('x', [0.23], 0.56)
-        cpd2 = LinearGaussianCPD('y', [0.67, 1, 4.56, 8], 2, ['x1', 'x2', 'x3'])
+        cpd1 = LinearGaussianCPD("x", [0.23], 0.56)
+        cpd2 = LinearGaussianCPD("y", [0.67, 1, 4.56, 8], 2, ["x1", "x2", "x3"])
         np_test.assert_almost_equal(cpd1.assignment(1), 0.3139868)
         np_test.assert_almost_equal(cpd2.assignment(1, 1.2, 2.3, 3.4), 1.076e-162)
 
     @unittest.skip("TODO")
     def test_copy(self):
-        cpd = LinearGaussianCPD('y', [0.67, 1, 4.56, 8], 2, ['x1', 'x2', 'x3'])
+        cpd = LinearGaussianCPD("y", [0.67, 1, 4.56, 8], 2, ["x1", "x2", "x3"])
         copy = cpd.copy()
 
         self.assertEqual(cpd.variable, copy.variable)
         self.assertEqual(cpd.beta_0, copy.beta_0)
         self.assertEqual(cpd.variance, copy.variance)
         np_test.assert_array_equal(cpd.beta_vector, copy.beta_vector)
         self.assertEqual(cpd.evidence, copy.evidence)
 
-        cpd.variable = 'z'
-        self.assertEqual(copy.variable, 'y')
+        cpd.variable = "z"
+        self.assertEqual(copy.variable, "y")
         cpd.variance = 0
         self.assertEqual(copy.variance, 2)
         cpd.beta_0 = 1
         self.assertEqual(copy.beta_0, 0.67)
-        cpd.evidence = ['p', 'q', 'r']
-        self.assertEqual(copy.evidence, ['x1', 'x2', 'x3'])
+        cpd.evidence = ["p", "q", "r"]
+        self.assertEqual(copy.evidence, ["x1", "x2", "x3"])
         cpd.beta_vector = [2, 2, 2]
         np_test.assert_array_equal(copy.beta_vector, [1, 4.56, 8])
 
         copy = cpd.copy()
 
-        copy.variable = 'k'
-        self.assertEqual(cpd.variable, 'z')
+        copy.variable = "k"
+        self.assertEqual(cpd.variable, "z")
         copy.variance = 0.3
         self.assertEqual(cpd.variance, 0)
         copy.beta_0 = 1.5
         self.assertEqual(cpd.beta_0, 1)
-        copy.evidence = ['a', 'b', 'c']
-        self.assertEqual(cpd.evidence, ['p', 'q', 'r'])
+        copy.evidence = ["a", "b", "c"]
+        self.assertEqual(cpd.evidence, ["p", "q", "r"])
         copy.beta_vector = [2.2, 2.2, 2.2]
         np_test.assert_array_equal(cpd.beta_vector, [2, 2, 2])
 
     @unittest.skip("TODO")
     def test_str(self):
-        cpd1 = LinearGaussianCPD('x', [0.23], 0.56)
-        cpd2 = LinearGaussianCPD('y', [0.67, 1, 4.56, 8], 2, ['x1', 'x2', 'x3'])
+        cpd1 = LinearGaussianCPD("x", [0.23], 0.56)
+        cpd2 = LinearGaussianCPD("y", [0.67, 1, 4.56, 8], 2, ["x1", "x2", "x3"])
         self.assertEqual(cpd1.__str__(), "P(x) = N(0.23; 0.56)")
-        self.assertEqual(cpd2.__str__(), "P(y | x1, x2, x3) = N(1.0*x1 + "
-                                         "4.56*x2 + 8.0*x3 + 0.67; 2)")
-        
+        self.assertEqual(
+            cpd2.__str__(),
+            "P(y | x1, x2, x3) = N(1.0*x1 + " "4.56*x2 + 8.0*x3 + 0.67; 2)",
+        )
+
+
 #    def test_mle_fit(self):
 #        cpd = LinearGaussianCPD('Y',  [0.2, -2, 3, 7], 9.6, ['X1', 'X2', 'X3'])
 #        gbn_values = pd.read_csv('gbn_values.csv')
 #        cpd.fit(gbn_values)
```

### Comparing `pgmpy-0.1.7/pgmpy/tests/test_factors/test_continuous/test_JointGaussianDistribution.py` & `pgmpy-0.1.9/pgmpy/tests/test_factors/test_continuous/test_JointGaussianDistribution.py`

 * *Files 12% similar despite different names*

```diff
@@ -4,204 +4,284 @@
 import numpy.testing as np_test
 
 from pgmpy.factors.distributions import GaussianDistribution as GD
 
 
 class TestGDInit(unittest.TestCase):
     def test_class_init(self):
-        phi1 = GD(['x1', 'x2', 'x3'], np.array([[1], [-3], [4]]),
-                  np.array([[4, 2, -2], [2, 5, -5], [-2, -5, 8]]))
-        self.assertEqual(phi1.variables, ['x1', 'x2', 'x3'])
+        phi1 = GD(
+            ["x1", "x2", "x3"],
+            np.array([[1], [-3], [4]]),
+            np.array([[4, 2, -2], [2, 5, -5], [-2, -5, 8]]),
+        )
+        self.assertEqual(phi1.variables, ["x1", "x2", "x3"])
         np_test.assert_array_equal(phi1.mean, np.asarray([[1], [-3], [4]], dtype=float))
-        np_test.assert_array_equal(phi1.covariance,
-                                   np.asarray([[4, 2, -2], [2, 5, -5], [-2, -5, 8]], dtype=float))
+        np_test.assert_array_equal(
+            phi1.covariance,
+            np.asarray([[4, 2, -2], [2, 5, -5], [-2, -5, 8]], dtype=float),
+        )
         self.assertEqual(phi1._precision_matrix, None)
 
-        phi2 = GD(['x1', 'x2', 'x3'], [1, 2, 5],
-                  np.array([[4, 2, -2], [2, 5, -5], [-2, -5, 8]]))
-        self.assertEqual(phi2.variables, ['x1', 'x2', 'x3'])
+        phi2 = GD(
+            ["x1", "x2", "x3"],
+            [1, 2, 5],
+            np.array([[4, 2, -2], [2, 5, -5], [-2, -5, 8]]),
+        )
+        self.assertEqual(phi2.variables, ["x1", "x2", "x3"])
         np_test.assert_array_equal(phi2.mean, np.asarray([[1], [2], [5]], dtype=float))
-        np_test.assert_array_equal(phi2.covariance,
-                                   np.asarray([[4, 2, -2], [2, 5, -5], [-2, -5, 8]], dtype=float))
+        np_test.assert_array_equal(
+            phi2.covariance,
+            np.asarray([[4, 2, -2], [2, 5, -5], [-2, -5, 8]], dtype=float),
+        )
         self.assertEqual(phi2._precision_matrix, None)
 
-        phi3 = GD(['x'], [0], [[1]])
-        self.assertEqual(phi3.variables, ['x'])
+        phi3 = GD(["x"], [0], [[1]])
+        self.assertEqual(phi3.variables, ["x"])
         np_test.assert_array_equal(phi3.mean, np.asarray([[0]], dtype=float))
         np_test.assert_array_equal(phi3.covariance, np.asarray([[1]], dtype=float))
         self.assertEqual(phi3._precision_matrix, None)
 
-        phi1 = GD(['1', 2, (1, 2, 'x')],
-                  np.array([[1], [-3], [4]]),
-                  np.array([[4, 2, -2], [2, 5, -5], [-2, -5, 8]]))
-        self.assertEqual(phi1.variables, ['1', 2, (1, 2, 'x')])
+        phi1 = GD(
+            ["1", 2, (1, 2, "x")],
+            np.array([[1], [-3], [4]]),
+            np.array([[4, 2, -2], [2, 5, -5], [-2, -5, 8]]),
+        )
+        self.assertEqual(phi1.variables, ["1", 2, (1, 2, "x")])
         np_test.assert_array_equal(phi1.mean, np.asarray([[1], [-3], [4]], dtype=float))
-        np_test.assert_array_equal(phi1.covariance,
-                                   np.asarray([[4, 2, -2], [2, 5, -5], [-2, -5, 8]], dtype=float))
+        np_test.assert_array_equal(
+            phi1.covariance,
+            np.asarray([[4, 2, -2], [2, 5, -5], [-2, -5, 8]], dtype=float),
+        )
         self.assertEqual(phi1._precision_matrix, None)
 
-        phi2 = GD(['1', 7, (1, 2, 'x')], [1, 2, 5],
-                  np.array([[4, 2, -2], [2, 5, -5], [-2, -5, 8]]))
-        self.assertEqual(phi2.variables, ['1', 7, (1, 2, 'x')])
+        phi2 = GD(
+            ["1", 7, (1, 2, "x")],
+            [1, 2, 5],
+            np.array([[4, 2, -2], [2, 5, -5], [-2, -5, 8]]),
+        )
+        self.assertEqual(phi2.variables, ["1", 7, (1, 2, "x")])
         np_test.assert_array_equal(phi2.mean, np.asarray([[1], [2], [5]], dtype=float))
-        np_test.assert_array_equal(phi2.covariance,
-                                   np.asarray([[4, 2, -2], [2, 5, -5], [-2, -5, 8]], dtype=float))
+        np_test.assert_array_equal(
+            phi2.covariance,
+            np.asarray([[4, 2, -2], [2, 5, -5], [-2, -5, 8]], dtype=float),
+        )
         self.assertEqual(phi2._precision_matrix, None)
 
         phi3 = GD([23], [0], [[1]])
         self.assertEqual(phi3.variables, [23])
         np_test.assert_array_equal(phi3.mean, np.asarray([[0]], dtype=float))
         np_test.assert_array_equal(phi3.covariance, np.asarray([[1]], dtype=float))
         self.assertEqual(phi3._precision_matrix, None)
 
     def test_class_init_valueerror(self):
-        self.assertRaises(ValueError, GD, ['x1', 'x2', 'x3'], [1, -3],
-                          np.array([[4, 2, -2], [2, 5, -5], [-2, -5, 8]]))
-        self.assertRaises(ValueError, GD, ['x1', 'x2'], [1, -3, 4],
-                          np.array([[4, 2, -2], [2, 5, -5], [-2, -5, 8]]))
-        self.assertRaises(ValueError, GD, ['x1', 'x2', 'x3'], [[1, -3, 4]],
-                          np.array([[4, 2, -2], [2, 5, -5], [-2, -5, 8]]))
-        self.assertRaises(ValueError, GD, ['x1', 'x2', 'x3'], [1],
-                          np.array([[4, 2, -2], [2, 5, -5], [-2, -5, 8]]))
-        self.assertRaises(ValueError, GD, ['x1', 'x2', 'x3'], [[1], [-3]],
-                          np.array([[4, 2, -2], [2, 5, -5], [-2, -5, 8]]))
-
-        self.assertRaises(ValueError, GD, ['x1', 'x2', 'x3'], [1, -3, 4],
-                          np.array([[4, 2, -2], [2, 5, -5]]))
-        self.assertRaises(ValueError, GD, ['x1', 'x2', 'x3'], [1, -3, 4],
-                          np.array([[4, 2, -2]]))
-        self.assertRaises(ValueError, GD, ['x1', 'x2', 'x3'], [1, -3],
-                          np.array([[4, 2], [2, 5], [-2, -5]]))
-        self.assertRaises(ValueError, GD, ['x1', 'x2', 'x3'], [1, -3],
-                          np.array([[4], [2], [-2]]))
-        self.assertRaises(ValueError, GD, ['x1', 'x2', 'x3'], [1, -3],
-                          np.array([[-2]]))
+        self.assertRaises(
+            ValueError,
+            GD,
+            ["x1", "x2", "x3"],
+            [1, -3],
+            np.array([[4, 2, -2], [2, 5, -5], [-2, -5, 8]]),
+        )
+        self.assertRaises(
+            ValueError,
+            GD,
+            ["x1", "x2"],
+            [1, -3, 4],
+            np.array([[4, 2, -2], [2, 5, -5], [-2, -5, 8]]),
+        )
+        self.assertRaises(
+            ValueError,
+            GD,
+            ["x1", "x2", "x3"],
+            [[1, -3, 4]],
+            np.array([[4, 2, -2], [2, 5, -5], [-2, -5, 8]]),
+        )
+        self.assertRaises(
+            ValueError,
+            GD,
+            ["x1", "x2", "x3"],
+            [1],
+            np.array([[4, 2, -2], [2, 5, -5], [-2, -5, 8]]),
+        )
+        self.assertRaises(
+            ValueError,
+            GD,
+            ["x1", "x2", "x3"],
+            [[1], [-3]],
+            np.array([[4, 2, -2], [2, 5, -5], [-2, -5, 8]]),
+        )
+
+        self.assertRaises(
+            ValueError,
+            GD,
+            ["x1", "x2", "x3"],
+            [1, -3, 4],
+            np.array([[4, 2, -2], [2, 5, -5]]),
+        )
+        self.assertRaises(
+            ValueError, GD, ["x1", "x2", "x3"], [1, -3, 4], np.array([[4, 2, -2]])
+        )
+        self.assertRaises(
+            ValueError,
+            GD,
+            ["x1", "x2", "x3"],
+            [1, -3],
+            np.array([[4, 2], [2, 5], [-2, -5]]),
+        )
+        self.assertRaises(
+            ValueError, GD, ["x1", "x2", "x3"], [1, -3], np.array([[4], [2], [-2]])
+        )
+        self.assertRaises(ValueError, GD, ["x1", "x2", "x3"], [1, -3], np.array([[-2]]))
 
 
 class TestJGDMethods(unittest.TestCase):
     def setUp(self):
-        self.phi1 = GD(['x1', 'x2', 'x3'], np.array([[1], [-3], [4]]),
-                       np.array([[4, 2, -2], [2, 5, -5], [-2, -5, 8]]))
-        self.phi2 = GD(['x'], [0], [[1]])
+        self.phi1 = GD(
+            ["x1", "x2", "x3"],
+            np.array([[1], [-3], [4]]),
+            np.array([[4, 2, -2], [2, 5, -5], [-2, -5, 8]]),
+        )
+        self.phi2 = GD(["x"], [0], [[1]])
         self.phi3 = self.phi1.copy()
 
     def test_precision_matrix(self):
         self.assertEqual(self.phi1._precision_matrix, None)
-        np_test.assert_almost_equal(self.phi1.precision_matrix,
-                                    np.array([[0.3125, -0.125, 0],
-                                             [-0.125, 0.5833333, 0.3333333],
-                                             [0, 0.3333333, 0.3333333]]))
-        np_test.assert_almost_equal(self.phi1._precision_matrix,
-                                    np.array([[0.3125, -0.125, 0],
-                                             [-0.125, 0.5833333, 0.3333333],
-                                             [0, 0.3333333, 0.3333333]]))
+        np_test.assert_almost_equal(
+            self.phi1.precision_matrix,
+            np.array(
+                [
+                    [0.3125, -0.125, 0],
+                    [-0.125, 0.5833333, 0.3333333],
+                    [0, 0.3333333, 0.3333333],
+                ]
+            ),
+        )
+        np_test.assert_almost_equal(
+            self.phi1._precision_matrix,
+            np.array(
+                [
+                    [0.3125, -0.125, 0],
+                    [-0.125, 0.5833333, 0.3333333],
+                    [0, 0.3333333, 0.3333333],
+                ]
+            ),
+        )
 
         self.assertEqual(self.phi2._precision_matrix, None)
         np_test.assert_almost_equal(self.phi2.precision_matrix, np.array([[1]]))
         np_test.assert_almost_equal(self.phi2._precision_matrix, np.array([[1]]))
 
     def test_marginalize(self):
-        phi = self.phi1.marginalize(['x3'], inplace=False)
-        self.assertEqual(phi.variables, ['x1', 'x2'])
+        phi = self.phi1.marginalize(["x3"], inplace=False)
+        self.assertEqual(phi.variables, ["x1", "x2"])
         np_test.assert_array_equal(phi.mean, np.asarray([[1], [-3]], dtype=float))
-        np_test.assert_array_equal(phi.covariance,
-                                   np.asarray([[4, 2], [2, 5]], dtype=float))
+        np_test.assert_array_equal(
+            phi.covariance, np.asarray([[4, 2], [2, 5]], dtype=float)
+        )
         self.assertEqual(phi._precision_matrix, None)
 
-        phi = self.phi1.marginalize(['x3', 'x2'], inplace=False)
-        self.assertEqual(phi.variables, ['x1'])
+        phi = self.phi1.marginalize(["x3", "x2"], inplace=False)
+        self.assertEqual(phi.variables, ["x1"])
         np_test.assert_array_equal(phi.mean, np.asarray([[1]], dtype=float))
-        np_test.assert_array_equal(phi.covariance,
-                                   np.asarray([[4]], dtype=float))
+        np_test.assert_array_equal(phi.covariance, np.asarray([[4]], dtype=float))
         self.assertEqual(phi._precision_matrix, None)
 
-        self.phi1.marginalize(['x3'])
-        self.assertEqual(self.phi1.variables, ['x1', 'x2'])
+        self.phi1.marginalize(["x3"])
+        self.assertEqual(self.phi1.variables, ["x1", "x2"])
         np_test.assert_array_equal(self.phi1.mean, np.asarray([[1], [-3]], dtype=float))
-        np_test.assert_array_equal(self.phi1.covariance,
-                                   np.asarray([[4, 2], [2, 5]], dtype=float))
+        np_test.assert_array_equal(
+            self.phi1.covariance, np.asarray([[4, 2], [2, 5]], dtype=float)
+        )
         self.assertEqual(self.phi1._precision_matrix, None)
 
         self.phi1 = self.phi3
-        self.phi1.marginalize(['x3', 'x2'])
-        self.assertEqual(self.phi1.variables, ['x1'])
+        self.phi1.marginalize(["x3", "x2"])
+        self.assertEqual(self.phi1.variables, ["x1"])
         np_test.assert_array_equal(self.phi1.mean, np.asarray([[1]], dtype=float))
-        np_test.assert_array_equal(self.phi1.covariance,
-                                   np.asarray([[4]], dtype=float))
+        np_test.assert_array_equal(self.phi1.covariance, np.asarray([[4]], dtype=float))
         self.assertEqual(self.phi1._precision_matrix, None)
 
         self.phi1 = self.phi3
 
     def test_copy(self):
         copy_phi1 = self.phi1.copy()
         self.assertEqual(copy_phi1.variables, self.phi1.variables)
         np_test.assert_array_equal(copy_phi1.mean, self.phi1.mean)
         np_test.assert_array_equal(copy_phi1.covariance, self.phi1.covariance)
-        np_test.assert_array_equal(copy_phi1._precision_matrix, self.phi1._precision_matrix)
-
-        copy_phi1.marginalize(['x3'])
-        self.assertEqual(self.phi1.variables, ['x1', 'x2', 'x3'])
-        np_test.assert_array_equal(self.phi1.mean, np.asarray([[1], [-3], [4]], dtype=float))
-        np_test.assert_array_equal(self.phi1.covariance,
-                                   np.asarray([[4, 2, -2], [2, 5, -5], [-2, -5, 8]], dtype=float))
+        np_test.assert_array_equal(
+            copy_phi1._precision_matrix, self.phi1._precision_matrix
+        )
+
+        copy_phi1.marginalize(["x3"])
+        self.assertEqual(self.phi1.variables, ["x1", "x2", "x3"])
+        np_test.assert_array_equal(
+            self.phi1.mean, np.asarray([[1], [-3], [4]], dtype=float)
+        )
+        np_test.assert_array_equal(
+            self.phi1.covariance,
+            np.asarray([[4, 2, -2], [2, 5, -5], [-2, -5, 8]], dtype=float),
+        )
         self.assertEqual(self.phi1._precision_matrix, None)
 
-        self.phi1.marginalize(['x2'])
-        self.assertEqual(copy_phi1.variables, ['x1', 'x2'])
+        self.phi1.marginalize(["x2"])
+        self.assertEqual(copy_phi1.variables, ["x1", "x2"])
         np_test.assert_array_equal(copy_phi1.mean, np.asarray([[1], [-3]], dtype=float))
-        np_test.assert_array_equal(copy_phi1.covariance,
-                                   np.asarray([[4, 2], [2, 5]], dtype=float))
+        np_test.assert_array_equal(
+            copy_phi1.covariance, np.asarray([[4, 2], [2, 5]], dtype=float)
+        )
         self.assertEqual(copy_phi1._precision_matrix, None)
 
         self.phi1 = self.phi3
 
     def test_assignment(self):
         np_test.assert_almost_equal(self.phi1.assignment(*[1, 2, 3]), 2.797826e-05)
-        np_test.assert_almost_equal(self.phi1.assignment(*[[1, 2, 3], [0, 0, 0]]),
-                                    np.array([2.79782602e-05, 1.48056313e-03]))
+        np_test.assert_almost_equal(
+            self.phi1.assignment(*[[1, 2, 3], [0, 0, 0]]),
+            np.array([2.79782602e-05, 1.48056313e-03]),
+        )
         np_test.assert_almost_equal(self.phi2.assignment(0), 0.3989422804)
-        np_test.assert_almost_equal(self.phi2.assignment(*[0, 1, -1]),
-                                    np.array([0.39894228, 0.24197072, 0.24197072]))
+        np_test.assert_almost_equal(
+            self.phi2.assignment(*[0, 1, -1]),
+            np.array([0.39894228, 0.24197072, 0.24197072]),
+        )
 
     def test_reduce(self):
-        phi = self.phi1.reduce([('x1', 7)], inplace=False)
-        self.assertEqual(phi.variables, ['x2', 'x3'])
+        phi = self.phi1.reduce([("x1", 7)], inplace=False)
+        self.assertEqual(phi.variables, ["x2", "x3"])
         np_test.assert_array_equal(phi.mean, np.asarray([[0], [1]], dtype=float))
-        np_test.assert_array_equal(phi.covariance,
-                                   np.asarray([[4, -4], [-4, 7]], dtype=float))
+        np_test.assert_array_equal(
+            phi.covariance, np.asarray([[4, -4], [-4, 7]], dtype=float)
+        )
         self.assertEqual(phi._precision_matrix, None)
 
-        phi = self.phi1.reduce([('x1', 3), ('x2', 1)], inplace=False)
-        self.assertEqual(phi.variables, ['x3'])
+        phi = self.phi1.reduce([("x1", 3), ("x2", 1)], inplace=False)
+        self.assertEqual(phi.variables, ["x3"])
         np_test.assert_array_equal(phi.mean, np.array([[0]], dtype=float))
-        np_test.assert_array_equal(phi.covariance,
-                                   np.asarray([[3]], dtype=float))
+        np_test.assert_array_equal(phi.covariance, np.asarray([[3]], dtype=float))
         self.assertEqual(phi._precision_matrix, None)
 
-        self.phi1.reduce([('x1', 7)])
-        self.assertEqual(self.phi1.variables, ['x2', 'x3'])
+        self.phi1.reduce([("x1", 7)])
+        self.assertEqual(self.phi1.variables, ["x2", "x3"])
         np_test.assert_array_equal(self.phi1.mean, np.asarray([[0], [1]], dtype=float))
-        np_test.assert_array_equal(self.phi1.covariance,
-                                   np.asarray([[4, -4], [-4, 7]], dtype=float))
+        np_test.assert_array_equal(
+            self.phi1.covariance, np.asarray([[4, -4], [-4, 7]], dtype=float)
+        )
         self.assertEqual(self.phi1._precision_matrix, None)
 
         self.phi1 = self.phi3.copy()
-        self.phi1.reduce([('x1', 3), ('x2', 1)])
-        self.assertEqual(self.phi1.variables, ['x3'])
+        self.phi1.reduce([("x1", 3), ("x2", 1)])
+        self.assertEqual(self.phi1.variables, ["x3"])
         np_test.assert_array_equal(self.phi1.mean, np.asarray([[0]], dtype=float))
-        np_test.assert_array_equal(self.phi1.covariance,
-                                   np.asarray([[3]], dtype=float))
+        np_test.assert_array_equal(self.phi1.covariance, np.asarray([[3]], dtype=float))
         self.assertEqual(self.phi1._precision_matrix, None)
 
         self.phi1 = self.phi3.copy()
-        self.phi1.reduce([('x2', 1), ('x1', 3)])
-        self.assertEqual(self.phi1.variables, ['x3'])
+        self.phi1.reduce([("x2", 1), ("x1", 3)])
+        self.assertEqual(self.phi1.variables, ["x3"])
         np_test.assert_array_equal(self.phi1.mean, np.asarray([[0]], dtype=float))
-        np_test.assert_array_equal(self.phi1.covariance,
-                                   np.asarray([[3]], dtype=float))
+        np_test.assert_array_equal(self.phi1.covariance, np.asarray([[3]], dtype=float))
         self.assertEqual(self.phi1._precision_matrix, None)
 
     def test_normalize(self):
         phi = self.phi1.copy()
         phi.normalize()
         self.assertEqual(self.phi1.variables, phi.variables)
         np_test.assert_array_equal(self.phi1.mean, phi.mean)
```

### Comparing `pgmpy-0.1.7/pgmpy/tests/test_factors/test_continuous/test_ContinuousFactor.py` & `pgmpy-0.1.9/pgmpy/tests/test_factors/test_continuous/test_ContinuousFactor.py`

 * *Files 22% similar despite different names*

```diff
@@ -15,360 +15,418 @@
     def pdf2(self, *args):
         return multivariate_normal.pdf(args, [0, 0], [[1, 0], [0, 1]])
 
     def pdf3(self, x, y, z):
         return z * (np.power(x, 1) * np.power(y, 2)) / beta(x, y)
 
     def test_class_init(self):
-        phi1 = ContinuousFactor(['x', 'y'], self.pdf1)
-        self.assertEqual(phi1.scope(), ['x', 'y'])
+        phi1 = ContinuousFactor(["x", "y"], self.pdf1)
+        self.assertEqual(phi1.scope(), ["x", "y"])
         self.assertEqual(phi1.pdf, self.pdf1)
 
-        phi2 = ContinuousFactor(['x1', 'x2'], self.pdf2)
-        self.assertEqual(phi2.scope(), ['x1', 'x2'])
+        phi2 = ContinuousFactor(["x1", "x2"], self.pdf2)
+        self.assertEqual(phi2.scope(), ["x1", "x2"])
         self.assertEqual(phi2.pdf, self.pdf2)
 
-        phi3 = ContinuousFactor(['x', 'y', 'z'], self.pdf3)
-        self.assertEqual(phi3.scope(), ['x', 'y', 'z'])
+        phi3 = ContinuousFactor(["x", "y", "z"], self.pdf3)
+        self.assertEqual(phi3.scope(), ["x", "y", "z"])
         self.assertEqual(phi3.pdf, self.pdf3)
 
     def test_class_init_typeerror(self):
-        self.assertRaises(TypeError, ContinuousFactor, 'x y', self.pdf1)
-        self.assertRaises(TypeError, ContinuousFactor, 'x', self.pdf1)
+        self.assertRaises(TypeError, ContinuousFactor, "x y", self.pdf1)
+        self.assertRaises(TypeError, ContinuousFactor, "x", self.pdf1)
 
-        self.assertRaises(TypeError, ContinuousFactor, 'x1 x2', self.pdf2)
-        self.assertRaises(TypeError, ContinuousFactor, 'x1', self.pdf1)
+        self.assertRaises(TypeError, ContinuousFactor, "x1 x2", self.pdf2)
+        self.assertRaises(TypeError, ContinuousFactor, "x1", self.pdf1)
 
-        self.assertRaises(TypeError, ContinuousFactor, 'x y z', self.pdf3)
-        self.assertRaises(TypeError, ContinuousFactor, 'x', self.pdf3)
+        self.assertRaises(TypeError, ContinuousFactor, "x y z", self.pdf3)
+        self.assertRaises(TypeError, ContinuousFactor, "x", self.pdf3)
 
-        self.assertRaises(TypeError, ContinuousFactor, set(['x', 'y']), self.pdf1)
-        self.assertRaises(TypeError, ContinuousFactor, {'x': 1, 'y': 2}, self.pdf1)
+        self.assertRaises(TypeError, ContinuousFactor, set(["x", "y"]), self.pdf1)
+        self.assertRaises(TypeError, ContinuousFactor, {"x": 1, "y": 2}, self.pdf1)
 
-        self.assertRaises(TypeError, ContinuousFactor, set(['x1', 'x2']), self.pdf2)
-        self.assertRaises(TypeError, ContinuousFactor, {'x1': 1, 'x2': 2}, self.pdf1)
+        self.assertRaises(TypeError, ContinuousFactor, set(["x1", "x2"]), self.pdf2)
+        self.assertRaises(TypeError, ContinuousFactor, {"x1": 1, "x2": 2}, self.pdf1)
 
-        self.assertRaises(TypeError, ContinuousFactor, set(['x', 'y', 'z']), self.pdf3)
-        self.assertRaises(TypeError, ContinuousFactor, {'x': 1, 'y': 2, 'z': 3}, self.pdf3)
+        self.assertRaises(TypeError, ContinuousFactor, set(["x", "y", "z"]), self.pdf3)
+        self.assertRaises(
+            TypeError, ContinuousFactor, {"x": 1, "y": 2, "z": 3}, self.pdf3
+        )
 
     def test_class_init_valueerror(self):
-        self.assertRaises(ValueError, ContinuousFactor, ['x', 'x'], self.pdf1)
-        self.assertRaises(ValueError, ContinuousFactor, ['x', 'y', 'y'], self.pdf1)
+        self.assertRaises(ValueError, ContinuousFactor, ["x", "x"], self.pdf1)
+        self.assertRaises(ValueError, ContinuousFactor, ["x", "y", "y"], self.pdf1)
 
-        self.assertRaises(ValueError, ContinuousFactor, ['x1', 'x1'], self.pdf2)
-        self.assertRaises(ValueError, ContinuousFactor, ['x1', 'x2', 'x2'], self.pdf2)
+        self.assertRaises(ValueError, ContinuousFactor, ["x1", "x1"], self.pdf2)
+        self.assertRaises(ValueError, ContinuousFactor, ["x1", "x2", "x2"], self.pdf2)
 
-        self.assertRaises(ValueError, ContinuousFactor, ['x', 'x'], self.pdf1)
-        self.assertRaises(ValueError, ContinuousFactor, ['x', 'y', 'y', 'z', 'z'], self.pdf1)
+        self.assertRaises(ValueError, ContinuousFactor, ["x", "x"], self.pdf1)
+        self.assertRaises(
+            ValueError, ContinuousFactor, ["x", "y", "y", "z", "z"], self.pdf1
+        )
 
 
 class TestContinuousFactorMethods(unittest.TestCase):
     def pdf1(self, x, y):
         return np.power(x, 1) * np.power(y, 2) / beta(x, y)
 
     def pdf2(self, x1, x2):
         return multivariate_normal.pdf([x1, x2], [0, 0], [[1, 0], [0, 1]])
 
     def pdf3(self, x, y, z):
         return z * (np.power(x, 1) * np.power(y, 2)) / beta(x, y)
 
     def pdf4(self, x1, x2, x3):
-        return multivariate_normal.pdf([x1, x2, x3], [0, 0, 0],
-                                       [[1, 0, 0], [0, 1, 0], [0, 0, 1]])
+        return multivariate_normal.pdf(
+            [x1, x2, x3], [0, 0, 0], [[1, 0, 0], [0, 1, 0], [0, 0, 1]]
+        )
 
     def setUp(self):
-        self.phi1 = ContinuousFactor(['x', 'y'], self.pdf1)
-        self.phi2 = ContinuousFactor(['x1', 'x2'], self.pdf2)
-        self.phi3 = ContinuousFactor(['x', 'y', 'z'], self.pdf3)
-        self.phi4 = ContinuousFactor(['x1', 'x2', 'x3'], self.pdf4)
+        self.phi1 = ContinuousFactor(["x", "y"], self.pdf1)
+        self.phi2 = ContinuousFactor(["x1", "x2"], self.pdf2)
+        self.phi3 = ContinuousFactor(["x", "y", "z"], self.pdf3)
+        self.phi4 = ContinuousFactor(["x1", "x2", "x3"], self.pdf4)
 
     def test_scope(self):
         self.assertEqual(self.phi1.scope(), self.phi1.scope())
         self.assertEqual(self.phi2.scope(), self.phi2.scope())
         self.assertEqual(self.phi3.scope(), self.phi3.scope())
 
     def test_assignment(self):
         self.assertEqual(self.phi1.assignment(1.212, 2), self.pdf1(1.212, 2))
         self.assertEqual(self.phi2.assignment(1, -2.231), self.pdf2(1, -2.231))
-        self.assertEqual(self.phi3.assignment(1.212, 2.213, -3), self.pdf3(1.212, 2.213, -3))
+        self.assertEqual(
+            self.phi3.assignment(1.212, 2.213, -3), self.pdf3(1.212, 2.213, -3)
+        )
 
     def test_reduce(self):
         phi1 = self.phi1.copy()
-        phi1.reduce([('x', 1)])
+        phi1.reduce([("x", 1)])
 
         def reduced_pdf1(y):
-            return (np.power(1, 1) * np.power(y, 2))/beta(1, y)
+            return (np.power(1, 1) * np.power(y, 2)) / beta(1, y)
 
-        self.assertEqual(phi1.scope(), ['y'])
+        self.assertEqual(phi1.scope(), ["y"])
         for inp in np.random.rand(4):
             self.assertEqual(phi1.pdf(inp), reduced_pdf1(inp))
             self.assertEqual(phi1.pdf(y=inp), reduced_pdf1(inp))
 
-        phi1 = self.phi1.reduce([('x', 1)], inplace=False)
-        self.assertEqual(phi1.scope(), ['y'])
+        phi1 = self.phi1.reduce([("x", 1)], inplace=False)
+        self.assertEqual(phi1.scope(), ["y"])
         for inp in np.random.rand(4):
             self.assertEqual(phi1.pdf(inp), reduced_pdf1(inp))
             self.assertEqual(phi1.pdf(y=inp), reduced_pdf1(inp))
 
         phi2 = self.phi2.copy()
-        phi2.reduce([('x2', 7.213)])
+        phi2.reduce([("x2", 7.213)])
 
         def reduced_pdf2(x1):
             return multivariate_normal.pdf([x1, 7.213], [0, 0], [[1, 0], [0, 1]])
 
-        self.assertEqual(phi2.scope(), ['x1'])
+        self.assertEqual(phi2.scope(), ["x1"])
         for inp in np.random.rand(4):
             self.assertEqual(phi2.pdf(inp), reduced_pdf2(inp))
             self.assertEqual(phi2.pdf(x1=inp), reduced_pdf2(inp))
 
-        phi2 = self.phi2.reduce([('x2', 7.213)], inplace=False)
-        self.assertEqual(phi2.scope(), ['x1'])
+        phi2 = self.phi2.reduce([("x2", 7.213)], inplace=False)
+        self.assertEqual(phi2.scope(), ["x1"])
         for inp in np.random.rand(4):
             self.assertEqual(phi2.pdf(inp), reduced_pdf2(inp))
             self.assertEqual(phi2.pdf(x1=inp), reduced_pdf2(inp))
 
         phi3 = self.phi3.copy()
-        phi3.reduce([('y', 0.112), ('z', 23)])
+        phi3.reduce([("y", 0.112), ("z", 23)])
 
         def reduced_pdf4(x):
-            return 23*(np.power(x, 1)*np.power(0.112, 2))/beta(x, 0.112)
+            return 23 * (np.power(x, 1) * np.power(0.112, 2)) / beta(x, 0.112)
 
-        self.assertEqual(phi3.scope(), ['x'])
+        self.assertEqual(phi3.scope(), ["x"])
         for inp in np.random.rand(4):
             self.assertEqual(phi3.pdf(inp), reduced_pdf4(inp))
             self.assertEqual(phi3.pdf(x=inp), reduced_pdf4(inp))
 
         phi3 = self.phi3.copy()
-        phi3.reduce([('y', 0.112)])
+        phi3.reduce([("y", 0.112)])
 
         def reduced_pdf3(x, z):
-            return z*(np.power(x, 1)*np.power(0.112, 2))/beta(x, 0.112)
+            return z * (np.power(x, 1) * np.power(0.112, 2)) / beta(x, 0.112)
 
-        self.assertEqual(phi3.scope(), ['x', 'z'])
+        self.assertEqual(phi3.scope(), ["x", "z"])
         for inp in np.random.rand(4, 2):
             self.assertEqual(phi3.pdf(inp[0], inp[1]), reduced_pdf3(inp[0], inp[1]))
             self.assertEqual(phi3.pdf(x=inp[0], z=inp[1]), reduced_pdf3(inp[0], inp[1]))
 
-        phi3 = self.phi3.reduce([('y', 0.112)], inplace=False)
-        self.assertEqual(phi3.scope(), ['x', 'z'])
+        phi3 = self.phi3.reduce([("y", 0.112)], inplace=False)
+        self.assertEqual(phi3.scope(), ["x", "z"])
         for inp in np.random.rand(4, 2):
             self.assertEqual(phi3.pdf(inp[0], inp[1]), reduced_pdf3(inp[0], inp[1]))
             self.assertEqual(phi3.pdf(x=inp[0], z=inp[1]), reduced_pdf3(inp[0], inp[1]))
             self.assertEqual(phi3.pdf(inp[0], z=inp[1]), reduced_pdf3(inp[0], inp[1]))
 
-        phi3 = self.phi3.reduce([('y', 0.112), ('z', 23)], inplace=False)
-        self.assertEqual(phi3.scope(), ['x'])
+        phi3 = self.phi3.reduce([("y", 0.112), ("z", 23)], inplace=False)
+        self.assertEqual(phi3.scope(), ["x"])
         for inp in np.random.rand(4):
             self.assertEqual(phi3.pdf(inp), reduced_pdf4(inp))
             self.assertEqual(phi3.pdf(x=inp), reduced_pdf4(inp))
 
     def test_reduce_error(self):
-        self.assertRaises(TypeError, self.phi1.reduce, 'x1')
-        self.assertRaises(TypeError, self.phi1.reduce, set(['x', 'y']))
-        self.assertRaises(TypeError, self.phi1.reduce, {'x': 1, 'y': 1})
-
-        self.assertRaises(TypeError, self.phi4.reduce, 'x4')
-        self.assertRaises(TypeError, self.phi4.reduce, set(['x1', 'x2', 'x3']))
-        self.assertRaises(TypeError, self.phi4.reduce, {'x1': 1, 'x2': 1, 'x3': 1})
-
-        self.assertRaises(ValueError, self.phi1.reduce, [('z', 3)])
-        self.assertRaises(ValueError, self.phi1.reduce, [('x', 0), ('y', 1), ('z', 4)])
-
-        self.assertRaises(ValueError, self.phi4.reduce, [('x4', 7)])
-        self.assertRaises(ValueError, self.phi4.reduce, [('x1', 1), ('x2', 2), ('x3', 3), ('x4', 4)])
+        self.assertRaises(TypeError, self.phi1.reduce, "x1")
+        self.assertRaises(TypeError, self.phi1.reduce, set(["x", "y"]))
+        self.assertRaises(TypeError, self.phi1.reduce, {"x": 1, "y": 1})
+
+        self.assertRaises(TypeError, self.phi4.reduce, "x4")
+        self.assertRaises(TypeError, self.phi4.reduce, set(["x1", "x2", "x3"]))
+        self.assertRaises(TypeError, self.phi4.reduce, {"x1": 1, "x2": 1, "x3": 1})
+
+        self.assertRaises(ValueError, self.phi1.reduce, [("z", 3)])
+        self.assertRaises(ValueError, self.phi1.reduce, [("x", 0), ("y", 1), ("z", 4)])
+
+        self.assertRaises(ValueError, self.phi4.reduce, [("x4", 7)])
+        self.assertRaises(
+            ValueError, self.phi4.reduce, [("x1", 1), ("x2", 2), ("x3", 3), ("x4", 4)]
+        )
 
     def test_marginalize(self):
         phi2 = self.phi2.copy()
-        phi2.marginalize(['x2'])
-        self.assertEqual(phi2.scope(), ['x1'])
+        phi2.marginalize(["x2"])
+        self.assertEqual(phi2.scope(), ["x1"])
         for inp in np.random.rand(4):
-            np_test.assert_almost_equal(phi2.pdf(inp),
-                                        multivariate_normal.pdf([inp], [0], [[1]]))
+            np_test.assert_almost_equal(
+                phi2.pdf(inp), multivariate_normal.pdf([inp], [0], [[1]])
+            )
 
-        phi2 = self.phi2.marginalize(['x2'], inplace=False)
-        self.assertEqual(phi2.scope(), ['x1'])
+        phi2 = self.phi2.marginalize(["x2"], inplace=False)
+        self.assertEqual(phi2.scope(), ["x1"])
         for inp in np.random.rand(4):
-            np_test.assert_almost_equal(phi2.pdf(inp),
-                                        multivariate_normal.pdf([inp], [0], [[1]]))
+            np_test.assert_almost_equal(
+                phi2.pdf(inp), multivariate_normal.pdf([inp], [0], [[1]])
+            )
 
         phi4 = self.phi4.copy()
-        phi4.marginalize(['x2'])
+        phi4.marginalize(["x2"])
 
-        self.assertEqual(phi4.scope(), ['x1', 'x3'])
+        self.assertEqual(phi4.scope(), ["x1", "x3"])
         for inp in np.random.rand(4, 2):
             np_test.assert_almost_equal(
-                phi4.pdf(inp[0], inp[1]), multivariate_normal.pdf(
-                    [inp[0], inp[1]], [0, 0], [[1, 0], [0, 1]]))
+                phi4.pdf(inp[0], inp[1]),
+                multivariate_normal.pdf([inp[0], inp[1]], [0, 0], [[1, 0], [0, 1]]),
+            )
 
-        phi4.marginalize(['x3'])
-        self.assertEqual(phi4.scope(), ['x1'])
+        phi4.marginalize(["x3"])
+        self.assertEqual(phi4.scope(), ["x1"])
         for inp in np.random.rand(1):
-            np_test.assert_almost_equal(phi4.pdf(inp), multivariate_normal.pdf([inp], [0], [[1]]))
+            np_test.assert_almost_equal(
+                phi4.pdf(inp), multivariate_normal.pdf([inp], [0], [[1]])
+            )
 
-        phi4 = self.phi4.marginalize(['x2'], inplace=False)
-        self.assertEqual(phi4.scope(), ['x1', 'x3'])
+        phi4 = self.phi4.marginalize(["x2"], inplace=False)
+        self.assertEqual(phi4.scope(), ["x1", "x3"])
         for inp in np.random.rand(4, 2):
-            np_test.assert_almost_equal(phi4.pdf(inp[0], inp[1]),
-                                        multivariate_normal.pdf([inp[0], inp[1]], [0, 0], [[1, 0], [0, 1]]))
+            np_test.assert_almost_equal(
+                phi4.pdf(inp[0], inp[1]),
+                multivariate_normal.pdf([inp[0], inp[1]], [0, 0], [[1, 0], [0, 1]]),
+            )
 
-        phi4 = phi4.marginalize(['x3'], inplace=False)
-        self.assertEqual(phi4.scope(), ['x1'])
+        phi4 = phi4.marginalize(["x3"], inplace=False)
+        self.assertEqual(phi4.scope(), ["x1"])
         for inp in np.random.rand(1):
-            np_test.assert_almost_equal(phi4.pdf(inp),
-                                        multivariate_normal.pdf([inp], [0], [[1]]))
+            np_test.assert_almost_equal(
+                phi4.pdf(inp), multivariate_normal.pdf([inp], [0], [[1]])
+            )
 
     def test_marginalize_error(self):
-        self.assertRaises(TypeError, self.phi1.marginalize, 'x1')
-        self.assertRaises(TypeError, self.phi1.marginalize, set(['x', 'y']))
-        self.assertRaises(TypeError, self.phi1.marginalize, {'x': 1, 'y': 1})
-
-        self.assertRaises(TypeError, self.phi4.marginalize, 'x4')
-        self.assertRaises(TypeError, self.phi4.marginalize, set(['x1', 'x2', 'x3']))
-        self.assertRaises(TypeError, self.phi4.marginalize, {'x1': 1, 'x2': 1, 'x3': 1})
+        self.assertRaises(TypeError, self.phi1.marginalize, "x1")
+        self.assertRaises(TypeError, self.phi1.marginalize, set(["x", "y"]))
+        self.assertRaises(TypeError, self.phi1.marginalize, {"x": 1, "y": 1})
+
+        self.assertRaises(TypeError, self.phi4.marginalize, "x4")
+        self.assertRaises(TypeError, self.phi4.marginalize, set(["x1", "x2", "x3"]))
+        self.assertRaises(TypeError, self.phi4.marginalize, {"x1": 1, "x2": 1, "x3": 1})
 
-        self.assertRaises(ValueError, self.phi1.marginalize, ['z'])
-        self.assertRaises(ValueError, self.phi1.marginalize, ['x', 'y', 'z'])
+        self.assertRaises(ValueError, self.phi1.marginalize, ["z"])
+        self.assertRaises(ValueError, self.phi1.marginalize, ["x", "y", "z"])
 
-        self.assertRaises(ValueError, self.phi4.marginalize, ['x4'])
-        self.assertRaises(ValueError, self.phi4.marginalize, ['x1', 'x2', 'x3', 'x4'])
+        self.assertRaises(ValueError, self.phi4.marginalize, ["x4"])
+        self.assertRaises(ValueError, self.phi4.marginalize, ["x1", "x2", "x3", "x4"])
 
     def test_normalize(self):
         def pdf2(x1, x2):
             return 2 * self.pdf2(x1, x2)
 
-        phi2 = ContinuousFactor(['x1', 'x2'], pdf2)
+        phi2 = ContinuousFactor(["x1", "x2"], pdf2)
         phi4 = phi2.copy()
 
         phi4.normalize()
         self.assertEqual(phi4.scope(), phi2.scope())
         for inp in np.random.rand(1, 2):
-            np_test.assert_almost_equal(phi4.pdf(inp[0], inp[1]), 2 * self.pdf2(inp[0], inp[1]))
+            np_test.assert_almost_equal(
+                phi4.pdf(inp[0], inp[1]), self.pdf2(inp[0], inp[1])
+            )
 
         phi2.normalize()
         self.assertEqual(phi4.scope(), phi2.scope())
         for inp in np.random.rand(1, 2):
-            np_test.assert_almost_equal(phi2.pdf(inp[0], inp[1]), phi4.pdf(inp[0], inp[1]))
+            np_test.assert_almost_equal(
+                phi2.pdf(inp[0], inp[1]), phi4.pdf(inp[0], inp[1])
+            )
 
     def test_operate(self):
         phi1 = self.phi1.copy()
-        phi1._operate(self.phi2, 'product')
-        self.assertEqual(phi1.scope(), ['x', 'y', 'x1', 'x2'])
+        phi1._operate(self.phi2, "product")
+        self.assertEqual(phi1.scope(), ["x", "y", "x1", "x2"])
         for inp in np.random.rand(4, 4):
-            self.assertEqual(phi1.pdf(*inp), self.phi1.pdf(inp[0], inp[1]) * self.phi2.pdf(inp[2], inp[3]))
+            self.assertEqual(
+                phi1.pdf(*inp),
+                self.phi1.pdf(inp[0], inp[1]) * self.phi2.pdf(inp[2], inp[3]),
+            )
 
-        phi1 = self.phi1._operate(self.phi2, 'product', inplace=False)
-        self.assertEqual(phi1.scope(), ['x', 'y', 'x1', 'x2'])
+        phi1 = self.phi1._operate(self.phi2, "product", inplace=False)
+        self.assertEqual(phi1.scope(), ["x", "y", "x1", "x2"])
         for inp in np.random.rand(4, 4):
-            self.assertEqual(phi1.pdf(*inp), self.phi1.pdf(inp[0], inp[1]) * self.phi2.pdf(inp[2], inp[3]))
+            self.assertEqual(
+                phi1.pdf(*inp),
+                self.phi1.pdf(inp[0], inp[1]) * self.phi2.pdf(inp[2], inp[3]),
+            )
 
         phi1 = self.phi1 * self.phi2
-        self.assertEqual(phi1.scope(), ['x', 'y', 'x1', 'x2'])
+        self.assertEqual(phi1.scope(), ["x", "y", "x1", "x2"])
         for inp in np.random.rand(4, 4):
-            self.assertEqual(phi1.pdf(*inp), self.phi1.pdf(inp[0], inp[1]) * self.phi2.pdf(inp[2], inp[3]))
+            self.assertEqual(
+                phi1.pdf(*inp),
+                self.phi1.pdf(inp[0], inp[1]) * self.phi2.pdf(inp[2], inp[3]),
+            )
 
         phi3 = self.phi3.copy()
-        phi3._operate(self.phi1, 'product')
-        self.assertEqual(phi3.scope(), ['x', 'y', 'z'])
+        phi3._operate(self.phi1, "product")
+        self.assertEqual(phi3.scope(), ["x", "y", "z"])
         for inp in np.random.rand(4, 3):
-            self.assertEqual(phi3.pdf(*inp), self.phi3.pdf(*inp) * self.phi1.pdf(inp[0], inp[1]))
-
-        phi3 = self.phi3._operate(self.phi1, 'product', inplace=False)
-        self.assertEqual(phi3.scope(), ['x', 'y', 'z'])
-        for inp in np.random.rand(4, 3):
-            self.assertEqual(phi3.pdf(*inp), self.phi3.pdf(*inp) * self.phi1.pdf(inp[0], inp[1]))
+            self.assertEqual(
+                phi3.pdf(*inp), self.phi3.pdf(*inp) * self.phi1.pdf(inp[0], inp[1])
+            )
+
+        phi3 = self.phi3._operate(self.phi1, "product", inplace=False)
+        self.assertEqual(phi3.scope(), ["x", "y", "z"])
+        for inp in np.random.rand(4, 3):
+            self.assertEqual(
+                phi3.pdf(*inp), self.phi3.pdf(*inp) * self.phi1.pdf(inp[0], inp[1])
+            )
 
         phi3 = self.phi3 * self.phi1
-        self.assertEqual(phi3.scope(), ['x', 'y', 'z'])
+        self.assertEqual(phi3.scope(), ["x", "y", "z"])
         for inp in np.random.rand(4, 3):
-            self.assertEqual(phi3.pdf(*inp), self.phi3.pdf(*inp) * self.phi1.pdf(inp[0], inp[1]))
+            self.assertEqual(
+                phi3.pdf(*inp), self.phi3.pdf(*inp) * self.phi1.pdf(inp[0], inp[1])
+            )
 
         phi3 = self.phi3.copy()
-        phi3._operate(self.phi1, 'divide')
-        self.assertEqual(phi3.scope(), ['x', 'y', 'z'])
-        for inp in np.random.rand(4, 3):
-            self.assertEqual(phi3.pdf(*inp), self.phi3.pdf(*inp) / self.phi1.pdf(inp[0], inp[1]))
-
-        phi3 = self.phi3._operate(self.phi1, 'divide', inplace=False)
-        self.assertEqual(phi3.scope(), ['x', 'y', 'z'])
+        phi3._operate(self.phi1, "divide")
+        self.assertEqual(phi3.scope(), ["x", "y", "z"])
         for inp in np.random.rand(4, 3):
-            self.assertEqual(phi3.pdf(*inp), self.phi3.pdf(*inp) / self.phi1.pdf(inp[0], inp[1]))
+            self.assertEqual(
+                phi3.pdf(*inp), self.phi3.pdf(*inp) / self.phi1.pdf(inp[0], inp[1])
+            )
+
+        phi3 = self.phi3._operate(self.phi1, "divide", inplace=False)
+        self.assertEqual(phi3.scope(), ["x", "y", "z"])
+        for inp in np.random.rand(4, 3):
+            self.assertEqual(
+                phi3.pdf(*inp), self.phi3.pdf(*inp) / self.phi1.pdf(inp[0], inp[1])
+            )
 
         phi3 = self.phi3 / self.phi1
-        self.assertEqual(phi3.scope(), ['x', 'y', 'z'])
+        self.assertEqual(phi3.scope(), ["x", "y", "z"])
         for inp in np.random.rand(4, 3):
-            self.assertEqual(phi3.pdf(*inp), self.phi3.pdf(*inp) / self.phi1.pdf(inp[0], inp[1]))
+            self.assertEqual(
+                phi3.pdf(*inp), self.phi3.pdf(*inp) / self.phi1.pdf(inp[0], inp[1])
+            )
 
         phi4 = self.phi4.copy()
-        phi4._operate(self.phi2, 'product')
-        self.assertEqual(phi4.scope(), ['x1', 'x2', 'x3'])
+        phi4._operate(self.phi2, "product")
+        self.assertEqual(phi4.scope(), ["x1", "x2", "x3"])
         for inp in np.random.rand(4, 3):
-            self.assertEqual(phi4.pdf(*inp), self.phi4.pdf(*inp) * self.phi2.pdf(inp[0], inp[1]))
-
-        phi4 = self.phi4._operate(self.phi2, 'product', inplace=False)
-        self.assertEqual(phi4.scope(), ['x1', 'x2', 'x3'])
-        for inp in np.random.rand(4, 3):
-            self.assertEqual(phi4.pdf(*inp), self.phi4.pdf(*inp) * self.phi2.pdf(inp[0], inp[1]))
+            self.assertEqual(
+                phi4.pdf(*inp), self.phi4.pdf(*inp) * self.phi2.pdf(inp[0], inp[1])
+            )
+
+        phi4 = self.phi4._operate(self.phi2, "product", inplace=False)
+        self.assertEqual(phi4.scope(), ["x1", "x2", "x3"])
+        for inp in np.random.rand(4, 3):
+            self.assertEqual(
+                phi4.pdf(*inp), self.phi4.pdf(*inp) * self.phi2.pdf(inp[0], inp[1])
+            )
 
         phi4 = self.phi4 * self.phi2
-        self.assertEqual(phi4.scope(), ['x1', 'x2', 'x3'])
+        self.assertEqual(phi4.scope(), ["x1", "x2", "x3"])
         for inp in np.random.rand(4, 3):
-            self.assertEqual(phi4.pdf(*inp), self.phi4.pdf(*inp) * self.phi2.pdf(inp[0], inp[1]))
+            self.assertEqual(
+                phi4.pdf(*inp), self.phi4.pdf(*inp) * self.phi2.pdf(inp[0], inp[1])
+            )
 
         phi4 = self.phi4.copy()
-        phi4._operate(self.phi2, 'divide')
-        self.assertEqual(phi4.scope(), ['x1', 'x2', 'x3'])
-        for inp in np.random.rand(4, 3):
-            self.assertEqual(phi4.pdf(*inp), self.phi4.pdf(*inp) / self.phi2.pdf(inp[0], inp[1]))
-
-        phi4 = self.phi4._operate(self.phi2, 'divide', inplace=False)
-        self.assertEqual(phi4.scope(), ['x1', 'x2', 'x3'])
+        phi4._operate(self.phi2, "divide")
+        self.assertEqual(phi4.scope(), ["x1", "x2", "x3"])
         for inp in np.random.rand(4, 3):
-            self.assertEqual(phi4.pdf(*inp), self.phi4.pdf(*inp) / self.phi2.pdf(inp[0], inp[1]))
+            self.assertEqual(
+                phi4.pdf(*inp), self.phi4.pdf(*inp) / self.phi2.pdf(inp[0], inp[1])
+            )
+
+        phi4 = self.phi4._operate(self.phi2, "divide", inplace=False)
+        self.assertEqual(phi4.scope(), ["x1", "x2", "x3"])
+        for inp in np.random.rand(4, 3):
+            self.assertEqual(
+                phi4.pdf(*inp), self.phi4.pdf(*inp) / self.phi2.pdf(inp[0], inp[1])
+            )
 
         phi4 = self.phi4 / self.phi2
-        self.assertEqual(phi4.scope(), ['x1', 'x2', 'x3'])
+        self.assertEqual(phi4.scope(), ["x1", "x2", "x3"])
         for inp in np.random.rand(4, 3):
-            self.assertEqual(phi4.pdf(*inp), self.phi4.pdf(*inp) / self.phi2.pdf(inp[0], inp[1]))
+            self.assertEqual(
+                phi4.pdf(*inp), self.phi4.pdf(*inp) / self.phi2.pdf(inp[0], inp[1])
+            )
 
     def test_operate_error(self):
-        self.assertRaises(TypeError, self.phi1._operate, 1, 'product')
-        self.assertRaises(TypeError, self.phi1._operate, 1, 'divide')
-        self.assertRaises(TypeError, self.phi1._operate, '1', 'product')
-        self.assertRaises(TypeError, self.phi1._operate, '1', 'divide')
-        self.assertRaises(TypeError, self.phi1._operate, self.phi2.pdf, 'product')
-        self.assertRaises(TypeError, self.phi1._operate, self.phi2.pdf, 'divide')
-        self.assertRaises(TypeError, self.phi1._operate, [1], 'product')
-        self.assertRaises(TypeError, self.phi1._operate, [1], 'divide')
-
-        self.assertRaises(TypeError, self.phi4._operate, 1, 'product')
-        self.assertRaises(TypeError, self.phi4._operate, 1, 'divide')
-        self.assertRaises(TypeError, self.phi4._operate, '1', 'product')
-        self.assertRaises(TypeError, self.phi4._operate, '1', 'divide')
-        self.assertRaises(TypeError, self.phi4._operate, self.phi2.pdf, 'product')
-        self.assertRaises(TypeError, self.phi4._operate, self.phi2.pdf, 'divide')
-        self.assertRaises(TypeError, self.phi4._operate, [1], 'product')
-        self.assertRaises(TypeError, self.phi4._operate, [1], 'divide')
-
-        self.assertRaises(TypeError, self.phi1._operate, 1, 'product', False)
-        self.assertRaises(TypeError, self.phi1._operate, 1, 'divide', False)
-        self.assertRaises(TypeError, self.phi1._operate, '1', 'product', False)
-        self.assertRaises(TypeError, self.phi1._operate, '1', 'divide', False)
-        self.assertRaises(TypeError, self.phi1._operate, self.phi2.pdf, 'product', False)
-        self.assertRaises(TypeError, self.phi1._operate, self.phi2.pdf, 'divide', False)
-        self.assertRaises(TypeError, self.phi1._operate, [1], 'product', False)
-        self.assertRaises(TypeError, self.phi1._operate, [1], 'divide', False)
-
-        self.assertRaises(TypeError, self.phi4._operate, 1, 'product', False)
-        self.assertRaises(TypeError, self.phi4._operate, 1, 'divide', False)
-        self.assertRaises(TypeError, self.phi4._operate, '1', 'product', False)
-        self.assertRaises(TypeError, self.phi4._operate, '1', 'divide', False)
-        self.assertRaises(TypeError, self.phi4._operate, self.phi2.pdf, 'product', False)
-        self.assertRaises(TypeError, self.phi4._operate, self.phi2.pdf, 'divide', False)
-        self.assertRaises(TypeError, self.phi4._operate, [1], 'product', False)
-        self.assertRaises(TypeError, self.phi4._operate, [1], 'divide', False)
+        self.assertRaises(TypeError, self.phi1._operate, 1, "product")
+        self.assertRaises(TypeError, self.phi1._operate, 1, "divide")
+        self.assertRaises(TypeError, self.phi1._operate, "1", "product")
+        self.assertRaises(TypeError, self.phi1._operate, "1", "divide")
+        self.assertRaises(TypeError, self.phi1._operate, self.phi2.pdf, "product")
+        self.assertRaises(TypeError, self.phi1._operate, self.phi2.pdf, "divide")
+        self.assertRaises(TypeError, self.phi1._operate, [1], "product")
+        self.assertRaises(TypeError, self.phi1._operate, [1], "divide")
+
+        self.assertRaises(TypeError, self.phi4._operate, 1, "product")
+        self.assertRaises(TypeError, self.phi4._operate, 1, "divide")
+        self.assertRaises(TypeError, self.phi4._operate, "1", "product")
+        self.assertRaises(TypeError, self.phi4._operate, "1", "divide")
+        self.assertRaises(TypeError, self.phi4._operate, self.phi2.pdf, "product")
+        self.assertRaises(TypeError, self.phi4._operate, self.phi2.pdf, "divide")
+        self.assertRaises(TypeError, self.phi4._operate, [1], "product")
+        self.assertRaises(TypeError, self.phi4._operate, [1], "divide")
+
+        self.assertRaises(TypeError, self.phi1._operate, 1, "product", False)
+        self.assertRaises(TypeError, self.phi1._operate, 1, "divide", False)
+        self.assertRaises(TypeError, self.phi1._operate, "1", "product", False)
+        self.assertRaises(TypeError, self.phi1._operate, "1", "divide", False)
+        self.assertRaises(
+            TypeError, self.phi1._operate, self.phi2.pdf, "product", False
+        )
+        self.assertRaises(TypeError, self.phi1._operate, self.phi2.pdf, "divide", False)
+        self.assertRaises(TypeError, self.phi1._operate, [1], "product", False)
+        self.assertRaises(TypeError, self.phi1._operate, [1], "divide", False)
+
+        self.assertRaises(TypeError, self.phi4._operate, 1, "product", False)
+        self.assertRaises(TypeError, self.phi4._operate, 1, "divide", False)
+        self.assertRaises(TypeError, self.phi4._operate, "1", "product", False)
+        self.assertRaises(TypeError, self.phi4._operate, "1", "divide", False)
+        self.assertRaises(
+            TypeError, self.phi4._operate, self.phi2.pdf, "product", False
+        )
+        self.assertRaises(TypeError, self.phi4._operate, self.phi2.pdf, "divide", False)
+        self.assertRaises(TypeError, self.phi4._operate, [1], "product", False)
+        self.assertRaises(TypeError, self.phi4._operate, [1], "divide", False)
 
         self.assertRaises(ValueError, self.phi1.__truediv__, self.phi2)
         self.assertRaises(ValueError, self.phi1.__truediv__, self.phi3)
         self.assertRaises(ValueError, self.phi1.__truediv__, self.phi4)
         self.assertRaises(ValueError, self.phi2.__truediv__, self.phi3)
         self.assertRaises(ValueError, self.phi2.__truediv__, self.phi4)
 
@@ -380,37 +438,37 @@
         copy5 = copy2.copy()
 
         self.assertEqual(copy1.scope(), copy4.scope())
         self.assertEqual(copy1.pdf, copy4.pdf)
         self.assertEqual(copy2.scope(), copy5.scope())
         self.assertEqual(copy2.pdf, copy5.pdf)
 
-# TODO: Fix these
-#        copy1.variables = ['A', 'B']
-#        self.assertEqual(copy4.scope(), self.phi1.scope())
-
-#        def pdf(a, b):
-#            return (a + b) / (a * a + b * b)
-#        copy1._pdf = pdf
-#        copy1_pdf = pdf
-#        self.assertEqual(copy4.pdf, self.phi1.pdf)
-#        copy4.variables = ['X', 'Y']
-#        self.assertEqual(copy1.scope(), ['x', 'y'])
-#        copy4._pdf = lambda a, b: a + b
-#        for inp in np.random.rand(4, 2):
-#            self.assertEqual(copy1.pdf(inp[0], inp[1]), copy1_pdf(inp[0], inp[1]))
-
-#        copy2.reduce([('x', 7.7)])
-
-#        def reduced_pdf(y, z):
-#            return z*(np.power(7.7, 1) * np.power(y, 2)) / beta(7.7, y)
-#        self.assertEqual(copy5.scope(), self.phi3.scope())
-#        self.assertEqual(copy5.pdf, self.phi3.pdf)
-#        copy5.reduce([('x', 11), ('z', 13)])
-#        self.assertEqual(copy2.scope(), ['y', 'z'])
-#        for inp in np.random.rand(4, 2):
-#            self.assertEqual(copy2.pdf(inp[0], inp[1]), reduced_pdf(inp[0], inp[1]))
+    # TODO: Fix these
+    #        copy1.variables = ['A', 'B']
+    #        self.assertEqual(copy4.scope(), self.phi1.scope())
+
+    #        def pdf(a, b):
+    #            return (a + b) / (a * a + b * b)
+    #        copy1._pdf = pdf
+    #        copy1_pdf = pdf
+    #        self.assertEqual(copy4.pdf, self.phi1.pdf)
+    #        copy4.variables = ['X', 'Y']
+    #        self.assertEqual(copy1.scope(), ['x', 'y'])
+    #        copy4._pdf = lambda a, b: a + b
+    #        for inp in np.random.rand(4, 2):
+    #            self.assertEqual(copy1.pdf(inp[0], inp[1]), copy1_pdf(inp[0], inp[1]))
+
+    #        copy2.reduce([('x', 7.7)])
+
+    #        def reduced_pdf(y, z):
+    #            return z*(np.power(7.7, 1) * np.power(y, 2)) / beta(7.7, y)
+    #        self.assertEqual(copy5.scope(), self.phi3.scope())
+    #        self.assertEqual(copy5.pdf, self.phi3.pdf)
+    #        copy5.reduce([('x', 11), ('z', 13)])
+    #        self.assertEqual(copy2.scope(), ['y', 'z'])
+    #        for inp in np.random.rand(4, 2):
+    #            self.assertEqual(copy2.pdf(inp[0], inp[1]), reduced_pdf(inp[0], inp[1]))
 
     def tearDown(self):
         del self.phi1
         del self.phi2
         del self.phi3
```

### Comparing `pgmpy-0.1.7/pgmpy/tests/test_factors/test_continuous/test_CustomDistribution.py` & `pgmpy-0.1.9/pgmpy/tests/test_factors/test_continuous/test_CustomDistribution.py`

 * *Files 16% similar despite different names*

```diff
@@ -15,394 +15,428 @@
     def pdf2(self, *args):
         return multivariate_normal.pdf(args, [0, 0], [[1, 0], [0, 1]])
 
     def pdf3(self, x, y, z):
         return z * (np.power(x, 1) * np.power(y, 2)) / beta(x, y)
 
     def test_class_init(self):
-        phi1 = CustomDistribution(['x', 'y'], self.pdf1)
-        self.assertEqual(phi1.variables, ['x', 'y'])
+        phi1 = CustomDistribution(["x", "y"], self.pdf1)
+        self.assertEqual(phi1.variables, ["x", "y"])
         self.assertEqual(phi1._pdf, self.pdf1)
 
-        phi2 = CustomDistribution(['x1', 'x2'], self.pdf2)
-        self.assertEqual(phi2.variables, ['x1', 'x2'])
+        phi2 = CustomDistribution(["x1", "x2"], self.pdf2)
+        self.assertEqual(phi2.variables, ["x1", "x2"])
         self.assertEqual(phi2._pdf, self.pdf2)
 
-        phi3 = CustomDistribution(['x', 'y', 'z'], self.pdf3)
-        self.assertEqual(phi3.variables, ['x', 'y', 'z'])
+        phi3 = CustomDistribution(["x", "y", "z"], self.pdf3)
+        self.assertEqual(phi3.variables, ["x", "y", "z"])
         self.assertEqual(phi3._pdf, self.pdf3)
 
     def test_class_init_typeerror(self):
-        self.assertRaises(TypeError, CustomDistribution, 'x y', self.pdf1)
-        self.assertRaises(TypeError, CustomDistribution, 'x', self.pdf1)
+        self.assertRaises(TypeError, CustomDistribution, "x y", self.pdf1)
+        self.assertRaises(TypeError, CustomDistribution, "x", self.pdf1)
 
-        self.assertRaises(TypeError, CustomDistribution, 'x1 x2', self.pdf2)
-        self.assertRaises(TypeError, CustomDistribution, 'x1', self.pdf1)
+        self.assertRaises(TypeError, CustomDistribution, "x1 x2", self.pdf2)
+        self.assertRaises(TypeError, CustomDistribution, "x1", self.pdf1)
 
-        self.assertRaises(TypeError, CustomDistribution, 'x y z', self.pdf3)
-        self.assertRaises(TypeError, CustomDistribution, 'x', self.pdf3)
+        self.assertRaises(TypeError, CustomDistribution, "x y z", self.pdf3)
+        self.assertRaises(TypeError, CustomDistribution, "x", self.pdf3)
 
-        self.assertRaises(TypeError, CustomDistribution, set(['x', 'y']), self.pdf1)
-        self.assertRaises(TypeError, CustomDistribution, {'x': 1, 'y': 2}, self.pdf1)
-
-        self.assertRaises(TypeError, CustomDistribution, set(['x1', 'x2']), self.pdf2)
-        self.assertRaises(TypeError, CustomDistribution, {'x1': 1, 'x2': 2}, self.pdf1)
-
-        self.assertRaises(TypeError, CustomDistribution, set(['x', 'y', 'z']), self.pdf3)
-        self.assertRaises(TypeError, CustomDistribution, {'x': 1, 'y': 2, 'z': 3}, self.pdf3)
+        self.assertRaises(TypeError, CustomDistribution, set(["x", "y"]), self.pdf1)
+        self.assertRaises(TypeError, CustomDistribution, {"x": 1, "y": 2}, self.pdf1)
+
+        self.assertRaises(TypeError, CustomDistribution, set(["x1", "x2"]), self.pdf2)
+        self.assertRaises(TypeError, CustomDistribution, {"x1": 1, "x2": 2}, self.pdf1)
+
+        self.assertRaises(
+            TypeError, CustomDistribution, set(["x", "y", "z"]), self.pdf3
+        )
+        self.assertRaises(
+            TypeError, CustomDistribution, {"x": 1, "y": 2, "z": 3}, self.pdf3
+        )
 
     def test_class_init_valueerror(self):
-        self.assertRaises(ValueError, CustomDistribution, ['x', 'x'], self.pdf1)
-        self.assertRaises(ValueError, CustomDistribution, ['x', 'y', 'y'], self.pdf1)
+        self.assertRaises(ValueError, CustomDistribution, ["x", "x"], self.pdf1)
+        self.assertRaises(ValueError, CustomDistribution, ["x", "y", "y"], self.pdf1)
 
-        self.assertRaises(ValueError, CustomDistribution, ['x1', 'x1'], self.pdf2)
-        self.assertRaises(ValueError, CustomDistribution, ['x1', 'x2', 'x2'], self.pdf2)
+        self.assertRaises(ValueError, CustomDistribution, ["x1", "x1"], self.pdf2)
+        self.assertRaises(ValueError, CustomDistribution, ["x1", "x2", "x2"], self.pdf2)
 
-        self.assertRaises(ValueError, CustomDistribution, ['x', 'x'], self.pdf1)
-        self.assertRaises(ValueError, CustomDistribution, ['x', 'y', 'y', 'z', 'z'], self.pdf1)
+        self.assertRaises(ValueError, CustomDistribution, ["x", "x"], self.pdf1)
+        self.assertRaises(
+            ValueError, CustomDistribution, ["x", "y", "y", "z", "z"], self.pdf1
+        )
 
 
 class TestCustomDistributionMethods(unittest.TestCase):
     def pdf1(self, x, y):
         return np.power(x, 1) * np.power(y, 2) / beta(x, y)
 
     def pdf2(self, x1, x2):
         return multivariate_normal.pdf([x1, x2], [0, 0], [[1, 0], [0, 1]])
 
     def pdf3(self, x, y, z):
         return z * (np.power(x, 1) * np.power(y, 2)) / beta(x, y)
 
     def pdf4(self, x1, x2, x3):
-        return multivariate_normal.pdf([x1, x2, x3], [0, 0, 0],
-                                       [[1, 0, 0], [0, 1, 0], [0, 0, 1]])
+        return multivariate_normal.pdf(
+            [x1, x2, x3], [0, 0, 0], [[1, 0, 0], [0, 1, 0], [0, 0, 1]]
+        )
 
     def setUp(self):
-        self.phi1 = CustomDistribution(['x', 'y'], self.pdf1)
-        self.phi2 = CustomDistribution(['x1', 'x2'], self.pdf2)
-        self.phi3 = CustomDistribution(['x', 'y', 'z'], self.pdf3)
-        self.phi4 = CustomDistribution(['x1', 'x2', 'x3'], self.pdf4)
+        self.phi1 = CustomDistribution(["x", "y"], self.pdf1)
+        self.phi2 = CustomDistribution(["x1", "x2"], self.pdf2)
+        self.phi3 = CustomDistribution(["x", "y", "z"], self.pdf3)
+        self.phi4 = CustomDistribution(["x1", "x2", "x3"], self.pdf4)
 
     def test_variables(self):
         self.assertEqual(self.phi1.variables, self.phi1._variables)
         self.assertEqual(self.phi2.variables, self.phi2._variables)
         self.assertEqual(self.phi3.variables, self.phi3._variables)
 
     def test_assignment(self):
         self.assertEqual(self.phi1.assignment(1.212, 2), self.pdf1(1.212, 2))
         self.assertEqual(self.phi2.assignment(1, -2.231), self.pdf2(1, -2.231))
-        self.assertEqual(self.phi3.assignment(1.212, 2.213, -3), self.pdf3(1.212, 2.213, -3))
+        self.assertEqual(
+            self.phi3.assignment(1.212, 2.213, -3), self.pdf3(1.212, 2.213, -3)
+        )
 
     def test_reduce(self):
         phi1 = self.phi1.copy()
-        phi1.reduce([('x', 1)])
+        phi1.reduce([("x", 1)])
 
         def reduced_pdf1(y):
             return (np.power(1, 1) * np.power(y, 2)) / beta(1, y)
 
-        self.assertEqual(phi1.variables, ['y'])
+        self.assertEqual(phi1.variables, ["y"])
         for inp in np.random.rand(4):
             self.assertEqual(phi1._pdf(inp), reduced_pdf1(inp))
             self.assertEqual(phi1._pdf(y=inp), reduced_pdf1(inp))
 
-        phi1 = self.phi1.reduce([('x', 1)], inplace=False)
-        self.assertEqual(phi1.variables, ['y'])
+        phi1 = self.phi1.reduce([("x", 1)], inplace=False)
+        self.assertEqual(phi1.variables, ["y"])
         for inp in np.random.rand(4):
             self.assertEqual(phi1._pdf(inp), reduced_pdf1(inp))
             self.assertEqual(phi1._pdf(y=inp), reduced_pdf1(inp))
 
         phi2 = self.phi2.copy()
-        phi2.reduce([('x2', 7.213)])
+        phi2.reduce([("x2", 7.213)])
 
         def reduced_pdf2(x1):
             return multivariate_normal.pdf([x1, 7.213], [0, 0], [[1, 0], [0, 1]])
 
-        self.assertEqual(phi2.variables, ['x1'])
+        self.assertEqual(phi2.variables, ["x1"])
         for inp in np.random.rand(4):
             self.assertEqual(phi2._pdf(inp), reduced_pdf2(inp))
             self.assertEqual(phi2._pdf(x1=inp), reduced_pdf2(inp))
 
-        phi2 = self.phi2.reduce([('x2', 7.213)], inplace=False)
-        self.assertEqual(phi2.variables, ['x1'])
+        phi2 = self.phi2.reduce([("x2", 7.213)], inplace=False)
+        self.assertEqual(phi2.variables, ["x1"])
         for inp in np.random.rand(4):
             self.assertEqual(phi2._pdf(inp), reduced_pdf2(inp))
             self.assertEqual(phi2._pdf(x1=inp), reduced_pdf2(inp))
 
         phi3 = self.phi3.copy()
-        phi3.reduce([('y', 0.112), ('z', 23)])
+        phi3.reduce([("y", 0.112), ("z", 23)])
 
         def reduced_pdf4(x):
-            return 23*(np.power(x, 1)*np.power(0.112, 2))/beta(x, 0.112)
+            return 23 * (np.power(x, 1) * np.power(0.112, 2)) / beta(x, 0.112)
 
-        self.assertEqual(phi3.variables, ['x'])
+        self.assertEqual(phi3.variables, ["x"])
         for inp in np.random.rand(4):
             self.assertEqual(phi3._pdf(inp), reduced_pdf4(inp))
             self.assertEqual(phi3._pdf(x=inp), reduced_pdf4(inp))
 
         phi3 = self.phi3.copy()
-        phi3.reduce([('y', 0.112)])
+        phi3.reduce([("y", 0.112)])
 
         def reduced_pdf3(x, z):
-            return z*(np.power(x, 1)*np.power(0.112, 2))/beta(x, 0.112)
+            return z * (np.power(x, 1) * np.power(0.112, 2)) / beta(x, 0.112)
 
-        self.assertEqual(phi3.variables, ['x', 'z'])
+        self.assertEqual(phi3.variables, ["x", "z"])
         for inp in np.random.rand(4, 2):
-            self.assertEqual(phi3._pdf(inp[0], inp[1]),
-                             reduced_pdf3(inp[0], inp[1]))
-            self.assertEqual(phi3._pdf(x=inp[0], z=inp[1]),
-                             reduced_pdf3(inp[0], inp[1]))
+            self.assertEqual(phi3._pdf(inp[0], inp[1]), reduced_pdf3(inp[0], inp[1]))
+            self.assertEqual(
+                phi3._pdf(x=inp[0], z=inp[1]), reduced_pdf3(inp[0], inp[1])
+            )
 
-        phi3 = self.phi3.reduce([('y', 0.112)], inplace=False)
-        self.assertEqual(phi3.variables, ['x', 'z'])
+        phi3 = self.phi3.reduce([("y", 0.112)], inplace=False)
+        self.assertEqual(phi3.variables, ["x", "z"])
         for inp in np.random.rand(4, 2):
-            self.assertEqual(phi3._pdf(inp[0], inp[1]),
-                             reduced_pdf3(inp[0], inp[1]))
-            self.assertEqual(phi3._pdf(x=inp[0], z=inp[1]),
-                             reduced_pdf3(inp[0], inp[1]))
-            self.assertEqual(phi3._pdf(inp[0], z=inp[1]),
-                             reduced_pdf3(inp[0], inp[1]))
+            self.assertEqual(phi3._pdf(inp[0], inp[1]), reduced_pdf3(inp[0], inp[1]))
+            self.assertEqual(
+                phi3._pdf(x=inp[0], z=inp[1]), reduced_pdf3(inp[0], inp[1])
+            )
+            self.assertEqual(phi3._pdf(inp[0], z=inp[1]), reduced_pdf3(inp[0], inp[1]))
 
-        phi3 = self.phi3.reduce([('y', 0.112), ('z', 23)], inplace=False)
-        self.assertEqual(phi3.variables, ['x'])
+        phi3 = self.phi3.reduce([("y", 0.112), ("z", 23)], inplace=False)
+        self.assertEqual(phi3.variables, ["x"])
         for inp in np.random.rand(4):
             self.assertEqual(phi3._pdf(inp), reduced_pdf4(inp))
             self.assertEqual(phi3._pdf(x=inp), reduced_pdf4(inp))
 
     def test_reduce_error(self):
-        self.assertRaises(TypeError, self.phi1.reduce, 'x1')
-        self.assertRaises(TypeError, self.phi1.reduce, set(['x', 'y']))
-        self.assertRaises(TypeError, self.phi1.reduce, {'x': 1, 'y': 1})
-
-        self.assertRaises(TypeError, self.phi4.reduce, 'x4')
-        self.assertRaises(TypeError, self.phi4.reduce, set(['x1', 'x2', 'x3']))
-        self.assertRaises(TypeError, self.phi4.reduce, {'x1': 1, 'x2': 1, 'x3': 1})
-
-        self.assertRaises(ValueError, self.phi1.reduce, [('z', 3)])
-        self.assertRaises(ValueError, self.phi1.reduce, [('x', 0), ('y', 1), ('z', 4)])
-
-        self.assertRaises(ValueError, self.phi4.reduce, [('x4', 7)])
-        self.assertRaises(ValueError, self.phi4.reduce, [('x1', 1), ('x2', 2), ('x3', 3), ('x4', 4)])
+        self.assertRaises(TypeError, self.phi1.reduce, "x1")
+        self.assertRaises(TypeError, self.phi1.reduce, set(["x", "y"]))
+        self.assertRaises(TypeError, self.phi1.reduce, {"x": 1, "y": 1})
+
+        self.assertRaises(TypeError, self.phi4.reduce, "x4")
+        self.assertRaises(TypeError, self.phi4.reduce, set(["x1", "x2", "x3"]))
+        self.assertRaises(TypeError, self.phi4.reduce, {"x1": 1, "x2": 1, "x3": 1})
+
+        self.assertRaises(ValueError, self.phi1.reduce, [("z", 3)])
+        self.assertRaises(ValueError, self.phi1.reduce, [("x", 0), ("y", 1), ("z", 4)])
+
+        self.assertRaises(ValueError, self.phi4.reduce, [("x4", 7)])
+        self.assertRaises(
+            ValueError, self.phi4.reduce, [("x1", 1), ("x2", 2), ("x3", 3), ("x4", 4)]
+        )
 
     def test_marginalize(self):
         phi2 = self.phi2.copy()
-        phi2.marginalize(['x2'])
-        self.assertEqual(phi2.variables, ['x1'])
+        phi2.marginalize(["x2"])
+        self.assertEqual(phi2.variables, ["x1"])
         for inp in np.random.rand(4):
-            np_test.assert_almost_equal(phi2._pdf(inp),
-                                        multivariate_normal.pdf([inp], [0], [[1]]))
+            np_test.assert_almost_equal(
+                phi2._pdf(inp), multivariate_normal.pdf([inp], [0], [[1]])
+            )
 
-        phi2 = self.phi2.marginalize(['x2'], inplace=False)
-        self.assertEqual(phi2.variables, ['x1'])
+        phi2 = self.phi2.marginalize(["x2"], inplace=False)
+        self.assertEqual(phi2.variables, ["x1"])
         for inp in np.random.rand(4):
-            np_test.assert_almost_equal(phi2._pdf(inp),
-                                        multivariate_normal.pdf([inp], [0], [[1]]))
+            np_test.assert_almost_equal(
+                phi2._pdf(inp), multivariate_normal.pdf([inp], [0], [[1]])
+            )
 
         phi4 = self.phi4.copy()
-        phi4.marginalize(['x2'])
+        phi4.marginalize(["x2"])
 
-        self.assertEqual(phi4.variables, ['x1', 'x3'])
+        self.assertEqual(phi4.variables, ["x1", "x3"])
         for inp in np.random.rand(4, 2):
             np_test.assert_almost_equal(
-                phi4._pdf(inp[0], inp[1]), multivariate_normal.pdf(
-                    [inp[0], inp[1]], [0, 0], [[1, 0], [0, 1]]))
+                phi4._pdf(inp[0], inp[1]),
+                multivariate_normal.pdf([inp[0], inp[1]], [0, 0], [[1, 0], [0, 1]]),
+            )
 
-        phi4.marginalize(['x3'])
-        self.assertEqual(phi4.variables, ['x1'])
+        phi4.marginalize(["x3"])
+        self.assertEqual(phi4.variables, ["x1"])
         for inp in np.random.rand(1):
-            np_test.assert_almost_equal(phi4._pdf(inp),
-                                        multivariate_normal.pdf([inp], [0], [[1]]))
+            np_test.assert_almost_equal(
+                phi4._pdf(inp), multivariate_normal.pdf([inp], [0], [[1]])
+            )
 
-        phi4 = self.phi4.marginalize(['x2'], inplace=False)
-        self.assertEqual(phi4.variables, ['x1', 'x3'])
+        phi4 = self.phi4.marginalize(["x2"], inplace=False)
+        self.assertEqual(phi4.variables, ["x1", "x3"])
         for inp in np.random.rand(4, 2):
-            np_test.assert_almost_equal(phi4._pdf(inp[0], inp[1]),
-                                        multivariate_normal.pdf([inp[0], inp[1]], [0, 0], [[1, 0], [0, 1]]))
+            np_test.assert_almost_equal(
+                phi4._pdf(inp[0], inp[1]),
+                multivariate_normal.pdf([inp[0], inp[1]], [0, 0], [[1, 0], [0, 1]]),
+            )
 
-        phi4 = phi4.marginalize(['x3'], inplace=False)
-        self.assertEqual(phi4.variables, ['x1'])
+        phi4 = phi4.marginalize(["x3"], inplace=False)
+        self.assertEqual(phi4.variables, ["x1"])
         for inp in np.random.rand(1):
-            np_test.assert_almost_equal(phi4._pdf(inp),
-                                        multivariate_normal.pdf([inp], [0], [[1]]))
+            np_test.assert_almost_equal(
+                phi4._pdf(inp), multivariate_normal.pdf([inp], [0], [[1]])
+            )
 
     def test_marginalize_error(self):
-        self.assertRaises(TypeError, self.phi1.marginalize, 'x1')
-        self.assertRaises(TypeError, self.phi1.marginalize, set(['x', 'y']))
-        self.assertRaises(TypeError, self.phi1.marginalize, {'x': 1, 'y': 1})
-
-        self.assertRaises(TypeError, self.phi4.marginalize, 'x4')
-        self.assertRaises(TypeError, self.phi4.marginalize, set(['x1', 'x2', 'x3']))
-        self.assertRaises(TypeError, self.phi4.marginalize, {'x1': 1, 'x2': 1, 'x3': 1})
+        self.assertRaises(TypeError, self.phi1.marginalize, "x1")
+        self.assertRaises(TypeError, self.phi1.marginalize, set(["x", "y"]))
+        self.assertRaises(TypeError, self.phi1.marginalize, {"x": 1, "y": 1})
+
+        self.assertRaises(TypeError, self.phi4.marginalize, "x4")
+        self.assertRaises(TypeError, self.phi4.marginalize, set(["x1", "x2", "x3"]))
+        self.assertRaises(TypeError, self.phi4.marginalize, {"x1": 1, "x2": 1, "x3": 1})
 
-        self.assertRaises(ValueError, self.phi1.marginalize, ['z'])
-        self.assertRaises(ValueError, self.phi1.marginalize, ['x', 'y', 'z'])
+        self.assertRaises(ValueError, self.phi1.marginalize, ["z"])
+        self.assertRaises(ValueError, self.phi1.marginalize, ["x", "y", "z"])
 
-        self.assertRaises(ValueError, self.phi4.marginalize, ['x4'])
-        self.assertRaises(ValueError, self.phi4.marginalize, ['x1', 'x2', 'x3', 'x4'])
+        self.assertRaises(ValueError, self.phi4.marginalize, ["x4"])
+        self.assertRaises(ValueError, self.phi4.marginalize, ["x1", "x2", "x3", "x4"])
 
     def test_normalize(self):
         def pdf2(x1, x2):
             return 2 * self.pdf2(x1, x2)
 
-        phi2 = CustomDistribution(['x1', 'x2'], pdf2)
+        phi2 = CustomDistribution(["x1", "x2"], pdf2)
         phi4 = phi2.copy()
 
         phi4.normalize()
         self.assertEqual(phi4.variables, phi2.variables)
         for inp in np.random.rand(1, 2):
-            np_test.assert_almost_equal(phi4._pdf(inp[0], inp[1]),
-                                        self.pdf2(inp[0], inp[1]))
+            np_test.assert_almost_equal(
+                phi4._pdf(inp[0], inp[1]), self.pdf2(inp[0], inp[1])
+            )
 
         phi4 = phi2.normalize(inplace=False)
         self.assertEqual(phi4.variables, phi4.variables)
         for inp in np.random.rand(1, 2):
-            np_test.assert_almost_equal(phi4._pdf(inp[0], inp[1]),
-                                        self.pdf2(inp[0], inp[1]))
+            np_test.assert_almost_equal(
+                phi4._pdf(inp[0], inp[1]), self.pdf2(inp[0], inp[1])
+            )
 
     def test_operate(self):
         phi1 = self.phi1.copy()
-        phi1._operate(self.phi2, 'product')
-        self.assertEqual(phi1.variables, ['x', 'y', 'x1', 'x2'])
+        phi1._operate(self.phi2, "product")
+        self.assertEqual(phi1.variables, ["x", "y", "x1", "x2"])
         for inp in np.random.rand(4, 4):
             self.assertEqual(
                 phi1._pdf(*inp),
-                self.phi1._pdf(inp[0], inp[1]) * self.phi2._pdf(inp[2], inp[3]))
+                self.phi1._pdf(inp[0], inp[1]) * self.phi2._pdf(inp[2], inp[3]),
+            )
 
-        phi1 = self.phi1._operate(self.phi2, 'product', inplace=False)
-        self.assertEqual(phi1.variables, ['x', 'y', 'x1', 'x2'])
+        phi1 = self.phi1._operate(self.phi2, "product", inplace=False)
+        self.assertEqual(phi1.variables, ["x", "y", "x1", "x2"])
         for inp in np.random.rand(4, 4):
             self.assertEqual(
                 phi1._pdf(*inp),
-                self.phi1._pdf(inp[0], inp[1]) * self.phi2._pdf(inp[2], inp[3]))
+                self.phi1._pdf(inp[0], inp[1]) * self.phi2._pdf(inp[2], inp[3]),
+            )
 
         phi1 = self.phi1 * self.phi2
-        self.assertEqual(phi1.variables, ['x', 'y', 'x1', 'x2'])
+        self.assertEqual(phi1.variables, ["x", "y", "x1", "x2"])
         for inp in np.random.rand(4, 4):
             self.assertEqual(
                 phi1._pdf(*inp),
-                self.phi1._pdf(inp[0], inp[1]) * self.phi2._pdf(inp[2], inp[3]))
+                self.phi1._pdf(inp[0], inp[1]) * self.phi2._pdf(inp[2], inp[3]),
+            )
 
         phi3 = self.phi3.copy()
-        phi3._operate(self.phi1, 'product')
-        self.assertEqual(phi3.variables, ['x', 'y', 'z'])
+        phi3._operate(self.phi1, "product")
+        self.assertEqual(phi3.variables, ["x", "y", "z"])
         for inp in np.random.rand(4, 3):
-            self.assertEqual(phi3._pdf(*inp), self.phi3._pdf(*inp) *
-                             self.phi1._pdf(inp[0], inp[1]))
+            self.assertEqual(
+                phi3._pdf(*inp), self.phi3._pdf(*inp) * self.phi1._pdf(inp[0], inp[1])
+            )
 
-        phi3 = self.phi3._operate(self.phi1, 'product', inplace=False)
-        self.assertEqual(phi3.variables, ['x', 'y', 'z'])
+        phi3 = self.phi3._operate(self.phi1, "product", inplace=False)
+        self.assertEqual(phi3.variables, ["x", "y", "z"])
         for inp in np.random.rand(4, 3):
-            self.assertEqual(phi3._pdf(*inp), self.phi3._pdf(*inp) *
-                             self.phi1._pdf(inp[0], inp[1]))
+            self.assertEqual(
+                phi3._pdf(*inp), self.phi3._pdf(*inp) * self.phi1._pdf(inp[0], inp[1])
+            )
 
         phi3 = self.phi3 * self.phi1
-        self.assertEqual(phi3.variables, ['x', 'y', 'z'])
+        self.assertEqual(phi3.variables, ["x", "y", "z"])
         for inp in np.random.rand(4, 3):
-            self.assertEqual(phi3._pdf(*inp), self.phi3._pdf(*inp) *
-                             self.phi1._pdf(inp[0], inp[1]))
+            self.assertEqual(
+                phi3._pdf(*inp), self.phi3._pdf(*inp) * self.phi1._pdf(inp[0], inp[1])
+            )
 
         phi3 = self.phi3.copy()
-        phi3._operate(self.phi1, 'divide')
-        self.assertEqual(phi3.variables, ['x', 'y', 'z'])
+        phi3._operate(self.phi1, "divide")
+        self.assertEqual(phi3.variables, ["x", "y", "z"])
         for inp in np.random.rand(4, 3):
-            self.assertEqual(phi3._pdf(*inp), self.phi3._pdf(*inp) /
-                             self.phi1._pdf(inp[0], inp[1]))
+            self.assertEqual(
+                phi3._pdf(*inp), self.phi3._pdf(*inp) / self.phi1._pdf(inp[0], inp[1])
+            )
 
-        phi3 = self.phi3._operate(self.phi1, 'divide', inplace=False)
-        self.assertEqual(phi3.variables, ['x', 'y', 'z'])
+        phi3 = self.phi3._operate(self.phi1, "divide", inplace=False)
+        self.assertEqual(phi3.variables, ["x", "y", "z"])
         for inp in np.random.rand(4, 3):
-            self.assertEqual(phi3._pdf(*inp), self.phi3._pdf(*inp) /
-                             self.phi1._pdf(inp[0], inp[1]))
+            self.assertEqual(
+                phi3._pdf(*inp), self.phi3._pdf(*inp) / self.phi1._pdf(inp[0], inp[1])
+            )
 
         phi3 = self.phi3 / self.phi1
-        self.assertEqual(phi3.variables, ['x', 'y', 'z'])
+        self.assertEqual(phi3.variables, ["x", "y", "z"])
         for inp in np.random.rand(4, 3):
-            self.assertEqual(phi3._pdf(*inp), self.phi3._pdf(*inp) /
-                             self.phi1._pdf(inp[0], inp[1]))
+            self.assertEqual(
+                phi3._pdf(*inp), self.phi3._pdf(*inp) / self.phi1._pdf(inp[0], inp[1])
+            )
 
         phi4 = self.phi4.copy()
-        phi4._operate(self.phi2, 'product')
-        self.assertEqual(phi4.variables, ['x1', 'x2', 'x3'])
+        phi4._operate(self.phi2, "product")
+        self.assertEqual(phi4.variables, ["x1", "x2", "x3"])
         for inp in np.random.rand(4, 3):
-            self.assertEqual(phi4._pdf(*inp), self.phi4._pdf(*inp) *
-                             self.phi2._pdf(inp[0], inp[1]))
+            self.assertEqual(
+                phi4._pdf(*inp), self.phi4._pdf(*inp) * self.phi2._pdf(inp[0], inp[1])
+            )
 
-        phi4 = self.phi4._operate(self.phi2, 'product', inplace=False)
-        self.assertEqual(phi4.variables, ['x1', 'x2', 'x3'])
+        phi4 = self.phi4._operate(self.phi2, "product", inplace=False)
+        self.assertEqual(phi4.variables, ["x1", "x2", "x3"])
         for inp in np.random.rand(4, 3):
-            self.assertEqual(phi4._pdf(*inp), self.phi4._pdf(*inp) *
-                             self.phi2._pdf(inp[0], inp[1]))
+            self.assertEqual(
+                phi4._pdf(*inp), self.phi4._pdf(*inp) * self.phi2._pdf(inp[0], inp[1])
+            )
 
         phi4 = self.phi4 * self.phi2
-        self.assertEqual(phi4.variables, ['x1', 'x2', 'x3'])
+        self.assertEqual(phi4.variables, ["x1", "x2", "x3"])
         for inp in np.random.rand(4, 3):
-            self.assertEqual(phi4._pdf(*inp), self.phi4._pdf(*inp) *
-                             self.phi2._pdf(inp[0], inp[1]))
+            self.assertEqual(
+                phi4._pdf(*inp), self.phi4._pdf(*inp) * self.phi2._pdf(inp[0], inp[1])
+            )
 
         phi4 = self.phi4.copy()
-        phi4._operate(self.phi2, 'divide')
-        self.assertEqual(phi4.variables, ['x1', 'x2', 'x3'])
+        phi4._operate(self.phi2, "divide")
+        self.assertEqual(phi4.variables, ["x1", "x2", "x3"])
         for inp in np.random.rand(4, 3):
-            self.assertEqual(phi4._pdf(*inp), self.phi4._pdf(*inp) /
-                             self.phi2._pdf(inp[0], inp[1]))
+            self.assertEqual(
+                phi4._pdf(*inp), self.phi4._pdf(*inp) / self.phi2._pdf(inp[0], inp[1])
+            )
 
-        phi4 = self.phi4._operate(self.phi2, 'divide', inplace=False)
-        self.assertEqual(phi4.variables, ['x1', 'x2', 'x3'])
+        phi4 = self.phi4._operate(self.phi2, "divide", inplace=False)
+        self.assertEqual(phi4.variables, ["x1", "x2", "x3"])
         for inp in np.random.rand(4, 3):
-            self.assertEqual(phi4._pdf(*inp), self.phi4._pdf(*inp) /
-                             self.phi2._pdf(inp[0], inp[1]))
+            self.assertEqual(
+                phi4._pdf(*inp), self.phi4._pdf(*inp) / self.phi2._pdf(inp[0], inp[1])
+            )
 
         phi4 = self.phi4 / self.phi2
-        self.assertEqual(phi4.variables, ['x1', 'x2', 'x3'])
+        self.assertEqual(phi4.variables, ["x1", "x2", "x3"])
         for inp in np.random.rand(4, 3):
-            self.assertEqual(phi4._pdf(*inp), self.phi4._pdf(*inp) /
-                             self.phi2._pdf(inp[0], inp[1]))
+            self.assertEqual(
+                phi4._pdf(*inp), self.phi4._pdf(*inp) / self.phi2._pdf(inp[0], inp[1])
+            )
 
     def test_operate_error(self):
-        self.assertRaises(TypeError, self.phi1._operate, 1, 'product')
-        self.assertRaises(TypeError, self.phi1._operate, 1, 'divide')
-        self.assertRaises(TypeError, self.phi1._operate, '1', 'product')
-        self.assertRaises(TypeError, self.phi1._operate, '1', 'divide')
-        self.assertRaises(TypeError, self.phi1._operate,
-                          self.phi2._pdf, 'product')
-        self.assertRaises(TypeError, self.phi1._operate,
-                          self.phi2._pdf, 'divide')
-        self.assertRaises(TypeError, self.phi1._operate, [1], 'product')
-        self.assertRaises(TypeError, self.phi1._operate, [1], 'divide')
-
-        self.assertRaises(TypeError, self.phi4._operate, 1, 'product')
-        self.assertRaises(TypeError, self.phi4._operate, 1, 'divide')
-        self.assertRaises(TypeError, self.phi4._operate, '1', 'product')
-        self.assertRaises(TypeError, self.phi4._operate, '1', 'divide')
-        self.assertRaises(TypeError, self.phi4._operate,
-                          self.phi2._pdf, 'product')
-        self.assertRaises(TypeError, self.phi4._operate,
-                          self.phi2._pdf, 'divide')
-        self.assertRaises(TypeError, self.phi4._operate, [1], 'product')
-        self.assertRaises(TypeError, self.phi4._operate, [1], 'divide')
-
-        self.assertRaises(TypeError, self.phi1._operate, 1, 'product', False)
-        self.assertRaises(TypeError, self.phi1._operate, 1, 'divide', False)
-        self.assertRaises(TypeError, self.phi1._operate, '1', 'product', False)
-        self.assertRaises(TypeError, self.phi1._operate, '1', 'divide', False)
-        self.assertRaises(TypeError, self.phi1._operate,
-                          self.phi2._pdf, 'product', False)
-        self.assertRaises(TypeError, self.phi1._operate,
-                          self.phi2._pdf, 'divide', False)
-        self.assertRaises(TypeError, self.phi1._operate, [1], 'product', False)
-        self.assertRaises(TypeError, self.phi1._operate, [1], 'divide', False)
-
-        self.assertRaises(TypeError, self.phi4._operate, 1, 'product', False)
-        self.assertRaises(TypeError, self.phi4._operate, 1, 'divide', False)
-        self.assertRaises(TypeError, self.phi4._operate, '1', 'product', False)
-        self.assertRaises(TypeError, self.phi4._operate, '1', 'divide', False)
-        self.assertRaises(TypeError, self.phi4._operate,
-                          self.phi2._pdf, 'product', False)
-        self.assertRaises(TypeError, self.phi4._operate,
-                          self.phi2._pdf, 'divide', False)
-        self.assertRaises(TypeError, self.phi4._operate, [1], 'product', False)
-        self.assertRaises(TypeError, self.phi4._operate, [1], 'divide', False)
+        self.assertRaises(TypeError, self.phi1._operate, 1, "product")
+        self.assertRaises(TypeError, self.phi1._operate, 1, "divide")
+        self.assertRaises(TypeError, self.phi1._operate, "1", "product")
+        self.assertRaises(TypeError, self.phi1._operate, "1", "divide")
+        self.assertRaises(TypeError, self.phi1._operate, self.phi2._pdf, "product")
+        self.assertRaises(TypeError, self.phi1._operate, self.phi2._pdf, "divide")
+        self.assertRaises(TypeError, self.phi1._operate, [1], "product")
+        self.assertRaises(TypeError, self.phi1._operate, [1], "divide")
+
+        self.assertRaises(TypeError, self.phi4._operate, 1, "product")
+        self.assertRaises(TypeError, self.phi4._operate, 1, "divide")
+        self.assertRaises(TypeError, self.phi4._operate, "1", "product")
+        self.assertRaises(TypeError, self.phi4._operate, "1", "divide")
+        self.assertRaises(TypeError, self.phi4._operate, self.phi2._pdf, "product")
+        self.assertRaises(TypeError, self.phi4._operate, self.phi2._pdf, "divide")
+        self.assertRaises(TypeError, self.phi4._operate, [1], "product")
+        self.assertRaises(TypeError, self.phi4._operate, [1], "divide")
+
+        self.assertRaises(TypeError, self.phi1._operate, 1, "product", False)
+        self.assertRaises(TypeError, self.phi1._operate, 1, "divide", False)
+        self.assertRaises(TypeError, self.phi1._operate, "1", "product", False)
+        self.assertRaises(TypeError, self.phi1._operate, "1", "divide", False)
+        self.assertRaises(
+            TypeError, self.phi1._operate, self.phi2._pdf, "product", False
+        )
+        self.assertRaises(
+            TypeError, self.phi1._operate, self.phi2._pdf, "divide", False
+        )
+        self.assertRaises(TypeError, self.phi1._operate, [1], "product", False)
+        self.assertRaises(TypeError, self.phi1._operate, [1], "divide", False)
+
+        self.assertRaises(TypeError, self.phi4._operate, 1, "product", False)
+        self.assertRaises(TypeError, self.phi4._operate, 1, "divide", False)
+        self.assertRaises(TypeError, self.phi4._operate, "1", "product", False)
+        self.assertRaises(TypeError, self.phi4._operate, "1", "divide", False)
+        self.assertRaises(
+            TypeError, self.phi4._operate, self.phi2._pdf, "product", False
+        )
+        self.assertRaises(
+            TypeError, self.phi4._operate, self.phi2._pdf, "divide", False
+        )
+        self.assertRaises(TypeError, self.phi4._operate, [1], "product", False)
+        self.assertRaises(TypeError, self.phi4._operate, [1], "divide", False)
 
         self.assertRaises(ValueError, self.phi1.__truediv__, self.phi2)
         self.assertRaises(ValueError, self.phi1.__truediv__, self.phi3)
         self.assertRaises(ValueError, self.phi1.__truediv__, self.phi4)
         self.assertRaises(ValueError, self.phi2.__truediv__, self.phi3)
         self.assertRaises(ValueError, self.phi2.__truediv__, self.phi4)
 
@@ -414,38 +448,38 @@
         copy5 = copy2.copy()
 
         self.assertEqual(copy1.variables, copy4.variables)
         self.assertEqual(copy1._pdf, copy4._pdf)
         self.assertEqual(copy2.variables, copy5.variables)
         self.assertEqual(copy2._pdf, copy5._pdf)
 
-        copy1.variables = ['A', 'B']
+        copy1.variables = ["A", "B"]
         self.assertEqual(copy4.variables, self.phi1.variables)
 
         def pdf(a, b):
             return (a + b) / (a * a + b * b)
+
         copy1._pdf = pdf
         copy1_pdf = pdf
         self.assertEqual(copy4._pdf, self.phi1._pdf)
-        copy4.variables = ['X', 'Y']
-        self.assertEqual(copy1.variables, ['A', 'B'])
+        copy4.variables = ["X", "Y"]
+        self.assertEqual(copy1.variables, ["A", "B"])
         copy4._pdf = lambda a, b: a + b
         for inp in np.random.rand(4, 2):
-            self.assertEqual(copy1._pdf(inp[0], inp[1]),
-                             copy1_pdf(inp[0], inp[1]))
+            self.assertEqual(copy1._pdf(inp[0], inp[1]), copy1_pdf(inp[0], inp[1]))
 
-        copy2.reduce([('x', 7.7)])
+        copy2.reduce([("x", 7.7)])
 
         def reduced_pdf(y, z):
-            return z*(np.power(7.7, 1) * np.power(y, 2)) / beta(7.7, y)
+            return z * (np.power(7.7, 1) * np.power(y, 2)) / beta(7.7, y)
+
         self.assertEqual(copy5.variables, self.phi3.variables)
         self.assertEqual(copy5._pdf, self.phi3._pdf)
-        copy5.reduce([('x', 11), ('z', 13)])
-        self.assertEqual(copy2.variables, ['y', 'z'])
+        copy5.reduce([("x", 11), ("z", 13)])
+        self.assertEqual(copy2.variables, ["y", "z"])
         for inp in np.random.rand(4, 2):
-            self.assertEqual(copy2._pdf(inp[0], inp[1]),
-                             reduced_pdf(inp[0], inp[1]))
+            self.assertEqual(copy2._pdf(inp[0], inp[1]), reduced_pdf(inp[0], inp[1]))
 
     def tearDown(self):
         del self.phi1
         del self.phi2
         del self.phi3
```

### Comparing `pgmpy-0.1.7/pgmpy/tests/test_factors/test_discrete/test_Factor.py` & `pgmpy-0.1.9/pgmpy/tests/test_factors/test_discrete/test_Factor.py`

 * *Files 14% similar despite different names*

```diff
@@ -13,346 +13,595 @@
 from pgmpy.factors.discrete.CPD import TabularCPD
 from pgmpy.independencies import Independencies
 from pgmpy.models import BayesianModel
 from pgmpy.models import MarkovModel
 
 
 class TestFactorInit(unittest.TestCase):
-
     def test_class_init(self):
-        phi = DiscreteFactor(['x1', 'x2', 'x3'], [2, 2, 2], np.ones(8))
-        self.assertEqual(phi.variables, ['x1', 'x2', 'x3'])
+        phi = DiscreteFactor(
+            variables=["x1", "x2", "x3"], cardinality=[2, 2, 2], values=np.ones(8)
+        )
+        self.assertEqual(phi.variables, ["x1", "x2", "x3"])
         np_test.assert_array_equal(phi.cardinality, np.array([2, 2, 2]))
         np_test.assert_array_equal(phi.values, np.ones(8).reshape(2, 2, 2))
 
-    def test_class_init1(self):
-        phi = DiscreteFactor([1, 2, 3], [2, 3, 2], np.arange(12))
+    def test_class_init_int_var(self):
+        phi = DiscreteFactor(
+            variables=[1, 2, 3], cardinality=[2, 3, 2], values=np.arange(12)
+        )
         self.assertEqual(phi.variables, [1, 2, 3])
         np_test.assert_array_equal(phi.cardinality, np.array([2, 3, 2]))
         np_test.assert_array_equal(phi.values, np.arange(12).reshape(2, 3, 2))
 
+    def test_class_init_statenames(self):
+        phi = DiscreteFactor(
+            variables=["x1", "x2", "x3"],
+            cardinality=[2, 2, 2],
+            values=np.ones(8),
+            state_names={
+                "x1": ["sn0", "sn1"],
+                "x2": ["sn0", "sn1"],
+                "x3": ["sn0", "sn1"],
+            },
+        )
+        self.assertEqual(phi.variables, ["x1", "x2", "x3"])
+        np_test.assert_array_equal(phi.cardinality, np.array([2, 2, 2]))
+        np_test.assert_array_equal(phi.values, np.ones(8).reshape(2, 2, 2))
+
+    def test_class_init_repeated_statename(self):
+        self.assertRaises(
+            ValueError,
+            DiscreteFactor,
+            ["x1", "x2", "x3"],
+            [2, 2, 2],
+            np.ones(8),
+            {"x1": ["sn0", "sn0"], "x2": ["sn0", "sn1"], "x3": ["sn0", "sn1"]},
+        )
+
+    def test_class_init_nonlist_statename(self):
+        self.assertRaises(
+            ValueError,
+            DiscreteFactor,
+            ["x1", "x2", "x3"],
+            [2, 2, 2],
+            np.ones(8),
+            {"x1": {"sn0", "sn0"}, "x2": ["sn0", "sn1"], "x3": ["sn0", "sn1"]},
+        )
+
     def test_class_init_sizeerror(self):
-        self.assertRaises(ValueError, DiscreteFactor, ['x1', 'x2', 'x3'], [2, 2, 2], np.ones(9))
+        self.assertRaises(
+            ValueError, DiscreteFactor, ["x1", "x2", "x3"], [2, 2, 2], np.ones(9)
+        )
 
     def test_class_init_typeerror(self):
-        self.assertRaises(TypeError, DiscreteFactor, 'x1', [3], [1, 2, 3])
-        self.assertRaises(ValueError, DiscreteFactor, ['x1', 'x1', 'x3'], [2, 3, 2], range(12))
+        self.assertRaises(TypeError, DiscreteFactor, "x1", [3], [1, 2, 3])
+        self.assertRaises(
+            ValueError, DiscreteFactor, ["x1", "x1", "x3"], [2, 3, 2], range(12)
+        )
 
     def test_init_size_var_card_not_equal(self):
-        self.assertRaises(ValueError, DiscreteFactor, ['x1', 'x2'], [2], np.ones(2))
+        self.assertRaises(ValueError, DiscreteFactor, ["x1", "x2"], [2], np.ones(2))
 
 
 class TestFactorMethods(unittest.TestCase):
-
     def setUp(self):
-        self.phi = DiscreteFactor(['x1', 'x2', 'x3'], [2, 2, 2], np.random.uniform(5, 10, size=8))
-        self.phi1 = DiscreteFactor(['x1', 'x2', 'x3'], [2, 3, 2], range(12))
-        self.phi2 = DiscreteFactor([('x1', 0), ('x2', 0), ('x3', 0)], [2, 3, 2], range(12))
+        self.phi = DiscreteFactor(
+            variables=["x1", "x2", "x3"],
+            cardinality=[2, 2, 2],
+            values=np.random.uniform(5, 10, size=8),
+        )
+        self.phi_sn = DiscreteFactor(
+            variables=["x1", "x2", "x3"],
+            cardinality=[2, 2, 2],
+            values=np.random.uniform(5, 10, size=8),
+            state_names={
+                "x1": ("sn0", "sn1"),
+                "x2": ("sn0", "sn1"),
+                "x3": ("sn0", "sn1"),
+            },
+        )
+
+        self.phi1 = DiscreteFactor(
+            variables=["x1", "x2", "x3"], cardinality=[2, 3, 2], values=range(12)
+        )
+        self.phi1_sn = DiscreteFactor(
+            variables=["x1", "x2", "x3"],
+            cardinality=[2, 3, 2],
+            values=range(12),
+            state_names={
+                "x1": ("sn0", "sn1"),
+                "x2": ("sn0", "sn1", "sn2"),
+                "x3": ("sn0", "sn1"),
+            },
+        )
+
+        self.phi2 = DiscreteFactor(
+            variables=[("x1", 0), ("x2", 0), ("x3", 0)],
+            cardinality=[2, 3, 2],
+            values=range(12),
+        )
+
+        self.phi2_sn = DiscreteFactor(
+            variables=[("x1", 0), ("x2", 0), ("x3", 0)],
+            cardinality=[2, 3, 2],
+            values=range(12),
+            state_names={
+                "x1": ("sn0", "sn1"),
+                "x2": ("sn0", "sn1", "sn2"),
+                "x3": ("sn0", "sn1"),
+            },
+        )
+
         # This larger factor (phi3) caused a bug in reduce
         card3 = [3, 3, 3, 2, 2, 2, 2, 2, 2]
-        self.phi3 = DiscreteFactor(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'],
-                                   card3, np.arange(np.prod(card3), dtype=np.float))
-
-        self.tup1 = ('x1', 'x2')
-        self.tup2 = ('x2', 'x3')
-        self.tup3 = ('x3', (1, 'x4'))
-        self.phi4 = DiscreteFactor([self.tup1, self.tup2, self.tup3], [2, 3, 4], np.random.uniform(3, 10, size=24))
-        self.phi5 = DiscreteFactor([self.tup1, self.tup2, self.tup3], [2, 3, 4], range(24))
+        self.phi3 = DiscreteFactor(
+            ["A", "B", "C", "D", "E", "F", "G", "H", "I"],
+            card3,
+            np.arange(np.prod(card3), dtype=np.float),
+        )
+
+        self.tup1 = ("x1", "x2")
+        self.tup2 = ("x2", "x3")
+        self.tup3 = ("x3", (1, "x4"))
+        self.phi4 = DiscreteFactor(
+            [self.tup1, self.tup2, self.tup3],
+            [2, 3, 4],
+            np.random.uniform(3, 10, size=24),
+        )
+        self.phi5 = DiscreteFactor(
+            [self.tup1, self.tup2, self.tup3], [2, 3, 4], range(24)
+        )
 
         self.card6 = [4, 2, 1, 3, 5, 6]
-        self.phi6 = DiscreteFactor([self.tup1, self.tup2, self.tup3, self.tup1 + self.tup2,
-                                    self.tup2 + self.tup3, self.tup3 + self.tup1], self.card6,
-                                   np.arange(np.prod(self.card6), dtype=np.float))
-
-        self.var1 = 'x1'
-        self.var2 = ('x2', 1)
-        self.var3 = frozenset(['x1', 'x2'])
+        self.phi6 = DiscreteFactor(
+            [
+                self.tup1,
+                self.tup2,
+                self.tup3,
+                self.tup1 + self.tup2,
+                self.tup2 + self.tup3,
+                self.tup3 + self.tup1,
+            ],
+            self.card6,
+            np.arange(np.prod(self.card6), dtype=np.float),
+        )
+
+        self.var1 = "x1"
+        self.var2 = ("x2", 1)
+        self.var3 = frozenset(["x1", "x2"])
         self.phi7 = DiscreteFactor([self.var1, self.var2], [3, 2], [3, 2, 4, 5, 9, 8])
         self.phi8 = DiscreteFactor([self.var2, self.var3], [2, 2], [2, 1, 5, 6])
         self.phi9 = DiscreteFactor([self.var1, self.var3], [3, 2], [3, 2, 4, 5, 9, 8])
         self.phi10 = DiscreteFactor([self.var3], [2], [3, 6])
 
     def test_scope(self):
-        self.assertListEqual(self.phi.scope(), ['x1', 'x2', 'x3'])
-        self.assertListEqual(self.phi1.scope(), ['x1', 'x2', 'x3'])
+        self.assertListEqual(self.phi.scope(), ["x1", "x2", "x3"])
+        self.assertListEqual(self.phi_sn.scope(), ["x1", "x2", "x3"])
+
+        self.assertListEqual(self.phi1.scope(), ["x1", "x2", "x3"])
+        self.assertListEqual(self.phi1_sn.scope(), ["x1", "x2", "x3"])
 
         self.assertListEqual(self.phi4.scope(), [self.tup1, self.tup2, self.tup3])
 
     def test_assignment(self):
-        self.assertListEqual(self.phi.assignment([0]), [[('x1', 0), ('x2', 0), ('x3', 0)]])
-        self.assertListEqual(self.phi.assignment([4, 5, 6]), [[('x1', 1), ('x2', 0), ('x3', 0)],
-                                                              [('x1', 1), ('x2', 0), ('x3', 1)],
-                                                              [('x1', 1), ('x2', 1), ('x3', 0)]])
-
-        self.assertListEqual(self.phi1.assignment(np.array([4, 5, 6])), [[('x1', 0), ('x2', 2), ('x3', 0)],
-                                                                         [('x1', 0), ('x2', 2), ('x3', 1)],
-                                                                         [('x1', 1), ('x2', 0), ('x3', 0)]])
-        self.assertListEqual(self.phi4.assignment(np.array([11, 12, 23])),
-                             [[(self.tup1, 0), (self.tup2, 2), (self.tup3, 3)],
-                              [(self.tup1, 1), (self.tup2, 0), (self.tup3, 0)],
-                              [(self.tup1, 1), (self.tup2, 2), (self.tup3, 3)]])
+        self.assertListEqual(
+            self.phi.assignment([0]), [[("x1", 0), ("x2", 0), ("x3", 0)]]
+        )
+        self.assertListEqual(
+            self.phi_sn.assignment([0]), [[("x1", "sn0"), ("x2", "sn0"), ("x3", "sn0")]]
+        )
+
+        self.assertListEqual(
+            self.phi.assignment([4, 5, 6]),
+            [
+                [("x1", 1), ("x2", 0), ("x3", 0)],
+                [("x1", 1), ("x2", 0), ("x3", 1)],
+                [("x1", 1), ("x2", 1), ("x3", 0)],
+            ],
+        )
+
+        self.assertListEqual(
+            self.phi_sn.assignment([4, 5, 6]),
+            [
+                [("x1", "sn1"), ("x2", "sn0"), ("x3", "sn0")],
+                [("x1", "sn1"), ("x2", "sn0"), ("x3", "sn1")],
+                [("x1", "sn1"), ("x2", "sn1"), ("x3", "sn0")],
+            ],
+        )
+
+        self.assertListEqual(
+            self.phi1.assignment(np.array([4, 5, 6])),
+            [
+                [("x1", 0), ("x2", 2), ("x3", 0)],
+                [("x1", 0), ("x2", 2), ("x3", 1)],
+                [("x1", 1), ("x2", 0), ("x3", 0)],
+            ],
+        )
+
+        self.assertListEqual(
+            self.phi1_sn.assignment(np.array([4, 5, 6])),
+            [
+                [("x1", "sn0"), ("x2", "sn2"), ("x3", "sn0")],
+                [("x1", "sn0"), ("x2", "sn2"), ("x3", "sn1")],
+                [("x1", "sn1"), ("x2", "sn0"), ("x3", "sn0")],
+            ],
+        )
+
+        self.assertListEqual(
+            self.phi4.assignment(np.array([11, 12, 23])),
+            [
+                [(self.tup1, 0), (self.tup2, 2), (self.tup3, 3)],
+                [(self.tup1, 1), (self.tup2, 0), (self.tup3, 0)],
+                [(self.tup1, 1), (self.tup2, 2), (self.tup3, 3)],
+            ],
+        )
 
     def test_assignment_indexerror(self):
         self.assertRaises(IndexError, self.phi.assignment, [10])
+        self.assertRaises(IndexError, self.phi_sn.assignment, [10])
         self.assertRaises(IndexError, self.phi.assignment, [1, 3, 10, 5])
+        self.assertRaises(IndexError, self.phi_sn.assignment, [1, 3, 10, 5])
         self.assertRaises(IndexError, self.phi.assignment, np.array([1, 3, 10, 5]))
+        self.assertRaises(IndexError, self.phi_sn.assignment, np.array([1, 3, 10, 5]))
 
         self.assertRaises(IndexError, self.phi4.assignment, [2, 24])
         self.assertRaises(IndexError, self.phi4.assignment, np.array([24, 2, 4, 30]))
 
     def test_get_cardinality(self):
-        self.assertEqual(self.phi.get_cardinality(['x1']), {'x1': 2})
-        self.assertEqual(self.phi.get_cardinality(['x2']), {'x2': 2})
-        self.assertEqual(self.phi.get_cardinality(['x3']), {'x3': 2})
-        self.assertEqual(self.phi.get_cardinality(['x1', 'x2']), {'x1': 2, 'x2': 2})
-        self.assertEqual(self.phi.get_cardinality(['x1', 'x3']), {'x1': 2, 'x3': 2})
-        self.assertEqual(self.phi.get_cardinality(['x1', 'x2', 'x3']), {'x1': 2, 'x2': 2, 'x3': 2})
+        self.assertEqual(self.phi.get_cardinality(["x1"]), {"x1": 2})
+        self.assertEqual(self.phi_sn.get_cardinality(["x1"]), {"x1": 2})
 
-        self.assertEqual(self.phi4.get_cardinality([self.tup1, self.tup3]),
-                         {self.tup1: 2, self.tup3: 4})
+        self.assertEqual(self.phi.get_cardinality(["x2"]), {"x2": 2})
+        self.assertEqual(self.phi_sn.get_cardinality(["x2"]), {"x2": 2})
+
+        self.assertEqual(self.phi.get_cardinality(["x3"]), {"x3": 2})
+        self.assertEqual(self.phi_sn.get_cardinality(["x3"]), {"x3": 2})
+
+        self.assertEqual(self.phi.get_cardinality(["x1", "x2"]), {"x1": 2, "x2": 2})
+        self.assertEqual(self.phi_sn.get_cardinality(["x1", "x2"]), {"x1": 2, "x2": 2})
+
+        self.assertEqual(self.phi.get_cardinality(["x1", "x3"]), {"x1": 2, "x3": 2})
+        self.assertEqual(self.phi_sn.get_cardinality(["x1", "x3"]), {"x1": 2, "x3": 2})
+
+        self.assertEqual(
+            self.phi.get_cardinality(["x1", "x2", "x3"]), {"x1": 2, "x2": 2, "x3": 2}
+        )
+        self.assertEqual(
+            self.phi_sn.get_cardinality(["x1", "x2", "x3"]), {"x1": 2, "x2": 2, "x3": 2}
+        )
+
+        self.assertEqual(
+            self.phi4.get_cardinality([self.tup1, self.tup3]),
+            {self.tup1: 2, self.tup3: 4},
+        )
 
     def test_get_cardinality_scopeerror(self):
-        self.assertRaises(ValueError, self.phi.get_cardinality, ['x4'])
-        self.assertRaises(ValueError, self.phi4.get_cardinality, [('x1', 'x4')])
+        self.assertRaises(ValueError, self.phi.get_cardinality, ["x4"])
+        self.assertRaises(ValueError, self.phi4.get_cardinality, [("x1", "x4")])
 
-        self.assertRaises(ValueError, self.phi4.get_cardinality, [('x3', (2, 'x4'))])
+        self.assertRaises(ValueError, self.phi4.get_cardinality, [("x3", (2, "x4"))])
 
     def test_get_cardinality_typeerror(self):
-        self.assertRaises(TypeError, self.phi.get_cardinality, 'x1')
+        self.assertRaises(TypeError, self.phi.get_cardinality, "x1")
+        self.assertRaises(TypeError, self.phi_sn.get_cardinality, "x1")
 
     def test_marginalize(self):
-        self.phi1.marginalize(['x1'])
-        np_test.assert_array_equal(self.phi1.values, np.array([[6, 8],
-                                                               [10, 12],
-                                                               [14, 16]]))
-        self.phi1.marginalize(['x2'])
+        self.phi1.marginalize(["x1"])
+        np_test.assert_array_equal(
+            self.phi1.values, np.array([[6, 8], [10, 12], [14, 16]])
+        )
+        self.phi1_sn.marginalize(["x1"])
+        np_test.assert_array_equal(
+            self.phi1_sn.values, np.array([[6, 8], [10, 12], [14, 16]])
+        )
+
+        self.phi1.marginalize(["x2"])
         np_test.assert_array_equal(self.phi1.values, np.array([30, 36]))
-        self.phi1.marginalize(['x3'])
+        self.phi1_sn.marginalize(["x2"])
+        np_test.assert_array_equal(self.phi1_sn.values, np.array([30, 36]))
+
+        self.phi1.marginalize(["x3"])
         np_test.assert_array_equal(self.phi1.values, np.array(66))
+        self.phi1_sn.marginalize(["x3"])
+        np_test.assert_array_equal(self.phi1_sn.values, np.array(66))
 
         self.phi5.marginalize([self.tup1])
-        np_test.assert_array_equal(self.phi5.values, np.array([[12, 14, 16, 18],
-                                                               [20, 22, 24, 26],
-                                                               [28, 30, 32, 34]]))
+        np_test.assert_array_equal(
+            self.phi5.values,
+            np.array([[12, 14, 16, 18], [20, 22, 24, 26], [28, 30, 32, 34]]),
+        )
         self.phi5.marginalize([self.tup2])
         np_test.assert_array_equal(self.phi5.values, np.array([60, 66, 72, 78]))
 
         self.phi5.marginalize([self.tup3])
         np_test.assert_array_equal(self.phi5.values, np.array([276]))
 
     def test_marginalize_scopeerror(self):
-        self.assertRaises(ValueError, self.phi.marginalize, ['x4'])
-        self.phi.marginalize(['x1'])
-        self.assertRaises(ValueError, self.phi.marginalize, ['x1'])
+        self.assertRaises(ValueError, self.phi.marginalize, ["x4"])
+        self.phi.marginalize(["x1"])
+        self.assertRaises(ValueError, self.phi.marginalize, ["x1"])
 
-        self.assertRaises(ValueError, self.phi4.marginalize, [('x1', 'x3')])
+        self.assertRaises(ValueError, self.phi4.marginalize, [("x1", "x3")])
         self.phi4.marginalize([self.tup2])
         self.assertRaises(ValueError, self.phi4.marginalize, [self.tup2])
 
     def test_marginalize_typeerror(self):
-        self.assertRaises(TypeError, self.phi.marginalize, 'x1')
+        self.assertRaises(TypeError, self.phi.marginalize, "x1")
 
     def test_marginalize_shape(self):
-        values = ['A', 'D', 'F', 'H']
+        values = ["A", "D", "F", "H"]
         phi3_mar = self.phi3.marginalize(values, inplace=False)
         # Previously a sorting error caused these to be different
         np_test.assert_array_equal(phi3_mar.values.shape, phi3_mar.cardinality)
 
         phi6_mar = self.phi6.marginalize([self.tup1, self.tup2], inplace=False)
         np_test.assert_array_equal(phi6_mar.values.shape, phi6_mar.cardinality)
 
         self.phi6.marginalize([self.tup1, self.tup3 + self.tup1], inplace=True)
         np_test.assert_array_equal(self.phi6.values.shape, self.phi6.cardinality)
 
     def test_normalize(self):
         self.phi1.normalize()
-        np_test.assert_almost_equal(self.phi1.values,
-                                    np.array([[[0, 0.01515152],
-                                               [0.03030303, 0.04545455],
-                                               [0.06060606, 0.07575758]],
-                                              [[0.09090909, 0.10606061],
-                                               [0.12121212, 0.13636364],
-                                               [0.15151515, 0.16666667]]]))
+        np_test.assert_almost_equal(
+            self.phi1.values,
+            np.array(
+                [
+                    [
+                        [0, 0.01515152],
+                        [0.03030303, 0.04545455],
+                        [0.06060606, 0.07575758],
+                    ],
+                    [
+                        [0.09090909, 0.10606061],
+                        [0.12121212, 0.13636364],
+                        [0.15151515, 0.16666667],
+                    ],
+                ]
+            ),
+        )
+
+        self.phi1_sn.normalize()
+        np_test.assert_almost_equal(
+            self.phi1_sn.values,
+            np.array(
+                [
+                    [
+                        [0, 0.01515152],
+                        [0.03030303, 0.04545455],
+                        [0.06060606, 0.07575758],
+                    ],
+                    [
+                        [0.09090909, 0.10606061],
+                        [0.12121212, 0.13636364],
+                        [0.15151515, 0.16666667],
+                    ],
+                ]
+            ),
+        )
+
         self.phi5.normalize()
-        np_test.assert_almost_equal(self.phi5.values,
-                                    [[[0., 0.00362319, 0.00724638, 0.01086957],
-                                      [0.01449275, 0.01811594, 0.02173913, 0.02536232],
-                                      [0.02898551, 0.0326087,  0.03623188, 0.03985507]],
-                                     [[0.04347826, 0.04710145, 0.05072464, 0.05434783],
-                                      [0.05797101, 0.0615942,  0.06521739, 0.06884058],
-                                      [0.07246377, 0.07608696, 0.07971014, 0.08333333]]])
+        np_test.assert_almost_equal(
+            self.phi5.values,
+            [
+                [
+                    [0.0, 0.00362319, 0.00724638, 0.01086957],
+                    [0.01449275, 0.01811594, 0.02173913, 0.02536232],
+                    [0.02898551, 0.0326087, 0.03623188, 0.03985507],
+                ],
+                [
+                    [0.04347826, 0.04710145, 0.05072464, 0.05434783],
+                    [0.05797101, 0.0615942, 0.06521739, 0.06884058],
+                    [0.07246377, 0.07608696, 0.07971014, 0.08333333],
+                ],
+            ],
+        )
 
     def test_reduce(self):
-        self.phi1.reduce([('x1', 0), ('x2', 0)])
+        self.phi1.reduce([("x1", 0), ("x2", 0)])
         np_test.assert_array_equal(self.phi1.values, np.array([0, 1]))
+        self.phi1_sn.reduce([("x1", "sn0"), ("x2", "sn0")])
+        np_test.assert_array_equal(self.phi1_sn.values, np.array([0, 1]))
 
         self.phi5.reduce([(self.tup1, 0), (self.tup3, 1)])
         np_test.assert_array_equal(self.phi5.values, np.array([1, 5, 9]))
 
     def test_reduce1(self):
-        self.phi1.reduce([('x2', 0), ('x1', 0)])
+        self.phi1.reduce([("x2", 0), ("x1", 0)])
         np_test.assert_array_equal(self.phi1.values, np.array([0, 1]))
+        self.phi1_sn.reduce([("x2", "sn0"), ("x1", "sn0")])
+        np_test.assert_array_equal(self.phi1_sn.values, np.array([0, 1]))
 
         self.phi5.reduce([(self.tup3, 1), (self.tup1, 0)])
         np_test.assert_array_equal(self.phi5.values, np.array([1, 5, 9]))
 
     def test_reduce_shape(self):
-        values = [('A', 0), ('D', 0), ('F', 0), ('H', 1)]
+        values = [("A", 0), ("D", 0), ("F", 0), ("H", 1)]
         phi3_reduced = self.phi3.reduce(values, inplace=False)
         # Previously a sorting error caused these to be different
         np_test.assert_array_equal(phi3_reduced.values.shape, phi3_reduced.cardinality)
 
         values = [(self.tup1, 2), (self.tup3, 0)]
         phi6_reduced = self.phi6.reduce(values, inplace=False)
         np_test.assert_array_equal(phi6_reduced.values.shape, phi6_reduced.cardinality)
 
         self.phi6.reduce(values, inplace=True)
         np_test.assert_array_equal(self.phi6.values.shape, self.phi6.cardinality)
 
     def test_complete_reduce(self):
-        self.phi1.reduce([('x1', 0), ('x2', 0), ('x3', 1)])
+        self.phi1.reduce([("x1", 0), ("x2", 0), ("x3", 1)])
         np_test.assert_array_equal(self.phi1.values, np.array([1]))
         np_test.assert_array_equal(self.phi1.cardinality, np.array([]))
         np_test.assert_array_equal(self.phi1.variables, OrderedDict())
 
-        self.phi5.reduce([(('x1', 'x2'), 1), (('x2', 'x3'), 0), (('x3', (1, 'x4')), 3)])
+        self.phi1_sn.reduce([("x1", "sn0"), ("x2", "sn0"), ("x3", "sn1")])
+        np_test.assert_array_equal(self.phi1_sn.values, np.array([1]))
+        np_test.assert_array_equal(self.phi1_sn.cardinality, np.array([]))
+        np_test.assert_array_equal(self.phi1_sn.variables, OrderedDict())
+
+        self.phi5.reduce([(("x1", "x2"), 1), (("x2", "x3"), 0), (("x3", (1, "x4")), 3)])
         np_test.assert_array_equal(self.phi5.values, np.array([15]))
         np_test.assert_array_equal(self.phi5.cardinality, np.array([]))
         np_test.assert_array_equal(self.phi5.variables, OrderedDict())
 
     def test_reduce_typeerror(self):
-        self.assertRaises(TypeError, self.phi1.reduce, 'x10')
-        self.assertRaises(TypeError, self.phi1.reduce, ['x10'])
-        self.assertRaises(TypeError, self.phi1.reduce, [('x1', 'x2')])
-        self.assertRaises(TypeError, self.phi1.reduce, [(0, 'x1')])
-        self.assertRaises(TypeError, self.phi1.reduce, [(0.1, 'x1')])
-        self.assertRaises(TypeError, self.phi1.reduce, [(0.1, 0.1)])
-        self.assertRaises(TypeError, self.phi1.reduce, [('x1', 0.1)])
-
-        self.assertRaises(TypeError, self.phi5.reduce, [(('x1', 'x2'), 0), (('x2', 'x3'), 0.2)])
+        self.assertRaises(TypeError, self.phi1.reduce, "x10")
+        self.assertRaises(TypeError, self.phi1.reduce, ["x10"])
 
     def test_reduce_scopeerror(self):
-        self.assertRaises(ValueError, self.phi1.reduce, [('x4', 1)])
-        self.assertRaises(ValueError, self.phi5.reduce, [((('x1', 0.1), 0))])
+        self.assertRaises(KeyError, self.phi1.reduce, [("x4", 1)])
+        self.assertRaises(KeyError, self.phi5.reduce, [((("x1", 0.1), 0))])
 
     def test_reduce_sizeerror(self):
-        self.assertRaises(IndexError, self.phi1.reduce, [('x3', 5)])
-        self.assertRaises(IndexError, self.phi5.reduce, [(('x2', 'x3'), 3)])
+        self.assertRaises(KeyError, self.phi1.reduce, [("x3", 5)])
+        self.assertRaises(KeyError, self.phi5.reduce, [(("x2", "x3"), 3)])
 
     def test_identity_factor(self):
         identity_factor = self.phi.identity_factor()
-        self.assertEqual(list(identity_factor.variables), ['x1', 'x2', 'x3'])
+        self.assertEqual(list(identity_factor.variables), ["x1", "x2", "x3"])
         np_test.assert_array_equal(identity_factor.cardinality, [2, 2, 2])
         np_test.assert_array_equal(identity_factor.values, np.ones(8).reshape(2, 2, 2))
 
         identity_factor1 = self.phi5.identity_factor()
-        self.assertEqual(list(identity_factor1.variables), [self.tup1, self.tup2, self.tup3])
+        self.assertEqual(
+            list(identity_factor1.variables), [self.tup1, self.tup2, self.tup3]
+        )
         np_test.assert_array_equal(identity_factor1.cardinality, [2, 3, 4])
-        np_test.assert_array_equal(identity_factor1.values, np.ones(24).reshape(2, 3, 4))
+        np_test.assert_array_equal(
+            identity_factor1.values, np.ones(24).reshape(2, 3, 4)
+        )
 
     def test_factor_product(self):
-        phi = DiscreteFactor(['x1', 'x2'], [2, 2], range(4))
-        phi1 = DiscreteFactor(['x3', 'x4'], [2, 2], range(4))
+        phi = DiscreteFactor(["x1", "x2"], [2, 2], range(4))
+        phi1 = DiscreteFactor(["x3", "x4"], [2, 2], range(4))
         prod = factor_product(phi, phi1)
-        expected_factor = DiscreteFactor(['x1', 'x2', 'x3', 'x4'], [2, 2, 2, 2],
-                                         [0, 0, 0, 0, 0, 1, 2, 3, 0, 2, 4, 6, 0, 3, 6, 9])
+        expected_factor = DiscreteFactor(
+            ["x1", "x2", "x3", "x4"],
+            [2, 2, 2, 2],
+            [0, 0, 0, 0, 0, 1, 2, 3, 0, 2, 4, 6, 0, 3, 6, 9],
+        )
         self.assertEqual(prod, expected_factor)
-        self.assertEqual(sorted(prod.variables), ['x1', 'x2', 'x3', 'x4'])
+        self.assertEqual(sorted(prod.variables), ["x1", "x2", "x3", "x4"])
 
-        phi = DiscreteFactor(['x1', 'x2'], [3, 2], range(6))
-        phi1 = DiscreteFactor(['x2', 'x3'], [2, 2], range(4))
+        phi = DiscreteFactor(["x1", "x2"], [3, 2], range(6))
+        phi1 = DiscreteFactor(["x2", "x3"], [2, 2], range(4))
         prod = factor_product(phi, phi1)
-        expected_factor = DiscreteFactor(['x1', 'x2', 'x3'], [3, 2, 2],
-                                         [0, 0, 2, 3, 0, 2, 6, 9, 0, 4, 10, 15])
+        expected_factor = DiscreteFactor(
+            ["x1", "x2", "x3"], [3, 2, 2], [0, 0, 2, 3, 0, 2, 6, 9, 0, 4, 10, 15]
+        )
         self.assertEqual(prod, expected_factor)
         self.assertEqual(prod.variables, expected_factor.variables)
 
         prod = factor_product(self.phi7, self.phi8)
-        expected_factor = DiscreteFactor([self.var1, self.var2, self.var3], [3, 2, 2],
-                                         [6, 3, 10, 12, 8, 4, 25, 30, 18, 9, 40, 48])
+        expected_factor = DiscreteFactor(
+            [self.var1, self.var2, self.var3],
+            [3, 2, 2],
+            [6, 3, 10, 12, 8, 4, 25, 30, 18, 9, 40, 48],
+        )
         self.assertEqual(prod, expected_factor)
         self.assertEqual(prod.variables, expected_factor.variables)
 
     def test_product(self):
-        phi = DiscreteFactor(['x1', 'x2'], [2, 2], range(4))
-        phi1 = DiscreteFactor(['x3', 'x4'], [2, 2], range(4))
+        phi = DiscreteFactor(["x1", "x2"], [2, 2], range(4))
+        phi1 = DiscreteFactor(["x3", "x4"], [2, 2], range(4))
         prod = phi.product(phi1, inplace=False)
-        expected_factor = DiscreteFactor(['x1', 'x2', 'x3', 'x4'], [2, 2, 2, 2],
-                                         [0, 0, 0, 0, 0, 1, 2, 3, 0, 2, 4, 6, 0, 3, 6, 9])
+        expected_factor = DiscreteFactor(
+            ["x1", "x2", "x3", "x4"],
+            [2, 2, 2, 2],
+            [0, 0, 0, 0, 0, 1, 2, 3, 0, 2, 4, 6, 0, 3, 6, 9],
+        )
         self.assertEqual(prod, expected_factor)
-        self.assertEqual(sorted(prod.variables), ['x1', 'x2', 'x3', 'x4'])
+        self.assertEqual(sorted(prod.variables), ["x1", "x2", "x3", "x4"])
 
-        phi = DiscreteFactor(['x1', 'x2'], [3, 2], range(6))
-        phi1 = DiscreteFactor(['x2', 'x3'], [2, 2], range(4))
+        phi = DiscreteFactor(["x1", "x2"], [3, 2], range(6))
+        phi1 = DiscreteFactor(["x2", "x3"], [2, 2], range(4))
         prod = phi.product(phi1, inplace=False)
-        expected_factor = DiscreteFactor(['x1', 'x2', 'x3'], [3, 2, 2],
-                                         [0, 0, 2, 3, 0, 2, 6, 9, 0, 4, 10, 15])
+        expected_factor = DiscreteFactor(
+            ["x1", "x2", "x3"], [3, 2, 2], [0, 0, 2, 3, 0, 2, 6, 9, 0, 4, 10, 15]
+        )
         self.assertEqual(prod, expected_factor)
-        self.assertEqual(sorted(prod.variables), ['x1', 'x2', 'x3'])
+        self.assertEqual(sorted(prod.variables), ["x1", "x2", "x3"])
 
         phi7_copy = self.phi7
         phi7_copy.product(self.phi8, inplace=True)
-        expected_factor = DiscreteFactor([self.var1, self.var2, self.var3], [3, 2, 2],
-                                         [6, 3, 10, 12, 8, 4, 25, 30, 18, 9, 40, 48])
+        expected_factor = DiscreteFactor(
+            [self.var1, self.var2, self.var3],
+            [3, 2, 2],
+            [6, 3, 10, 12, 8, 4, 25, 30, 18, 9, 40, 48],
+        )
         self.assertEqual(expected_factor, phi7_copy)
         self.assertEqual(phi7_copy.variables, [self.var1, self.var2, self.var3])
 
     def test_factor_product_non_factor_arg(self):
         self.assertRaises(TypeError, factor_product, 1, 2)
 
     def test_factor_mul(self):
-        phi = DiscreteFactor(['x1', 'x2'], [2, 2], range(4))
-        phi1 = DiscreteFactor(['x3', 'x4'], [2, 2], range(4))
+        phi = DiscreteFactor(["x1", "x2"], [2, 2], range(4))
+        phi1 = DiscreteFactor(["x3", "x4"], [2, 2], range(4))
         prod = phi * phi1
 
-        sorted_vars = ['x1', 'x2', 'x3', 'x4']
+        sorted_vars = ["x1", "x2", "x3", "x4"]
         for axis in range(prod.values.ndim):
             exchange_index = prod.variables.index(sorted_vars[axis])
-            prod.variables[axis], prod.variables[exchange_index] = prod.variables[exchange_index], prod.variables[axis]
+            prod.variables[axis], prod.variables[exchange_index] = (
+                prod.variables[exchange_index],
+                prod.variables[axis],
+            )
             prod.values = prod.values.swapaxes(axis, exchange_index)
 
-        np_test.assert_almost_equal(prod.values.ravel(),
-                                    np.array([0, 0, 0, 0, 0, 1, 2, 3,
-                                              0, 2, 4, 6, 0, 3, 6, 9]))
+        np_test.assert_almost_equal(
+            prod.values.ravel(),
+            np.array([0, 0, 0, 0, 0, 1, 2, 3, 0, 2, 4, 6, 0, 3, 6, 9]),
+        )
 
-        self.assertEqual(prod.variables, ['x1', 'x2', 'x3', 'x4'])
+        self.assertEqual(prod.variables, ["x1", "x2", "x3", "x4"])
 
     def test_factor_divide(self):
-        phi1 = DiscreteFactor(['x1', 'x2'], [2, 2], [1, 2, 2, 4])
-        phi2 = DiscreteFactor(['x1'], [2], [1, 2])
+        phi1 = DiscreteFactor(["x1", "x2"], [2, 2], [1, 2, 2, 4])
+        phi2 = DiscreteFactor(["x1"], [2], [1, 2])
         expected_factor = phi1.divide(phi2, inplace=False)
-        phi3 = DiscreteFactor(['x1', 'x2'], [2, 2], [1, 2, 1, 2])
+        phi3 = DiscreteFactor(["x1", "x2"], [2, 2], [1, 2, 1, 2])
         self.assertEqual(phi3, expected_factor)
 
         self.phi9.divide(self.phi10, inplace=True)
-        np_test.assert_array_almost_equal(self.phi9.values, np.array([1.000000, 0.333333, 1.333333,
-                                                                      0.833333, 3.000000, 1.333333]).reshape(3, 2))
+        np_test.assert_array_almost_equal(
+            self.phi9.values,
+            np.array(
+                [1.000000, 0.333333, 1.333333, 0.833333, 3.000000, 1.333333]
+            ).reshape(3, 2),
+        )
         self.assertEqual(self.phi9.variables, [self.var1, self.var3])
 
     def test_factor_divide_truediv(self):
-        phi1 = DiscreteFactor(['x1', 'x2'], [2, 2], [1, 2, 2, 4])
-        phi2 = DiscreteFactor(['x1'], [2], [1, 2])
+        phi1 = DiscreteFactor(["x1", "x2"], [2, 2], [1, 2, 2, 4])
+        phi2 = DiscreteFactor(["x1"], [2], [1, 2])
         div = phi1 / phi2
-        phi3 = DiscreteFactor(['x1', 'x2'], [2, 2], [1, 2, 1, 2])
+        phi3 = DiscreteFactor(["x1", "x2"], [2, 2], [1, 2, 1, 2])
         self.assertEqual(phi3, div)
 
         self.phi9 = self.phi9 / self.phi10
-        np_test.assert_array_almost_equal(self.phi9.values, np.array([1.000000, 0.333333, 1.333333,
-                                                                      0.833333, 3.000000, 1.333333]).reshape(3, 2))
+        np_test.assert_array_almost_equal(
+            self.phi9.values,
+            np.array(
+                [1.000000, 0.333333, 1.333333, 0.833333, 3.000000, 1.333333]
+            ).reshape(3, 2),
+        )
         self.assertEqual(self.phi9.variables, [self.var1, self.var3])
 
     def test_factor_divide_invalid(self):
-        phi1 = DiscreteFactor(['x1', 'x2'], [2, 2], [1, 2, 3, 4])
-        phi2 = DiscreteFactor(['x1'], [2], [0, 2])
+        phi1 = DiscreteFactor(["x1", "x2"], [2, 2], [1, 2, 3, 4])
+        phi2 = DiscreteFactor(["x1"], [2], [0, 2])
         div = phi1.divide(phi2, inplace=False)
-        np_test.assert_array_equal(div.values.ravel(), np.array([np.inf, np.inf, 1.5, 2]))
+        np_test.assert_array_equal(
+            div.values.ravel(), np.array([np.inf, np.inf, 1.5, 2])
+        )
 
     def test_factor_divide_no_common_scope(self):
-        phi1 = DiscreteFactor(['x1', 'x2'], [2, 2], [1, 2, 3, 4])
-        phi2 = DiscreteFactor(['x3'], [2], [0, 2])
+        phi1 = DiscreteFactor(["x1", "x2"], [2, 2], [1, 2, 3, 4])
+        phi2 = DiscreteFactor(["x3"], [2], [0, 2])
         self.assertRaises(ValueError, factor_divide, phi1, phi2)
 
         phi2 = DiscreteFactor([self.var3], [2], [2, 1])
         self.assertRaises(ValueError, factor_divide, self.phi7, phi2)
 
     def test_factor_divide_non_factor_arg(self):
         self.assertRaises(TypeError, factor_divide, 1, 1)
@@ -363,88 +612,192 @@
         self.assertTrue(self.phi1 == self.phi1)
 
         self.assertTrue(self.phi5 == self.phi5)
         self.assertFalse(self.phi5 == self.phi6)
         self.assertTrue(self.phi6 == self.phi6)
 
     def test_eq1(self):
-        phi1 = DiscreteFactor(['x1', 'x2', 'x3'], [2, 4, 3], range(24))
-        phi2 = DiscreteFactor(['x2', 'x1', 'x3'], [4, 2, 3],
-                              [0, 1, 2, 12, 13, 14, 3, 4, 5, 15, 16, 17, 6, 7,
-                               8, 18, 19, 20, 9, 10, 11, 21, 22, 23])
+        phi1 = DiscreteFactor(["x1", "x2", "x3"], [2, 4, 3], range(24))
+        phi2 = DiscreteFactor(
+            ["x2", "x1", "x3"],
+            [4, 2, 3],
+            [
+                0,
+                1,
+                2,
+                12,
+                13,
+                14,
+                3,
+                4,
+                5,
+                15,
+                16,
+                17,
+                6,
+                7,
+                8,
+                18,
+                19,
+                20,
+                9,
+                10,
+                11,
+                21,
+                22,
+                23,
+            ],
+        )
         self.assertTrue(phi1 == phi2)
-        self.assertEqual(phi2.variables, ['x2', 'x1', 'x3'])
+        self.assertEqual(phi2.variables, ["x2", "x1", "x3"])
 
         phi3 = DiscreteFactor([self.tup1, self.tup2, self.tup3], [2, 4, 3], range(24))
-        phi4 = DiscreteFactor([self.tup2, self.tup1, self.tup3], [4, 2, 3],
-                              [0, 1, 2, 12, 13, 14, 3, 4, 5, 15, 16, 17,
-                               6, 7, 8, 18, 19, 20, 9, 10, 11, 21, 22, 23])
+        phi4 = DiscreteFactor(
+            [self.tup2, self.tup1, self.tup3],
+            [4, 2, 3],
+            [
+                0,
+                1,
+                2,
+                12,
+                13,
+                14,
+                3,
+                4,
+                5,
+                15,
+                16,
+                17,
+                6,
+                7,
+                8,
+                18,
+                19,
+                20,
+                9,
+                10,
+                11,
+                21,
+                22,
+                23,
+            ],
+        )
         self.assertTrue(phi3 == phi4)
 
     def test_hash(self):
-        phi1 = DiscreteFactor(['x1', 'x2'], [2, 2], [1, 2, 3, 4])
-        phi2 = DiscreteFactor(['x2', 'x1'], [2, 2], [1, 3, 2, 4])
+        phi1 = DiscreteFactor(["x1", "x2"], [2, 2], [1, 2, 3, 4])
+        phi2 = DiscreteFactor(["x2", "x1"], [2, 2], [1, 3, 2, 4])
         self.assertEqual(hash(phi1), hash(phi2))
 
-        phi1 = DiscreteFactor(['x1', 'x2', 'x3'], [2, 2, 2], range(8))
-        phi2 = DiscreteFactor(['x3', 'x1', 'x2'], [2, 2, 2], [0, 2, 4, 6, 1, 3, 5, 7])
+        phi1 = DiscreteFactor(["x1", "x2", "x3"], [2, 2, 2], range(8))
+        phi2 = DiscreteFactor(["x3", "x1", "x2"], [2, 2, 2], [0, 2, 4, 6, 1, 3, 5, 7])
         self.assertEqual(hash(phi1), hash(phi2))
 
         var1 = TestHash(1, 2)
         phi3 = DiscreteFactor([var1, self.var2, self.var3], [2, 4, 3], range(24))
-        phi4 = DiscreteFactor([self.var2, var1, self.var3], [4, 2, 3],
-                              [0, 1, 2, 12, 13, 14, 3, 4, 5, 15, 16, 17,
-                               6, 7, 8, 18, 19, 20, 9, 10, 11, 21, 22, 23])
+        phi4 = DiscreteFactor(
+            [self.var2, var1, self.var3],
+            [4, 2, 3],
+            [
+                0,
+                1,
+                2,
+                12,
+                13,
+                14,
+                3,
+                4,
+                5,
+                15,
+                16,
+                17,
+                6,
+                7,
+                8,
+                18,
+                19,
+                20,
+                9,
+                10,
+                11,
+                21,
+                22,
+                23,
+            ],
+        )
         self.assertEqual(hash(phi3), hash(phi4))
 
         var1 = TestHash(2, 3)
-        var2 = TestHash('x2', 1)
+        var2 = TestHash("x2", 1)
         phi3 = DiscreteFactor([var1, var2, self.var3], [2, 2, 2], range(8))
-        phi4 = DiscreteFactor([self.var3, var1, var2], [2, 2, 2], [0, 2, 4, 6, 1, 3, 5, 7])
+        phi4 = DiscreteFactor(
+            [self.var3, var1, var2], [2, 2, 2], [0, 2, 4, 6, 1, 3, 5, 7]
+        )
         self.assertEqual(hash(phi3), hash(phi4))
 
     def test_maximize_single(self):
-        self.phi1.maximize(['x1'])
-        self.assertEqual(self.phi1, DiscreteFactor(['x2', 'x3'], [3, 2], [6, 7, 8, 9, 10, 11]))
-        self.phi1.maximize(['x2'])
-        self.assertEqual(self.phi1, DiscreteFactor(['x3'], [2], [10, 11]))
-        self.phi2 = DiscreteFactor(['x1', 'x2', 'x3'], [3, 2, 2], [0.25, 0.35, 0.08, 0.16, 0.05, 0.07,
-                                                                   0.00, 0.00, 0.15, 0.21, 0.08, 0.18])
-        self.phi2.maximize(['x2'])
-        self.assertEqual(self.phi2, DiscreteFactor(['x1', 'x3'], [3, 2], [0.25, 0.35, 0.05,
-                                                                          0.07, 0.15, 0.21]))
-
-        self.phi5.maximize([('x1', 'x2')])
-        self.assertEqual(self.phi5, DiscreteFactor([('x2', 'x3'), ('x3', (1, 'x4'))], [3, 4],
-                                                   [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]))
-        self.phi5.maximize([('x2', 'x3')])
-        self.assertEqual(self.phi5, DiscreteFactor([('x3', (1, 'x4'))], [4], [20, 21, 22, 23]))
+        self.phi1.maximize(["x1"])
+        self.assertEqual(
+            self.phi1, DiscreteFactor(["x2", "x3"], [3, 2], [6, 7, 8, 9, 10, 11])
+        )
+        self.phi1.maximize(["x2"])
+        self.assertEqual(self.phi1, DiscreteFactor(["x3"], [2], [10, 11]))
+        self.phi2 = DiscreteFactor(
+            ["x1", "x2", "x3"],
+            [3, 2, 2],
+            [0.25, 0.35, 0.08, 0.16, 0.05, 0.07, 0.00, 0.00, 0.15, 0.21, 0.08, 0.18],
+        )
+        self.phi2.maximize(["x2"])
+        self.assertEqual(
+            self.phi2,
+            DiscreteFactor(["x1", "x3"], [3, 2], [0.25, 0.35, 0.05, 0.07, 0.15, 0.21]),
+        )
+
+        self.phi5.maximize([("x1", "x2")])
+        self.assertEqual(
+            self.phi5,
+            DiscreteFactor(
+                [("x2", "x3"), ("x3", (1, "x4"))],
+                [3, 4],
+                [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],
+            ),
+        )
+        self.phi5.maximize([("x2", "x3")])
+        self.assertEqual(
+            self.phi5, DiscreteFactor([("x3", (1, "x4"))], [4], [20, 21, 22, 23])
+        )
 
     def test_maximize_list(self):
-        self.phi1.maximize(['x1', 'x2'])
-        self.assertEqual(self.phi1, DiscreteFactor(['x3'], [2], [10, 11]))
+        self.phi1.maximize(["x1", "x2"])
+        self.assertEqual(self.phi1, DiscreteFactor(["x3"], [2], [10, 11]))
 
-        self.phi5.maximize([('x1', 'x2'), ('x2', 'x3')])
-        self.assertEqual(self.phi5, DiscreteFactor([('x3', (1, 'x4'))], [4], [20, 21, 22, 23]))
+        self.phi5.maximize([("x1", "x2"), ("x2", "x3")])
+        self.assertEqual(
+            self.phi5, DiscreteFactor([("x3", (1, "x4"))], [4], [20, 21, 22, 23])
+        )
 
     def test_maximize_shape(self):
-        values = ['A', 'D', 'F', 'H']
+        values = ["A", "D", "F", "H"]
         phi3_max = self.phi3.maximize(values, inplace=False)
         # Previously a sorting error caused these to be different
         np_test.assert_array_equal(phi3_max.values.shape, phi3_max.cardinality)
 
-        phi = DiscreteFactor([self.var1, self.var2, self.var3], [3, 2, 2], [3, 2, 4, 5, 9, 8, 3, 2, 4, 5, 9, 8])
+        phi = DiscreteFactor(
+            [self.var1, self.var2, self.var3],
+            [3, 2, 2],
+            [3, 2, 4, 5, 9, 8, 3, 2, 4, 5, 9, 8],
+        )
         phi_max = phi.marginalize([self.var1, self.var2], inplace=False)
         np_test.assert_array_equal(phi_max.values.shape, phi_max.cardinality)
 
     def test_maximize_scopeerror(self):
-        self.assertRaises(ValueError, self.phi.maximize, ['x10'])
+        self.assertRaises(ValueError, self.phi.maximize, ["x10"])
 
     def test_maximize_typeerror(self):
-        self.assertRaises(TypeError, self.phi.maximize, 'x1')
+        self.assertRaises(TypeError, self.phi.maximize, "x1")
 
     def tearDown(self):
         del self.phi
         del self.phi1
         del self.phi2
         del self.phi3
         del self.phi4
@@ -463,352 +816,640 @@
         self.x = x
         self.y = y
 
     def __hash__(self):
         return hash(str(self.x) + str(self.y))
 
     def __eq__(self, other):
-        return isinstance(other, self.__class__) and self.x == other.x and self.y == other.y
+        return (
+            isinstance(other, self.__class__)
+            and self.x == other.x
+            and self.y == other.y
+        )
 
 
 class TestTabularCPDInit(unittest.TestCase):
-
     def test_cpd_init(self):
-        cpd = TabularCPD('grade', 3, [[0.1, 0.1, 0.1]])
-        self.assertEqual(cpd.variable, 'grade')
+        cpd = TabularCPD("grade", 3, [[0.1, 0.1, 0.1]])
+        self.assertEqual(cpd.variable, "grade")
         self.assertEqual(cpd.variable_card, 3)
-        self.assertEqual(list(cpd.variables), ['grade'])
+        self.assertEqual(list(cpd.variables), ["grade"])
         np_test.assert_array_equal(cpd.cardinality, np.array([3]))
         np_test.assert_array_almost_equal(cpd.values, np.array([0.1, 0.1, 0.1]))
 
-        values = [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
-                  [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
-                  [0.8, 0.8, 0.8, 0.8, 0.8, 0.8]]
+        values = [
+            [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
+            [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
+            [0.8, 0.8, 0.8, 0.8, 0.8, 0.8],
+        ]
 
-        evidence = ['intel', 'diff']
+        evidence = ["intel", "diff"]
         evidence_card = [3, 2]
 
         valid_value_inputs = [values, np.asarray(values)]
         valid_evidence_inputs = [evidence, set(evidence), np.asarray(evidence)]
         valid_evidence_card_inputs = [evidence_card, np.asarray(evidence_card)]
 
         for value in valid_value_inputs:
             for evidence in valid_evidence_inputs:
                 for evidence_card in valid_evidence_card_inputs:
-                    cpd = TabularCPD('grade', 3, values, evidence=['intel', 'diff'], evidence_card=[3, 2])
-                    self.assertEqual(cpd.variable, 'grade')
+                    cpd = TabularCPD(
+                        "grade",
+                        3,
+                        values,
+                        evidence=["intel", "diff"],
+                        evidence_card=[3, 2],
+                    )
+                    self.assertEqual(cpd.variable, "grade")
                     self.assertEqual(cpd.variable_card, 3)
                     np_test.assert_array_equal(cpd.cardinality, np.array([3, 3, 2]))
-                    self.assertListEqual(list(cpd.variables), ['grade', 'intel', 'diff'])
-                    np_test.assert_array_equal(cpd.values, np.array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1,
-                                                                     0.1, 0.1, 0.1, 0.1, 0.1, 0.1,
-                                                                     0.8, 0.8, 0.8, 0.8, 0.8, 0.8]).reshape(3, 3, 2))
-
-        cpd = TabularCPD('grade', 3, [[0.1, 0.1],
-                                      [0.1, 0.1],
-                                      [0.8, 0.8]],
-                         evidence=['evi1'], evidence_card=[2.0])
-        self.assertEqual(cpd.variable, 'grade')
+                    self.assertListEqual(
+                        list(cpd.variables), ["grade", "intel", "diff"]
+                    )
+                    np_test.assert_array_equal(
+                        cpd.values,
+                        np.array(
+                            [
+                                0.1,
+                                0.1,
+                                0.1,
+                                0.1,
+                                0.1,
+                                0.1,
+                                0.1,
+                                0.1,
+                                0.1,
+                                0.1,
+                                0.1,
+                                0.1,
+                                0.8,
+                                0.8,
+                                0.8,
+                                0.8,
+                                0.8,
+                                0.8,
+                            ]
+                        ).reshape(3, 3, 2),
+                    )
+
+        cpd = TabularCPD(
+            "grade",
+            3,
+            [[0.1, 0.1], [0.1, 0.1], [0.8, 0.8]],
+            evidence=["evi1"],
+            evidence_card=[2.0],
+        )
+        self.assertEqual(cpd.variable, "grade")
         self.assertEqual(cpd.variable_card, 3)
         np_test.assert_array_equal(cpd.cardinality, np.array([3, 2]))
-        self.assertListEqual(list(cpd.variables), ['grade', 'evi1'])
-        np_test.assert_array_equal(cpd.values, np.array([0.1, 0.1,
-                                                         0.1, 0.1,
-                                                         0.8, 0.8]).reshape(3, 2))
+        self.assertListEqual(list(cpd.variables), ["grade", "evi1"])
+        np_test.assert_array_equal(
+            cpd.values, np.array([0.1, 0.1, 0.1, 0.1, 0.8, 0.8]).reshape(3, 2)
+        )
 
     def test_cpd_init_event_card_not_int(self):
-        self.assertRaises(TypeError, TabularCPD, 'event', '2', [[0.1, 0.9]])
+        self.assertRaises(TypeError, TabularCPD, "event", "2", [[0.1, 0.9]])
 
     def test_cpd_init_cardinality_not_specified(self):
-        self.assertRaises(ValueError, TabularCPD, 'event', 3, [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
-                                                               [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
-                                                               [0.8, 0.8, 0.8, 0.8, 0.8, 0.8]],
-                          ['evi1', 'evi2'], [5])
-        self.assertRaises(ValueError, TabularCPD, 'event', 3, [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
-                                                               [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
-                                                               [0.8, 0.8, 0.8, 0.8, 0.8, 0.8]],
-                          ['evi1', 'evi2'], [5.0])
-        self.assertRaises(ValueError, TabularCPD, 'event', 3, [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
-                                                               [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
-                                                               [0.8, 0.8, 0.8, 0.8, 0.8, 0.8]],
-                          ['evi1'], [5, 6])
-        self.assertRaises(TypeError, TabularCPD, 'event', 3, [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
-                                                              [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
-                                                              [0.8, 0.8, 0.8, 0.8, 0.8, 0.8]],
-                          'evi1', [5, 6])
+        self.assertRaises(
+            ValueError,
+            TabularCPD,
+            "event",
+            3,
+            [
+                [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
+                [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
+                [0.8, 0.8, 0.8, 0.8, 0.8, 0.8],
+            ],
+            ["evi1", "evi2"],
+            [5],
+        )
+        self.assertRaises(
+            ValueError,
+            TabularCPD,
+            "event",
+            3,
+            [
+                [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
+                [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
+                [0.8, 0.8, 0.8, 0.8, 0.8, 0.8],
+            ],
+            ["evi1", "evi2"],
+            [5.0],
+        )
+        self.assertRaises(
+            ValueError,
+            TabularCPD,
+            "event",
+            3,
+            [
+                [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
+                [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
+                [0.8, 0.8, 0.8, 0.8, 0.8, 0.8],
+            ],
+            ["evi1"],
+            [5, 6],
+        )
+        self.assertRaises(
+            TypeError,
+            TabularCPD,
+            "event",
+            3,
+            [
+                [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
+                [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
+                [0.8, 0.8, 0.8, 0.8, 0.8, 0.8],
+            ],
+            "evi1",
+            [5, 6],
+        )
 
     def test_cpd_init_value_not_2d(self):
-        self.assertRaises(TypeError, TabularCPD, 'event', 3, [[[0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
-                                                               [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
-                                                               [0.8, 0.8, 0.8, 0.8, 0.8, 0.8]]],
-                          ['evi1', 'evi2'], [5, 6])
+        self.assertRaises(
+            TypeError,
+            TabularCPD,
+            "event",
+            3,
+            [
+                [
+                    [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
+                    [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
+                    [0.8, 0.8, 0.8, 0.8, 0.8, 0.8],
+                ]
+            ],
+            ["evi1", "evi2"],
+            [5, 6],
+        )
 
 
 class TestTabularCPDMethods(unittest.TestCase):
-
     def setUp(self):
-        self.cpd = TabularCPD('grade', 3, [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
-                                           [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
-                                           [0.8, 0.8, 0.8, 0.8, 0.8, 0.8]],
-                              evidence=['intel', 'diff'], evidence_card=[3, 2])
-
-        self.cpd2 = TabularCPD('J', 2, [[0.9, 0.3, 0.9, 0.3, 0.8, 0.8, 0.4, 0.4],
-                                        [0.1, 0.7, 0.1, 0.7, 0.2, 0.2, 0.6, 0.6]],
-                               evidence=['A', 'B', 'C'], evidence_card=[2, 2, 2])
+        sn = {
+            "intel": ["low", "medium", "high"],
+            "diff": ["low", "high"],
+            "grade": [
+                "grade(0)",
+                "grade(1)",
+                "grade(2)",
+                "grade(3)",
+                "grade(4)",
+                "grade(5)",
+            ],
+        }
+        self.cpd = TabularCPD(
+            "grade",
+            3,
+            [
+                [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
+                [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
+                [0.8, 0.8, 0.8, 0.8, 0.8, 0.8],
+            ],
+            evidence=["intel", "diff"],
+            evidence_card=[3, 2],
+            state_names=sn,
+        )
+
+        self.cpd2 = TabularCPD(
+            "J",
+            2,
+            [
+                [0.9, 0.3, 0.9, 0.3, 0.8, 0.8, 0.4, 0.4],
+                [0.1, 0.7, 0.1, 0.7, 0.2, 0.2, 0.6, 0.6],
+            ],
+            evidence=["A", "B", "C"],
+            evidence_card=[2, 2, 2],
+        )
 
     def test_marginalize_1(self):
-        self.cpd.marginalize(['diff'])
-        self.assertEqual(self.cpd.variable, 'grade')
+        self.cpd.marginalize(["diff"])
+        self.assertEqual(self.cpd.variable, "grade")
         self.assertEqual(self.cpd.variable_card, 3)
-        self.assertListEqual(list(self.cpd.variables), ['grade', 'intel'])
+        self.assertListEqual(list(self.cpd.variables), ["grade", "intel"])
         np_test.assert_array_equal(self.cpd.cardinality, np.array([3, 3]))
-        np_test.assert_array_equal(self.cpd.values.ravel(), np.array([0.1, 0.1, 0.1,
-                                                                      0.1, 0.1, 0.1,
-                                                                      0.8, 0.8, 0.8]))
+        np_test.assert_array_equal(
+            self.cpd.values.ravel(),
+            np.array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.8, 0.8, 0.8]),
+        )
 
     def test_marginalize_2(self):
-        self.assertRaises(ValueError, self.cpd.marginalize, ['grade'])
+        self.assertRaises(ValueError, self.cpd.marginalize, ["grade"])
 
     def test_marginalize_3(self):
         copy_cpd = self.cpd.copy()
-        copy_cpd.marginalize(['intel', 'diff'])
-        self.cpd.marginalize(['intel'])
-        self.cpd.marginalize(['diff'])
+        copy_cpd.marginalize(["intel", "diff"])
+        self.cpd.marginalize(["intel"])
+        self.cpd.marginalize(["diff"])
         np_test.assert_array_almost_equal(self.cpd.values, copy_cpd.values)
 
     def test_normalize(self):
-        cpd_un_normalized = TabularCPD('grade', 2, [[0.7, 0.2, 0.6, 0.2], [0.4, 0.4, 0.4, 0.8]],
-                                       ['intel', 'diff'], [2, 2])
+        cpd_un_normalized = TabularCPD(
+            "grade",
+            2,
+            [[0.7, 0.2, 0.6, 0.2], [0.4, 0.4, 0.4, 0.8]],
+            ["intel", "diff"],
+            [2, 2],
+        )
         cpd_un_normalized.normalize()
-        np_test.assert_array_almost_equal(cpd_un_normalized.values, np.array([[[0.63636364, 0.33333333],
-                                                                               [0.6, 0.2]],
-                                                                              [[0.36363636, 0.66666667],
-                                                                               [0.4, 0.8]]]))
+        np_test.assert_array_almost_equal(
+            cpd_un_normalized.values,
+            np.array(
+                [
+                    [[0.63636364, 0.33333333], [0.6, 0.2]],
+                    [[0.36363636, 0.66666667], [0.4, 0.8]],
+                ]
+            ),
+        )
 
     def test_normalize_not_in_place(self):
-        cpd_un_normalized = TabularCPD('grade', 2, [[0.7, 0.2, 0.6, 0.2], [0.4, 0.4, 0.4, 0.8]],
-                                       ['intel', 'diff'], [2, 2])
-        np_test.assert_array_almost_equal(cpd_un_normalized.normalize(inplace=False).values,
-                                          np.array([[[0.63636364, 0.33333333],
-                                                     [0.6, 0.2]],
-                                                    [[0.36363636, 0.66666667],
-                                                     [0.4, 0.8]]]))
+        cpd_un_normalized = TabularCPD(
+            "grade",
+            2,
+            [[0.7, 0.2, 0.6, 0.2], [0.4, 0.4, 0.4, 0.8]],
+            ["intel", "diff"],
+            [2, 2],
+        )
+        np_test.assert_array_almost_equal(
+            cpd_un_normalized.normalize(inplace=False).values,
+            np.array(
+                [
+                    [[0.63636364, 0.33333333], [0.6, 0.2]],
+                    [[0.36363636, 0.66666667], [0.4, 0.8]],
+                ]
+            ),
+        )
 
     def test_normalize_original_safe(self):
-        cpd_un_normalized = TabularCPD('grade', 2, [[0.7, 0.2, 0.6, 0.2], [0.4, 0.4, 0.4, 0.8]],
-                                       ['intel', 'diff'], [2, 2])
+        cpd_un_normalized = TabularCPD(
+            "grade",
+            2,
+            [[0.7, 0.2, 0.6, 0.2], [0.4, 0.4, 0.4, 0.8]],
+            ["intel", "diff"],
+            [2, 2],
+        )
         cpd_un_normalized.normalize(inplace=False)
-        np_test.assert_array_almost_equal(cpd_un_normalized.values, np.array([[[0.7, 0.2], [0.6, 0.2]],
-                                                                              [[0.4, 0.4], [0.4, 0.8]]]))
+        np_test.assert_array_almost_equal(
+            cpd_un_normalized.values,
+            np.array([[[0.7, 0.2], [0.6, 0.2]], [[0.4, 0.4], [0.4, 0.8]]]),
+        )
 
     def test__repr__(self):
-        grade_cpd = TabularCPD('grade', 3, [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
-                                            [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
-                                            [0.8, 0.8, 0.8, 0.8, 0.8, 0.8]],
-                               evidence=['intel', 'diff'], evidence_card=[3, 2])
-        intel_cpd = TabularCPD('intel', 3, [[0.5], [0.3], [0.2]])
-        diff_cpd = TabularCPD('grade', 3, [[0.1, 0.1], [0.1, 0.1],  [0.8, 0.8]], evidence=['diff'], evidence_card=[2])
-        self.assertEqual(repr(grade_cpd), '<TabularCPD representing P(grade:3 | intel:3, diff:2) at {address}>'
-                         .format(address=hex(id(grade_cpd))))
-        self.assertEqual(repr(intel_cpd), '<TabularCPD representing P(intel:3) at {address}>'
-                         .format(address=hex(id(intel_cpd))))
-        self.assertEqual(repr(diff_cpd), '<TabularCPD representing P(grade:3 | diff:2) at {address}>'
-                         .format(address=hex(id(diff_cpd))))
+        grade_cpd = TabularCPD(
+            "grade",
+            3,
+            [
+                [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
+                [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
+                [0.8, 0.8, 0.8, 0.8, 0.8, 0.8],
+            ],
+            evidence=["intel", "diff"],
+            evidence_card=[3, 2],
+        )
+        intel_cpd = TabularCPD("intel", 3, [[0.5], [0.3], [0.2]])
+        diff_cpd = TabularCPD(
+            "grade",
+            3,
+            [[0.1, 0.1], [0.1, 0.1], [0.8, 0.8]],
+            evidence=["diff"],
+            evidence_card=[2],
+        )
+        self.assertEqual(
+            repr(grade_cpd),
+            "<TabularCPD representing P(grade:3 | intel:3, diff:2) at {address}>".format(
+                address=hex(id(grade_cpd))
+            ),
+        )
+        self.assertEqual(
+            repr(intel_cpd),
+            "<TabularCPD representing P(intel:3) at {address}>".format(
+                address=hex(id(intel_cpd))
+            ),
+        )
+        self.assertEqual(
+            repr(diff_cpd),
+            "<TabularCPD representing P(grade:3 | diff:2) at {address}>".format(
+                address=hex(id(diff_cpd))
+            ),
+        )
 
     def test_copy(self):
         copy_cpd = self.cpd.copy()
         np_test.assert_array_equal(self.cpd.get_values(), copy_cpd.get_values())
 
     def test_copy_original_safe(self):
         copy_cpd = self.cpd.copy()
-        copy_cpd.reorder_parents(['diff', 'intel'])
-        np_test.assert_array_equal(self.cpd.get_values(),
-                                   np.array([[0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
-                                             [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
-                                             [0.8, 0.8, 0.8, 0.8, 0.8, 0.8]]))
+        copy_cpd.reorder_parents(["diff", "intel"])
+        np_test.assert_array_equal(
+            self.cpd.get_values(),
+            np.array(
+                [
+                    [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
+                    [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
+                    [0.8, 0.8, 0.8, 0.8, 0.8, 0.8],
+                ]
+            ),
+        )
+
+    def test_copy_state_names(self):
+        copy_cpd = self.cpd.copy()
+        self.assertEqual(self.cpd.state_names, copy_cpd.state_names)
 
     def test_reduce_1(self):
-        self.cpd.reduce([('diff', 0)])
-        np_test.assert_array_equal(self.cpd.get_values(), np.array([[0.1, 0.1, 0.1],
-                                                                    [0.1, 0.1, 0.1],
-                                                                    [0.8, 0.8, 0.8]]))
+        self.cpd.reduce([("diff", "low")])
+        np_test.assert_array_equal(
+            self.cpd.get_values(),
+            np.array([[0.1, 0.1, 0.1], [0.1, 0.1, 0.1], [0.8, 0.8, 0.8]]),
+        )
 
     def test_reduce_2(self):
-        self.cpd.reduce([('intel', 0)])
-        np_test.assert_array_equal(self.cpd.get_values(), np.array([[0.1, 0.1],
-                                                                    [0.1, 0.1],
-                                                                    [0.8, 0.8]]))
+        self.cpd.reduce([("intel", "low")])
+        np_test.assert_array_equal(
+            self.cpd.get_values(), np.array([[0.1, 0.1], [0.1, 0.1], [0.8, 0.8]])
+        )
 
     def test_reduce_3(self):
-        self.cpd.reduce([('intel', 0), ('diff', 0)])
-        np_test.assert_array_equal(self.cpd.get_values(), np.array([[0.1],
-                                                                    [0.1],
-                                                                    [0.8]]))
+        self.cpd.reduce([("intel", "low"), ("diff", "low")])
+        np_test.assert_array_equal(
+            self.cpd.get_values(), np.array([[0.1], [0.1], [0.8]])
+        )
 
     def test_reduce_4(self):
-        self.assertRaises(ValueError, self.cpd.reduce, [('grade', 0)])
+        self.assertRaises(ValueError, self.cpd.reduce, [("grade", "grade(0)")])
 
     def test_reduce_5(self):
         copy_cpd = self.cpd.copy()
-        copy_cpd.reduce([('intel', 2), ('diff', 1)])
-        self.cpd.reduce([('intel', 2)])
-        self.cpd.reduce([('diff', 1)])
+        copy_cpd.reduce([("intel", "high"), ("diff", "low")])
+        self.cpd.reduce([("intel", "high")])
+        self.cpd.reduce([("diff", "high")])
         np_test.assert_array_almost_equal(self.cpd.values, copy_cpd.values)
 
     def test_get_values(self):
-        np_test.assert_array_equal(self.cpd.get_values(),
-                                   np.array([[0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
-                                             [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
-                                             [0.8, 0.8, 0.8, 0.8, 0.8, 0.8]]))
+        np_test.assert_array_equal(
+            self.cpd.get_values(),
+            np.array(
+                [
+                    [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
+                    [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
+                    [0.8, 0.8, 0.8, 0.8, 0.8, 0.8],
+                ]
+            ),
+        )
 
     def test_reorder_parents_inplace(self):
-        new_vals = self.cpd2.reorder_parents(['B', 'A', 'C'])
-        np_test.assert_array_equal(new_vals, np.array([[0.9, 0.3, 0.8, 0.8, 0.9, 0.3, 0.4, 0.4],
-                                                       [0.1, 0.7, 0.2, 0.2, 0.1, 0.7, 0.6, 0.6]]))
-        np_test.assert_array_equal(self.cpd2.get_values(),
-                                   np.array([[0.9, 0.3, 0.8, 0.8, 0.9, 0.3, 0.4, 0.4],
-                                             [0.1, 0.7, 0.2, 0.2, 0.1, 0.7, 0.6, 0.6]]))
+        new_vals = self.cpd2.reorder_parents(["B", "A", "C"])
+        np_test.assert_array_equal(
+            new_vals,
+            np.array(
+                [
+                    [0.9, 0.3, 0.8, 0.8, 0.9, 0.3, 0.4, 0.4],
+                    [0.1, 0.7, 0.2, 0.2, 0.1, 0.7, 0.6, 0.6],
+                ]
+            ),
+        )
+        np_test.assert_array_equal(
+            self.cpd2.get_values(),
+            np.array(
+                [
+                    [0.9, 0.3, 0.8, 0.8, 0.9, 0.3, 0.4, 0.4],
+                    [0.1, 0.7, 0.2, 0.2, 0.1, 0.7, 0.6, 0.6],
+                ]
+            ),
+        )
 
     def test_reorder_parents(self):
-        new_vals = self.cpd2.reorder_parents(['B', 'A', 'C'])
-        np_test.assert_array_equal(new_vals, np.array([[0.9, 0.3, 0.8, 0.8, 0.9, 0.3, 0.4, 0.4],
-                                                       [0.1, 0.7, 0.2, 0.2, 0.1, 0.7, 0.6, 0.6]]))
+        new_vals = self.cpd2.reorder_parents(["B", "A", "C"])
+        np_test.assert_array_equal(
+            new_vals,
+            np.array(
+                [
+                    [0.9, 0.3, 0.8, 0.8, 0.9, 0.3, 0.4, 0.4],
+                    [0.1, 0.7, 0.2, 0.2, 0.1, 0.7, 0.6, 0.6],
+                ]
+            ),
+        )
 
     def test_reorder_parents_no_effect(self):
-        self.cpd2.reorder_parents(['C', 'A', 'B'], inplace=False)
-        np_test.assert_array_equal(self.cpd2.get_values(),
-                                   np.array([[0.9, 0.3, 0.9, 0.3, 0.8, 0.8, 0.4, 0.4],
-                                             [0.1, 0.7, 0.1, 0.7, 0.2, 0.2, 0.6, 0.6]]))
+        self.cpd2.reorder_parents(["C", "A", "B"], inplace=False)
+        np_test.assert_array_equal(
+            self.cpd2.get_values(),
+            np.array(
+                [
+                    [0.9, 0.3, 0.9, 0.3, 0.8, 0.8, 0.4, 0.4],
+                    [0.1, 0.7, 0.1, 0.7, 0.2, 0.2, 0.6, 0.6],
+                ]
+            ),
+        )
 
     def test_reorder_parents_warning(self):
         with warnings.catch_warnings(record=True) as w:
             warnings.simplefilter("always")
-            self.cpd2.reorder_parents(['A', 'B', 'C'], inplace=False)
-            assert("Same ordering provided as current" in str(w[-1].message))
-            np_test.assert_array_equal(self.cpd2.get_values(),
-                                       np.array([[0.9, 0.3, 0.9, 0.3, 0.8, 0.8, 0.4, 0.4],
-                                                 [0.1, 0.7, 0.1, 0.7, 0.2, 0.2, 0.6, 0.6]]))
+            self.cpd2.reorder_parents(["A", "B", "C"], inplace=False)
+            assert "Same ordering provided as current" in str(w[-1].message)
+            np_test.assert_array_equal(
+                self.cpd2.get_values(),
+                np.array(
+                    [
+                        [0.9, 0.3, 0.9, 0.3, 0.8, 0.8, 0.4, 0.4],
+                        [0.1, 0.7, 0.1, 0.7, 0.2, 0.2, 0.6, 0.6],
+                    ]
+                ),
+            )
 
     def tearDown(self):
         del self.cpd
 
 
 class TestJointProbabilityDistributionInit(unittest.TestCase):
-
     def test_jpd_init(self):
-        jpd = JPD(['x1', 'x2', 'x3'], [2, 3, 2], np.ones(12) / 12)
+        jpd = JPD(["x1", "x2", "x3"], [2, 3, 2], np.ones(12) / 12)
         np_test.assert_array_equal(jpd.cardinality, np.array([2, 3, 2]))
         np_test.assert_array_equal(jpd.values, np.ones(12).reshape(2, 3, 2) / 12)
-        self.assertEqual(jpd.get_cardinality(['x1', 'x2', 'x3']), {'x1': 2, 'x2': 3, 'x3': 2})
+        self.assertEqual(
+            jpd.get_cardinality(["x1", "x2", "x3"]), {"x1": 2, "x2": 3, "x3": 2}
+        )
 
     def test_jpd_init_exception(self):
-        self.assertRaises(ValueError, JPD, ['x1', 'x2', 'x3'], [2, 2, 2], np.ones(8))
+        self.assertRaises(ValueError, JPD, ["x1", "x2", "x3"], [2, 2, 2], np.ones(8))
 
 
 class TestJointProbabilityDistributionMethods(unittest.TestCase):
-
     def setUp(self):
-        self.jpd = JPD(['x1', 'x2', 'x3'], [2, 3, 2], values=np.ones(12) / 12)
-        self.jpd1 = JPD(['x1', 'x2', 'x3'], [2, 3, 2], values=np.ones(12) / 12)
-        self.jpd2 = JPD(['x1', 'x2', 'x3'], [2, 2, 3],
-                        [0.126, 0.168, 0.126, 0.009, 0.045, 0.126, 0.252, 0.0224, 0.0056, 0.06, 0.036, 0.024])
-        self.jpd3 = JPD(['x1', 'x2', 'x3'], [2, 2, 2],
-                        [5.0e-04, 5.225e-04, 0.00, 8.9775e-03, 9.9e-03, 5.39055e-02, 0.00, 9.261945e-01])
+        self.jpd = JPD(["x1", "x2", "x3"], [2, 3, 2], values=np.ones(12) / 12)
+        self.jpd1 = JPD(["x1", "x2", "x3"], [2, 3, 2], values=np.ones(12) / 12)
+        self.jpd2 = JPD(
+            ["x1", "x2", "x3"],
+            [2, 2, 3],
+            [
+                0.126,
+                0.168,
+                0.126,
+                0.009,
+                0.045,
+                0.126,
+                0.252,
+                0.0224,
+                0.0056,
+                0.06,
+                0.036,
+                0.024,
+            ],
+        )
+        self.jpd3 = JPD(
+            ["x1", "x2", "x3"],
+            [2, 2, 2],
+            [
+                5.0e-04,
+                5.225e-04,
+                0.00,
+                8.9775e-03,
+                9.9e-03,
+                5.39055e-02,
+                0.00,
+                9.261945e-01,
+            ],
+        )
 
     def test_jpd_marginal_distribution_list(self):
-        self.jpd.marginal_distribution(['x1', 'x2'])
-        np_test.assert_array_almost_equal(self.jpd.values,
-                                          np.array([[0.16666667, 0.16666667, 0.16666667],
-                                                    [0.16666667, 0.16666667, 0.16666667]]))
+        self.jpd.marginal_distribution(["x1", "x2"])
+        np_test.assert_array_almost_equal(
+            self.jpd.values,
+            np.array(
+                [
+                    [0.16666667, 0.16666667, 0.16666667],
+                    [0.16666667, 0.16666667, 0.16666667],
+                ]
+            ),
+        )
         np_test.assert_array_equal(self.jpd.cardinality, np.array([2, 3]))
-        dic = {'x1': 2, 'x2': 3}
-        self.assertEqual(self.jpd.get_cardinality(['x1', 'x2']), dic)
-        self.assertEqual(self.jpd.scope(), ['x1', 'x2'])
+        dic = {"x1": 2, "x2": 3}
+        self.assertEqual(self.jpd.get_cardinality(["x1", "x2"]), dic)
+        self.assertEqual(self.jpd.scope(), ["x1", "x2"])
         np_test.assert_almost_equal(np.sum(self.jpd.values), 1)
-        new_jpd = self.jpd1.marginal_distribution(['x1', 'x2'], inplace=False)
+        new_jpd = self.jpd1.marginal_distribution(["x1", "x2"], inplace=False)
         self.assertTrue(self.jpd1 != self.jpd)
         self.assertTrue(new_jpd == self.jpd)
 
     def test_marginal_distribution_str(self):
-        self.jpd.marginal_distribution('x1')
+        self.jpd.marginal_distribution("x1")
         np_test.assert_array_almost_equal(self.jpd.values, np.array([0.5, 0.5]))
         np_test.assert_array_equal(self.jpd.cardinality, np.array([2]))
-        self.assertEqual(self.jpd.scope(), ['x1'])
+        self.assertEqual(self.jpd.scope(), ["x1"])
         np_test.assert_almost_equal(np.sum(self.jpd.values), 1)
-        new_jpd = self.jpd1.marginal_distribution('x1', inplace=False)
+        new_jpd = self.jpd1.marginal_distribution("x1", inplace=False)
         self.assertTrue(self.jpd1 != self.jpd)
         self.assertTrue(self.jpd == new_jpd)
 
     def test_conditional_distribution_list(self):
         self.jpd = self.jpd1.copy()
-        self.jpd.conditional_distribution([('x1', 1), ('x2', 0)])
+        self.jpd.conditional_distribution([("x1", 1), ("x2", 0)])
         np_test.assert_array_almost_equal(self.jpd.values, np.array([0.5, 0.5]))
         np_test.assert_array_equal(self.jpd.cardinality, np.array([2]))
-        self.assertEqual(self.jpd.scope(), ['x3'])
+        self.assertEqual(self.jpd.scope(), ["x3"])
         np_test.assert_almost_equal(np.sum(self.jpd.values), 1)
-        new_jpd = self.jpd1.conditional_distribution([('x1', 1), ('x2', 0)], inplace=False)
+        new_jpd = self.jpd1.conditional_distribution(
+            [("x1", 1), ("x2", 0)], inplace=False
+        )
         self.assertTrue(self.jpd1 != self.jpd)
         self.assertTrue(self.jpd == new_jpd)
 
     def test_check_independence(self):
-        self.assertTrue(self.jpd2.check_independence(['x1'], ['x2']))
-        self.assertRaises(TypeError, self.jpd2.check_independence, 'x1', ['x2'])
-        self.assertRaises(TypeError, self.jpd2.check_independence, ['x1'], 'x2')
-        self.assertRaises(TypeError, self.jpd2.check_independence, ['x1'], ['x2'], 'x3')
-        self.assertFalse(self.jpd2.check_independence(['x1'], ['x2'], ('x3',), condition_random_variable=True))
-        self.assertFalse(self.jpd2.check_independence(['x1'], ['x2'], [('x3', 0)]))
-        self.assertTrue(self.jpd1.check_independence(['x1'], ['x2'], ('x3',), condition_random_variable=True))
-        self.assertTrue(self.jpd1.check_independence(['x1'], ['x2'], [('x3', 1)]))
-        self.assertTrue(self.jpd3.check_independence(['x1'], ['x2'], ('x3',), condition_random_variable=True))
+        self.assertTrue(self.jpd2.check_independence(["x1"], ["x2"]))
+        self.assertRaises(TypeError, self.jpd2.check_independence, "x1", ["x2"])
+        self.assertRaises(TypeError, self.jpd2.check_independence, ["x1"], "x2")
+        self.assertRaises(TypeError, self.jpd2.check_independence, ["x1"], ["x2"], "x3")
+        self.assertFalse(
+            self.jpd2.check_independence(
+                ["x1"], ["x2"], ("x3",), condition_random_variable=True
+            )
+        )
+        self.assertFalse(self.jpd2.check_independence(["x1"], ["x2"], [("x3", 0)]))
+        self.assertTrue(
+            self.jpd1.check_independence(
+                ["x1"], ["x2"], ("x3",), condition_random_variable=True
+            )
+        )
+        self.assertTrue(self.jpd1.check_independence(["x1"], ["x2"], [("x3", 1)]))
+        self.assertTrue(
+            self.jpd3.check_independence(
+                ["x1"], ["x2"], ("x3",), condition_random_variable=True
+            )
+        )
 
     def test_get_independencies(self):
-        independencies = Independencies(['x1', 'x2'], ['x2', 'x3'], ['x3', 'x1'])
-        independencies1 = Independencies(['x1', 'x2'])
+        independencies = Independencies(["x1", "x2"], ["x2", "x3"], ["x3", "x1"])
+        independencies1 = Independencies(["x1", "x2"])
         self.assertEqual(self.jpd1.get_independencies(), independencies)
         self.assertEqual(self.jpd2.get_independencies(), independencies1)
-        self.assertEqual(self.jpd1.get_independencies([('x3', 0)]), independencies1)
-        self.assertEqual(self.jpd2.get_independencies([('x3', 0)]), Independencies())
+        self.assertEqual(self.jpd1.get_independencies([("x3", 0)]), independencies1)
+        self.assertEqual(self.jpd2.get_independencies([("x3", 0)]), Independencies())
 
     def test_minimal_imap(self):
-        bm = self.jpd1.minimal_imap(order=['x1', 'x2', 'x3'])
-        self.assertEqual(sorted(bm.edges()), sorted([('x1', 'x3'), ('x2', 'x3')]))
-        bm = self.jpd1.minimal_imap(order=['x2', 'x3', 'x1'])
-        self.assertEqual(sorted(bm.edges()), sorted([('x2', 'x1'), ('x3', 'x1')]))
-        bm = self.jpd2.minimal_imap(order=['x1', 'x2', 'x3'])
-        self.assertEqual(bm.edges(), [])
-        bm = self.jpd2.minimal_imap(order=['x1', 'x2'])
-        self.assertEqual(bm.edges(), [])
+        bm = self.jpd1.minimal_imap(order=["x1", "x2", "x3"])
+        self.assertEqual(sorted(bm.edges()), sorted([("x1", "x3"), ("x2", "x3")]))
+        bm = self.jpd1.minimal_imap(order=["x2", "x3", "x1"])
+        self.assertEqual(sorted(bm.edges()), sorted([("x2", "x1"), ("x3", "x1")]))
+        bm = self.jpd2.minimal_imap(order=["x1", "x2", "x3"])
+        self.assertEqual(list(bm.edges()), [])
+        bm = self.jpd2.minimal_imap(order=["x1", "x2"])
+        self.assertEqual(list(bm.edges()), [])
 
     def test_repr(self):
-        self.assertEqual(repr(self.jpd1), '<Joint Distribution representing P(x1:2, x2:3, x3:2) at {address}>'.format(
-            address=hex(id(self.jpd1))))
+        self.assertEqual(
+            repr(self.jpd1),
+            "<Joint Distribution representing P(x1:2, x2:3, x3:2) at {address}>".format(
+                address=hex(id(self.jpd1))
+            ),
+        )
 
     def test_is_imap(self):
-        G1 = BayesianModel([('diff', 'grade'), ('intel', 'grade')])
-        diff_cpd = TabularCPD('diff', 2, [[0.2], [0.8]])
-        intel_cpd = TabularCPD('intel', 3, [[0.5], [0.3], [0.2]])
-        grade_cpd = TabularCPD('grade', 3,
-                               [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
-                                [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
-                                [0.8, 0.8, 0.8, 0.8, 0.8, 0.8]],
-                               evidence=['diff', 'intel'],
-                               evidence_card=[2, 3])
+        G1 = BayesianModel([("diff", "grade"), ("intel", "grade")])
+        diff_cpd = TabularCPD("diff", 2, [[0.2], [0.8]])
+        intel_cpd = TabularCPD("intel", 3, [[0.5], [0.3], [0.2]])
+        grade_cpd = TabularCPD(
+            "grade",
+            3,
+            [
+                [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
+                [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
+                [0.8, 0.8, 0.8, 0.8, 0.8, 0.8],
+            ],
+            evidence=["diff", "intel"],
+            evidence_card=[2, 3],
+        )
         G1.add_cpds(diff_cpd, intel_cpd, grade_cpd)
-        val = [0.01, 0.01, 0.08, 0.006, 0.006, 0.048, 0.004, 0.004, 0.032,
-               0.04, 0.04, 0.32, 0.024, 0.024, 0.192, 0.016, 0.016, 0.128]
-        jpd = JPD(['diff', 'intel', 'grade'], [2, 3, 3], val)
+        val = [
+            0.01,
+            0.01,
+            0.08,
+            0.006,
+            0.006,
+            0.048,
+            0.004,
+            0.004,
+            0.032,
+            0.04,
+            0.04,
+            0.32,
+            0.024,
+            0.024,
+            0.192,
+            0.016,
+            0.016,
+            0.128,
+        ]
+        jpd = JPD(["diff", "intel", "grade"], [2, 3, 3], val)
         self.assertTrue(jpd.is_imap(G1))
         self.assertRaises(TypeError, jpd.is_imap, MarkovModel())
 
     def tearDown(self):
         del self.jpd
         del self.jpd1
         del self.jpd2
         del self.jpd3
 
+
 #
 # class TestTreeCPDInit(unittest.TestCase):
 #     def test_init_single_variable_nodes(self):
 #         tree = TreeCPD([('B', DiscreteFactor(['A'], [2], [0.8, 0.2]), 0),
 #                         ('B', 'C', 1),
 #                         ('C', DiscreteFactor(['A'], [2], [0.1, 0.9]), 0),
 #                         ('C', 'D', 1),
```

### Comparing `pgmpy-0.1.7/pgmpy/tests/test_factors/test_FactorSet.py` & `pgmpy-0.1.9/pgmpy/tests/test_factors/test_FactorSet.py`

 * *Files 11% similar despite different names*

```diff
@@ -3,73 +3,101 @@
 from pgmpy.extern.six.moves import filter, range
 from pgmpy.factors import FactorSet
 from pgmpy.factors.discrete import DiscreteFactor
 
 
 class TestFactorSet(unittest.TestCase):
     def setUp(self):
-        self.phi1 = DiscreteFactor(['x1', 'x2', 'x3'], [2, 3, 2], range(12))
-        self.phi2 = DiscreteFactor(['x3', 'x4', 'x1'], [2, 2, 2], range(8))
-        self.phi3 = DiscreteFactor(['x5', 'x6', 'x7'], [2, 2, 2], range(8))
-        self.phi4 = DiscreteFactor(['x5', 'x7', 'x8'], [2, 2, 2], range(8))
+        self.phi1 = DiscreteFactor(["x1", "x2", "x3"], [2, 3, 2], range(12))
+        self.phi2 = DiscreteFactor(["x3", "x4", "x1"], [2, 2, 2], range(8))
+        self.phi3 = DiscreteFactor(["x5", "x6", "x7"], [2, 2, 2], range(8))
+        self.phi4 = DiscreteFactor(["x5", "x7", "x8"], [2, 2, 2], range(8))
 
     def test_class_init(self):
-        phi1 = DiscreteFactor(['x1', 'x2', 'x3'], [2, 3, 2], range(12))
-        phi2 = DiscreteFactor(['x3', 'x4', 'x1'], [2, 2, 2], range(8))
+        phi1 = DiscreteFactor(["x1", "x2", "x3"], [2, 3, 2], range(12))
+        phi2 = DiscreteFactor(["x3", "x4", "x1"], [2, 2, 2], range(8))
         factor_set1 = FactorSet(phi1, phi2)
         self.assertEqual({phi1, phi2}, factor_set1.get_factors())
 
     def test_factorset_add_remove_factors(self):
         self.factor_set1 = FactorSet()
         self.factor_set1.add_factors(self.phi1, self.phi2)
         self.assertEqual({self.phi1, self.phi2}, self.factor_set1.get_factors())
         self.factor_set1.remove_factors(self.phi2)
         self.assertEqual({self.phi1}, self.factor_set1.get_factors())
 
     def test_factorset_product(self):
         factor_set1 = FactorSet(self.phi1, self.phi2)
         factor_set2 = FactorSet(self.phi3, self.phi4)
         factor_set3 = factor_set2.product(factor_set1, inplace=False)
-        self.assertEqual({self.phi1, self.phi2, self.phi3, self.phi4}, factor_set3.factors)
+        self.assertEqual(
+            {self.phi1, self.phi2, self.phi3, self.phi4}, factor_set3.factors
+        )
 
     def test_factorset_divide(self):
-        phi1 = DiscreteFactor(['x1', 'x2', 'x3'], [2, 3, 2], range(1, 13))
-        phi2 = DiscreteFactor(['x3', 'x4', 'x1'], [2, 2, 2], range(1, 9))
+        phi1 = DiscreteFactor(["x1", "x2", "x3"], [2, 3, 2], range(1, 13))
+        phi2 = DiscreteFactor(["x3", "x4", "x1"], [2, 2, 2], range(1, 9))
         factor_set1 = FactorSet(phi1, phi2)
-        phi3 = DiscreteFactor(['x5', 'x6', 'x7'], [2, 2, 2], range(1, 9))
-        phi4 = DiscreteFactor(['x5', 'x7', 'x8'], [2, 2, 2], range(1, 9))
+        phi3 = DiscreteFactor(["x5", "x6", "x7"], [2, 2, 2], range(1, 9))
+        phi4 = DiscreteFactor(["x5", "x7", "x8"], [2, 2, 2], range(1, 9))
         factor_set2 = FactorSet(phi3, phi4)
         factor_set3 = factor_set2.divide(factor_set1, inplace=False)
-        self.assertEqual({phi3, phi4, phi1.identity_factor() / phi1, phi2.identity_factor() / phi2},
-                         factor_set3.factors)
+        self.assertEqual(
+            {phi3, phi4, phi1.identity_factor() / phi1, phi2.identity_factor() / phi2},
+            factor_set3.factors,
+        )
 
     def test_factorset_marginalize_inplace(self):
         factor_set = FactorSet(self.phi1, self.phi2, self.phi3, self.phi4)
-        factor_set.marginalize(['x1', 'x5'], inplace=True)
-        phi1_equivalent_in_factor_set = list(filter(lambda x: set(x.scope()) == {'x2', 'x3'},
-                                                    factor_set.factors))[0]
-        self.assertEqual(self.phi1.marginalize(['x1'], inplace=False), phi1_equivalent_in_factor_set)
-        phi2_equivalent_in_factor_set = list(filter(lambda x: set(x.scope()) == {'x4', 'x3'},
-                                                    factor_set.factors))[0]
-        self.assertEqual(self.phi2.marginalize(['x1'], inplace=False), phi2_equivalent_in_factor_set)
-        phi3_equivalent_in_factor_set = list(filter(lambda x: set(x.scope()) == {'x6', 'x7'},
-                                                    factor_set.factors))[0]
-        self.assertEqual(self.phi3.marginalize(['x5'], inplace=False), phi3_equivalent_in_factor_set)
-        phi4_equivalent_in_factor_set = list(filter(lambda x: set(x.scope()) == {'x8', 'x7'},
-                                                    factor_set.factors))[0]
-        self.assertEqual(self.phi4.marginalize(['x5'], inplace=False), phi4_equivalent_in_factor_set)
+        factor_set.marginalize(["x1", "x5"], inplace=True)
+        phi1_equivalent_in_factor_set = list(
+            filter(lambda x: set(x.scope()) == {"x2", "x3"}, factor_set.factors)
+        )[0]
+        self.assertEqual(
+            self.phi1.marginalize(["x1"], inplace=False), phi1_equivalent_in_factor_set
+        )
+        phi2_equivalent_in_factor_set = list(
+            filter(lambda x: set(x.scope()) == {"x4", "x3"}, factor_set.factors)
+        )[0]
+        self.assertEqual(
+            self.phi2.marginalize(["x1"], inplace=False), phi2_equivalent_in_factor_set
+        )
+        phi3_equivalent_in_factor_set = list(
+            filter(lambda x: set(x.scope()) == {"x6", "x7"}, factor_set.factors)
+        )[0]
+        self.assertEqual(
+            self.phi3.marginalize(["x5"], inplace=False), phi3_equivalent_in_factor_set
+        )
+        phi4_equivalent_in_factor_set = list(
+            filter(lambda x: set(x.scope()) == {"x8", "x7"}, factor_set.factors)
+        )[0]
+        self.assertEqual(
+            self.phi4.marginalize(["x5"], inplace=False), phi4_equivalent_in_factor_set
+        )
 
     def test_factorset_marginalize_not_inplace(self):
         factor_set = FactorSet(self.phi1, self.phi2, self.phi3, self.phi4)
-        new_factor_set = factor_set.marginalize(['x1', 'x5'], inplace=False)
-        phi1_equivalent_in_factor_set = list(filter(lambda x: set(x.scope()) == {'x2', 'x3'},
-                                                    new_factor_set.factors))[0]
-        self.assertEqual(self.phi1.marginalize(['x1'], inplace=False), phi1_equivalent_in_factor_set)
-        phi2_equivalent_in_factor_set = list(filter(lambda x: set(x.scope()) == {'x4', 'x3'},
-                                                    new_factor_set.factors))[0]
-        self.assertEqual(self.phi2.marginalize(['x1'], inplace=False), phi2_equivalent_in_factor_set)
-        phi3_equivalent_in_factor_set = list(filter(lambda x: set(x.scope()) == {'x6', 'x7'},
-                                                    new_factor_set.factors))[0]
-        self.assertEqual(self.phi3.marginalize(['x5'], inplace=False), phi3_equivalent_in_factor_set)
-        phi4_equivalent_in_factor_set = list(filter(lambda x: set(x.scope()) == {'x8', 'x7'},
-                                                    new_factor_set.factors))[0]
-        self.assertEqual(self.phi4.marginalize(['x5'], inplace=False), phi4_equivalent_in_factor_set)
+        new_factor_set = factor_set.marginalize(["x1", "x5"], inplace=False)
+        phi1_equivalent_in_factor_set = list(
+            filter(lambda x: set(x.scope()) == {"x2", "x3"}, new_factor_set.factors)
+        )[0]
+        self.assertEqual(
+            self.phi1.marginalize(["x1"], inplace=False), phi1_equivalent_in_factor_set
+        )
+        phi2_equivalent_in_factor_set = list(
+            filter(lambda x: set(x.scope()) == {"x4", "x3"}, new_factor_set.factors)
+        )[0]
+        self.assertEqual(
+            self.phi2.marginalize(["x1"], inplace=False), phi2_equivalent_in_factor_set
+        )
+        phi3_equivalent_in_factor_set = list(
+            filter(lambda x: set(x.scope()) == {"x6", "x7"}, new_factor_set.factors)
+        )[0]
+        self.assertEqual(
+            self.phi3.marginalize(["x5"], inplace=False), phi3_equivalent_in_factor_set
+        )
+        phi4_equivalent_in_factor_set = list(
+            filter(lambda x: set(x.scope()) == {"x8", "x7"}, new_factor_set.factors)
+        )[0]
+        self.assertEqual(
+            self.phi4.marginalize(["x5"], inplace=False), phi4_equivalent_in_factor_set
+        )
```

### Comparing `pgmpy-0.1.7/pgmpy/tests/test_models/test_ClusterGraph.py` & `pgmpy-0.1.9/pgmpy/tests/test_models/test_ClusterGraph.py`

 * *Files 22% similar despite different names*

```diff
@@ -9,198 +9,210 @@
 
 
 class TestClusterGraphCreation(unittest.TestCase):
     def setUp(self):
         self.graph = ClusterGraph()
 
     def test_add_single_node(self):
-        self.graph.add_node(('a', 'b'))
-        self.assertListEqual(self.graph.nodes(), [('a', 'b')])
+        self.graph.add_node(("a", "b"))
+        self.assertListEqual(list(self.graph.nodes()), [("a", "b")])
 
     def test_add_single_node_raises_error(self):
-        self.assertRaises(TypeError, self.graph.add_node, 'a')
+        self.assertRaises(TypeError, self.graph.add_node, "a")
 
     def test_add_multiple_nodes(self):
-        self.graph.add_nodes_from([('a', 'b'), ('b', 'c')])
-        self.assertListEqual(hf.recursive_sorted(self.graph.nodes()), [['a', 'b'], ['b', 'c']])
+        self.graph.add_nodes_from([("a", "b"), ("b", "c")])
+        self.assertListEqual(
+            hf.recursive_sorted(self.graph.nodes()), [["a", "b"], ["b", "c"]]
+        )
 
     def test_add_single_edge(self):
-        self.graph.add_edge(('a', 'b'), ('b', 'c'))
-        self.assertListEqual(hf.recursive_sorted(self.graph.nodes()), [['a', 'b'], ['b', 'c']])
-        self.assertListEqual(sorted([node for edge in self.graph.edges() for node in edge]),
-                             [('a', 'b'), ('b', 'c')])
+        self.graph.add_edge(("a", "b"), ("b", "c"))
+        self.assertListEqual(
+            hf.recursive_sorted(self.graph.nodes()), [["a", "b"], ["b", "c"]]
+        )
+        self.assertListEqual(
+            sorted([node for edge in self.graph.edges() for node in edge]),
+            [("a", "b"), ("b", "c")],
+        )
 
     def test_add_single_edge_raises_error(self):
-        self.assertRaises(ValueError, self.graph.add_edge, ('a', 'b'), ('c', 'd'))
+        self.assertRaises(ValueError, self.graph.add_edge, ("a", "b"), ("c", "d"))
 
     def tearDown(self):
         del self.graph
 
 
 class TestClusterGraphFactorOperations(unittest.TestCase):
     def setUp(self):
         self.graph = ClusterGraph()
 
     def test_add_single_factor(self):
-        self.graph.add_node(('a', 'b'))
-        phi1 = DiscreteFactor(['a', 'b'], [2, 2], np.random.rand(4))
+        self.graph.add_node(("a", "b"))
+        phi1 = DiscreteFactor(["a", "b"], [2, 2], np.random.rand(4))
         self.graph.add_factors(phi1)
         six.assertCountEqual(self, self.graph.factors, [phi1])
 
     def test_add_single_factor_raises_error(self):
-        self.graph.add_node(('a', 'b'))
-        phi1 = DiscreteFactor(['b', 'c'], [2, 2], np.random.rand(4))
+        self.graph.add_node(("a", "b"))
+        phi1 = DiscreteFactor(["b", "c"], [2, 2], np.random.rand(4))
         self.assertRaises(ValueError, self.graph.add_factors, phi1)
 
     def test_add_multiple_factors(self):
-        self.graph.add_edges_from([[('a', 'b'), ('b', 'c')]])
-        phi1 = DiscreteFactor(['a', 'b'], [2, 2], np.random.rand(4))
-        phi2 = DiscreteFactor(['b', 'c'], [2, 2], np.random.rand(4))
+        self.graph.add_edges_from([[("a", "b"), ("b", "c")]])
+        phi1 = DiscreteFactor(["a", "b"], [2, 2], np.random.rand(4))
+        phi2 = DiscreteFactor(["b", "c"], [2, 2], np.random.rand(4))
         self.graph.add_factors(phi1, phi2)
         six.assertCountEqual(self, self.graph.factors, [phi1, phi2])
 
     def test_get_factors(self):
-        self.graph.add_edges_from([[('a', 'b'), ('b', 'c')]])
-        phi1 = DiscreteFactor(['a', 'b'], [2, 2], np.random.rand(4))
-        phi2 = DiscreteFactor(['b', 'c'], [2, 2], np.random.rand(4))
+        self.graph.add_edges_from([[("a", "b"), ("b", "c")]])
+        phi1 = DiscreteFactor(["a", "b"], [2, 2], np.random.rand(4))
+        phi2 = DiscreteFactor(["b", "c"], [2, 2], np.random.rand(4))
         six.assertCountEqual(self, self.graph.get_factors(), [])
         self.graph.add_factors(phi1, phi2)
-        self.assertEqual(self.graph.get_factors(node=('b', 'a')), phi1)
-        self.assertEqual(self.graph.get_factors(node=('b', 'c')), phi2)
+        self.assertEqual(self.graph.get_factors(node=("b", "a")), phi1)
+        self.assertEqual(self.graph.get_factors(node=("b", "c")), phi2)
         six.assertCountEqual(self, self.graph.get_factors(), [phi1, phi2])
 
     def test_remove_factors(self):
-        self.graph.add_edges_from([[('a', 'b'), ('b', 'c')]])
-        phi1 = DiscreteFactor(['a', 'b'], [2, 2], np.random.rand(4))
-        phi2 = DiscreteFactor(['b', 'c'], [2, 2], np.random.rand(4))
+        self.graph.add_edges_from([[("a", "b"), ("b", "c")]])
+        phi1 = DiscreteFactor(["a", "b"], [2, 2], np.random.rand(4))
+        phi2 = DiscreteFactor(["b", "c"], [2, 2], np.random.rand(4))
         self.graph.add_factors(phi1, phi2)
         self.graph.remove_factors(phi1)
         six.assertCountEqual(self, self.graph.factors, [phi2])
 
     def test_get_partition_function(self):
-        self.graph.add_edges_from([[('a', 'b'), ('b', 'c')]])
-        phi1 = DiscreteFactor(['a', 'b'], [2, 2], range(4))
-        phi2 = DiscreteFactor(['b', 'c'], [2, 2], range(4))
+        self.graph.add_edges_from([[("a", "b"), ("b", "c")]])
+        phi1 = DiscreteFactor(["a", "b"], [2, 2], range(4))
+        phi2 = DiscreteFactor(["b", "c"], [2, 2], range(4))
         self.graph.add_factors(phi1, phi2)
         self.assertEqual(self.graph.get_partition_function(), 22.0)
 
     def tearDown(self):
         del self.graph
 
 
 class TestClusterGraphMethods(unittest.TestCase):
     def setUp(self):
         self.graph = ClusterGraph()
 
     def test_get_cardinality(self):
 
-        self.graph.add_edges_from([(('a', 'b', 'c'), ('a', 'b')),
-                                   (('a', 'b', 'c'), ('a', 'c'))])
+        self.graph.add_edges_from(
+            [(("a", "b", "c"), ("a", "b")), (("a", "b", "c"), ("a", "c"))]
+        )
 
         self.assertDictEqual(self.graph.get_cardinality(), {})
 
-        phi1 = DiscreteFactor(['a', 'b', 'c'], [1, 2, 2], np.random.rand(4))
+        phi1 = DiscreteFactor(["a", "b", "c"], [1, 2, 2], np.random.rand(4))
         self.graph.add_factors(phi1)
-        self.assertDictEqual(self.graph.get_cardinality(), {'a': 1, 'b': 2, 'c': 2})
+        self.assertDictEqual(self.graph.get_cardinality(), {"a": 1, "b": 2, "c": 2})
         self.graph.remove_factors(phi1)
         self.assertDictEqual(self.graph.get_cardinality(), {})
 
-        phi1 = DiscreteFactor(['a', 'b'], [1, 2], np.random.rand(2))
-        phi2 = DiscreteFactor(['a', 'c'], [1, 2], np.random.rand(2))
+        phi1 = DiscreteFactor(["a", "b"], [1, 2], np.random.rand(2))
+        phi2 = DiscreteFactor(["a", "c"], [1, 2], np.random.rand(2))
         self.graph.add_factors(phi1, phi2)
-        self.assertDictEqual(self.graph.get_cardinality(), {'a': 1, 'b': 2, 'c': 2})
+        self.assertDictEqual(self.graph.get_cardinality(), {"a": 1, "b": 2, "c": 2})
 
-        phi3 = DiscreteFactor(['a', 'c'], [1, 1], np.random.rand(1))
+        phi3 = DiscreteFactor(["a", "c"], [1, 1], np.random.rand(1))
         self.graph.add_factors(phi3)
-        self.assertDictEqual(self.graph.get_cardinality(), {'c': 1, 'b': 2, 'a': 1})
+        self.assertDictEqual(self.graph.get_cardinality(), {"c": 1, "b": 2, "a": 1})
 
         self.graph.remove_factors(phi1, phi2, phi3)
         self.assertDictEqual(self.graph.get_cardinality(), {})
 
     def test_get_cardinality_with_node(self):
-        self.graph.add_edges_from([(('a', 'b'), ('a', 'c'))])
-        phi1 = DiscreteFactor(['a', 'b'], [1, 2], np.random.rand(2))
-        phi2 = DiscreteFactor(['a', 'c'], [1, 2], np.random.rand(2))
-        self.graph.add_factors(phi1, phi2)
-        self.assertEqual(self.graph.get_cardinality('a'), 1)
-        self.assertEqual(self.graph.get_cardinality('b'), 2)
-        self.assertEqual(self.graph.get_cardinality('c'), 2)
+        self.graph.add_edges_from([(("a", "b"), ("a", "c"))])
+        phi1 = DiscreteFactor(["a", "b"], [1, 2], np.random.rand(2))
+        phi2 = DiscreteFactor(["a", "c"], [1, 2], np.random.rand(2))
+        self.graph.add_factors(phi1, phi2)
+        self.assertEqual(self.graph.get_cardinality("a"), 1)
+        self.assertEqual(self.graph.get_cardinality("b"), 2)
+        self.assertEqual(self.graph.get_cardinality("c"), 2)
 
     def test_check_model(self):
-        self.graph.add_edges_from([(('a', 'b'), ('a', 'c'))])
-        phi1 = DiscreteFactor(['a', 'b'], [1, 2], np.random.rand(2))
-        phi2 = DiscreteFactor(['a', 'c'], [1, 2], np.random.rand(2))
+        self.graph.add_edges_from([(("a", "b"), ("a", "c"))])
+        phi1 = DiscreteFactor(["a", "b"], [1, 2], np.random.rand(2))
+        phi2 = DiscreteFactor(["a", "c"], [1, 2], np.random.rand(2))
         self.graph.add_factors(phi1, phi2)
         self.assertTrue(self.graph.check_model())
 
         self.graph.remove_factors(phi2)
-        phi2 = DiscreteFactor(['a', 'c'], [1, 2], np.random.rand(2))
+        phi2 = DiscreteFactor(["a", "c"], [1, 2], np.random.rand(2))
         self.graph.add_factors(phi2)
         self.assertTrue(self.graph.check_model())
 
     def test_check_model1(self):
-        self.graph.add_edges_from([(('a', 'b'), ('a', 'c')),
-                                   (('a', 'c'), ('a', 'd'))])
-        phi1 = DiscreteFactor(['a', 'b'], [1, 2], np.random.rand(2))
+        self.graph.add_edges_from([(("a", "b"), ("a", "c")), (("a", "c"), ("a", "d"))])
+        phi1 = DiscreteFactor(["a", "b"], [1, 2], np.random.rand(2))
         self.graph.add_factors(phi1)
         self.assertRaises(ValueError, self.graph.check_model)
-        phi2 = DiscreteFactor(['a', 'c'], [1, 2], np.random.rand(2))
+        phi2 = DiscreteFactor(["a", "c"], [1, 2], np.random.rand(2))
         self.graph.add_factors(phi2)
         self.assertRaises(ValueError, self.graph.check_model)
 
     def test_check_model2(self):
-        self.graph.add_edges_from([(('a', 'b'), ('a', 'c')),
-                                   (('a', 'c'), ('a', 'd'))])
+        self.graph.add_edges_from([(("a", "b"), ("a", "c")), (("a", "c"), ("a", "d"))])
 
-        phi1 = DiscreteFactor(['a', 'b'], [1, 2], np.random.rand(2))
-        phi2 = DiscreteFactor(['a', 'c'], [3, 3], np.random.rand(9))
-        phi3 = DiscreteFactor(['a', 'd'], [4, 4], np.random.rand(16))
+        phi1 = DiscreteFactor(["a", "b"], [1, 2], np.random.rand(2))
+        phi2 = DiscreteFactor(["a", "c"], [3, 3], np.random.rand(9))
+        phi3 = DiscreteFactor(["a", "d"], [4, 4], np.random.rand(16))
         self.graph.add_factors(phi1, phi2, phi3)
         self.assertRaises(ValueError, self.graph.check_model)
         self.graph.remove_factors(phi2)
-        phi2 = DiscreteFactor(['a', 'c'], [1, 3], np.random.rand(3))
+        phi2 = DiscreteFactor(["a", "c"], [1, 3], np.random.rand(3))
         self.graph.add_factors(phi2)
         self.assertRaises(ValueError, self.graph.check_model)
         self.graph.remove_factors(phi3)
 
-        phi3 = DiscreteFactor(['a', 'd'], [1, 4], np.random.rand(4))
+        phi3 = DiscreteFactor(["a", "d"], [1, 4], np.random.rand(4))
         self.graph.add_factors(phi3)
         self.assertTrue(self.graph.check_model())
 
     def test_copy_with_factors(self):
-        self.graph.add_edges_from([[('a', 'b'), ('b', 'c')]])
-        phi1 = DiscreteFactor(['a', 'b'], [2, 2], np.random.rand(4))
-        phi2 = DiscreteFactor(['b', 'c'], [2, 2], np.random.rand(4))
+        self.graph.add_edges_from([[("a", "b"), ("b", "c")]])
+        phi1 = DiscreteFactor(["a", "b"], [2, 2], np.random.rand(4))
+        phi2 = DiscreteFactor(["b", "c"], [2, 2], np.random.rand(4))
         self.graph.add_factors(phi1, phi2)
         graph_copy = self.graph.copy()
         self.assertIsInstance(graph_copy, ClusterGraph)
-        self.assertEqual(hf.recursive_sorted(self.graph.nodes()),
-                         hf.recursive_sorted(graph_copy.nodes()))
-        self.assertEqual(hf.recursive_sorted(self.graph.edges()),
-                         hf.recursive_sorted(graph_copy.edges()))
+        self.assertEqual(
+            hf.recursive_sorted(self.graph.nodes()),
+            hf.recursive_sorted(graph_copy.nodes()),
+        )
+        self.assertEqual(
+            hf.recursive_sorted(self.graph.edges()),
+            hf.recursive_sorted(graph_copy.edges()),
+        )
         self.assertTrue(graph_copy.check_model())
         self.assertEqual(self.graph.get_factors(), graph_copy.get_factors())
         self.graph.remove_factors(phi1, phi2)
-        self.assertTrue(phi1 not in self.graph.factors and phi2 not in self.graph.factors)
+        self.assertTrue(
+            phi1 not in self.graph.factors and phi2 not in self.graph.factors
+        )
         self.assertTrue(phi1 in graph_copy.factors and phi2 in graph_copy.factors)
         self.graph.add_factors(phi1, phi2)
-        self.graph.factors[0] = DiscreteFactor(['a', 'b'], [2, 2], np.random.rand(4))
+        self.graph.factors[0] = DiscreteFactor(["a", "b"], [2, 2], np.random.rand(4))
         self.assertNotEqual(self.graph.get_factors()[0], graph_copy.get_factors()[0])
         self.assertNotEqual(self.graph.factors, graph_copy.factors)
 
     def test_copy_without_factors(self):
-        self.graph.add_nodes_from([('a', 'b', 'c'), ('a', 'b'), ('a', 'c')])
-        self.graph.add_edges_from([(('a', 'b', 'c'), ('a', 'b')),
-                                   (('a', 'b', 'c'), ('a', 'c'))])
+        self.graph.add_nodes_from([("a", "b", "c"), ("a", "b"), ("a", "c")])
+        self.graph.add_edges_from(
+            [(("a", "b", "c"), ("a", "b")), (("a", "b", "c"), ("a", "c"))]
+        )
         graph_copy = self.graph.copy()
-        self.graph.remove_edge(('a', 'b', 'c'), ('a', 'c'))
-        self.assertFalse(self.graph.has_edge(('a', 'b', 'c'), ('a', 'c')))
-        self.assertTrue(graph_copy.has_edge(('a', 'b', 'c'), ('a', 'c')))
-        self.graph.remove_node(('a', 'c'))
-        self.assertFalse(self.graph.has_node(('a', 'c')))
-        self.assertTrue(graph_copy.has_node(('a', 'c')))
-        self.graph.add_node(('c', 'd'))
-        self.assertTrue(self.graph.has_node(('c', 'd')))
-        self.assertFalse(graph_copy.has_node(('c', 'd')))
+        self.graph.remove_edge(("a", "b", "c"), ("a", "c"))
+        self.assertFalse(self.graph.has_edge(("a", "b", "c"), ("a", "c")))
+        self.assertTrue(graph_copy.has_edge(("a", "b", "c"), ("a", "c")))
+        self.graph.remove_node(("a", "c"))
+        self.assertFalse(self.graph.has_node(("a", "c")))
+        self.assertTrue(graph_copy.has_node(("a", "c")))
+        self.graph.add_node(("c", "d"))
+        self.assertTrue(self.graph.has_node(("c", "d")))
+        self.assertFalse(graph_copy.has_node(("c", "d")))
 
     def tearDown(self):
         del self.graph
```

### Comparing `pgmpy-0.1.7/pgmpy/tests/test_models/test_LinearGaussianBayesianNetwork.py` & `pgmpy-0.1.9/pgmpy/tests/test_models/test_LinearGaussianBayesianNetwork.py`

 * *Files 16% similar despite different names*

```diff
@@ -7,72 +7,84 @@
 from pgmpy.factors.discrete import TabularCPD
 from pgmpy.models import LinearGaussianBayesianNetwork
 
 
 class TestLGBNMethods(unittest.TestCase):
     @unittest.skip("TODO")
     def setUp(self):
-        self.model = LinearGaussianBayesianNetwork([('x1', 'x2'), ('x2', 'x3')])
-        self.cpd1 = LinearGaussianCPD('x1', [1], 4)
-        self.cpd2 = LinearGaussianCPD('x2', [-5, 0.5], 4, ['x1'])
-        self.cpd3 = LinearGaussianCPD('x3', [4, -1], 3, ['x2'])
+        self.model = LinearGaussianBayesianNetwork([("x1", "x2"), ("x2", "x3")])
+        self.cpd1 = LinearGaussianCPD("x1", [1], 4)
+        self.cpd2 = LinearGaussianCPD("x2", [-5, 0.5], 4, ["x1"])
+        self.cpd3 = LinearGaussianCPD("x3", [4, -1], 3, ["x2"])
 
     @unittest.skip("TODO")
     def test_add_cpds(self):
         self.model.add_cpds(self.cpd1)
-        cpd = self.model.get_cpds('x1')
+        cpd = self.model.get_cpds("x1")
         self.assertEqual(cpd.variable, self.cpd1.variable)
         self.assertEqual(cpd.variance, self.cpd1.variance)
         self.assertEqual(cpd.beta_0, self.cpd1.beta_0)
 
         self.model.add_cpds(self.cpd2)
-        cpd = self.model.get_cpds('x2')
+        cpd = self.model.get_cpds("x2")
         self.assertEqual(cpd.variable, self.cpd2.variable)
         self.assertEqual(cpd.variance, self.cpd2.variance)
         self.assertEqual(cpd.beta_0, self.cpd2.beta_0)
         self.assertEqual(cpd.evidence, self.cpd2.evidence)
         np_test.assert_array_equal(cpd.beta_vector, self.cpd2.beta_vector)
 
         self.model.add_cpds(self.cpd3)
-        cpd = self.model.get_cpds('x3')
+        cpd = self.model.get_cpds("x3")
         self.assertEqual(cpd.variable, self.cpd3.variable)
         self.assertEqual(cpd.variance, self.cpd3.variance)
         self.assertEqual(cpd.beta_0, self.cpd3.beta_0)
         self.assertEqual(cpd.evidence, self.cpd3.evidence)
         np_test.assert_array_equal(cpd.beta_vector, self.cpd3.beta_vector)
 
-        tab_cpd = TabularCPD('grade', 3, [[0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
-                                          [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
-                                          [0.8, 0.8, 0.8, 0.8, 0.8, 0.8]],
-                             evidence=['diff', 'intel'], evidence_card=[2, 3])
+        tab_cpd = TabularCPD(
+            "grade",
+            3,
+            [
+                [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
+                [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
+                [0.8, 0.8, 0.8, 0.8, 0.8, 0.8],
+            ],
+            evidence=["diff", "intel"],
+            evidence_card=[2, 3],
+        )
         self.assertRaises(ValueError, self.model.add_cpds, tab_cpd)
         self.assertRaises(ValueError, self.model.add_cpds, 1)
         self.assertRaises(ValueError, self.model.add_cpds, 1, tab_cpd)
 
     @unittest.skip("TODO")
     def test_to_joint_gaussian(self):
         self.model.add_cpds(self.cpd1, self.cpd2, self.cpd3)
         jgd = self.model.to_joint_gaussian()
-        self.assertEqual(jgd.variables, ['x1', 'x2', 'x3'])
+        self.assertEqual(jgd.variables, ["x1", "x2", "x3"])
         np_test.assert_array_equal(jgd.mean, np.array([[1.0], [-4.5], [8.5]]))
-        np_test.assert_array_equal(jgd.covariance, np.array([[4.0, 2.0, -2.0],
-                                                             [2.0, 5.0, -5.0],
-                                                             [-2.0, -5.0, 8.0]]))
+        np_test.assert_array_equal(
+            jgd.covariance,
+            np.array([[4.0, 2.0, -2.0], [2.0, 5.0, -5.0], [-2.0, -5.0, 8.0]]),
+        )
 
     @unittest.skip("TODO")
     def test_check_model(self):
         self.model.add_cpds(self.cpd1, self.cpd2, self.cpd3)
         self.assertEqual(self.model.check_model(), True)
 
-        self.model.add_edge('x1', 'x4')
-        cpd4 = LinearGaussianCPD('x4', [4, -1], 3, ['x2'])
+        self.model.add_edge("x1", "x4")
+        cpd4 = LinearGaussianCPD("x4", [4, -1], 3, ["x2"])
         self.model.add_cpds(cpd4)
 
         self.assertRaises(ValueError, self.model.check_model)
 
     @unittest.skip("TODO")
     def test_not_implemented_methods(self):
-        self.assertRaises(ValueError, self.model.get_cardinality, 'x1')
+        self.assertRaises(ValueError, self.model.get_cardinality, "x1")
         self.assertRaises(NotImplementedError, self.model.fit, [[1, 2, 3], [1, 5, 6]])
-        self.assertRaises(NotImplementedError, self.model.predict, [[1, 2, 3], [1, 5, 6]])
+        self.assertRaises(
+            NotImplementedError, self.model.predict, [[1, 2, 3], [1, 5, 6]]
+        )
         self.assertRaises(NotImplementedError, self.model.to_markov_model)
-        self.assertRaises(NotImplementedError, self.model.is_imap, [[1, 2, 3], [1, 5, 6]])
+        self.assertRaises(
+            NotImplementedError, self.model.is_imap, [[1, 2, 3], [1, 5, 6]]
+        )
```

### Comparing `pgmpy-0.1.7/pgmpy/tests/test_models/test_MarkovModel.py` & `pgmpy-0.1.9/pgmpy/tests/test_models/test_MarkovModel.py`

 * *Files 22% similar despite different names*

```diff
@@ -16,605 +16,620 @@
     def setUp(self):
         self.graph = MarkovModel()
 
     def test_class_init_without_data(self):
         self.assertIsInstance(self.graph, MarkovModel)
 
     def test_class_init_with_data_string(self):
-        self.g = MarkovModel([('a', 'b'), ('b', 'c')])
-        self.assertListEqual(sorted(self.g.nodes()), ['a', 'b', 'c'])
-        self.assertListEqual(hf.recursive_sorted(self.g.edges()),
-                             [['a', 'b'], ['b', 'c']])
+        self.g = MarkovModel([("a", "b"), ("b", "c")])
+        self.assertListEqual(sorted(self.g.nodes()), ["a", "b", "c"])
+        self.assertListEqual(
+            hf.recursive_sorted(self.g.edges()), [["a", "b"], ["b", "c"]]
+        )
 
     def test_class_init_with_data_nonstring(self):
         self.g = MarkovModel([(1, 2), (2, 3)])
 
     def test_add_node_string(self):
-        self.graph.add_node('a')
-        self.assertListEqual(self.graph.nodes(), ['a'])
+        self.graph.add_node("a")
+        self.assertListEqual(list(self.graph.nodes()), ["a"])
 
     def test_add_node_nonstring(self):
         self.graph.add_node(1)
 
     def test_add_nodes_from_string(self):
-        self.graph.add_nodes_from(['a', 'b', 'c', 'd'])
-        self.assertListEqual(sorted(self.graph.nodes()), ['a', 'b', 'c', 'd'])
+        self.graph.add_nodes_from(["a", "b", "c", "d"])
+        self.assertListEqual(sorted(self.graph.nodes()), ["a", "b", "c", "d"])
 
     def test_add_nodes_from_non_string(self):
         self.graph.add_nodes_from([1, 2, 3, 4])
 
     def test_add_edge_string(self):
-        self.graph.add_edge('d', 'e')
-        self.assertListEqual(sorted(self.graph.nodes()), ['d', 'e'])
-        self.assertListEqual(hf.recursive_sorted(self.graph.edges()),
-                             [['d', 'e']])
-        self.graph.add_nodes_from(['a', 'b', 'c'])
-        self.graph.add_edge('a', 'b')
-        self.assertListEqual(hf.recursive_sorted(self.graph.edges()),
-                             [['a', 'b'], ['d', 'e']])
+        self.graph.add_edge("d", "e")
+        self.assertListEqual(sorted(self.graph.nodes()), ["d", "e"])
+        self.assertListEqual(hf.recursive_sorted(self.graph.edges()), [["d", "e"]])
+        self.graph.add_nodes_from(["a", "b", "c"])
+        self.graph.add_edge("a", "b")
+        self.assertListEqual(
+            hf.recursive_sorted(self.graph.edges()), [["a", "b"], ["d", "e"]]
+        )
 
     def test_add_edge_nonstring(self):
         self.graph.add_edge(1, 2)
 
     def test_add_edge_selfloop(self):
-        self.assertRaises(ValueError, self.graph.add_edge, 'a', 'a')
+        self.assertRaises(ValueError, self.graph.add_edge, "a", "a")
 
     def test_add_edges_from_string(self):
-        self.graph.add_edges_from([('a', 'b'), ('b', 'c')])
-        self.assertListEqual(sorted(self.graph.nodes()), ['a', 'b', 'c'])
-        self.assertListEqual(hf.recursive_sorted(self.graph.edges()),
-                             [['a', 'b'], ['b', 'c']])
-        self.graph.add_nodes_from(['d', 'e', 'f'])
-        self.graph.add_edges_from([('d', 'e'), ('e', 'f')])
-        self.assertListEqual(sorted(self.graph.nodes()),
-                             ['a', 'b', 'c', 'd', 'e', 'f'])
-        self.assertListEqual(hf.recursive_sorted(self.graph.edges()),
-                             hf.recursive_sorted([('a', 'b'), ('b', 'c'),
-                                                  ('d', 'e'), ('e', 'f')]))
+        self.graph.add_edges_from([("a", "b"), ("b", "c")])
+        self.assertListEqual(sorted(self.graph.nodes()), ["a", "b", "c"])
+        self.assertListEqual(
+            hf.recursive_sorted(self.graph.edges()), [["a", "b"], ["b", "c"]]
+        )
+        self.graph.add_nodes_from(["d", "e", "f"])
+        self.graph.add_edges_from([("d", "e"), ("e", "f")])
+        self.assertListEqual(sorted(self.graph.nodes()), ["a", "b", "c", "d", "e", "f"])
+        self.assertListEqual(
+            hf.recursive_sorted(self.graph.edges()),
+            hf.recursive_sorted([("a", "b"), ("b", "c"), ("d", "e"), ("e", "f")]),
+        )
 
     def test_add_edges_from_nonstring(self):
         self.graph.add_edges_from([(1, 2), (2, 3)])
 
     def test_add_edges_from_self_loop(self):
-        self.assertRaises(ValueError, self.graph.add_edges_from,
-                          [('a', 'a')])
+        self.assertRaises(ValueError, self.graph.add_edges_from, [("a", "a")])
 
     def test_number_of_neighbors(self):
-        self.graph.add_edges_from([('a', 'b'), ('b', 'c')])
-        self.assertEqual(len(self.graph.neighbors('b')), 2)
+        self.graph.add_edges_from([("a", "b"), ("b", "c")])
+        self.assertEqual(len(list(self.graph.neighbors("b"))), 2)
 
     def tearDown(self):
         del self.graph
 
 
 class TestMarkovModelMethods(unittest.TestCase):
     def setUp(self):
         self.graph = MarkovModel()
 
     def test_get_cardinality(self):
 
-        self.graph.add_edges_from([('a', 'b'), ('b', 'c'), ('c', 'd'),
-                                   ('d', 'a')])
+        self.graph.add_edges_from([("a", "b"), ("b", "c"), ("c", "d"), ("d", "a")])
 
         self.assertDictEqual(self.graph.get_cardinality(), {})
 
-        phi1 = DiscreteFactor(['a', 'b'], [1, 2], np.random.rand(2))
+        phi1 = DiscreteFactor(["a", "b"], [1, 2], np.random.rand(2))
         self.graph.add_factors(phi1)
-        self.assertDictEqual(self.graph.get_cardinality(), {'a': 1, 'b': 2})
+        self.assertDictEqual(self.graph.get_cardinality(), {"a": 1, "b": 2})
         self.graph.remove_factors(phi1)
         self.assertDictEqual(self.graph.get_cardinality(), {})
 
-        phi1 = DiscreteFactor(['a', 'b'], [2, 2], np.random.rand(4))
-        phi2 = DiscreteFactor(['c', 'd'], [1, 2], np.random.rand(2))
+        phi1 = DiscreteFactor(["a", "b"], [2, 2], np.random.rand(4))
+        phi2 = DiscreteFactor(["c", "d"], [1, 2], np.random.rand(2))
         self.graph.add_factors(phi1, phi2)
-        self.assertDictEqual(self.graph.get_cardinality(), {'d': 2, 'a': 2, 'b': 2, 'c': 1})
+        self.assertDictEqual(
+            self.graph.get_cardinality(), {"d": 2, "a": 2, "b": 2, "c": 1}
+        )
 
-        phi3 = DiscreteFactor(['d', 'a'], [1, 2], np.random.rand(2))
+        phi3 = DiscreteFactor(["d", "a"], [1, 2], np.random.rand(2))
         self.graph.add_factors(phi3)
-        self.assertDictEqual(self.graph.get_cardinality(), {'d': 1, 'c': 1, 'b': 2, 'a': 2})
+        self.assertDictEqual(
+            self.graph.get_cardinality(), {"d": 1, "c": 1, "b": 2, "a": 2}
+        )
 
         self.graph.remove_factors(phi1, phi2, phi3)
         self.assertDictEqual(self.graph.get_cardinality(), {})
 
     def test_get_cardinality_with_node(self):
 
-        self.graph.add_edges_from([('a', 'b'), ('b', 'c'), ('c', 'd'),
-                                   ('d', 'a')])
+        self.graph.add_edges_from([("a", "b"), ("b", "c"), ("c", "d"), ("d", "a")])
 
-        phi1 = DiscreteFactor(['a', 'b'], [2, 2], np.random.rand(4))
-        phi2 = DiscreteFactor(['c', 'd'], [1, 2], np.random.rand(2))
+        phi1 = DiscreteFactor(["a", "b"], [2, 2], np.random.rand(4))
+        phi2 = DiscreteFactor(["c", "d"], [1, 2], np.random.rand(2))
         self.graph.add_factors(phi1, phi2)
-        self.assertEqual(self.graph.get_cardinality('a'), 2)
-        self.assertEqual(self.graph.get_cardinality('b'), 2)
-        self.assertEqual(self.graph.get_cardinality('c'), 1)
-        self.assertEqual(self.graph.get_cardinality('d'), 2)
+        self.assertEqual(self.graph.get_cardinality("a"), 2)
+        self.assertEqual(self.graph.get_cardinality("b"), 2)
+        self.assertEqual(self.graph.get_cardinality("c"), 1)
+        self.assertEqual(self.graph.get_cardinality("d"), 2)
 
     def test_check_model(self):
-        self.graph.add_edges_from([('a', 'b'), ('b', 'c'), ('c', 'd'),
-                                   ('d', 'a')])
+        self.graph.add_edges_from([("a", "b"), ("b", "c"), ("c", "d"), ("d", "a")])
 
-        phi1 = DiscreteFactor(['a', 'b'], [1, 2], np.random.rand(2))
+        phi1 = DiscreteFactor(["a", "b"], [1, 2], np.random.rand(2))
         self.graph.add_factors(phi1)
         self.assertRaises(ValueError, self.graph.check_model)
 
-        phi2 = DiscreteFactor(['a', 'c'], [1, 2], np.random.rand(2))
+        phi2 = DiscreteFactor(["a", "c"], [1, 2], np.random.rand(2))
         self.graph.add_factors(phi2)
         self.assertRaises(ValueError, self.graph.check_model)
 
     def test_check_model1(self):
-        self.graph.add_edges_from([('a', 'b'), ('b', 'c'), ('c', 'd'),
-                                   ('d', 'a')])
-        phi1 = DiscreteFactor(['a', 'b'], [1, 2], np.random.rand(2))
-        phi2 = DiscreteFactor(['c', 'b'], [3, 2], np.random.rand(6))
-        phi3 = DiscreteFactor(['c', 'd'], [3, 4], np.random.rand(12))
-        phi4 = DiscreteFactor(['d', 'a'], [4, 1], np.random.rand(4))
+        self.graph.add_edges_from([("a", "b"), ("b", "c"), ("c", "d"), ("d", "a")])
+        phi1 = DiscreteFactor(["a", "b"], [1, 2], np.random.rand(2))
+        phi2 = DiscreteFactor(["c", "b"], [3, 2], np.random.rand(6))
+        phi3 = DiscreteFactor(["c", "d"], [3, 4], np.random.rand(12))
+        phi4 = DiscreteFactor(["d", "a"], [4, 1], np.random.rand(4))
 
         self.graph.add_factors(phi1, phi2, phi3, phi4)
         self.assertTrue(self.graph.check_model())
 
         self.graph.remove_factors(phi1, phi4)
-        phi1 = DiscreteFactor(['a', 'b'], [4, 2], np.random.rand(8))
+        phi1 = DiscreteFactor(["a", "b"], [4, 2], np.random.rand(8))
         self.graph.add_factors(phi1)
         self.assertTrue(self.graph.check_model())
 
     def test_check_model2(self):
-        self.graph.add_edges_from([('a', 'b'), ('b', 'c'), ('c', 'd'),
-                                   ('d', 'a')])
+        self.graph.add_edges_from([("a", "b"), ("b", "c"), ("c", "d"), ("d", "a")])
 
-        phi1 = DiscreteFactor(['a', 'b'], [1, 2], np.random.rand(2))
+        phi1 = DiscreteFactor(["a", "b"], [1, 2], np.random.rand(2))
 
-        phi2 = DiscreteFactor(['b', 'c'], [3, 3], np.random.rand(9))
+        phi2 = DiscreteFactor(["b", "c"], [3, 3], np.random.rand(9))
         self.graph.add_factors(phi1, phi2)
         self.assertRaises(ValueError, self.graph.check_model)
         self.graph.remove_factors(phi2)
 
-        phi3 = DiscreteFactor(['c', 'a'], [4, 4], np.random.rand(16))
+        phi3 = DiscreteFactor(["c", "a"], [4, 4], np.random.rand(16))
         self.graph.add_factors(phi3)
         self.assertRaises(ValueError, self.graph.check_model)
         self.graph.remove_factors(phi3)
 
-        phi2 = DiscreteFactor(['b', 'c'], [2, 3], np.random.rand(6))
-        phi3 = DiscreteFactor(['c', 'd'], [3, 4], np.random.rand(12))
-        phi4 = DiscreteFactor(['d', 'a'], [4, 3], np.random.rand(12))
+        phi2 = DiscreteFactor(["b", "c"], [2, 3], np.random.rand(6))
+        phi3 = DiscreteFactor(["c", "d"], [3, 4], np.random.rand(12))
+        phi4 = DiscreteFactor(["d", "a"], [4, 3], np.random.rand(12))
         self.graph.add_factors(phi2, phi3, phi4)
         self.assertRaises(ValueError, self.graph.check_model)
         self.graph.remove_factors(phi2, phi3, phi4)
 
-        phi2 = DiscreteFactor(['a', 'b'], [1, 3], np.random.rand(3))
+        phi2 = DiscreteFactor(["a", "b"], [1, 3], np.random.rand(3))
         self.graph.add_factors(phi1, phi2)
         self.assertRaises(ValueError, self.graph.check_model)
         self.graph.remove_factors(phi2)
 
     def test_check_model3(self):
-        self.graph.add_edges_from([('a', 'b'), ('b', 'c'), ('c', 'd'),
-                                   ('d', 'a')])
+        self.graph.add_edges_from([("a", "b"), ("b", "c"), ("c", "d"), ("d", "a")])
 
-        phi1 = DiscreteFactor(['a', 'c'], [1, 2], np.random.rand(2))
+        phi1 = DiscreteFactor(["a", "c"], [1, 2], np.random.rand(2))
         self.graph.add_factors(phi1)
         self.assertRaises(ValueError, self.graph.check_model)
         self.graph.remove_factors(phi1)
 
-        phi1 = DiscreteFactor(['a', 'b'], [1, 2], np.random.rand(2))
-        phi2 = DiscreteFactor(['a', 'c'], [1, 2], np.random.rand(2))
+        phi1 = DiscreteFactor(["a", "b"], [1, 2], np.random.rand(2))
+        phi2 = DiscreteFactor(["a", "c"], [1, 2], np.random.rand(2))
         self.graph.add_factors(phi1, phi2)
         self.assertRaises(ValueError, self.graph.check_model)
         self.graph.remove_factors(phi1, phi2)
 
-        phi1 = DiscreteFactor(['a', 'b'], [1, 2], np.random.rand(2))
-        phi2 = DiscreteFactor(['b', 'c'], [2, 3], np.random.rand(6))
-        phi3 = DiscreteFactor(['c', 'd'], [3, 4], np.random.rand(12))
-        phi4 = DiscreteFactor(['d', 'a'], [4, 1], np.random.rand(4))
-        phi5 = DiscreteFactor(['d', 'b'], [4, 2], np.random.rand(8))
+        phi1 = DiscreteFactor(["a", "b"], [1, 2], np.random.rand(2))
+        phi2 = DiscreteFactor(["b", "c"], [2, 3], np.random.rand(6))
+        phi3 = DiscreteFactor(["c", "d"], [3, 4], np.random.rand(12))
+        phi4 = DiscreteFactor(["d", "a"], [4, 1], np.random.rand(4))
+        phi5 = DiscreteFactor(["d", "b"], [4, 2], np.random.rand(8))
         self.graph.add_factors(phi1, phi2, phi3, phi4, phi5)
         self.assertRaises(ValueError, self.graph.check_model)
         self.graph.remove_factors(phi1, phi2, phi3, phi4, phi5)
 
     def test_factor_graph(self):
-        phi1 = DiscreteFactor(['Alice', 'Bob'], [3, 2], np.random.rand(6))
-        phi2 = DiscreteFactor(['Bob', 'Charles'], [2, 2], np.random.rand(4))
-        self.graph.add_edges_from([('Alice', 'Bob'), ('Bob', 'Charles')])
+        phi1 = DiscreteFactor(["Alice", "Bob"], [3, 2], np.random.rand(6))
+        phi2 = DiscreteFactor(["Bob", "Charles"], [2, 2], np.random.rand(4))
+        self.graph.add_edges_from([("Alice", "Bob"), ("Bob", "Charles")])
         self.graph.add_factors(phi1, phi2)
 
         factor_graph = self.graph.to_factor_graph()
         self.assertIsInstance(factor_graph, FactorGraph)
-        self.assertListEqual(sorted(factor_graph.nodes()),
-                             ['Alice', 'Bob', 'Charles', 'phi_Alice_Bob',
-                              'phi_Bob_Charles'])
-        self.assertListEqual(hf.recursive_sorted(factor_graph.edges()),
-                             [['Alice', 'phi_Alice_Bob'], ['Bob', 'phi_Alice_Bob'],
-                              ['Bob', 'phi_Bob_Charles'], ['Charles', 'phi_Bob_Charles']])
+        self.assertListEqual(
+            sorted(factor_graph.nodes()),
+            ["Alice", "Bob", "Charles", "phi_Alice_Bob", "phi_Bob_Charles"],
+        )
+        self.assertListEqual(
+            hf.recursive_sorted(factor_graph.edges()),
+            [
+                ["Alice", "phi_Alice_Bob"],
+                ["Bob", "phi_Alice_Bob"],
+                ["Bob", "phi_Bob_Charles"],
+                ["Charles", "phi_Bob_Charles"],
+            ],
+        )
         self.assertListEqual(factor_graph.get_factors(), [phi1, phi2])
 
     def test_factor_graph_raises_error(self):
-        self.graph.add_edges_from([('Alice', 'Bob'), ('Bob', 'Charles')])
+        self.graph.add_edges_from([("Alice", "Bob"), ("Bob", "Charles")])
         self.assertRaises(ValueError, self.graph.to_factor_graph)
 
     def test_junction_tree(self):
-        self.graph.add_edges_from([('a', 'b'), ('b', 'c'), ('c', 'd'),
-                                   ('d', 'a')])
-        phi1 = DiscreteFactor(['a', 'b'], [2, 3], np.random.rand(6))
-        phi2 = DiscreteFactor(['b', 'c'], [3, 4], np.random.rand(12))
-        phi3 = DiscreteFactor(['c', 'd'], [4, 5], np.random.rand(20))
-        phi4 = DiscreteFactor(['d', 'a'], [5, 2], np.random.random(10))
+        self.graph.add_edges_from([("a", "b"), ("b", "c"), ("c", "d"), ("d", "a")])
+        phi1 = DiscreteFactor(["a", "b"], [2, 3], np.random.rand(6))
+        phi2 = DiscreteFactor(["b", "c"], [3, 4], np.random.rand(12))
+        phi3 = DiscreteFactor(["c", "d"], [4, 5], np.random.rand(20))
+        phi4 = DiscreteFactor(["d", "a"], [5, 2], np.random.random(10))
         self.graph.add_factors(phi1, phi2, phi3, phi4)
 
         junction_tree = self.graph.to_junction_tree()
-        self.assertListEqual(hf.recursive_sorted(junction_tree.nodes()),
-                             [['a', 'b', 'd'], ['b', 'c', 'd']])
+        self.assertListEqual(
+            hf.recursive_sorted(junction_tree.nodes()),
+            [["a", "b", "d"], ["b", "c", "d"]],
+        )
         self.assertEqual(len(junction_tree.edges()), 1)
 
     def test_junction_tree_single_clique(self):
 
-        self.graph.add_edges_from([('x1', 'x2'), ('x2', 'x3'), ('x1', 'x3')])
-        phi = [DiscreteFactor(edge, [2, 2], np.random.rand(4)) for edge in self.graph.edges()]
+        self.graph.add_edges_from([("x1", "x2"), ("x2", "x3"), ("x1", "x3")])
+        phi = [
+            DiscreteFactor(edge, [2, 2], np.random.rand(4))
+            for edge in self.graph.edges()
+        ]
         self.graph.add_factors(*phi)
 
         junction_tree = self.graph.to_junction_tree()
-        self.assertListEqual(hf.recursive_sorted(junction_tree.nodes()),
-                             [['x1', 'x2', 'x3']])
+        self.assertListEqual(
+            hf.recursive_sorted(junction_tree.nodes()), [["x1", "x2", "x3"]]
+        )
         factors = junction_tree.get_factors()
         self.assertEqual(factors[0], factor_product(*phi))
 
     def test_markov_blanket(self):
-        self.graph.add_edges_from([('a', 'b'), ('b', 'c')])
-        self.assertListEqual(self.graph.markov_blanket('a'), ['b'])
-        self.assertListEqual(sorted(self.graph.markov_blanket('b')),
-                             ['a', 'c'])
+        self.graph.add_edges_from([("a", "b"), ("b", "c")])
+        self.assertListEqual(list(self.graph.markov_blanket("a")), ["b"])
+        self.assertListEqual(sorted(self.graph.markov_blanket("b")), ["a", "c"])
 
     def test_local_independencies(self):
-        self.graph.add_edges_from([('a', 'b'), ('b', 'c')])
+        self.graph.add_edges_from([("a", "b"), ("b", "c")])
         independencies = self.graph.get_local_independencies()
         self.assertIsInstance(independencies, Independencies)
-        self.assertEqual(independencies, Independencies(['a', 'c', 'b']))
+        self.assertEqual(independencies, Independencies(["a", "c", "b"]))
 
     def test_bayesian_model(self):
-        self.graph.add_edges_from([('a', 'b'), ('b', 'c'), ('c', 'd'),
-                                   ('d', 'a')])
-        phi1 = DiscreteFactor(['a', 'b'], [2, 3], np.random.rand(6))
-        phi2 = DiscreteFactor(['b', 'c'], [3, 4], np.random.rand(12))
-        phi3 = DiscreteFactor(['c', 'd'], [4, 5], np.random.rand(20))
-        phi4 = DiscreteFactor(['d', 'a'], [5, 2], np.random.random(10))
+        self.graph.add_edges_from([("a", "b"), ("b", "c"), ("c", "d"), ("d", "a")])
+        phi1 = DiscreteFactor(["a", "b"], [2, 3], np.random.rand(6))
+        phi2 = DiscreteFactor(["b", "c"], [3, 4], np.random.rand(12))
+        phi3 = DiscreteFactor(["c", "d"], [4, 5], np.random.rand(20))
+        phi4 = DiscreteFactor(["d", "a"], [5, 2], np.random.random(10))
         self.graph.add_factors(phi1, phi2, phi3, phi4)
 
         bm = self.graph.to_bayesian_model()
         self.assertIsInstance(bm, BayesianModel)
-        self.assertListEqual(sorted(bm.nodes()), ['a', 'b', 'c', 'd'])
+        self.assertListEqual(sorted(bm.nodes()), ["a", "b", "c", "d"])
         self.assertTrue(nx.is_chordal(bm.to_undirected()))
 
     def tearDown(self):
         del self.graph
 
 
 class TestUndirectedGraphFactorOperations(unittest.TestCase):
     def setUp(self):
         self.graph = MarkovModel()
 
     def test_add_factor_raises_error(self):
-        self.graph.add_edges_from([('Alice', 'Bob'), ('Bob', 'Charles'),
-                                   ('Charles', 'Debbie'), ('Debbie', 'Alice')])
-        factor = DiscreteFactor(['Alice', 'Bob', 'John'], [2, 2, 2], np.random.rand(8))
+        self.graph.add_edges_from(
+            [
+                ("Alice", "Bob"),
+                ("Bob", "Charles"),
+                ("Charles", "Debbie"),
+                ("Debbie", "Alice"),
+            ]
+        )
+        factor = DiscreteFactor(["Alice", "Bob", "John"], [2, 2, 2], np.random.rand(8))
         self.assertRaises(ValueError, self.graph.add_factors, factor)
 
     def test_add_single_factor(self):
-        self.graph.add_nodes_from(['a', 'b', 'c'])
-        phi = DiscreteFactor(['a', 'b'], [2, 2], range(4))
+        self.graph.add_nodes_from(["a", "b", "c"])
+        phi = DiscreteFactor(["a", "b"], [2, 2], range(4))
         self.graph.add_factors(phi)
         six.assertCountEqual(self, self.graph.factors, [phi])
 
     def test_add_multiple_factors(self):
-        self.graph.add_nodes_from(['a', 'b', 'c'])
-        phi1 = DiscreteFactor(['a', 'b'], [2, 2], range(4))
-        phi2 = DiscreteFactor(['b', 'c'], [2, 2], range(4))
+        self.graph.add_nodes_from(["a", "b", "c"])
+        phi1 = DiscreteFactor(["a", "b"], [2, 2], range(4))
+        phi2 = DiscreteFactor(["b", "c"], [2, 2], range(4))
         self.graph.add_factors(phi1, phi2)
         six.assertCountEqual(self, self.graph.factors, [phi1, phi2])
 
     def test_get_factors(self):
-        self.graph.add_nodes_from(['a', 'b', 'c'])
-        phi1 = DiscreteFactor(['a', 'b'], [2, 2], range(4))
-        phi2 = DiscreteFactor(['b', 'c'], [2, 2], range(4))
+        self.graph.add_nodes_from(["a", "b", "c"])
+        phi1 = DiscreteFactor(["a", "b"], [2, 2], range(4))
+        phi2 = DiscreteFactor(["b", "c"], [2, 2], range(4))
         six.assertCountEqual(self, self.graph.get_factors(), [])
         self.graph.add_factors(phi1, phi2)
         six.assertCountEqual(self, self.graph.get_factors(), [phi1, phi2])
-        six.assertCountEqual(self, self.graph.get_factors('a'), [phi1])
+        six.assertCountEqual(self, self.graph.get_factors("a"), [phi1])
 
     def test_remove_single_factor(self):
-        self.graph.add_nodes_from(['a', 'b', 'c'])
-        phi1 = DiscreteFactor(['a', 'b'], [2, 2], range(4))
-        phi2 = DiscreteFactor(['b', 'c'], [2, 2], range(4))
+        self.graph.add_nodes_from(["a", "b", "c"])
+        phi1 = DiscreteFactor(["a", "b"], [2, 2], range(4))
+        phi2 = DiscreteFactor(["b", "c"], [2, 2], range(4))
         self.graph.add_factors(phi1, phi2)
         self.graph.remove_factors(phi1)
         six.assertCountEqual(self, self.graph.factors, [phi2])
 
     def test_remove_multiple_factors(self):
-        self.graph.add_nodes_from(['a', 'b', 'c'])
-        phi1 = DiscreteFactor(['a', 'b'], [2, 2], range(4))
-        phi2 = DiscreteFactor(['b', 'c'], [2, 2], range(4))
+        self.graph.add_nodes_from(["a", "b", "c"])
+        phi1 = DiscreteFactor(["a", "b"], [2, 2], range(4))
+        phi2 = DiscreteFactor(["b", "c"], [2, 2], range(4))
         self.graph.add_factors(phi1, phi2)
         self.graph.remove_factors(phi1, phi2)
         six.assertCountEqual(self, self.graph.factors, [])
 
     def test_partition_function(self):
-        self.graph.add_nodes_from(['a', 'b', 'c'])
-        phi1 = DiscreteFactor(['a', 'b'], [2, 2], range(4))
-        phi2 = DiscreteFactor(['b', 'c'], [2, 2], range(4))
+        self.graph.add_nodes_from(["a", "b", "c"])
+        phi1 = DiscreteFactor(["a", "b"], [2, 2], range(4))
+        phi2 = DiscreteFactor(["b", "c"], [2, 2], range(4))
         self.graph.add_factors(phi1, phi2)
-        self.graph.add_edges_from([('a', 'b'), ('b', 'c')])
+        self.graph.add_edges_from([("a", "b"), ("b", "c")])
         self.assertEqual(self.graph.get_partition_function(), 22.0)
 
     def test_partition_function_raises_error(self):
-        self.graph.add_nodes_from(['a', 'b', 'c', 'd'])
-        phi1 = DiscreteFactor(['a', 'b'], [2, 2], range(4))
-        phi2 = DiscreteFactor(['b', 'c'], [2, 2], range(4))
+        self.graph.add_nodes_from(["a", "b", "c", "d"])
+        phi1 = DiscreteFactor(["a", "b"], [2, 2], range(4))
+        phi2 = DiscreteFactor(["b", "c"], [2, 2], range(4))
         self.graph.add_factors(phi1, phi2)
-        self.assertRaises(ValueError,
-                          self.graph.get_partition_function)
+        self.assertRaises(ValueError, self.graph.get_partition_function)
 
     def tearDown(self):
         del self.graph
 
 
 class TestUndirectedGraphTriangulation(unittest.TestCase):
     def setUp(self):
         self.graph = MarkovModel()
 
     def test_check_clique(self):
-        self.graph.add_edges_from([('a', 'b'), ('b', 'c'), ('c', 'a')])
-        self.assertTrue(self.graph.is_clique(['a', 'b', 'c']))
+        self.graph.add_edges_from([("a", "b"), ("b", "c"), ("c", "a")])
+        self.assertTrue(self.graph.is_clique(["a", "b", "c"]))
 
     def test_is_triangulated(self):
-        self.graph.add_edges_from([('a', 'b'), ('b', 'c'), ('c', 'a')])
+        self.graph.add_edges_from([("a", "b"), ("b", "c"), ("c", "a")])
         self.assertTrue(self.graph.is_triangulated())
 
     def test_triangulation_h1_inplace(self):
-        self.graph.add_edges_from([('a', 'b'), ('b', 'c'), ('c', 'd'),
-                                   ('d', 'a')])
-        phi1 = DiscreteFactor(['a', 'b'], [2, 3], np.random.rand(6))
-        phi2 = DiscreteFactor(['b', 'c'], [3, 4], np.random.rand(12))
-        phi3 = DiscreteFactor(['c', 'd'], [4, 5], np.random.rand(20))
-        phi4 = DiscreteFactor(['d', 'a'], [5, 2], np.random.random(10))
+        self.graph.add_edges_from([("a", "b"), ("b", "c"), ("c", "d"), ("d", "a")])
+        phi1 = DiscreteFactor(["a", "b"], [2, 3], np.random.rand(6))
+        phi2 = DiscreteFactor(["b", "c"], [3, 4], np.random.rand(12))
+        phi3 = DiscreteFactor(["c", "d"], [4, 5], np.random.rand(20))
+        phi4 = DiscreteFactor(["d", "a"], [5, 2], np.random.random(10))
         self.graph.add_factors(phi1, phi2, phi3, phi4)
-        self.graph.triangulate(heuristic='H1', inplace=True)
+        self.graph.triangulate(heuristic="H1", inplace=True)
         self.assertTrue(self.graph.is_triangulated())
-        self.assertListEqual(hf.recursive_sorted(self.graph.edges()),
-                             [['a', 'b'], ['a', 'c'], ['a', 'd'],
-                              ['b', 'c'], ['c', 'd']])
+        self.assertListEqual(
+            hf.recursive_sorted(self.graph.edges()),
+            [["a", "b"], ["a", "c"], ["a", "d"], ["b", "c"], ["c", "d"]],
+        )
 
     def test_triangulation_h2_inplace(self):
-        self.graph.add_edges_from([('a', 'b'), ('b', 'c'), ('c', 'd'),
-                                   ('d', 'a')])
-        phi1 = DiscreteFactor(['a', 'b'], [2, 3], np.random.rand(6))
-        phi2 = DiscreteFactor(['b', 'c'], [3, 4], np.random.rand(12))
-        phi3 = DiscreteFactor(['c', 'd'], [4, 5], np.random.rand(20))
-        phi4 = DiscreteFactor(['d', 'a'], [5, 2], np.random.random(10))
+        self.graph.add_edges_from([("a", "b"), ("b", "c"), ("c", "d"), ("d", "a")])
+        phi1 = DiscreteFactor(["a", "b"], [2, 3], np.random.rand(6))
+        phi2 = DiscreteFactor(["b", "c"], [3, 4], np.random.rand(12))
+        phi3 = DiscreteFactor(["c", "d"], [4, 5], np.random.rand(20))
+        phi4 = DiscreteFactor(["d", "a"], [5, 2], np.random.random(10))
         self.graph.add_factors(phi1, phi2, phi3, phi4)
-        self.graph.triangulate(heuristic='H2', inplace=True)
+        self.graph.triangulate(heuristic="H2", inplace=True)
         self.assertTrue(self.graph.is_triangulated())
-        self.assertListEqual(hf.recursive_sorted(self.graph.edges()),
-                             [['a', 'b'], ['a', 'c'], ['a', 'd'],
-                              ['b', 'c'], ['c', 'd']])
+        self.assertListEqual(
+            hf.recursive_sorted(self.graph.edges()),
+            [["a", "b"], ["a", "c"], ["a", "d"], ["b", "c"], ["c", "d"]],
+        )
 
     def test_triangulation_h3_inplace(self):
-        self.graph.add_edges_from([('a', 'b'), ('b', 'c'), ('c', 'd'),
-                                   ('d', 'a')])
-        phi1 = DiscreteFactor(['a', 'b'], [2, 3], np.random.rand(6))
-        phi2 = DiscreteFactor(['b', 'c'], [3, 4], np.random.rand(12))
-        phi3 = DiscreteFactor(['c', 'd'], [4, 5], np.random.rand(20))
-        phi4 = DiscreteFactor(['d', 'a'], [5, 2], np.random.random(10))
+        self.graph.add_edges_from([("a", "b"), ("b", "c"), ("c", "d"), ("d", "a")])
+        phi1 = DiscreteFactor(["a", "b"], [2, 3], np.random.rand(6))
+        phi2 = DiscreteFactor(["b", "c"], [3, 4], np.random.rand(12))
+        phi3 = DiscreteFactor(["c", "d"], [4, 5], np.random.rand(20))
+        phi4 = DiscreteFactor(["d", "a"], [5, 2], np.random.random(10))
         self.graph.add_factors(phi1, phi2, phi3, phi4)
-        self.graph.triangulate(heuristic='H3', inplace=True)
+        self.graph.triangulate(heuristic="H3", inplace=True)
         self.assertTrue(self.graph.is_triangulated())
-        self.assertListEqual(hf.recursive_sorted(self.graph.edges()),
-                             [['a', 'b'], ['a', 'd'], ['b', 'c'],
-                              ['b', 'd'], ['c', 'd']])
+        self.assertListEqual(
+            hf.recursive_sorted(self.graph.edges()),
+            [["a", "b"], ["a", "d"], ["b", "c"], ["b", "d"], ["c", "d"]],
+        )
 
     def test_triangulation_h4_inplace(self):
-        self.graph.add_edges_from([('a', 'b'), ('b', 'c'), ('c', 'd'),
-                                   ('d', 'a')])
-        phi1 = DiscreteFactor(['a', 'b'], [2, 3], np.random.rand(6))
-        phi2 = DiscreteFactor(['b', 'c'], [3, 4], np.random.rand(12))
-        phi3 = DiscreteFactor(['c', 'd'], [4, 5], np.random.rand(20))
-        phi4 = DiscreteFactor(['d', 'a'], [5, 2], np.random.random(10))
+        self.graph.add_edges_from([("a", "b"), ("b", "c"), ("c", "d"), ("d", "a")])
+        phi1 = DiscreteFactor(["a", "b"], [2, 3], np.random.rand(6))
+        phi2 = DiscreteFactor(["b", "c"], [3, 4], np.random.rand(12))
+        phi3 = DiscreteFactor(["c", "d"], [4, 5], np.random.rand(20))
+        phi4 = DiscreteFactor(["d", "a"], [5, 2], np.random.random(10))
         self.graph.add_factors(phi1, phi2, phi3, phi4)
-        self.graph.triangulate(heuristic='H4', inplace=True)
+        self.graph.triangulate(heuristic="H4", inplace=True)
         self.assertTrue(self.graph.is_triangulated())
-        self.assertListEqual(hf.recursive_sorted(self.graph.edges()),
-                             [['a', 'b'], ['a', 'd'], ['b', 'c'],
-                              ['b', 'd'], ['c', 'd']])
+        self.assertListEqual(
+            hf.recursive_sorted(self.graph.edges()),
+            [["a", "b"], ["a", "d"], ["b", "c"], ["b", "d"], ["c", "d"]],
+        )
 
     def test_triangulation_h5_inplace(self):
-        self.graph.add_edges_from([('a', 'b'), ('b', 'c'), ('c', 'd'),
-                                   ('d', 'a')])
-        phi1 = DiscreteFactor(['a', 'b'], [2, 3], np.random.rand(6))
-        phi2 = DiscreteFactor(['b', 'c'], [3, 4], np.random.rand(12))
-        phi3 = DiscreteFactor(['c', 'd'], [4, 5], np.random.rand(20))
-        phi4 = DiscreteFactor(['d', 'a'], [5, 2], np.random.random(10))
+        self.graph.add_edges_from([("a", "b"), ("b", "c"), ("c", "d"), ("d", "a")])
+        phi1 = DiscreteFactor(["a", "b"], [2, 3], np.random.rand(6))
+        phi2 = DiscreteFactor(["b", "c"], [3, 4], np.random.rand(12))
+        phi3 = DiscreteFactor(["c", "d"], [4, 5], np.random.rand(20))
+        phi4 = DiscreteFactor(["d", "a"], [5, 2], np.random.random(10))
         self.graph.add_factors(phi1, phi2, phi3, phi4)
-        self.graph.triangulate(heuristic='H4', inplace=True)
+        self.graph.triangulate(heuristic="H4", inplace=True)
         self.assertTrue(self.graph.is_triangulated())
-        self.assertListEqual(hf.recursive_sorted(self.graph.edges()),
-                             [['a', 'b'], ['a', 'd'], ['b', 'c'],
-                              ['b', 'd'], ['c', 'd']])
+        self.assertListEqual(
+            hf.recursive_sorted(self.graph.edges()),
+            [["a", "b"], ["a", "d"], ["b", "c"], ["b", "d"], ["c", "d"]],
+        )
 
     def test_triangulation_h6_inplace(self):
-        self.graph.add_edges_from([('a', 'b'), ('b', 'c'), ('c', 'd'),
-                                   ('d', 'a')])
-        phi1 = DiscreteFactor(['a', 'b'], [2, 3], np.random.rand(6))
-        phi2 = DiscreteFactor(['b', 'c'], [3, 4], np.random.rand(12))
-        phi3 = DiscreteFactor(['c', 'd'], [4, 5], np.random.rand(20))
-        phi4 = DiscreteFactor(['d', 'a'], [5, 2], np.random.random(10))
+        self.graph.add_edges_from([("a", "b"), ("b", "c"), ("c", "d"), ("d", "a")])
+        phi1 = DiscreteFactor(["a", "b"], [2, 3], np.random.rand(6))
+        phi2 = DiscreteFactor(["b", "c"], [3, 4], np.random.rand(12))
+        phi3 = DiscreteFactor(["c", "d"], [4, 5], np.random.rand(20))
+        phi4 = DiscreteFactor(["d", "a"], [5, 2], np.random.random(10))
         self.graph.add_factors(phi1, phi2, phi3, phi4)
-        self.graph.triangulate(heuristic='H4', inplace=True)
+        self.graph.triangulate(heuristic="H4", inplace=True)
         self.assertTrue(self.graph.is_triangulated())
-        self.assertListEqual(hf.recursive_sorted(self.graph.edges()),
-                             [['a', 'b'], ['a', 'd'], ['b', 'c'],
-                              ['b', 'd'], ['c', 'd']])
+        self.assertListEqual(
+            hf.recursive_sorted(self.graph.edges()),
+            [["a", "b"], ["a", "d"], ["b", "c"], ["b", "d"], ["c", "d"]],
+        )
 
     def test_cardinality_mismatch_raises_error(self):
-        self.graph.add_edges_from([('a', 'b'), ('b', 'c'), ('c', 'd'),
-                                   ('d', 'a')])
-        factor_list = [DiscreteFactor(edge, [2, 2], np.random.rand(4)) for edge in
-                       self.graph.edges()]
+        self.graph.add_edges_from([("a", "b"), ("b", "c"), ("c", "d"), ("d", "a")])
+        factor_list = [
+            DiscreteFactor(edge, [2, 2], np.random.rand(4))
+            for edge in self.graph.edges()
+        ]
         self.graph.add_factors(*factor_list)
-        self.graph.add_factors(DiscreteFactor(['a', 'b'], [2, 3], np.random.rand(6)))
+        self.graph.add_factors(DiscreteFactor(["a", "b"], [2, 3], np.random.rand(6)))
         self.assertRaises(ValueError, self.graph.triangulate)
 
     def test_triangulation_h1_create_new(self):
-        self.graph.add_edges_from([('a', 'b'), ('b', 'c'), ('c', 'd'),
-                                   ('d', 'a')])
-        phi1 = DiscreteFactor(['a', 'b'], [2, 3], np.random.rand(6))
-        phi2 = DiscreteFactor(['b', 'c'], [3, 4], np.random.rand(12))
-        phi3 = DiscreteFactor(['c', 'd'], [4, 5], np.random.rand(20))
-        phi4 = DiscreteFactor(['d', 'a'], [5, 2], np.random.random(10))
-        self.graph.add_factors(phi1, phi2, phi3, phi4)
-        H = self.graph.triangulate(heuristic='H1', inplace=True)
-        self.assertListEqual(hf.recursive_sorted(H.edges()),
-                             [['a', 'b'], ['a', 'c'], ['a', 'd'],
-                              ['b', 'c'], ['c', 'd']])
+        self.graph.add_edges_from([("a", "b"), ("b", "c"), ("c", "d"), ("d", "a")])
+        phi1 = DiscreteFactor(["a", "b"], [2, 3], np.random.rand(6))
+        phi2 = DiscreteFactor(["b", "c"], [3, 4], np.random.rand(12))
+        phi3 = DiscreteFactor(["c", "d"], [4, 5], np.random.rand(20))
+        phi4 = DiscreteFactor(["d", "a"], [5, 2], np.random.random(10))
+        self.graph.add_factors(phi1, phi2, phi3, phi4)
+        H = self.graph.triangulate(heuristic="H1", inplace=True)
+        self.assertListEqual(
+            hf.recursive_sorted(H.edges()),
+            [["a", "b"], ["a", "c"], ["a", "d"], ["b", "c"], ["c", "d"]],
+        )
 
     def test_triangulation_h2_create_new(self):
-        self.graph.add_edges_from([('a', 'b'), ('b', 'c'), ('c', 'd'),
-                                   ('d', 'a')])
-        phi1 = DiscreteFactor(['a', 'b'], [2, 3], np.random.rand(6))
-        phi2 = DiscreteFactor(['b', 'c'], [3, 4], np.random.rand(12))
-        phi3 = DiscreteFactor(['c', 'd'], [4, 5], np.random.rand(20))
-        phi4 = DiscreteFactor(['d', 'a'], [5, 2], np.random.random(10))
-        self.graph.add_factors(phi1, phi2, phi3, phi4)
-        H = self.graph.triangulate(heuristic='H2', inplace=True)
-        self.assertListEqual(hf.recursive_sorted(H.edges()),
-                             [['a', 'b'], ['a', 'c'], ['a', 'd'],
-                              ['b', 'c'], ['c', 'd']])
+        self.graph.add_edges_from([("a", "b"), ("b", "c"), ("c", "d"), ("d", "a")])
+        phi1 = DiscreteFactor(["a", "b"], [2, 3], np.random.rand(6))
+        phi2 = DiscreteFactor(["b", "c"], [3, 4], np.random.rand(12))
+        phi3 = DiscreteFactor(["c", "d"], [4, 5], np.random.rand(20))
+        phi4 = DiscreteFactor(["d", "a"], [5, 2], np.random.random(10))
+        self.graph.add_factors(phi1, phi2, phi3, phi4)
+        H = self.graph.triangulate(heuristic="H2", inplace=True)
+        self.assertListEqual(
+            hf.recursive_sorted(H.edges()),
+            [["a", "b"], ["a", "c"], ["a", "d"], ["b", "c"], ["c", "d"]],
+        )
 
     def test_triangulation_h3_create_new(self):
-        self.graph.add_edges_from([('a', 'b'), ('b', 'c'), ('c', 'd'),
-                                   ('d', 'a')])
-        phi1 = DiscreteFactor(['a', 'b'], [2, 3], np.random.rand(6))
-        phi2 = DiscreteFactor(['b', 'c'], [3, 4], np.random.rand(12))
-        phi3 = DiscreteFactor(['c', 'd'], [4, 5], np.random.rand(20))
-        phi4 = DiscreteFactor(['d', 'a'], [5, 2], np.random.random(10))
-        self.graph.add_factors(phi1, phi2, phi3, phi4)
-        H = self.graph.triangulate(heuristic='H3', inplace=True)
-        self.assertListEqual(hf.recursive_sorted(H.edges()),
-                             [['a', 'b'], ['a', 'd'], ['b', 'c'],
-                              ['b', 'd'], ['c', 'd']])
+        self.graph.add_edges_from([("a", "b"), ("b", "c"), ("c", "d"), ("d", "a")])
+        phi1 = DiscreteFactor(["a", "b"], [2, 3], np.random.rand(6))
+        phi2 = DiscreteFactor(["b", "c"], [3, 4], np.random.rand(12))
+        phi3 = DiscreteFactor(["c", "d"], [4, 5], np.random.rand(20))
+        phi4 = DiscreteFactor(["d", "a"], [5, 2], np.random.random(10))
+        self.graph.add_factors(phi1, phi2, phi3, phi4)
+        H = self.graph.triangulate(heuristic="H3", inplace=True)
+        self.assertListEqual(
+            hf.recursive_sorted(H.edges()),
+            [["a", "b"], ["a", "d"], ["b", "c"], ["b", "d"], ["c", "d"]],
+        )
 
     def test_triangulation_h4_create_new(self):
-        self.graph.add_edges_from([('a', 'b'), ('b', 'c'), ('c', 'd'),
-                                   ('d', 'a')])
-        phi1 = DiscreteFactor(['a', 'b'], [2, 3], np.random.rand(6))
-        phi2 = DiscreteFactor(['b', 'c'], [3, 4], np.random.rand(12))
-        phi3 = DiscreteFactor(['c', 'd'], [4, 5], np.random.rand(20))
-        phi4 = DiscreteFactor(['d', 'a'], [5, 2], np.random.random(10))
-        self.graph.add_factors(phi1, phi2, phi3, phi4)
-        H = self.graph.triangulate(heuristic='H4', inplace=True)
-        self.assertListEqual(hf.recursive_sorted(H.edges()),
-                             [['a', 'b'], ['a', 'd'], ['b', 'c'],
-                              ['b', 'd'], ['c', 'd']])
+        self.graph.add_edges_from([("a", "b"), ("b", "c"), ("c", "d"), ("d", "a")])
+        phi1 = DiscreteFactor(["a", "b"], [2, 3], np.random.rand(6))
+        phi2 = DiscreteFactor(["b", "c"], [3, 4], np.random.rand(12))
+        phi3 = DiscreteFactor(["c", "d"], [4, 5], np.random.rand(20))
+        phi4 = DiscreteFactor(["d", "a"], [5, 2], np.random.random(10))
+        self.graph.add_factors(phi1, phi2, phi3, phi4)
+        H = self.graph.triangulate(heuristic="H4", inplace=True)
+        self.assertListEqual(
+            hf.recursive_sorted(H.edges()),
+            [["a", "b"], ["a", "d"], ["b", "c"], ["b", "d"], ["c", "d"]],
+        )
 
     def test_triangulation_h5_create_new(self):
-        self.graph.add_edges_from([('a', 'b'), ('b', 'c'), ('c', 'd'),
-                                   ('d', 'a')])
-        phi1 = DiscreteFactor(['a', 'b'], [2, 3], np.random.rand(6))
-        phi2 = DiscreteFactor(['b', 'c'], [3, 4], np.random.rand(12))
-        phi3 = DiscreteFactor(['c', 'd'], [4, 5], np.random.rand(20))
-        phi4 = DiscreteFactor(['d', 'a'], [5, 2], np.random.random(10))
-        self.graph.add_factors(phi1, phi2, phi3, phi4)
-        H = self.graph.triangulate(heuristic='H5', inplace=True)
-        self.assertListEqual(hf.recursive_sorted(H.edges()),
-                             [['a', 'b'], ['a', 'd'], ['b', 'c'],
-                              ['b', 'd'], ['c', 'd']])
+        self.graph.add_edges_from([("a", "b"), ("b", "c"), ("c", "d"), ("d", "a")])
+        phi1 = DiscreteFactor(["a", "b"], [2, 3], np.random.rand(6))
+        phi2 = DiscreteFactor(["b", "c"], [3, 4], np.random.rand(12))
+        phi3 = DiscreteFactor(["c", "d"], [4, 5], np.random.rand(20))
+        phi4 = DiscreteFactor(["d", "a"], [5, 2], np.random.random(10))
+        self.graph.add_factors(phi1, phi2, phi3, phi4)
+        H = self.graph.triangulate(heuristic="H5", inplace=True)
+        self.assertListEqual(
+            hf.recursive_sorted(H.edges()),
+            [["a", "b"], ["a", "d"], ["b", "c"], ["b", "d"], ["c", "d"]],
+        )
 
     def test_triangulation_h6_create_new(self):
-        self.graph.add_edges_from([('a', 'b'), ('b', 'c'), ('c', 'd'),
-                                   ('d', 'a')])
-        phi1 = DiscreteFactor(['a', 'b'], [2, 3], np.random.rand(6))
-        phi2 = DiscreteFactor(['b', 'c'], [3, 4], np.random.rand(12))
-        phi3 = DiscreteFactor(['c', 'd'], [4, 5], np.random.rand(20))
-        phi4 = DiscreteFactor(['d', 'a'], [5, 2], np.random.random(10))
-        self.graph.add_factors(phi1, phi2, phi3, phi4)
-        H = self.graph.triangulate(heuristic='H6', inplace=True)
-        self.assertListEqual(hf.recursive_sorted(H.edges()),
-                             [['a', 'b'], ['a', 'd'], ['b', 'c'],
-                              ['b', 'd'], ['c', 'd']])
+        self.graph.add_edges_from([("a", "b"), ("b", "c"), ("c", "d"), ("d", "a")])
+        phi1 = DiscreteFactor(["a", "b"], [2, 3], np.random.rand(6))
+        phi2 = DiscreteFactor(["b", "c"], [3, 4], np.random.rand(12))
+        phi3 = DiscreteFactor(["c", "d"], [4, 5], np.random.rand(20))
+        phi4 = DiscreteFactor(["d", "a"], [5, 2], np.random.random(10))
+        self.graph.add_factors(phi1, phi2, phi3, phi4)
+        H = self.graph.triangulate(heuristic="H6", inplace=True)
+        self.assertListEqual(
+            hf.recursive_sorted(H.edges()),
+            [["a", "b"], ["a", "d"], ["b", "c"], ["b", "d"], ["c", "d"]],
+        )
 
     def test_copy(self):
         # Setup the original graph
-        self.graph.add_nodes_from(['a', 'b'])
-        self.graph.add_edges_from([('a', 'b')])
+        self.graph.add_nodes_from(["a", "b"])
+        self.graph.add_edges_from([("a", "b")])
 
         # Generate the copy
         copy = self.graph.copy()
 
         # Ensure the copied model is correct
         self.assertTrue(copy.check_model())
 
         # Basic sanity checks to ensure the graph was copied correctly
         self.assertEqual(len(copy.nodes()), 2)
-        self.assertListEqual(copy.neighbors('a'), ['b'])
-        self.assertListEqual(copy.neighbors('b'), ['a'])
+        self.assertListEqual(list(copy.neighbors("a")), ["b"])
+        self.assertListEqual(list(copy.neighbors("b")), ["a"])
 
         # Modify the original graph ...
-        self.graph.add_nodes_from(['c'])
-        self.graph.add_edges_from([('c', 'b')])
+        self.graph.add_nodes_from(["c"])
+        self.graph.add_edges_from([("c", "b")])
 
         # ... and ensure none of those changes get propagated
         self.assertEqual(len(copy.nodes()), 2)
-        self.assertListEqual(copy.neighbors('a'), ['b'])
-        self.assertListEqual(copy.neighbors('b'), ['a'])
+        self.assertListEqual(list(copy.neighbors("a")), ["b"])
+        self.assertListEqual(list(copy.neighbors("b")), ["a"])
         with self.assertRaises(nx.NetworkXError):
-            copy.neighbors('c')
+            list(copy.neighbors("c"))
 
         # Ensure the copy has no factors at this point
         self.assertEqual(len(copy.get_factors()), 0)
 
         # Add factors to the original graph
-        phi1 = DiscreteFactor(['a', 'b'], [2, 2], [[0.3, 0.7], [0.9, 0.1]])
+        phi1 = DiscreteFactor(["a", "b"], [2, 2], [[0.3, 0.7], [0.9, 0.1]])
         self.graph.add_factors(phi1)
 
         # The factors should not get copied over
         with self.assertRaises(AssertionError):
-            self.assertListEqual(copy.get_factors(), self.graph.get_factors())
+            self.assertListEqual(list(copy.get_factors()), self.graph.get_factors())
 
         # Create a fresh copy
         del copy
         copy = self.graph.copy()
-        self.assertListEqual(copy.get_factors(), self.graph.get_factors())
+        self.assertListEqual(list(copy.get_factors()), self.graph.get_factors())
 
         # If we change factors in the original, it should not be passed to the clone
         phi1.values = np.array([[0.5, 0.5], [0.5, 0.5]])
         self.assertNotEqual(self.graph.get_factors(), copy.get_factors())
 
         # Start with a fresh copy
         del copy
-        self.graph.add_nodes_from(['d'])
+        self.graph.add_nodes_from(["d"])
         copy = self.graph.copy()
 
         # Ensure an unconnected node gets copied over as well
         self.assertEqual(len(copy.nodes()), 4)
-        self.assertListEqual(self.graph.neighbors('a'), ['b'])
-        self.assertTrue('a' in self.graph.neighbors('b'))
-        self.assertTrue('c' in self.graph.neighbors('b'))
-        self.assertListEqual(self.graph.neighbors('c'), ['b'])
-        self.assertListEqual(self.graph.neighbors('d'), [])
+        self.assertListEqual(list(self.graph.neighbors("a")), ["b"])
+        self.assertTrue("a" in self.graph.neighbors("b"))
+        self.assertTrue("c" in self.graph.neighbors("b"))
+        self.assertListEqual(list(self.graph.neighbors("c")), ["b"])
+        self.assertListEqual(list(self.graph.neighbors("d")), [])
 
         # Verify that changing the copied model should not update the original
-        copy.add_nodes_from(['e'])
-        self.assertListEqual(copy.neighbors('e'), [])
+        copy.add_nodes_from(["e"])
+        self.assertListEqual(list(copy.neighbors("e")), [])
         with self.assertRaises(nx.NetworkXError):
-            self.graph.neighbors('e')
+            self.graph.neighbors("e")
 
         # Verify that changing edges in the copy doesn't create edges in the original
-        copy.add_edges_from([('d', 'b')])
+        copy.add_edges_from([("d", "b")])
 
-        self.assertTrue('a' in copy.neighbors('b'))
-        self.assertTrue('c' in copy.neighbors('b'))
-        self.assertTrue('d' in copy.neighbors('b'))
-
-        self.assertTrue('a' in self.graph.neighbors('b'))
-        self.assertTrue('c' in self.graph.neighbors('b'))
-        self.assertFalse('d' in self.graph.neighbors('b'))
+        self.assertTrue("a" in copy.neighbors("b"))
+        self.assertTrue("c" in copy.neighbors("b"))
+        self.assertTrue("d" in copy.neighbors("b"))
+
+        self.assertTrue("a" in self.graph.neighbors("b"))
+        self.assertTrue("c" in self.graph.neighbors("b"))
+        self.assertFalse("d" in self.graph.neighbors("b"))
 
         # If we remove factors from the copied model, it should not reflect in the original
         copy.remove_factors(phi1)
         self.assertEqual(len(self.graph.get_factors()), 1)
         self.assertEqual(len(copy.get_factors()), 0)
 
     def tearDown(self):
```

### Comparing `pgmpy-0.1.7/pgmpy/tests/test_models/test_MarkovChain.py` & `pgmpy-0.1.9/pgmpy/tests/test_models/test_MarkovChain.py`

 * *Files 9% similar despite different names*

```diff
@@ -9,27 +9,37 @@
 from pgmpy.factors.discrete import State
 from pgmpy.models import MarkovChain as MC
 from pgmpy.extern.six.moves import range, zip
 
 
 class TestMarkovChain(unittest.TestCase):
     def setUp(self):
-        self.variables = ['intel', 'diff', 'grade']
+        self.variables = ["intel", "diff", "grade"]
         self.card = [3, 2, 3]
-        self.cardinalities = {'intel': 3, 'diff': 2, 'grade': 3}
-        self.intel_tm = {0: {0: 0.1, 1: 0.25, 2: 0.65}, 1: {0: 0.5, 1: 0.3, 2: 0.2}, 2: {0: 0.3, 1: 0.3, 2: 0.4}}
-        self.intel_tm_matrix = np.array([[0.1, 0.25, 0.65], [0.5, 0.3, 0.2], [0.3, 0.3, 0.4]])
+        self.cardinalities = {"intel": 3, "diff": 2, "grade": 3}
+        self.intel_tm = {
+            0: {0: 0.1, 1: 0.25, 2: 0.65},
+            1: {0: 0.5, 1: 0.3, 2: 0.2},
+            2: {0: 0.3, 1: 0.3, 2: 0.4},
+        }
+        self.intel_tm_matrix = np.array(
+            [[0.1, 0.25, 0.65], [0.5, 0.3, 0.2], [0.3, 0.3, 0.4]]
+        )
         self.diff_tm = {0: {0: 0.3, 1: 0.7}, 1: {0: 0.75, 1: 0.25}}
         self.diff_tm_matrix = np.array([[0.3, 0.7], [0.75, 0.25]])
-        self.grade_tm = {0: {0: 0.4, 1: 0.2, 2: 0.4}, 1: {0: 0.9, 1: 0.05, 2: 0.05}, 2: {0: 0.1, 1: 0.4, 2: 0.5}}
+        self.grade_tm = {
+            0: {0: 0.4, 1: 0.2, 2: 0.4},
+            1: {0: 0.9, 1: 0.05, 2: 0.05},
+            2: {0: 0.1, 1: 0.4, 2: 0.5},
+        }
         self.grade_tm_matrix = [[0.4, 0.2, 0.4], [0.9, 0.05, 0.05], [0.1, 0.4, 0.5]]
-        self.start_state = [State('intel', 0), State('diff', 1), State('grade', 2)]
+        self.start_state = [State("intel", 0), State("diff", 1), State("grade", 2)]
         self.model = MC()
 
-        self.sample = DataFrame(index=range(200), columns=['a', 'b'])
+        self.sample = DataFrame(index=range(200), columns=["a", "b"])
         self.sample.a = [1] * 100 + [0] * 100
         self.sample.b = [0] * 100 + [1] * 100
 
     def tearDown(self):
         del self.variables
         del self.card
         del self.cardinalities
@@ -41,243 +51,271 @@
         del self.sample
 
     @patch("pgmpy.models.MarkovChain._check_state", autospec=True)
     def test_init(self, check_state):
         model = MC(self.variables, self.card, self.start_state)
         self.assertListEqual(model.variables, self.variables)
         self.assertDictEqual(model.cardinalities, self.cardinalities)
-        self.assertDictEqual(model.transition_models, {var: {} for var in self.variables})
+        self.assertDictEqual(
+            model.transition_models, {var: {} for var in self.variables}
+        )
         check_state.assert_called_once_with(model, self.start_state)
         self.assertListEqual(model.state, self.start_state)
 
     def test_init_bad_variables_type(self):
         # variables is non-iterable
         self.assertRaises(ValueError, MC, variables=123)
         # variables is a string
-        self.assertRaises(ValueError, MC, variables='abc')
+        self.assertRaises(ValueError, MC, variables="abc")
 
     def test_init_bad_card_type(self):
         # card is non-iterable
         self.assertRaises(ValueError, MC, card=123)
         # card is a string
-        self.assertRaises(ValueError, MC, card='abc')
+        self.assertRaises(ValueError, MC, card="abc")
 
     def test_init_less_args(self):
         model = MC()
         self.assertListEqual(model.variables, [])
         self.assertDictEqual(model.cardinalities, {})
         self.assertDictEqual(model.transition_models, {})
         self.assertIsNone(model.state)
 
     @patch("pgmpy.models.MarkovChain._check_state", autospec=True)
     def test_set_start_state_list(self, check_state):
-        model = MC(['b', 'a'], [1, 2])
+        model = MC(["b", "a"], [1, 2])
         check_state.return_value = True
-        model.set_start_state([State('a', 0), State('b', 1)])
-        model_state = [State('b', 1), State('a', 0)]
+        model.set_start_state([State("a", 0), State("b", 1)])
+        model_state = [State("b", 1), State("a", 0)]
         check_state.assert_called_once_with(model, model_state)
         self.assertEqual(model.state, model_state)
 
     def test_set_start_state_none(self):
         model = MC()
-        model.state = 'state'
+        model.state = "state"
         model.set_start_state(None)
         self.assertIsNone(model.state)
 
     def test_check_state_bad_type(self):
         model = MC()
         # state is non-iterable
         self.assertRaises(ValueError, model._check_state, 123)
         # state is a string
-        self.assertRaises(ValueError, model._check_state, 'abc')
+        self.assertRaises(ValueError, model._check_state, "abc")
 
     def test_check_state_bad_vars(self):
         model = MC()
         # state_vars and model_vars differ
         self.assertRaises(ValueError, model._check_state, [State(1, 2)])
 
     def test_check_state_bad_var_value(self):
-        model = MC(['a'], [2])
+        model = MC(["a"], [2])
         # value of variable >= cardinaliity
-        self.assertRaises(ValueError, model._check_state, [State('a', 3)])
+        self.assertRaises(ValueError, model._check_state, [State("a", 3)])
 
     def test_check_state_success(self):
-        model = MC(['a'], [2])
-        self.assertTrue(model._check_state([State('a', 1)]))
+        model = MC(["a"], [2])
+        self.assertTrue(model._check_state([State("a", 1)]))
 
     def test_add_variable_new(self):
-        model = MC(['a'], [2])
-        model.add_variable('p', 3)
-        self.assertIn('p', model.variables)
-        self.assertEqual(model.cardinalities['p'], 3)
-        self.assertDictEqual(model.transition_models['p'], {})
+        model = MC(["a"], [2])
+        model.add_variable("p", 3)
+        self.assertIn("p", model.variables)
+        self.assertEqual(model.cardinalities["p"], 3)
+        self.assertDictEqual(model.transition_models["p"], {})
 
     def test_copy(self):
-        model = MC(['a', 'b'], [2, 2], [State('a', 0), State('b', 1)])
-        model.add_transition_model('a', {0: {0: 0.1, 1: 0.9}, 1: {0: 0.2, 1: 0.8}})
-        model.add_transition_model('b', {0: {0: 0.3, 1: 0.7}, 1: {0: 0.4, 1: 0.6}})
+        model = MC(["a", "b"], [2, 2], [State("a", 0), State("b", 1)])
+        model.add_transition_model("a", {0: {0: 0.1, 1: 0.9}, 1: {0: 0.2, 1: 0.8}})
+        model.add_transition_model("b", {0: {0: 0.3, 1: 0.7}, 1: {0: 0.4, 1: 0.6}})
         copy = model.copy()
 
         self.assertIsInstance(copy, MC)
         self.assertEqual(sorted(model.variables), sorted(copy.variables))
         self.assertEqual(model.cardinalities, copy.cardinalities)
         self.assertEqual(model.transition_models, copy.transition_models)
         self.assertEqual(model.state, copy.state)
 
-        model.add_variable('p', 1)
-        model.set_start_state([State('a', 0), State('b', 1), State('p', 0)])
-        model.add_transition_model('p', {0: {0: 1}})
+        model.add_variable("p", 1)
+        model.set_start_state([State("a", 0), State("b", 1), State("p", 0)])
+        model.add_transition_model("p", {0: {0: 1}})
 
         self.assertNotEqual(sorted(model.variables), sorted(copy.variables))
-        self.assertEqual(sorted(['a', 'b']), sorted(copy.variables))
+        self.assertEqual(sorted(["a", "b"]), sorted(copy.variables))
         self.assertNotEqual(model.cardinalities, copy.cardinalities)
-        self.assertEqual({'a': 2, 'b': 2}, copy.cardinalities)
+        self.assertEqual({"a": 2, "b": 2}, copy.cardinalities)
         self.assertNotEqual(model.state, copy.state)
-        self.assertEqual([State('a', 0), State('b', 1)], copy.state)
+        self.assertEqual([State("a", 0), State("b", 1)], copy.state)
         self.assertNotEqual(model.transition_models, copy.transition_models)
         self.assertEqual(len(copy.transition_models), 2)
-        self.assertEqual(copy.transition_models['a'], {0: {0: 0.1, 1: 0.9}, 1: {0: 0.2, 1: 0.8}})
-        self.assertEqual(copy.transition_models['b'], {0: {0: 0.3, 1: 0.7}, 1: {0: 0.4, 1: 0.6}})
+        self.assertEqual(
+            copy.transition_models["a"], {0: {0: 0.1, 1: 0.9}, 1: {0: 0.2, 1: 0.8}}
+        )
+        self.assertEqual(
+            copy.transition_models["b"], {0: {0: 0.3, 1: 0.7}, 1: {0: 0.4, 1: 0.6}}
+        )
 
     @patch.object(sys.modules["pgmpy.models.MarkovChain"], "warn")
     def test_add_variable_existing(self, warn):
-        model = MC(['p'], [2])
-        model.add_variable('p', 3)
+        model = MC(["p"], [2])
+        model.add_variable("p", 3)
         self.assertEqual(warn.call_count, 1)
 
     @patch("pgmpy.models.MarkovChain.add_variable", autospec=True)
     def test_add_variables_from(self, add_var):
         model = MC()
         model.add_variables_from(self.variables, self.card)
         calls = [call(model, *p) for p in zip(self.variables, self.card)]
         add_var.assert_has_calls(calls)
 
     def test_add_transition_model_bad_type(self):
         model = MC()
-        grade_tm_matrix_bad = [[0.1, 0.5, 0.4], [0.2, 0.2, 0.6], 'abc']
+        grade_tm_matrix_bad = [[0.1, 0.5, 0.4], [0.2, 0.2, 0.6], "abc"]
         # if transition_model is not a dict or np.array
-        self.assertRaises(ValueError, model.add_transition_model, 'var', 123)
-        self.assertRaises(ValueError, model.add_transition_model, 'var', grade_tm_matrix_bad)
+        self.assertRaises(ValueError, model.add_transition_model, "var", 123)
+        self.assertRaises(
+            ValueError, model.add_transition_model, "var", grade_tm_matrix_bad
+        )
 
     def test_add_transition_model_bad_states(self):
-        model = MC(['var'], [2])
+        model = MC(["var"], [2])
         # transition for state=1 not defined
         transition_model = {0: {0: 0.1, 1: 0.9}}
-        self.assertRaises(ValueError, model.add_transition_model, 'var', transition_model)
+        self.assertRaises(
+            ValueError, model.add_transition_model, "var", transition_model
+        )
 
     def test_add_transition_model_bad_transition(self):
-        model = MC(['var'], [2])
+        model = MC(["var"], [2])
         # transition for state=1 is not a dict
-        transition_model = {0: {0: 0.1, 1: 0.9}, 1: 'abc'}
-        self.assertRaises(ValueError, model.add_transition_model, 'var', transition_model)
+        transition_model = {0: {0: 0.1, 1: 0.9}, 1: "abc"}
+        self.assertRaises(
+            ValueError, model.add_transition_model, "var", transition_model
+        )
 
     def test_add_transition_model_bad_probability(self):
-        model = MC(['var'], [2])
+        model = MC(["var"], [2])
         transition_model = {0: {0: -0.1, 1: 1.1}, 1: {0: 0.5, 1: 0.5}}
-        self.assertRaises(ValueError, model.add_transition_model, 'var', transition_model)
+        self.assertRaises(
+            ValueError, model.add_transition_model, "var", transition_model
+        )
 
     def test_add_transition_model_bad_probability_sum(self):
-        model = MC(['var'], [2])
+        model = MC(["var"], [2])
         # transition probabilities from state=0 do not sum to 1.0
         transition_model = {0: {0: 0.1, 1: 0.2}, 1: {0: 0.5, 1: 0.5}}
-        self.assertRaises(ValueError, model.add_transition_model, 'var', transition_model)
+        self.assertRaises(
+            ValueError, model.add_transition_model, "var", transition_model
+        )
 
     def test_add_transition_model_success(self):
-        model = MC(['var'], [2])
+        model = MC(["var"], [2])
         transition_model = {0: {0: 0.3, 1: 0.7}, 1: {0: 0.5, 1: 0.5}}
-        model.add_transition_model('var', transition_model)
-        self.assertDictEqual(model.transition_models['var'], transition_model)
+        model.add_transition_model("var", transition_model)
+        self.assertDictEqual(model.transition_models["var"], transition_model)
 
     def test_transition_model_bad_matrix_dimension(self):
-        model = MC(['var'], [2])
+        model = MC(["var"], [2])
         transition_model = np.array([0.3, 0.7])
         # check for square dimension of the matrix
-        self.assertRaises(ValueError, model.add_transition_model, 'var', transition_model)
+        self.assertRaises(
+            ValueError, model.add_transition_model, "var", transition_model
+        )
         transition_model = np.array([[0.3, 0.6, 0.1], [0.3, 0.3, 0.4]])
-        self.assertRaises(ValueError, model.add_transition_model, 'var', transition_model)
+        self.assertRaises(
+            ValueError, model.add_transition_model, "var", transition_model
+        )
 
     def test_transition_model_dict_to_matrix(self):
-        model = MC(['var'], [2])
+        model = MC(["var"], [2])
         transition_model = {0: {0: 0.3, 1: 0.7}, 1: {0: 0.5, 1: 0.5}}
         transition_model_matrix = np.array([[0.3, 0.7], [0.5, 0.5]])
-        model.add_transition_model('var', transition_model_matrix)
-        self.assertDictEqual(model.transition_models['var'], transition_model)
+        model.add_transition_model("var", transition_model_matrix)
+        self.assertDictEqual(model.transition_models["var"], transition_model)
 
     def test_sample(self):
-        model = MC(['a', 'b'], [2, 2])
-        model.transition_models['a'] = {0: {0: 0.1, 1: 0.9}, 1: {0: 0.2, 1: 0.8}}
-        model.transition_models['b'] = {0: {0: 0.3, 1: 0.7}, 1: {0: 0.4, 1: 0.6}}
-        sample = model.sample(start_state=[State('a', 0), State('b', 1)], size=2)
+        model = MC(["a", "b"], [2, 2])
+        model.transition_models["a"] = {0: {0: 0.1, 1: 0.9}, 1: {0: 0.2, 1: 0.8}}
+        model.transition_models["b"] = {0: {0: 0.3, 1: 0.7}, 1: {0: 0.4, 1: 0.6}}
+        sample = model.sample(start_state=[State("a", 0), State("b", 1)], size=2)
         self.assertEqual(len(sample), 2)
-        self.assertEqual(list(sample.columns), ['a', 'b'])
+        self.assertEqual(list(sample.columns), ["a", "b"])
         self.assertTrue(list(sample.loc[0]) in [[0, 0], [0, 1], [1, 0], [1, 1]])
         self.assertTrue(list(sample.loc[1]) in [[0, 0], [0, 1], [1, 0], [1, 1]])
 
     @patch("pgmpy.models.MarkovChain.random_state", autospec=True)
     def test_sample_less_arg(self, random_state):
-        model = MC(['a', 'b'], [2, 2])
-        random_state.return_value = [State('a', 0), State('b', 1)]
+        model = MC(["a", "b"], [2, 2])
+        random_state.return_value = [State("a", 0), State("b", 1)]
         sample = model.sample(size=1)
         random_state.assert_called_once_with(model)
         self.assertEqual(model.state, random_state.return_value)
         self.assertEqual(len(sample), 1)
-        self.assertEqual(list(sample.columns), ['a', 'b'])
+        self.assertEqual(list(sample.columns), ["a", "b"])
         self.assertEqual(list(sample.loc[0]), [0, 1])
 
     @patch("pgmpy.models.MarkovChain.sample", autospec=True)
     def test_prob_from_sample(self, sample):
-        model = MC(['a', 'b'], [2, 2])
+        model = MC(["a", "b"], [2, 2])
         sample.return_value = self.sample
-        probabilites = model.prob_from_sample([State('a', 1), State('b', 0)])
+        probabilites = model.prob_from_sample([State("a", 1), State("b", 0)])
         self.assertEqual(list(probabilites), [1] * 50 + [0] * 50)
 
     def test_is_stationarity_success(self):
-        model = MC(['intel', 'diff'], [2, 3])
-        model.set_start_state([State('intel', 0), State('diff', 2)])
+        model = MC(["intel", "diff"], [2, 3])
+        model.set_start_state([State("intel", 0), State("diff", 2)])
         intel_tm = {0: {0: 0.25, 1: 0.75}, 1: {0: 0.5, 1: 0.5}}
-        model.add_transition_model('intel', intel_tm)
-        diff_tm = {0: {0: 0.1, 1: 0.5, 2: 0.4}, 1: {0: 0.2, 1: 0.2, 2: 0.6}, 2: {0: 0.7, 1: 0.15, 2: 0.15}}
-        model.add_transition_model('diff', diff_tm)
+        model.add_transition_model("intel", intel_tm)
+        diff_tm = {
+            0: {0: 0.1, 1: 0.5, 2: 0.4},
+            1: {0: 0.2, 1: 0.2, 2: 0.6},
+            2: {0: 0.7, 1: 0.15, 2: 0.15},
+        }
+        model.add_transition_model("diff", diff_tm)
         self.assertTrue(model.is_stationarity)
 
     def test_is_stationarity_failure(self):
-        model = MC(['intel', 'diff'], [2, 3])
-        model.set_start_state([State('intel', 0), State('diff', 2)])
+        model = MC(["intel", "diff"], [2, 3])
+        model.set_start_state([State("intel", 0), State("diff", 2)])
         intel_tm = {0: {0: 0.25, 1: 0.75}, 1: {0: 0.5, 1: 0.5}}
-        model.add_transition_model('intel', intel_tm)
-        diff_tm = {0: {0: 0.1, 1: 0.5, 2: 0.4}, 1: {0: 0.2, 1: 0.2, 2: 0.6}, 2: {0: 0.7, 1: 0.15, 2: 0.15}}
-        model.add_transition_model('diff', diff_tm)
+        model.add_transition_model("intel", intel_tm)
+        diff_tm = {
+            0: {0: 0.1, 1: 0.5, 2: 0.4},
+            1: {0: 0.2, 1: 0.2, 2: 0.6},
+            2: {0: 0.7, 1: 0.15, 2: 0.15},
+        }
+        model.add_transition_model("diff", diff_tm)
         self.assertFalse(model.is_stationarity(0.002, None))
 
     @patch.object(sys.modules["pgmpy.models.MarkovChain"], "sample_discrete")
     def test_generate_sample(self, sample_discrete):
-        model = MC(['a', 'b'], [2, 2])
-        model.transition_models['a'] = {0: {0: 0.1, 1: 0.9}, 1: {0: 0.2, 1: 0.8}}
-        model.transition_models['b'] = {0: {0: 0.3, 1: 0.7}, 1: {0: 0.4, 1: 0.6}}
+        model = MC(["a", "b"], [2, 2])
+        model.transition_models["a"] = {0: {0: 0.1, 1: 0.9}, 1: {0: 0.2, 1: 0.8}}
+        model.transition_models["b"] = {0: {0: 0.3, 1: 0.7}, 1: {0: 0.4, 1: 0.6}}
         sample_discrete.side_effect = [[1], [0]] * 2
-        gen = model.generate_sample(start_state=[State('a', 0), State('b', 1)], size=2)
+        gen = model.generate_sample(start_state=[State("a", 0), State("b", 1)], size=2)
         samples = [sample for sample in gen]
-        expected_samples = [[State('a', 1), State('b', 0)]] * 2
+        expected_samples = [[State("a", 1), State("b", 0)]] * 2
         self.assertEqual(samples, expected_samples)
 
     @patch.object(sys.modules["pgmpy.models.MarkovChain"], "sample_discrete")
     @patch("pgmpy.models.MarkovChain.random_state", autospec=True)
     def test_generate_sample_less_arg(self, random_state, sample_discrete):
-        model = MC(['a', 'b'], [2, 2])
-        model.transition_models['a'] = {0: {0: 0.1, 1: 0.9}, 1: {0: 0.2, 1: 0.8}}
-        model.transition_models['b'] = {0: {0: 0.3, 1: 0.7}, 1: {0: 0.4, 1: 0.6}}
-        random_state.return_value = [State('a', 0), State('b', 1)]
+        model = MC(["a", "b"], [2, 2])
+        model.transition_models["a"] = {0: {0: 0.1, 1: 0.9}, 1: {0: 0.2, 1: 0.8}}
+        model.transition_models["b"] = {0: {0: 0.3, 1: 0.7}, 1: {0: 0.4, 1: 0.6}}
+        random_state.return_value = [State("a", 0), State("b", 1)]
         sample_discrete.side_effect = [[1], [0]] * 2
         gen = model.generate_sample(size=2)
         samples = [sample for sample in gen]
-        expected_samples = [[State('a', 1), State('b', 0)]] * 2
+        expected_samples = [[State("a", 1), State("b", 0)]] * 2
         self.assertEqual(samples, expected_samples)
 
     def test_random_state(self):
-        model = MC(['a', 'b'], [2, 3])
+        model = MC(["a", "b"], [2, 3])
         state = model.random_state()
         vars = [v for v, s in state]
-        self.assertEqual(vars, ['a', 'b'])
+        self.assertEqual(vars, ["a", "b"])
         self.assertGreaterEqual(state[0].state, 0)
         self.assertGreaterEqual(state[1].state, 0)
         self.assertLessEqual(state[0].state, 1)
         self.assertLessEqual(state[1].state, 2)
```

### Comparing `pgmpy-0.1.7/pgmpy/tests/test_models/test_FactorGraph.py` & `pgmpy-0.1.9/pgmpy/tests/test_models/test_FactorGraph.py`

 * *Files 27% similar despite different names*

```diff
@@ -14,284 +14,285 @@
     def setUp(self):
         self.graph = FactorGraph()
 
     def test_class_init_without_data(self):
         self.assertIsInstance(self.graph, FactorGraph)
 
     def test_class_init_data_string(self):
-        self.graph = FactorGraph([('a', 'phi1'), ('b', 'phi1')])
-        self.assertListEqual(sorted(self.graph.nodes()), ['a', 'b', 'phi1'])
-        self.assertListEqual(hf.recursive_sorted(self.graph.edges()),
-                             [['a', 'phi1'], ['b', 'phi1']])
+        self.graph = FactorGraph([("a", "phi1"), ("b", "phi1")])
+        self.assertListEqual(sorted(self.graph.nodes()), ["a", "b", "phi1"])
+        self.assertListEqual(
+            hf.recursive_sorted(self.graph.edges()), [["a", "phi1"], ["b", "phi1"]]
+        )
 
     def test_add_single_node(self):
-        self.graph.add_node('phi1')
-        self.assertEqual(self.graph.nodes(), ['phi1'])
+        self.graph.add_node("phi1")
+        self.assertEqual(list(self.graph.nodes()), ["phi1"])
 
     def test_add_multiple_nodes(self):
-        self.graph.add_nodes_from(['a', 'b', 'phi1'])
-        self.assertListEqual(sorted(self.graph.nodes()), ['a', 'b', 'phi1'])
+        self.graph.add_nodes_from(["a", "b", "phi1"])
+        self.assertListEqual(sorted(self.graph.nodes()), ["a", "b", "phi1"])
 
     def test_add_single_edge(self):
-        self.graph.add_edge('a', 'phi1')
-        self.assertListEqual(sorted(self.graph.nodes()), ['a', 'phi1'])
-        self.assertListEqual(hf.recursive_sorted(self.graph.edges()),
-                             [['a', 'phi1']])
+        self.graph.add_edge("a", "phi1")
+        self.assertListEqual(sorted(self.graph.nodes()), ["a", "phi1"])
+        self.assertListEqual(hf.recursive_sorted(self.graph.edges()), [["a", "phi1"]])
 
     def test_add_multiple_edges(self):
-        self.graph.add_edges_from([('a', 'phi1'), ('b', 'phi1')])
-        self.assertListEqual(sorted(self.graph.nodes()), ['a', 'b', 'phi1'])
-        self.assertListEqual(hf.recursive_sorted(self.graph.edges()),
-                             [['a', 'phi1'], ['b', 'phi1']])
+        self.graph.add_edges_from([("a", "phi1"), ("b", "phi1")])
+        self.assertListEqual(sorted(self.graph.nodes()), ["a", "b", "phi1"])
+        self.assertListEqual(
+            hf.recursive_sorted(self.graph.edges()), [["a", "phi1"], ["b", "phi1"]]
+        )
 
     def test_add_self_loop_raises_error(self):
-        self.assertRaises(ValueError, self.graph.add_edge, 'a', 'a')
+        self.assertRaises(ValueError, self.graph.add_edge, "a", "a")
 
     def tearDown(self):
         del self.graph
 
 
 class TestFactorGraphFactorOperations(unittest.TestCase):
     def setUp(self):
         self.graph = FactorGraph()
 
     def test_add_single_factor(self):
-        self.graph.add_edges_from([('a', 'phi1'), ('b', 'phi1')])
-        phi1 = DiscreteFactor(['a', 'b'], [2, 2], np.random.rand(4))
+        self.graph.add_edges_from([("a", "phi1"), ("b", "phi1")])
+        phi1 = DiscreteFactor(["a", "b"], [2, 2], np.random.rand(4))
         self.graph.add_factors(phi1)
         six.assertCountEqual(self, self.graph.factors, [phi1])
 
     def test_add_multiple_factors(self):
-        phi1 = DiscreteFactor(['a', 'b'], [2, 2], np.random.rand(4))
-        phi2 = DiscreteFactor(['b', 'c'], [2, 2], np.random.rand(4))
-        self.graph.add_edges_from([('a', phi1), ('b', phi1),
-                                   ('b', phi2), ('c', phi2)])
+        phi1 = DiscreteFactor(["a", "b"], [2, 2], np.random.rand(4))
+        phi2 = DiscreteFactor(["b", "c"], [2, 2], np.random.rand(4))
+        self.graph.add_edges_from([("a", phi1), ("b", phi1), ("b", phi2), ("c", phi2)])
         self.graph.add_factors(phi1, phi2)
         six.assertCountEqual(self, self.graph.factors, [phi1, phi2])
 
     def test_get_factors(self):
-        phi1 = DiscreteFactor(['a', 'b'], [2, 2], np.random.rand(4))
-        phi2 = DiscreteFactor(['b', 'c'], [2, 2], np.random.rand(4))
-        self.graph.add_edges_from([('a', phi1), ('b', phi1),
-                                   ('b', phi2), ('c', phi2)])
+        phi1 = DiscreteFactor(["a", "b"], [2, 2], np.random.rand(4))
+        phi2 = DiscreteFactor(["b", "c"], [2, 2], np.random.rand(4))
+        self.graph.add_edges_from([("a", phi1), ("b", phi1), ("b", phi2), ("c", phi2)])
 
         six.assertCountEqual(self, self.graph.get_factors(), [])
 
         self.graph.add_factors(phi1, phi2)
         self.assertEqual(self.graph.get_factors(node=phi1), phi1)
         self.assertEqual(self.graph.get_factors(node=phi2), phi2)
         six.assertCountEqual(self, self.graph.get_factors(), [phi1, phi2])
 
         self.graph.remove_factors(phi1)
         self.assertRaises(ValueError, self.graph.get_factors, node=phi1)
 
     def test_remove_factors(self):
-        self.graph.add_edges_from([('a', 'phi1'), ('b', 'phi1'),
-                                   ('b', 'phi2'), ('c', 'phi2')])
-        phi1 = DiscreteFactor(['a', 'b'], [2, 2], np.random.rand(4))
-        phi2 = DiscreteFactor(['b', 'c'], [2, 2], np.random.rand(4))
+        self.graph.add_edges_from(
+            [("a", "phi1"), ("b", "phi1"), ("b", "phi2"), ("c", "phi2")]
+        )
+        phi1 = DiscreteFactor(["a", "b"], [2, 2], np.random.rand(4))
+        phi2 = DiscreteFactor(["b", "c"], [2, 2], np.random.rand(4))
         self.graph.add_factors(phi1, phi2)
         self.graph.remove_factors(phi1)
         six.assertCountEqual(self, self.graph.factors, [phi2])
 
     def test_get_partition_function(self):
-        phi1 = DiscreteFactor(['a', 'b'], [2, 2], range(4))
-        phi2 = DiscreteFactor(['b', 'c'], [2, 2], range(4))
-        self.graph.add_edges_from([('a', phi1), ('b', phi1),
-                                   ('b', phi2), ('c', phi2)])
+        phi1 = DiscreteFactor(["a", "b"], [2, 2], range(4))
+        phi2 = DiscreteFactor(["b", "c"], [2, 2], range(4))
+        self.graph.add_edges_from([("a", phi1), ("b", phi1), ("b", phi2), ("c", phi2)])
         self.graph.add_factors(phi1, phi2)
         self.assertEqual(self.graph.get_partition_function(), 22.0)
 
     def tearDown(self):
         del self.graph
 
 
 class TestFactorGraphMethods(unittest.TestCase):
     def setUp(self):
         self.graph = FactorGraph()
 
     def test_get_cardinality(self):
-        self.graph.add_edges_from([('a', 'phi1'), ('b', 'phi1'),
-                                   ('c', 'phi2'), ('d', 'phi2'),
-                                   ('a', 'phi3'), ('d', 'phi3')])
+        self.graph.add_edges_from(
+            [
+                ("a", "phi1"),
+                ("b", "phi1"),
+                ("c", "phi2"),
+                ("d", "phi2"),
+                ("a", "phi3"),
+                ("d", "phi3"),
+            ]
+        )
 
         self.assertDictEqual(self.graph.get_cardinality(), {})
 
-        phi1 = DiscreteFactor(['a', 'b'], [1, 2], np.random.rand(2))
+        phi1 = DiscreteFactor(["a", "b"], [1, 2], np.random.rand(2))
         self.graph.add_factors(phi1)
-        self.assertDictEqual(self.graph.get_cardinality(), {'a': 1, 'b': 2})
+        self.assertDictEqual(self.graph.get_cardinality(), {"a": 1, "b": 2})
         self.graph.remove_factors(phi1)
         self.assertDictEqual(self.graph.get_cardinality(), {})
 
-        phi1 = DiscreteFactor(['a', 'b'], [2, 2], np.random.rand(4))
-        phi2 = DiscreteFactor(['c', 'd'], [1, 2], np.random.rand(2))
+        phi1 = DiscreteFactor(["a", "b"], [2, 2], np.random.rand(4))
+        phi2 = DiscreteFactor(["c", "d"], [1, 2], np.random.rand(2))
         self.graph.add_factors(phi1, phi2)
-        self.assertDictEqual(self.graph.get_cardinality(), {'d': 2, 'a': 2, 'b': 2, 'c': 1})
+        self.assertDictEqual(
+            self.graph.get_cardinality(), {"d": 2, "a": 2, "b": 2, "c": 1}
+        )
 
-        phi3 = DiscreteFactor(['d', 'a'], [1, 2], np.random.rand(2))
+        phi3 = DiscreteFactor(["d", "a"], [1, 2], np.random.rand(2))
         self.graph.add_factors(phi3)
-        self.assertDictEqual(self.graph.get_cardinality(), {'d': 1, 'c': 1, 'b': 2, 'a': 2})
+        self.assertDictEqual(
+            self.graph.get_cardinality(), {"d": 1, "c": 1, "b": 2, "a": 2}
+        )
 
         self.graph.remove_factors(phi1, phi2, phi3)
         self.assertDictEqual(self.graph.get_cardinality(), {})
 
     def test_get_cardinality_with_node(self):
-        self.graph.add_nodes_from(['a', 'b', 'c'])
-        phi1 = DiscreteFactor(['a', 'b'], [2, 2], np.random.rand(4))
-        phi2 = DiscreteFactor(['b', 'c'], [2, 2], np.random.rand(4))
+        self.graph.add_nodes_from(["a", "b", "c"])
+        phi1 = DiscreteFactor(["a", "b"], [2, 2], np.random.rand(4))
+        phi2 = DiscreteFactor(["b", "c"], [2, 2], np.random.rand(4))
         self.graph.add_nodes_from([phi1, phi2])
-        self.graph.add_edges_from([('a', phi1), ('b', phi1),
-                                   ('b', phi2), ('c', phi2)])
+        self.graph.add_edges_from([("a", phi1), ("b", phi1), ("b", phi2), ("c", phi2)])
         self.graph.add_factors(phi1, phi2)
-        self.assertEqual(self.graph.get_cardinality('a'), 2)
-        self.assertEqual(self.graph.get_cardinality('b'), 2)
-        self.assertEqual(self.graph.get_cardinality('c'), 2)
+        self.assertEqual(self.graph.get_cardinality("a"), 2)
+        self.assertEqual(self.graph.get_cardinality("b"), 2)
+        self.assertEqual(self.graph.get_cardinality("c"), 2)
 
     def test_get_factor_nodes(self):
-        phi1 = DiscreteFactor(['a', 'b'], [2, 2], np.random.rand(4))
-        phi2 = DiscreteFactor(['b', 'c'], [2, 2], np.random.rand(4))
+        phi1 = DiscreteFactor(["a", "b"], [2, 2], np.random.rand(4))
+        phi2 = DiscreteFactor(["b", "c"], [2, 2], np.random.rand(4))
 
-        self.graph.add_edges_from([('a', phi1), ('b', phi1),
-                                   ('b', phi2), ('c', phi2)])
+        self.graph.add_edges_from([("a", phi1), ("b", phi1), ("b", phi2), ("c", phi2)])
         self.graph.add_factors(phi1, phi2)
         six.assertCountEqual(self, self.graph.get_factor_nodes(), [phi1, phi2])
 
     def test_get_variable_nodes(self):
-        phi1 = DiscreteFactor(['a', 'b'], [2, 2], np.random.rand(4))
-        phi2 = DiscreteFactor(['b', 'c'], [2, 2], np.random.rand(4))
-        self.graph.add_edges_from([('a', phi1), ('b', phi1),
-                                   ('b', phi2), ('c', phi2)])
+        phi1 = DiscreteFactor(["a", "b"], [2, 2], np.random.rand(4))
+        phi2 = DiscreteFactor(["b", "c"], [2, 2], np.random.rand(4))
+        self.graph.add_edges_from([("a", phi1), ("b", phi1), ("b", phi2), ("c", phi2)])
         self.graph.add_factors(phi1, phi2)
-        six.assertCountEqual(self, self.graph.get_variable_nodes(), ['a', 'b', 'c'])
+        six.assertCountEqual(self, self.graph.get_variable_nodes(), ["a", "b", "c"])
 
     def test_get_variable_nodes_raises_error(self):
-        self.graph.add_edges_from([('a', 'phi1'), ('b', 'phi1'),
-                                   ('b', 'phi2'), ('c', 'phi2')])
+        self.graph.add_edges_from(
+            [("a", "phi1"), ("b", "phi1"), ("b", "phi2"), ("c", "phi2")]
+        )
         self.assertRaises(ValueError, self.graph.get_variable_nodes)
 
     def test_to_markov_model(self):
-        phi1 = DiscreteFactor(['a', 'b'], [2, 2], np.random.rand(4))
-        phi2 = DiscreteFactor(['b', 'c'], [2, 2], np.random.rand(4))
-        self.graph.add_edges_from([('a', phi1), ('b', phi1),
-                                   ('b', phi2), ('c', phi2)])
+        phi1 = DiscreteFactor(["a", "b"], [2, 2], np.random.rand(4))
+        phi2 = DiscreteFactor(["b", "c"], [2, 2], np.random.rand(4))
+        self.graph.add_edges_from([("a", phi1), ("b", phi1), ("b", phi2), ("c", phi2)])
         self.graph.add_factors(phi1, phi2)
         mm = self.graph.to_markov_model()
         self.assertIsInstance(mm, MarkovModel)
-        self.assertListEqual(sorted(mm.nodes()), ['a', 'b', 'c'])
-        self.assertListEqual(hf.recursive_sorted(mm.edges()),
-                             [['a', 'b'], ['b', 'c']])
-        self.assertListEqual(sorted(mm.get_factors(),
-                             key=lambda x: x.scope()), [phi1, phi2])
+        self.assertListEqual(sorted(mm.nodes()), ["a", "b", "c"])
+        self.assertListEqual(hf.recursive_sorted(mm.edges()), [["a", "b"], ["b", "c"]])
+        self.assertListEqual(
+            sorted(mm.get_factors(), key=lambda x: x.scope()), [phi1, phi2]
+        )
 
     def test_to_junction_tree(self):
-        phi1 = DiscreteFactor(['a', 'b'], [2, 2], np.random.rand(4))
-        phi2 = DiscreteFactor(['b', 'c'], [2, 2], np.random.rand(4))
-        self.graph.add_edges_from([('a', phi1), ('b', phi1),
-                                   ('b', phi2), ('c', phi2)])
+        phi1 = DiscreteFactor(["a", "b"], [2, 2], np.random.rand(4))
+        phi2 = DiscreteFactor(["b", "c"], [2, 2], np.random.rand(4))
+        self.graph.add_edges_from([("a", phi1), ("b", phi1), ("b", phi2), ("c", phi2)])
 
         self.graph.add_factors(phi1, phi2)
         jt = self.graph.to_junction_tree()
         self.assertIsInstance(jt, JunctionTree)
-        self.assertListEqual(hf.recursive_sorted(jt.nodes()),
-                             [['a', 'b'], ['b', 'c']])
+        self.assertListEqual(hf.recursive_sorted(jt.nodes()), [["a", "b"], ["b", "c"]])
         self.assertEqual(len(jt.edges()), 1)
 
     def test_check_model(self):
-        self.graph.add_nodes_from(['a', 'b', 'c'])
-        phi1 = DiscreteFactor(['a', 'b'], [2, 2], np.random.rand(4))
-        phi2 = DiscreteFactor(['b', 'c'], [2, 2], np.random.rand(4))
+        self.graph.add_nodes_from(["a", "b", "c"])
+        phi1 = DiscreteFactor(["a", "b"], [2, 2], np.random.rand(4))
+        phi2 = DiscreteFactor(["b", "c"], [2, 2], np.random.rand(4))
         self.graph.add_nodes_from([phi1, phi2])
-        self.graph.add_edges_from([('a', phi1), ('b', phi1),
-                                   ('b', phi2), ('c', phi2)])
+        self.graph.add_edges_from([("a", phi1), ("b", phi1), ("b", phi2), ("c", phi2)])
         self.graph.add_factors(phi1, phi2)
         self.assertTrue(self.graph.check_model())
 
         self.graph.remove_factors(phi1)
         self.graph.remove_node(phi1)
-        phi1 = DiscreteFactor(['a', 'b'], [4, 2], np.random.rand(8))
+        phi1 = DiscreteFactor(["a", "b"], [4, 2], np.random.rand(8))
         self.graph.add_factors(phi1)
-        self.graph.add_edges_from([('a', phi1)])
+        self.graph.add_edges_from([("a", phi1)])
         self.assertTrue(self.graph.check_model())
 
     def test_check_model1(self):
-        self.graph.add_nodes_from(['a', 'b', 'c', 'd'])
-        phi1 = DiscreteFactor(['a', 'b'], [2, 2], np.random.rand(4))
-        phi2 = DiscreteFactor(['b', 'c'], [2, 2], np.random.rand(4))
+        self.graph.add_nodes_from(["a", "b", "c", "d"])
+        phi1 = DiscreteFactor(["a", "b"], [2, 2], np.random.rand(4))
+        phi2 = DiscreteFactor(["b", "c"], [2, 2], np.random.rand(4))
         self.graph.add_nodes_from([phi1, phi2])
-        self.graph.add_edges_from([('a', phi1), ('b', phi1),
-                                   ('b', phi2), ('c', phi2)])
+        self.graph.add_edges_from([("a", phi1), ("b", phi1), ("b", phi2), ("c", phi2)])
         self.graph.add_factors(phi1, phi2)
         self.assertRaises(ValueError, self.graph.check_model)
 
-        self.graph.remove_node('d')
+        self.graph.remove_node("d")
         self.assertTrue(self.graph.check_model())
 
     def test_check_model2(self):
-        self.graph.add_nodes_from(['a', 'b', 'c'])
-        phi1 = DiscreteFactor(['a', 'b'], [2, 2], np.random.rand(4))
-        phi2 = DiscreteFactor(['b', 'c'], [2, 2], np.random.rand(4))
+        self.graph.add_nodes_from(["a", "b", "c"])
+        phi1 = DiscreteFactor(["a", "b"], [2, 2], np.random.rand(4))
+        phi2 = DiscreteFactor(["b", "c"], [2, 2], np.random.rand(4))
         self.graph.add_nodes_from([phi1, phi2])
-        self.graph.add_edges_from([('a', phi1), ('b', phi1),
-                                   ('b', phi2), ('c', phi2)])
+        self.graph.add_edges_from([("a", phi1), ("b", phi1), ("b", phi2), ("c", phi2)])
         self.graph.add_factors(phi1, phi2)
 
-        self.graph.add_edges_from([('a', 'b')])
+        self.graph.add_edges_from([("a", "b")])
         self.assertRaises(ValueError, self.graph.check_model)
 
         self.graph.add_edges_from([(phi1, phi2)])
         self.assertRaises(ValueError, self.graph.check_model)
 
-        self.graph.remove_edges_from([('a', 'b'), (phi1, phi2)])
+        self.graph.remove_edges_from([("a", "b"), (phi1, phi2)])
         self.assertTrue(self.graph.check_model())
 
     def test_check_model3(self):
-        self.graph.add_nodes_from(['a', 'b', 'c'])
-        phi1 = DiscreteFactor(['a', 'b'], [2, 2], np.random.rand(4))
-        phi2 = DiscreteFactor(['b', 'c'], [2, 2], np.random.rand(4))
-        phi3 = DiscreteFactor(['a', 'c'], [2, 2], np.random.rand(4))
+        self.graph.add_nodes_from(["a", "b", "c"])
+        phi1 = DiscreteFactor(["a", "b"], [2, 2], np.random.rand(4))
+        phi2 = DiscreteFactor(["b", "c"], [2, 2], np.random.rand(4))
+        phi3 = DiscreteFactor(["a", "c"], [2, 2], np.random.rand(4))
         self.graph.add_nodes_from([phi1, phi2])
-        self.graph.add_edges_from([('a', phi1), ('b', phi1),
-                                   ('b', phi2), ('c', phi2)])
+        self.graph.add_edges_from([("a", phi1), ("b", phi1), ("b", phi2), ("c", phi2)])
         self.graph.add_factors(phi1, phi2, phi3)
         self.assertRaises(ValueError, self.graph.check_model)
         self.graph.remove_factors(phi3)
         self.assertTrue(self.graph.check_model())
 
     def test_check_model4(self):
-        self.graph.add_nodes_from(['a', 'b', 'c'])
-        phi1 = DiscreteFactor(['a', 'b'], [2, 2], np.random.rand(4))
-        phi2 = DiscreteFactor(['b', 'c'], [3, 2], np.random.rand(6))
+        self.graph.add_nodes_from(["a", "b", "c"])
+        phi1 = DiscreteFactor(["a", "b"], [2, 2], np.random.rand(4))
+        phi2 = DiscreteFactor(["b", "c"], [3, 2], np.random.rand(6))
         self.graph.add_nodes_from([phi1, phi2])
-        self.graph.add_edges_from([('a', phi1), ('b', phi1),
-                                   ('b', phi2), ('c', phi2)])
+        self.graph.add_edges_from([("a", phi1), ("b", phi1), ("b", phi2), ("c", phi2)])
         self.graph.add_factors(phi1, phi2)
         self.assertRaises(ValueError, self.graph.check_model)
 
         self.graph.remove_factors(phi2)
         self.graph.remove_node(phi2)
-        phi3 = DiscreteFactor(['c', 'a'], [4, 4], np.random.rand(16))
+        phi3 = DiscreteFactor(["c", "a"], [4, 4], np.random.rand(16))
         self.graph.add_factors(phi3)
-        self.graph.add_edges_from([('a', phi3), ('c', phi3)])
+        self.graph.add_edges_from([("a", phi3), ("c", phi3)])
         self.assertRaises(ValueError, self.graph.check_model)
 
     def test_copy(self):
-        self.graph.add_nodes_from(['a', 'b', 'c'])
-        phi1 = DiscreteFactor(['a', 'b'], [2, 2], np.random.rand(4))
-        phi2 = DiscreteFactor(['b', 'c'], [2, 2], np.random.rand(4))
+        self.graph.add_nodes_from(["a", "b", "c"])
+        phi1 = DiscreteFactor(["a", "b"], [2, 2], np.random.rand(4))
+        phi2 = DiscreteFactor(["b", "c"], [2, 2], np.random.rand(4))
         self.graph.add_factors(phi1, phi2)
         self.graph.add_nodes_from([phi1, phi2])
-        self.graph.add_edges_from([('a', phi1), ('b', phi1),
-                                   ('b', phi2), ('c', phi2)])
+        self.graph.add_edges_from([("a", phi1), ("b", phi1), ("b", phi2), ("c", phi2)])
         graph_copy = self.graph.copy()
         self.assertIsInstance(graph_copy, FactorGraph)
         self.assertTrue(graph_copy.check_model())
         self.assertEqual(self.graph.get_factors(), graph_copy.get_factors())
         self.graph.remove_factors(phi1, phi2)
-        self.assertTrue(phi1 not in self.graph.factors and phi2 not in self.graph.factors)
+        self.assertTrue(
+            phi1 not in self.graph.factors and phi2 not in self.graph.factors
+        )
         self.assertTrue(phi1 in graph_copy.factors and phi2 in graph_copy.factors)
         self.graph.add_factors(phi1, phi2)
-        self.graph.factors[0] = DiscreteFactor(['a', 'b'], [2, 2], np.random.rand(4))
+        self.graph.factors[0] = DiscreteFactor(["a", "b"], [2, 2], np.random.rand(4))
         self.assertNotEqual(self.graph.get_factors()[0], graph_copy.get_factors()[0])
         self.assertNotEqual(self.graph.factors, graph_copy.factors)
 
     def tearDown(self):
         del self.graph
```

### Comparing `pgmpy-0.1.7/pgmpy/tests/test_models/test_NaiveBayes.py` & `pgmpy-0.1.9/pgmpy/tests/test_models/test_NaiveBayes.py`

 * *Files 25% similar despite different names*

```diff
@@ -13,183 +13,216 @@
     def setUp(self):
         self.G = NaiveBayes()
 
     def test_class_init_without_data(self):
         self.assertIsInstance(self.G, nx.DiGraph)
 
     def test_class_init_with_data_string(self):
-        self.g = NaiveBayes([('a', 'b'), ('a', 'c')])
-        six.assertCountEqual(self, self.g.nodes(), ['a', 'b', 'c'])
-        six.assertCountEqual(self, self.g.edges(), [('a', 'b'), ('a', 'c')])
-        self.assertEqual(self.g.parent_node, 'a')
-        self.assertSetEqual(self.g.children_nodes, {'b', 'c'})
-
-        self.assertRaises(ValueError, NaiveBayes, [('a', 'b'), ('b', 'c')])
-        self.assertRaises(ValueError, NaiveBayes, [('a', 'b'), ('c', 'b')])
-        self.assertRaises(ValueError, NaiveBayes, [('a', 'b'), ('d', 'e')])
+        self.g = NaiveBayes([("a", "b"), ("a", "c")])
+        six.assertCountEqual(self, list(self.g.nodes()), ["a", "b", "c"])
+        six.assertCountEqual(self, list(self.g.edges()), [("a", "b"), ("a", "c")])
+        self.assertEqual(self.g.parent_node, "a")
+        self.assertSetEqual(self.g.children_nodes, {"b", "c"})
+
+        self.assertRaises(ValueError, NaiveBayes, [("a", "b"), ("b", "c")])
+        self.assertRaises(ValueError, NaiveBayes, [("a", "b"), ("c", "b")])
+        self.assertRaises(ValueError, NaiveBayes, [("a", "b"), ("d", "e")])
 
     def test_class_init_with_data_nonstring(self):
         self.g = NaiveBayes([(1, 2), (1, 3)])
-        six.assertCountEqual(self, self.g.nodes(), [1, 2, 3])
-        six.assertCountEqual(self, self.g.edges(), [(1, 2), (1, 3)])
+        six.assertCountEqual(self, list(self.g.nodes()), [1, 2, 3])
+        six.assertCountEqual(self, list(self.g.edges()), [(1, 2), (1, 3)])
         self.assertEqual(self.g.parent_node, 1)
         self.assertSetEqual(self.g.children_nodes, {2, 3})
 
         self.assertRaises(ValueError, NaiveBayes, [(1, 2), (2, 3)])
         self.assertRaises(ValueError, NaiveBayes, [(1, 2), (3, 2)])
         self.assertRaises(ValueError, NaiveBayes, [(1, 2), (3, 4)])
 
     def test_add_node_string(self):
-        self.G.add_node('a')
-        self.assertListEqual(self.G.nodes(), ['a'])
+        self.G.add_node("a")
+        self.assertListEqual(list(self.G.nodes()), ["a"])
 
     def test_add_node_nonstring(self):
         self.G.add_node(1)
-        self.assertListEqual(self.G.nodes(), [1])
+        self.assertListEqual(list(self.G.nodes()), [1])
 
     def test_add_nodes_from_string(self):
-        self.G.add_nodes_from(['a', 'b', 'c', 'd'])
-        six.assertCountEqual(self, self.G.nodes(), ['a', 'b', 'c', 'd'])
+        self.G.add_nodes_from(["a", "b", "c", "d"])
+        six.assertCountEqual(self, list(self.G.nodes()), ["a", "b", "c", "d"])
 
     def test_add_nodes_from_non_string(self):
         self.G.add_nodes_from([1, 2, 3, 4])
-        six.assertCountEqual(self, self.G.nodes(), [1, 2, 3, 4])
+        six.assertCountEqual(self, list(self.G.nodes()), [1, 2, 3, 4])
 
     def test_add_edge_string(self):
-        self.G.add_edge('a', 'b')
-        six.assertCountEqual(self, self.G.nodes(), ['a', 'b'])
-        self.assertListEqual(self.G.edges(), [('a', 'b')])
-        self.assertEqual(self.G.parent_node, 'a')
-        self.assertSetEqual(self.G.children_nodes, {'b'})
-
-        self.G.add_nodes_from(['c', 'd'])
-        self.G.add_edge('a', 'c')
-        self.G.add_edge('a', 'd')
-        six.assertCountEqual(self, self.G.nodes(), ['a', 'b', 'c', 'd'])
-        six.assertCountEqual(self, self.G.edges(), [('a', 'b'), ('a', 'c'), ('a', 'd')])
-        self.assertEqual(self.G.parent_node, 'a')
-        self.assertSetEqual(self.G.children_nodes, {'b', 'c', 'd'})
-
-        self.assertRaises(ValueError, self.G.add_edge, 'b', 'c')
-        self.assertRaises(ValueError, self.G.add_edge, 'd', 'f')
-        self.assertRaises(ValueError, self.G.add_edge, 'e', 'f')
-        self.assertRaises(ValueError, self.G.add_edges_from, [('a', 'e'), ('b', 'f')])
-        self.assertRaises(ValueError, self.G.add_edges_from, [('b', 'f')])
+        self.G.add_edge("a", "b")
+        six.assertCountEqual(self, list(self.G.nodes()), ["a", "b"])
+        self.assertListEqual(list(self.G.edges()), [("a", "b")])
+        self.assertEqual(self.G.parent_node, "a")
+        self.assertSetEqual(self.G.children_nodes, {"b"})
+
+        self.G.add_nodes_from(["c", "d"])
+        self.G.add_edge("a", "c")
+        self.G.add_edge("a", "d")
+        six.assertCountEqual(self, list(self.G.nodes()), ["a", "b", "c", "d"])
+        six.assertCountEqual(
+            self, list(self.G.edges()), [("a", "b"), ("a", "c"), ("a", "d")]
+        )
+        self.assertEqual(self.G.parent_node, "a")
+        self.assertSetEqual(self.G.children_nodes, {"b", "c", "d"})
+
+        self.assertRaises(ValueError, self.G.add_edge, "b", "c")
+        self.assertRaises(ValueError, self.G.add_edge, "d", "f")
+        self.assertRaises(ValueError, self.G.add_edge, "e", "f")
+        self.assertRaises(ValueError, self.G.add_edges_from, [("a", "e"), ("b", "f")])
+        self.assertRaises(ValueError, self.G.add_edges_from, [("b", "f")])
 
     def test_add_edge_nonstring(self):
         self.G.add_edge(1, 2)
-        six.assertCountEqual(self, self.G.nodes(), [1, 2])
-        self.assertListEqual(self.G.edges(), [(1, 2)])
+        six.assertCountEqual(self, list(self.G.nodes()), [1, 2])
+        self.assertListEqual(list(self.G.edges()), [(1, 2)])
         self.assertEqual(self.G.parent_node, 1)
         self.assertSetEqual(self.G.children_nodes, {2})
 
         self.G.add_nodes_from([3, 4])
         self.G.add_edge(1, 3)
         self.G.add_edge(1, 4)
-        six.assertCountEqual(self, self.G.nodes(), [1, 2, 3, 4])
-        six.assertCountEqual(self, self.G.edges(), [(1, 2), (1, 3), (1, 4)])
+        six.assertCountEqual(self, list(self.G.nodes()), [1, 2, 3, 4])
+        six.assertCountEqual(self, list(self.G.edges()), [(1, 2), (1, 3), (1, 4)])
         self.assertEqual(self.G.parent_node, 1)
         self.assertSetEqual(self.G.children_nodes, {2, 3, 4})
 
         self.assertRaises(ValueError, self.G.add_edge, 2, 3)
         self.assertRaises(ValueError, self.G.add_edge, 3, 6)
         self.assertRaises(ValueError, self.G.add_edge, 5, 6)
         self.assertRaises(ValueError, self.G.add_edges_from, [(1, 5), (2, 6)])
         self.assertRaises(ValueError, self.G.add_edges_from, [(2, 6)])
 
     def test_add_edge_selfloop(self):
-        self.assertRaises(ValueError, self.G.add_edge, 'a', 'a')
+        self.assertRaises(ValueError, self.G.add_edge, "a", "a")
         self.assertRaises(ValueError, self.G.add_edge, 1, 1)
 
     def test_add_edges_from_self_loop(self):
-        self.assertRaises(ValueError, self.G.add_edges_from,
-                          [('a', 'a')])
+        self.assertRaises(ValueError, self.G.add_edges_from, [("a", "a")])
 
     def test_update_node_parents_bm_constructor(self):
-        self.g = NaiveBayes([('a', 'b'), ('a', 'c')])
-        self.assertListEqual(self.g.predecessors('a'), [])
-        self.assertListEqual(self.g.predecessors('b'), ['a'])
-        self.assertListEqual(self.g.predecessors('c'), ['a'])
+        self.g = NaiveBayes([("a", "b"), ("a", "c")])
+        self.assertListEqual(list(self.g.predecessors("a")), [])
+        self.assertListEqual(list(self.g.predecessors("b")), ["a"])
+        self.assertListEqual(list(self.g.predecessors("c")), ["a"])
 
     def test_update_node_parents(self):
-        self.G.add_nodes_from(['a', 'b', 'c'])
-        self.G.add_edges_from([('a', 'b'), ('a', 'c')])
-        self.assertListEqual(self.G.predecessors('a'), [])
-        self.assertListEqual(self.G.predecessors('b'), ['a'])
-        self.assertListEqual(self.G.predecessors('c'), ['a'])
+        self.G.add_nodes_from(["a", "b", "c"])
+        self.G.add_edges_from([("a", "b"), ("a", "c")])
+        self.assertListEqual(list(self.G.predecessors("a")), [])
+        self.assertListEqual(list(self.G.predecessors("b")), ["a"])
+        self.assertListEqual(list(self.G.predecessors("c")), ["a"])
 
     def tearDown(self):
         del self.G
 
 
 class TestNaiveBayesMethods(unittest.TestCase):
     def setUp(self):
-        self.G1 = NaiveBayes([('a', 'b'), ('a', 'c'),
-                             ('a', 'd'), ('a', 'e')])
-        self.G2 = NaiveBayes([('d', 'g'), ('d', 'l'),
-                              ('d', 's')])
+        self.G1 = NaiveBayes([("a", "b"), ("a", "c"), ("a", "d"), ("a", "e")])
+        self.G2 = NaiveBayes([("d", "g"), ("d", "l"), ("d", "s")])
 
     def test_local_independencies(self):
-        self.assertEqual(self.G1.local_independencies('a'), Independencies())
-        self.assertEqual(self.G1.local_independencies('b'), Independencies(['b', ['e', 'c', 'd'], 'a']))
-        self.assertEqual(self.G1.local_independencies('c'), Independencies(['c', ['e', 'b', 'd'], 'a']))
-        self.assertEqual(self.G1.local_independencies('d'), Independencies(['d', ['b', 'c', 'e'], 'a']))
+        self.assertEqual(self.G1.local_independencies("a"), Independencies())
+        self.assertEqual(
+            self.G1.local_independencies("b"),
+            Independencies(["b", ["e", "c", "d"], "a"]),
+        )
+        self.assertEqual(
+            self.G1.local_independencies("c"),
+            Independencies(["c", ["e", "b", "d"], "a"]),
+        )
+        self.assertEqual(
+            self.G1.local_independencies("d"),
+            Independencies(["d", ["b", "c", "e"], "a"]),
+        )
 
     def test_active_trail_nodes(self):
-        self.assertListEqual(sorted(self.G2.active_trail_nodes('d')), ['d', 'g', 'l', 's'])
-        self.assertListEqual(sorted(self.G2.active_trail_nodes('g')), ['d', 'g', 'l', 's'])
-        self.assertListEqual(sorted(self.G2.active_trail_nodes('l')), ['d', 'g', 'l', 's'])
-        self.assertListEqual(sorted(self.G2.active_trail_nodes('s')), ['d', 'g', 'l', 's'])
+        self.assertListEqual(
+            sorted(self.G2.active_trail_nodes("d")), ["d", "g", "l", "s"]
+        )
+        self.assertListEqual(
+            sorted(self.G2.active_trail_nodes("g")), ["d", "g", "l", "s"]
+        )
+        self.assertListEqual(
+            sorted(self.G2.active_trail_nodes("l")), ["d", "g", "l", "s"]
+        )
+        self.assertListEqual(
+            sorted(self.G2.active_trail_nodes("s")), ["d", "g", "l", "s"]
+        )
 
     def test_active_trail_nodes_args(self):
-        self.assertListEqual(sorted(self.G2.active_trail_nodes('d', observed='g')), ['d', 'l', 's'])
-        self.assertListEqual(sorted(self.G2.active_trail_nodes('l', observed='g')), ['d', 'l', 's'])
-        self.assertListEqual(sorted(self.G2.active_trail_nodes('s', observed=['g', 'l'])), ['d', 's'])
-        self.assertListEqual(sorted(self.G2.active_trail_nodes('s', observed=['d', 'l'])), ['s'])
+        self.assertListEqual(
+            sorted(self.G2.active_trail_nodes("d", observed="g")), ["d", "l", "s"]
+        )
+        self.assertListEqual(
+            sorted(self.G2.active_trail_nodes("l", observed="g")), ["d", "l", "s"]
+        )
+        self.assertListEqual(
+            sorted(self.G2.active_trail_nodes("s", observed=["g", "l"])), ["d", "s"]
+        )
+        self.assertListEqual(
+            sorted(self.G2.active_trail_nodes("s", observed=["d", "l"])), ["s"]
+        )
 
     def test_get_ancestors_of(self):
-        self.assertListEqual(sorted(self.G1._get_ancestors_of('b')), ['a', 'b'])
-        self.assertListEqual(sorted(self.G1._get_ancestors_of('e')), ['a', 'e'])
-        self.assertListEqual(sorted(self.G1._get_ancestors_of('a')), ['a'])
-        self.assertListEqual(sorted(self.G1._get_ancestors_of(['b', 'e'])), ['a', 'b', 'e'])
+        self.assertListEqual(sorted(self.G1._get_ancestors_of("b")), ["a", "b"])
+        self.assertListEqual(sorted(self.G1._get_ancestors_of("e")), ["a", "e"])
+        self.assertListEqual(sorted(self.G1._get_ancestors_of("a")), ["a"])
+        self.assertListEqual(
+            sorted(self.G1._get_ancestors_of(["b", "e"])), ["a", "b", "e"]
+        )
 
     def tearDown(self):
         del self.G1
         del self.G2
 
 
 class TestNaiveBayesFit(unittest.TestCase):
     def setUp(self):
         self.model1 = NaiveBayes()
-        self.model2 = NaiveBayes([('A', 'B')])
+        self.model2 = NaiveBayes([("A", "B")])
 
     def test_fit_model_creation(self):
-        values = pd.DataFrame(np.random.randint(low=0, high=2, size=(1000, 5)),
-                              columns=['A', 'B', 'C', 'D', 'E'])
-
-        self.model1.fit(values, 'A')
-        six.assertCountEqual(self, self.model1.nodes(), ['A', 'B', 'C', 'D', 'E'])
-        six.assertCountEqual(self, self.model1.edges(), [('A', 'B'), ('A', 'C'), ('A', 'D'),
-                                                         ('A', 'E')])
-        self.assertEqual(self.model1.parent_node, 'A')
-        self.assertSetEqual(self.model1.children_nodes, {'B', 'C', 'D', 'E'})
+        values = pd.DataFrame(
+            np.random.randint(low=0, high=2, size=(1000, 5)),
+            columns=["A", "B", "C", "D", "E"],
+        )
+
+        self.model1.fit(values, "A")
+        six.assertCountEqual(self, self.model1.nodes(), ["A", "B", "C", "D", "E"])
+        six.assertCountEqual(
+            self, self.model1.edges(), [("A", "B"), ("A", "C"), ("A", "D"), ("A", "E")]
+        )
+        self.assertEqual(self.model1.parent_node, "A")
+        self.assertSetEqual(self.model1.children_nodes, {"B", "C", "D", "E"})
 
         self.model2.fit(values)
-        six.assertCountEqual(self, self.model1.nodes(), ['A', 'B', 'C', 'D', 'E'])
-        six.assertCountEqual(self, self.model1.edges(), [('A', 'B'), ('A', 'C'), ('A', 'D'),
-                                                         ('A', 'E')])
-        self.assertEqual(self.model2.parent_node, 'A')
-        self.assertSetEqual(self.model2.children_nodes, {'B', 'C', 'D', 'E'})
+        six.assertCountEqual(self, self.model1.nodes(), ["A", "B", "C", "D", "E"])
+        six.assertCountEqual(
+            self, self.model1.edges(), [("A", "B"), ("A", "C"), ("A", "D"), ("A", "E")]
+        )
+        self.assertEqual(self.model2.parent_node, "A")
+        self.assertSetEqual(self.model2.children_nodes, {"B", "C", "D", "E"})
 
     def test_fit_model_creation_exception(self):
-        values = pd.DataFrame(np.random.randint(low=0, high=2, size=(1000, 5)),
-                              columns=['A', 'B', 'C', 'D', 'E'])
-        values2 = pd.DataFrame(np.random.randint(low=0, high=2, size=(1000, 3)),
-                               columns=['C', 'D', 'E'])
+        values = pd.DataFrame(
+            np.random.randint(low=0, high=2, size=(1000, 5)),
+            columns=["A", "B", "C", "D", "E"],
+        )
+        values2 = pd.DataFrame(
+            np.random.randint(low=0, high=2, size=(1000, 3)), columns=["C", "D", "E"]
+        )
 
         self.assertRaises(ValueError, self.model1.fit, values)
         self.assertRaises(ValueError, self.model1.fit, values2)
-        self.assertRaises(ValueError, self.model2.fit, values2, 'A')
+        self.assertRaises(ValueError, self.model2.fit, values2, "A")
 
     def tearDown(self):
         del self.model1
         del self.model2
```

### Comparing `pgmpy-0.1.7/pgmpy/tests/test_estimators/test_K2Score.py` & `pgmpy-0.1.9/pgmpy/tests/test_estimators/test_K2Score.py`

 * *Files 14% similar despite different names*

```diff
@@ -4,31 +4,35 @@
 
 from pgmpy.models import BayesianModel
 from pgmpy.estimators import K2Score
 
 
 class TestK2Score(unittest.TestCase):
     def setUp(self):
-        self.d1 = pd.DataFrame(data={'A': [0, 0, 1], 'B': [0, 1, 0], 'C': [1, 1, 0], 'D': ['X', 'Y', 'Z']})
-        self.m1 = BayesianModel([('A', 'C'), ('B', 'C'), ('D', 'B')])
-        self.m2 = BayesianModel([('C', 'A'), ('C', 'B'), ('A', 'D')])
+        self.d1 = pd.DataFrame(
+            data={"A": [0, 0, 1], "B": [0, 1, 0], "C": [1, 1, 0], "D": ["X", "Y", "Z"]}
+        )
+        self.m1 = BayesianModel([("A", "C"), ("B", "C"), ("D", "B")])
+        self.m2 = BayesianModel([("C", "A"), ("C", "B"), ("A", "D")])
 
         # data_link - "https://www.kaggle.com/c/titanic/download/train.csv"
-        self.titanic_data = pd.read_csv('pgmpy/tests/test_estimators/testdata/titanic_train.csv')
+        self.titanic_data = pd.read_csv(
+            "pgmpy/tests/test_estimators/testdata/titanic_train.csv"
+        )
         self.titanic_data2 = self.titanic_data[["Survived", "Sex", "Pclass"]]
 
     def test_score(self):
         self.assertAlmostEqual(K2Score(self.d1).score(self.m1), -10.73813429536977)
         self.assertEqual(K2Score(self.d1).score(BayesianModel()), 0)
 
     def test_score_titanic(self):
         scorer = K2Score(self.titanic_data2)
         titanic = BayesianModel([("Sex", "Survived"), ("Pclass", "Survived")])
         self.assertAlmostEqual(scorer.score(titanic), -1891.0630673606006)
-        titanic2 = BayesianModel([("Pclass", "Sex"), ])
+        titanic2 = BayesianModel([("Pclass", "Sex")])
         titanic2.add_nodes_from(["Sex", "Survived", "Pclass"])
         self.assertLess(scorer.score(titanic2), scorer.score(titanic))
 
     def tearDown(self):
         del self.d1
         del self.m1
         del self.m2
```

### Comparing `pgmpy-0.1.7/pgmpy/tests/test_estimators/test_ScoreCache.py` & `pgmpy-0.1.9/pgmpy/tests/test_estimators/test_ScoreCache.py`

 * *Files 8% similar despite different names*

```diff
@@ -2,15 +2,14 @@
 from mock import Mock, MagicMock, call
 from pgmpy.estimators.ScoreCache import LRUCache, ScoreCache
 from pgmpy.estimators import BicScore
 import pandas as pd
 
 
 class TestScoreCache(unittest.TestCase):
-
     def test_caching(self):
         function_mock = Mock(side_effect=[1, 2])
         cache = LRUCache(function_mock, max_size=2)
 
         self.assertEqual(cache("key1"), 1)
         self.assertEqual(cache("key2"), 2)
 
@@ -30,32 +29,38 @@
         cache("key2")  # cached
         cache("key1")
 
         expected_function_calls = [call("key1"), call("key2"), call("key1")]
         function_mock.assert_has_calls(expected_function_calls, any_order=False)
 
     def test_remove_least_recently_used(self):
-        function_mock = Mock(side_effect=lambda key: {"key1": 1, "key2": 2, "key3": 3}[key])
+        function_mock = Mock(
+            side_effect=lambda key: {"key1": 1, "key2": 2, "key3": 3}[key]
+        )
         cache = LRUCache(function_mock, max_size=2)
 
         cache("key1")
         cache("key2")
         cache("key3")  # kicks out 'key1'
         cache("key2")  # cached
         cache("key1")
 
-        expected_function_calls = [call("key1"), call("key2"), call("key3"), call("key1")]
+        expected_function_calls = [
+            call("key1"),
+            call("key2"),
+            call("key3"),
+            call("key1"),
+        ]
         function_mock.assert_has_calls(expected_function_calls, any_order=False)
 
     def test_score_cache_invalid_scorer(self):
         with self.assertRaises(AssertionError) as e:
             ScoreCache("invalid_scorer", None)
 
     def test_score_cache(self):
-
         def local_scores(*args):
             if args == ("key1", ["key2", "key3"]):
                 return -1
             if args == ("key2", ["key3"]):
                 return -2
 
             assert False, "Unhandled arguments"
@@ -66,9 +71,14 @@
         cache = ScoreCache(base_scorer, data)
 
         self.assertEqual(cache.local_score("key1", ["key2", "key3"]), -1)
         self.assertEqual(cache.local_score("key1", ["key2", "key3"]), -1)  # cached
         self.assertEqual(cache.local_score("key2", ["key3"]), -2)
         self.assertEqual(cache.local_score("key2", ["key3"]), -2)  # cached
 
-        expected_function_calls = [call("key1", ["key2", "key3"]), call("key2", ["key3"])]
-        base_scorer.local_score.assert_has_calls(expected_function_calls, any_order=False)
+        expected_function_calls = [
+            call("key1", ["key2", "key3"]),
+            call("key2", ["key3"]),
+        ]
+        base_scorer.local_score.assert_has_calls(
+            expected_function_calls, any_order=False
+        )
```

### Comparing `pgmpy-0.1.7/pgmpy/tests/test_estimators/test_MaximumLikelihoodEstimator.py` & `pgmpy-0.1.9/pgmpy/tests/test_estimators/test_MaximumLikelihoodEstimator.py`

 * *Files 23% similar despite different names*

```diff
@@ -6,84 +6,140 @@
 from pgmpy.models import BayesianModel
 from pgmpy.estimators import MaximumLikelihoodEstimator
 from pgmpy.factors.discrete import TabularCPD
 
 
 class TestMLE(unittest.TestCase):
     def setUp(self):
-        self.m1 = BayesianModel([('A', 'C'), ('B', 'C')])
-        self.d1 = pd.DataFrame(data={'A': [0, 0, 1],
-                                     'B': [0, 1, 0],
-                                     'C': [1, 1, 0]})
-        self.d2 = pd.DataFrame(data={'A': [0, np.NaN, 1],
-                                     'B': [0, 1, 0],
-                                     'C': [1, 1, np.NaN],
-                                     'D': [np.NaN, 'Y', np.NaN]})
-        self.cpds = [TabularCPD('A', 2, [[2.0/3], [1.0/3]]),
-                     TabularCPD('B', 2, [[2.0/3], [1.0/3]]),
-                     TabularCPD('C', 2, [[0.0, 0.0, 1.0, 0.5],
-                                         [1.0, 1.0, 0.0, 0.5]],
-                                evidence=['A', 'B'], evidence_card=[2, 2])]
+        self.m1 = BayesianModel([("A", "C"), ("B", "C")])
+        self.d1 = pd.DataFrame(data={"A": [0, 0, 1], "B": [0, 1, 0], "C": [1, 1, 0]})
+        self.d2 = pd.DataFrame(
+            data={
+                "A": [0, np.NaN, 1],
+                "B": [0, 1, 0],
+                "C": [1, 1, np.NaN],
+                "D": [np.NaN, "Y", np.NaN],
+            }
+        )
+        self.cpds = [
+            TabularCPD("A", 2, [[2.0 / 3], [1.0 / 3]]),
+            TabularCPD("B", 2, [[2.0 / 3], [1.0 / 3]]),
+            TabularCPD(
+                "C",
+                2,
+                [[0.0, 0.0, 1.0, 0.5], [1.0, 1.0, 0.0, 0.5]],
+                evidence=["A", "B"],
+                evidence_card=[2, 2],
+            ),
+        ]
         self.mle1 = MaximumLikelihoodEstimator(self.m1, self.d1)
 
     def test_get_parameters_incomplete_data(self):
         self.assertSetEqual(set(self.mle1.get_parameters()), set(self.cpds))
 
     def test_estimate_cpd(self):
-        self.assertEqual(self.mle1.estimate_cpd('A'), self.cpds[0])
-        self.assertEqual(self.mle1.estimate_cpd('B'), self.cpds[1])
-        self.assertEqual(self.mle1.estimate_cpd('C'), self.cpds[2])
+        self.assertEqual(self.mle1.estimate_cpd("A"), self.cpds[0])
+        self.assertEqual(self.mle1.estimate_cpd("B"), self.cpds[1])
+        self.assertEqual(self.mle1.estimate_cpd("C"), self.cpds[2])
 
     def test_state_names1(self):
-        m = BayesianModel([('A', 'B')])
-        d = pd.DataFrame(data={'A': [2, 3, 8, 8, 8], 'B': ['X', 'O', 'X', 'O', 'X']})
-        cpd_b = TabularCPD('B', 2, [[0, 1, 1.0 / 3], [1, 0, 2.0 / 3]],
-                           evidence=['A'], evidence_card=[3])
+        m = BayesianModel([("A", "B")])
+        d = pd.DataFrame(data={"A": [2, 3, 8, 8, 8], "B": ["X", "O", "X", "O", "X"]})
+        cpd_b = TabularCPD(
+            "B",
+            2,
+            [[0, 1, 1.0 / 3], [1, 0, 2.0 / 3]],
+            evidence=["A"],
+            evidence_card=[3],
+        )
         mle2 = MaximumLikelihoodEstimator(m, d)
-        self.assertEqual(mle2.estimate_cpd('B'), cpd_b)
+        self.assertEqual(mle2.estimate_cpd("B"), cpd_b)
 
     def test_state_names2(self):
-        m = BayesianModel([('Light?', 'Color'), ('Fruit', 'Color')])
-        d = pd.DataFrame(data={'Fruit': ['Apple', 'Apple', 'Apple', 'Banana', 'Banana'],
-                               'Light?': [True,   True,   False,   False,    True],
-                               'Color': ['red',   'green', 'black', 'black',  'yellow']})
-        color_cpd = TabularCPD('Color', 4, [[1, 0, 1, 0], [0, 0.5, 0, 0],
-                                            [0, 0.5, 0, 0], [0, 0, 0, 1]],
-                               evidence=['Fruit', 'Light?'], evidence_card=[2, 2])
+        m = BayesianModel([("Light?", "Color"), ("Fruit", "Color")])
+        d = pd.DataFrame(
+            data={
+                "Fruit": ["Apple", "Apple", "Apple", "Banana", "Banana"],
+                "Light?": [True, True, False, False, True],
+                "Color": ["red", "green", "black", "black", "yellow"],
+            }
+        )
+        color_cpd = TabularCPD(
+            "Color",
+            4,
+            [[1, 0, 1, 0], [0, 0.5, 0, 0], [0, 0.5, 0, 0], [0, 0, 0, 1]],
+            evidence=["Fruit", "Light?"],
+            evidence_card=[2, 2],
+        )
         mle2 = MaximumLikelihoodEstimator(m, d)
-        self.assertEqual(mle2.estimate_cpd('Color'), color_cpd)
+        self.assertEqual(mle2.estimate_cpd("Color"), color_cpd)
 
     def test_class_init(self):
-        mle = MaximumLikelihoodEstimator(self.m1, self.d1,
-                                         state_names={'A': [0, 1], 'B': [0, 1], 'C': [0, 1]})
+        mle = MaximumLikelihoodEstimator(
+            self.m1, self.d1, state_names={"A": [0, 1], "B": [0, 1], "C": [0, 1]}
+        )
         self.assertSetEqual(set(mle.get_parameters()), set(self.cpds))
 
     def test_nonoccurring_values(self):
-        mle = MaximumLikelihoodEstimator(self.m1, self.d1,
-                                         state_names={'A': [0, 1, 23], 'B': [0, 1], 'C': [0, 42, 1], 1: [2]})
-        cpds = [TabularCPD('A', 3, [[2.0/3], [1.0/3], [0]]),
-                TabularCPD('B', 2, [[2.0/3], [1.0/3]]),
-                TabularCPD('C', 3, [[0.0, 0.0, 1.0, 1.0/3, 1.0/3, 1.0/3],
-                                    [1.0, 1.0, 0.0, 1.0/3, 1.0/3, 1.0/3],
-                                    [0.0, 0.0, 0.0, 1.0/3, 1.0/3, 1.0/3]],
-                           evidence=['A', 'B'], evidence_card=[3, 2])]
+        mle = MaximumLikelihoodEstimator(
+            self.m1,
+            self.d1,
+            state_names={"A": [0, 1, 23], "B": [0, 1], "C": [0, 42, 1], 1: [2]},
+        )
+        cpds = [
+            TabularCPD("A", 3, [[2.0 / 3], [1.0 / 3], [0]]),
+            TabularCPD("B", 2, [[2.0 / 3], [1.0 / 3]]),
+            TabularCPD(
+                "C",
+                3,
+                [
+                    [0.0, 0.0, 1.0, 1.0 / 3, 1.0 / 3, 1.0 / 3],
+                    [1.0, 1.0, 0.0, 1.0 / 3, 1.0 / 3, 1.0 / 3],
+                    [0.0, 0.0, 0.0, 1.0 / 3, 1.0 / 3, 1.0 / 3],
+                ],
+                evidence=["A", "B"],
+                evidence_card=[3, 2],
+            ),
+        ]
         self.assertSetEqual(set(mle.get_parameters()), set(cpds))
 
     def test_missing_data(self):
-        e1 = MaximumLikelihoodEstimator(self.m1, self.d2, state_names={'C': [0, 1]}, complete_samples_only=False)
-        cpds1 = set([TabularCPD('A', 2, [[0.5], [0.5]]),
-                     TabularCPD('B', 2, [[2./3], [1./3]]),
-                     TabularCPD('C', 2, [[0, 0.5, 0.5, 0.5], [1, 0.5, 0.5, 0.5]],
-                                evidence=['A', 'B'], evidence_card=[2, 2])])
+        e1 = MaximumLikelihoodEstimator(
+            self.m1, self.d2, state_names={"C": [0, 1]}, complete_samples_only=False
+        )
+        cpds1 = set(
+            [
+                TabularCPD("A", 2, [[0.5], [0.5]]),
+                TabularCPD("B", 2, [[2.0 / 3], [1.0 / 3]]),
+                TabularCPD(
+                    "C",
+                    2,
+                    [[0, 0.5, 0.5, 0.5], [1, 0.5, 0.5, 0.5]],
+                    evidence=["A", "B"],
+                    evidence_card=[2, 2],
+                ),
+            ]
+        )
         self.assertSetEqual(cpds1, set(e1.get_parameters()))
 
-        e2 = MaximumLikelihoodEstimator(self.m1, self.d2, state_names={'C': [0, 1]}, complete_samples_only=True)
-        cpds2 = set([TabularCPD('A', 2, [[0.5], [0.5]]),
-                     TabularCPD('B', 2, [[0.5], [0.5]]),
-                     TabularCPD('C', 2, [[0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5]],
-                                evidence=['A', 'B'], evidence_card=[2, 2])])
+        e2 = MaximumLikelihoodEstimator(
+            self.m1, self.d2, state_names={"C": [0, 1]}, complete_samples_only=True
+        )
+        cpds2 = set(
+            [
+                TabularCPD("A", 2, [[0.5], [0.5]]),
+                TabularCPD("B", 2, [[0.5], [0.5]]),
+                TabularCPD(
+                    "C",
+                    2,
+                    [[0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5]],
+                    evidence=["A", "B"],
+                    evidence_card=[2, 2],
+                ),
+            ]
+        )
         self.assertSetEqual(cpds2, set(e2.get_parameters()))
 
     def tearDown(self):
         del self.m1
         del self.d1
         del self.d2
```

### Comparing `pgmpy-0.1.7/pgmpy/tests/test_estimators/test_BdeuScore.py` & `pgmpy-0.1.9/pgmpy/tests/test_estimators/test_BicScore.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,34 +1,38 @@
 import unittest
 
 import pandas as pd
 
 from pgmpy.models import BayesianModel
-from pgmpy.estimators import BdeuScore
+from pgmpy.estimators import BicScore
 
 
-class TestBdeuScore(unittest.TestCase):
+class TestBicScore(unittest.TestCase):
     def setUp(self):
-        self.d1 = pd.DataFrame(data={'A': [0, 0, 1], 'B': [0, 1, 0], 'C': [1, 1, 0], 'D': ['X', 'Y', 'Z']})
-        self.m1 = BayesianModel([('A', 'C'), ('B', 'C'), ('D', 'B')])
-        self.m2 = BayesianModel([('C', 'A'), ('C', 'B'), ('A', 'D')])
+        self.d1 = pd.DataFrame(
+            data={"A": [0, 0, 1], "B": [0, 1, 0], "C": [1, 1, 0], "D": ["X", "Y", "Z"]}
+        )
+        self.m1 = BayesianModel([("A", "C"), ("B", "C"), ("D", "B")])
+        self.m2 = BayesianModel([("C", "A"), ("C", "B"), ("A", "D")])
 
         # data_link - "https://www.kaggle.com/c/titanic/download/train.csv"
-        self.titanic_data = pd.read_csv('pgmpy/tests/test_estimators/testdata/titanic_train.csv')
+        self.titanic_data = pd.read_csv(
+            "pgmpy/tests/test_estimators/testdata/titanic_train.csv"
+        )
         self.titanic_data2 = self.titanic_data[["Survived", "Sex", "Pclass"]]
 
     def test_score(self):
-        self.assertAlmostEqual(BdeuScore(self.d1).score(self.m1), -9.907103407446435)
-        self.assertEqual(BdeuScore(self.d1).score(BayesianModel()), 0)
+        self.assertAlmostEqual(BicScore(self.d1).score(self.m1), -10.698440814229318)
+        self.assertEqual(BicScore(self.d1).score(BayesianModel()), 0)
 
     def test_score_titanic(self):
-        scorer = BdeuScore(self.titanic_data2, equivalent_sample_size=25)
+        scorer = BicScore(self.titanic_data2)
         titanic = BayesianModel([("Sex", "Survived"), ("Pclass", "Survived")])
-        self.assertAlmostEqual(scorer.score(titanic),  -1892.7383393910427)
-        titanic2 = BayesianModel([("Pclass", "Sex"), ])
+        self.assertAlmostEqual(scorer.score(titanic), -1896.7250012840179)
+        titanic2 = BayesianModel([("Pclass", "Sex")])
         titanic2.add_nodes_from(["Sex", "Survived", "Pclass"])
         self.assertLess(scorer.score(titanic2), scorer.score(titanic))
 
     def tearDown(self):
         del self.d1
         del self.m1
         del self.m2
```

### Comparing `pgmpy-0.1.7/pgmpy/tests/test_estimators/test_ParameterEstimator.py` & `pgmpy-0.1.9/pgmpy/tests/test_estimators/test_ParameterEstimator.py`

 * *Files 25% similar despite different names*

```diff
@@ -5,29 +5,47 @@
 
 from pgmpy.models import BayesianModel
 from pgmpy.estimators import ParameterEstimator
 
 
 class TestParameterEstimator(unittest.TestCase):
     def setUp(self):
-        self.m1 = BayesianModel([('A', 'C'), ('B', 'C'), ('D', 'B')])
-        self.d1 = DataFrame(data={'A': [0, 0, 1], 'B': [0, 1, 0], 'C': [1, 1, 0], 'D': ['X', 'Y', 'Z']})
-        self.d2 = DataFrame(data={'A': [0, NaN, 1], 'B': [0, 1, 0], 'C': [1, 1, NaN], 'D': [NaN, 'Y', NaN]})
+        self.m1 = BayesianModel([("A", "C"), ("B", "C"), ("D", "B")])
+        self.d1 = DataFrame(
+            data={"A": [0, 0, 1], "B": [0, 1, 0], "C": [1, 1, 0], "D": ["X", "Y", "Z"]}
+        )
+        self.d2 = DataFrame(
+            data={
+                "A": [0, NaN, 1],
+                "B": [0, 1, 0],
+                "C": [1, 1, NaN],
+                "D": [NaN, "Y", NaN],
+            }
+        )
 
     def test_state_count(self):
         e = ParameterEstimator(self.m1, self.d1)
-        self.assertEqual(e.state_counts('A').values.tolist(), [[2], [1]])
-        self.assertEqual(e.state_counts('C').values.tolist(),
-                         [[0., 0., 1., 0.], [1., 1., 0., 0.]])
+        self.assertEqual(e.state_counts("A").values.tolist(), [[2], [1]])
+        self.assertEqual(
+            e.state_counts("C").values.tolist(),
+            [[0.0, 0.0, 1.0, 0.0], [1.0, 1.0, 0.0, 0.0]],
+        )
 
     def test_missing_data(self):
-        e = ParameterEstimator(self.m1, self.d2, state_names={'C': [0, 1]}, complete_samples_only=False)
-        self.assertEqual(e.state_counts('A', complete_samples_only=True).values.tolist(), [[0], [0]])
-        self.assertEqual(e.state_counts('A').values.tolist(), [[1], [1]])
-        self.assertEqual(e.state_counts('C', complete_samples_only=True).values.tolist(),
-                         [[0, 0, 0, 0], [0, 0, 0, 0]])
-        self.assertEqual(e.state_counts('C').values.tolist(),
-                         [[0, 0, 0, 0], [1, 0, 0, 0]])
+        e = ParameterEstimator(
+            self.m1, self.d2, state_names={"C": [0, 1]}, complete_samples_only=False
+        )
+        self.assertEqual(
+            e.state_counts("A", complete_samples_only=True).values.tolist(), [[0], [0]]
+        )
+        self.assertEqual(e.state_counts("A").values.tolist(), [[1], [1]])
+        self.assertEqual(
+            e.state_counts("C", complete_samples_only=True).values.tolist(),
+            [[0, 0, 0, 0], [0, 0, 0, 0]],
+        )
+        self.assertEqual(
+            e.state_counts("C").values.tolist(), [[0, 0, 0, 0], [1, 0, 0, 0]]
+        )
 
     def tearDown(self):
         del self.m1
         del self.d1
```

### Comparing `pgmpy-0.1.7/pgmpy/tests/test_estimators/test_BicScore.py` & `pgmpy-0.1.9/pgmpy/tests/test_estimators/test_BdeuScore.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,34 +1,38 @@
 import unittest
 
 import pandas as pd
 
 from pgmpy.models import BayesianModel
-from pgmpy.estimators import BicScore
+from pgmpy.estimators import BdeuScore
 
 
-class TestBicScore(unittest.TestCase):
+class TestBdeuScore(unittest.TestCase):
     def setUp(self):
-        self.d1 = pd.DataFrame(data={'A': [0, 0, 1], 'B': [0, 1, 0], 'C': [1, 1, 0], 'D': ['X', 'Y', 'Z']})
-        self.m1 = BayesianModel([('A', 'C'), ('B', 'C'), ('D', 'B')])
-        self.m2 = BayesianModel([('C', 'A'), ('C', 'B'), ('A', 'D')])
+        self.d1 = pd.DataFrame(
+            data={"A": [0, 0, 1], "B": [0, 1, 0], "C": [1, 1, 0], "D": ["X", "Y", "Z"]}
+        )
+        self.m1 = BayesianModel([("A", "C"), ("B", "C"), ("D", "B")])
+        self.m2 = BayesianModel([("C", "A"), ("C", "B"), ("A", "D")])
 
         # data_link - "https://www.kaggle.com/c/titanic/download/train.csv"
-        self.titanic_data = pd.read_csv('pgmpy/tests/test_estimators/testdata/titanic_train.csv')
+        self.titanic_data = pd.read_csv(
+            "pgmpy/tests/test_estimators/testdata/titanic_train.csv"
+        )
         self.titanic_data2 = self.titanic_data[["Survived", "Sex", "Pclass"]]
 
     def test_score(self):
-        self.assertAlmostEqual(BicScore(self.d1).score(self.m1), -10.698440814229318)
-        self.assertEqual(BicScore(self.d1).score(BayesianModel()), 0)
+        self.assertAlmostEqual(BdeuScore(self.d1).score(self.m1), -9.907103407446435)
+        self.assertEqual(BdeuScore(self.d1).score(BayesianModel()), 0)
 
     def test_score_titanic(self):
-        scorer = BicScore(self.titanic_data2)
+        scorer = BdeuScore(self.titanic_data2, equivalent_sample_size=25)
         titanic = BayesianModel([("Sex", "Survived"), ("Pclass", "Survived")])
-        self.assertAlmostEqual(scorer.score(titanic), -1896.7250012840179)
-        titanic2 = BayesianModel([("Pclass", "Sex"), ])
+        self.assertAlmostEqual(scorer.score(titanic), -1892.7383393910427)
+        titanic2 = BayesianModel([("Pclass", "Sex")])
         titanic2.add_nodes_from(["Sex", "Survived", "Pclass"])
         self.assertLess(scorer.score(titanic2), scorer.score(titanic))
 
     def tearDown(self):
         del self.d1
         del self.m1
         del self.m2
```

### Comparing `pgmpy-0.1.7/pgmpy/tests/test_readwrite/test_PomdpX.py` & `pgmpy-0.1.9/pgmpy/tests/test_readwrite/test_PomdpX.py`

 * *Files 26% similar despite different names*

```diff
@@ -9,18 +9,20 @@
 
 try:
     from lxml import etree
 except ImportError:
     try:
         # Python 2.5
         import xml.etree.cElementTree as etree
+
         six.print_("running with cElementTree on Python 2.5+")
     except ImportError:
         try:
             import xml.etree.ElementTree as etree
+
             print("running with ElementTree on Python 2.5+")
         except ImportError:
             warnings.warn("Failed to import ElementTree from any known place")
 
 
 class TestPomdpXReaderString(unittest.TestCase):
     def setUp(self):
@@ -202,135 +204,150 @@
     </RewardFunction>
  </pomdpx>
  """
         self.reader_string = PomdpXReader(string=string)
         self.reader_file = PomdpXReader(path=six.StringIO(string))
 
     def test_get_variables(self):
-        var_expected = {'StateVar': [
-                        {'vnamePrev': 'rover_0',
-                         'vnameCurr': 'rover_1',
-                         'ValueEnum': ['s0', 's1', 's2'],
-                         'fullyObs': True},
-                        {'vnamePrev': 'rock_0',
-                         'vnameCurr': 'rock_1',
-                         'fullyObs': False,
-                         'ValueEnum': ['good', 'bad']}],
-                        'ObsVar': [{'vname': 'obs_sensor',
-                                    'ValueEnum': ['ogood', 'obad']}],
-                        'RewardVar': [{'vname': 'reward_rover'}],
-                        'ActionVar': [{'vname': 'action_rover',
-                                       'ValueEnum': ['amw', 'ame',
-                                                     'ac', 'as']}]
-                        }
+        var_expected = {
+            "StateVar": [
+                {
+                    "vnamePrev": "rover_0",
+                    "vnameCurr": "rover_1",
+                    "ValueEnum": ["s0", "s1", "s2"],
+                    "fullyObs": True,
+                },
+                {
+                    "vnamePrev": "rock_0",
+                    "vnameCurr": "rock_1",
+                    "fullyObs": False,
+                    "ValueEnum": ["good", "bad"],
+                },
+            ],
+            "ObsVar": [{"vname": "obs_sensor", "ValueEnum": ["ogood", "obad"]}],
+            "RewardVar": [{"vname": "reward_rover"}],
+            "ActionVar": [
+                {"vname": "action_rover", "ValueEnum": ["amw", "ame", "ac", "as"]}
+            ],
+        }
         self.maxDiff = None
         self.assertEqual(self.reader_string.get_variables(), var_expected)
         self.assertEqual(self.reader_file.get_variables(), var_expected)
 
     def test_get_initial_belief_system(self):
-        belief_expected = [{'Var': 'rover_0',
-                            'Parent': ['null'],
-                            'Type': 'TBL',
-                            'Parameter': [{'Instance': ['-'],
-                                           'ProbTable': ['0.0', '1.0', '0.0']}]
-                            },
-                           {'Var': 'rock_0',
-                            'Parent': ['null'],
-                            'Type': 'TBL',
-                            'Parameter': [{'Instance': ['-'],
-                                           'ProbTable': ['uniform']}]
-                            }]
+        belief_expected = [
+            {
+                "Var": "rover_0",
+                "Parent": ["null"],
+                "Type": "TBL",
+                "Parameter": [{"Instance": ["-"], "ProbTable": ["0.0", "1.0", "0.0"]}],
+            },
+            {
+                "Var": "rock_0",
+                "Parent": ["null"],
+                "Type": "TBL",
+                "Parameter": [{"Instance": ["-"], "ProbTable": ["uniform"]}],
+            },
+        ]
         self.maxDiff = None
         self.assertEqual(self.reader_string.get_initial_beliefs(), belief_expected)
         self.assertEqual(self.reader_file.get_initial_beliefs(), belief_expected)
 
     def test_get_state_transition_function(self):
-        state_transition_function_expected = \
-            [{'Var': 'rover_1',
-              'Parent': ['action_rover', 'rover_0'],
-              'Type': 'TBL',
-              'Parameter': [{'Instance': ['amw', 's0', 's2'],
-                             'ProbTable': ['1.0']},
-                            {'Instance': ['amw', 's1', 's0'],
-                             'ProbTable': ['1.0']},
-                            {'Instance': ['ame', 's0', 's1'],
-                             'ProbTable': ['1.0']},
-                            {'Instance': ['ame', 's1', 's2'],
-                             'ProbTable': ['1.0']},
-                            {'Instance': ['ac', 's0', 's0'],
-                             'ProbTable': ['1.0']},
-                            {'Instance': ['ac', 's1', 's1'],
-                             'ProbTable': ['1.0']},
-                            {'Instance': ['as', 's0', 's0'],
-                             'ProbTable': ['1.0']},
-                            {'Instance': ['as', 's1', 's2'],
-                             'ProbTable': ['1.0']},
-                            {'Instance': ['*', 's2', 's2'],
-                             'ProbTable': ['1.0']}]},
-             {'Var': 'rock_1',
-              'Parent': ['action_rover', 'rover_0', 'rock_0'],
-              'Type': 'TBL',
-              'Parameter': [{'Instance': ['amw', '*', '-', '-'],
-                             'ProbTable': ['1.0', '0.0', '0.0', '1.0']},
-                            {'Instance': ['ame', '*', '-', '-'],
-                             'ProbTable': ['identity']},
-                            {'Instance': ['ac', '*', '-', '-'],
-                             'ProbTable': ['identity']},
-                            {'Instance': ['as', '*', '-', '-'],
-                             'ProbTable': ['identity']},
-                            {'Instance': ['as', 's0', '*', '-'],
-                             'ProbTable': ['0.0', '1.0']},
-                            ]}]
+        state_transition_function_expected = [
+            {
+                "Var": "rover_1",
+                "Parent": ["action_rover", "rover_0"],
+                "Type": "TBL",
+                "Parameter": [
+                    {"Instance": ["amw", "s0", "s2"], "ProbTable": ["1.0"]},
+                    {"Instance": ["amw", "s1", "s0"], "ProbTable": ["1.0"]},
+                    {"Instance": ["ame", "s0", "s1"], "ProbTable": ["1.0"]},
+                    {"Instance": ["ame", "s1", "s2"], "ProbTable": ["1.0"]},
+                    {"Instance": ["ac", "s0", "s0"], "ProbTable": ["1.0"]},
+                    {"Instance": ["ac", "s1", "s1"], "ProbTable": ["1.0"]},
+                    {"Instance": ["as", "s0", "s0"], "ProbTable": ["1.0"]},
+                    {"Instance": ["as", "s1", "s2"], "ProbTable": ["1.0"]},
+                    {"Instance": ["*", "s2", "s2"], "ProbTable": ["1.0"]},
+                ],
+            },
+            {
+                "Var": "rock_1",
+                "Parent": ["action_rover", "rover_0", "rock_0"],
+                "Type": "TBL",
+                "Parameter": [
+                    {
+                        "Instance": ["amw", "*", "-", "-"],
+                        "ProbTable": ["1.0", "0.0", "0.0", "1.0"],
+                    },
+                    {"Instance": ["ame", "*", "-", "-"], "ProbTable": ["identity"]},
+                    {"Instance": ["ac", "*", "-", "-"], "ProbTable": ["identity"]},
+                    {"Instance": ["as", "*", "-", "-"], "ProbTable": ["identity"]},
+                    {"Instance": ["as", "s0", "*", "-"], "ProbTable": ["0.0", "1.0"]},
+                ],
+            },
+        ]
         self.maxDiff = None
-        self.assertEqual(self.reader_string.get_state_transition_function(),
-                         state_transition_function_expected)
-        self.assertEqual(self.reader_file.get_state_transition_function(),
-                         state_transition_function_expected)
+        self.assertEqual(
+            self.reader_string.get_state_transition_function(),
+            state_transition_function_expected,
+        )
+        self.assertEqual(
+            self.reader_file.get_state_transition_function(),
+            state_transition_function_expected,
+        )
 
     def test_obs_function(self):
-        obs_function_expected = \
-            [{'Var': 'obs_sensor',
-              'Parent': ['action_rover', 'rover_1', 'rock_1'],
-              'Type': 'TBL',
-              'Parameter': [{'Instance': ['amw', '*', '*', '-'],
-                             'ProbTable': ['1.0', '0.0']},
-                            {'Instance': ['ame', '*', '*', '-'],
-                             'ProbTable': ['1.0', '0.0']},
-                            {'Instance': ['as', '*', '*', '-'],
-                             'ProbTable': ['1.0', '0.0']},
-                            {'Instance': ['ac', 's0', '-', '-'],
-                             'ProbTable': ['1.0', '0.0', '0.0', '1.0']},
-                            {'Instance': ['ac', 's1', '-', '-'],
-                             'ProbTable': ['0.8', '0.2', '0.2', '0.8']},
-                            {'Instance': ['ac', 's2', '*', '-'],
-                             'ProbTable': ['1.0', '0.0']}]}]
+        obs_function_expected = [
+            {
+                "Var": "obs_sensor",
+                "Parent": ["action_rover", "rover_1", "rock_1"],
+                "Type": "TBL",
+                "Parameter": [
+                    {"Instance": ["amw", "*", "*", "-"], "ProbTable": ["1.0", "0.0"]},
+                    {"Instance": ["ame", "*", "*", "-"], "ProbTable": ["1.0", "0.0"]},
+                    {"Instance": ["as", "*", "*", "-"], "ProbTable": ["1.0", "0.0"]},
+                    {
+                        "Instance": ["ac", "s0", "-", "-"],
+                        "ProbTable": ["1.0", "0.0", "0.0", "1.0"],
+                    },
+                    {
+                        "Instance": ["ac", "s1", "-", "-"],
+                        "ProbTable": ["0.8", "0.2", "0.2", "0.8"],
+                    },
+                    {"Instance": ["ac", "s2", "*", "-"], "ProbTable": ["1.0", "0.0"]},
+                ],
+            }
+        ]
         self.maxDiff = None
         self.assertEqual(self.reader_string.get_obs_function(), obs_function_expected)
         self.assertEqual(self.reader_file.get_obs_function(), obs_function_expected)
 
     def test_reward_function(self):
-        reward_function_expected = \
-            [{'Var': 'reward_rover',
-              'Parent': ['action_rover', 'rover_0', 'rock_0'],
-              'Type': 'TBL',
-              'Parameter': [{'Instance': ['ame', 's1', '*'],
-                             'ValueTable': ['10']},
-                            {'Instance': ['amw', 's0', '*'],
-                             'ValueTable': ['-100']},
-                            {'Instance': ['as', 's1', '*'],
-                             'ValueTable': ['-100']},
-                            {'Instance': ['as', 's0', 'good'],
-                             'ValueTable': ['10']},
-                            {'Instance': ['as', 's0', 'bad'],
-                             'ValueTable': ['-10']}]}]
+        reward_function_expected = [
+            {
+                "Var": "reward_rover",
+                "Parent": ["action_rover", "rover_0", "rock_0"],
+                "Type": "TBL",
+                "Parameter": [
+                    {"Instance": ["ame", "s1", "*"], "ValueTable": ["10"]},
+                    {"Instance": ["amw", "s0", "*"], "ValueTable": ["-100"]},
+                    {"Instance": ["as", "s1", "*"], "ValueTable": ["-100"]},
+                    {"Instance": ["as", "s0", "good"], "ValueTable": ["10"]},
+                    {"Instance": ["as", "s0", "bad"], "ValueTable": ["-10"]},
+                ],
+            }
+        ]
         self.maxDiff = None
-        self.assertEqual(self.reader_string.get_reward_function(),
-                         reward_function_expected)
-        self.assertEqual(self.reader_file.get_reward_function(),
-                         reward_function_expected)
+        self.assertEqual(
+            self.reader_string.get_reward_function(), reward_function_expected
+        )
+        self.assertEqual(
+            self.reader_file.get_reward_function(), reward_function_expected
+        )
 
     def test_get_parameter_dd(self):
         string = """
         <pomdpx version="1.0" id="rockSample"
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
       xsi:noNamespaceSchemaLocation="pomdpx.xsd">
          <Description>RockSample problem for map size 1 x 3.
@@ -362,27 +379,33 @@
   </Parameter>
   </CondProb>
   </InitialStateBelief>
   </pomdpx>
  """
         self.reader_string = PomdpXReader(string=string)
         self.reader_file = PomdpXReader(path=six.StringIO(string))
-        expected_dd_parameter = [{
-            'Var': 'rover_0',
-            'Parent': ['null'],
-            'Type': 'DD',
-            'Parameter': {'rover_0': {'s0': '0.0',
-                                      's1': {'rock_0': {'good': '0.5',
-                                                        'bad': '0.5'}},
-                                      's2': '0.0'}}}]
+        expected_dd_parameter = [
+            {
+                "Var": "rover_0",
+                "Parent": ["null"],
+                "Type": "DD",
+                "Parameter": {
+                    "rover_0": {
+                        "s0": "0.0",
+                        "s1": {"rock_0": {"good": "0.5", "bad": "0.5"}},
+                        "s2": "0.0",
+                    }
+                },
+            }
+        ]
         self.maxDiff = None
-        self.assertEqual(expected_dd_parameter,
-                         self.reader_string.get_initial_beliefs())
-        self.assertEqual(expected_dd_parameter,
-                         self.reader_file.get_initial_beliefs())
+        self.assertEqual(
+            expected_dd_parameter, self.reader_string.get_initial_beliefs()
+        )
+        self.assertEqual(expected_dd_parameter, self.reader_file.get_initial_beliefs())
 
     def test_initial_belief_dd(self):
         string = """
     <pomdpx version="1.0" id="rockSample"
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
       xsi:noNamespaceSchemaLocation="pomdpx.xsd">
          <Description>RockSample problem for map size 1 x 3.
@@ -411,27 +434,31 @@
             </Parameter>
         </CondProb>
     </InitialStateBelief>
     </pomdpx>
     """
         self.reader_string = PomdpXReader(string=string)
         self.reader_file = PomdpXReader(path=six.StringIO(string))
-        expected_belief_dd = [{
-            'Var': 'rover_0',
-            'Parent': ['null'],
-            'Type': 'DD',
-            'Parameter': {'rover_0': {'s0': '0.0',
-                                      's1': {'type': 'uniform',
-                                             'var': 'rock_0'},
-                                      's2': '0.0'}}}]
+        expected_belief_dd = [
+            {
+                "Var": "rover_0",
+                "Parent": ["null"],
+                "Type": "DD",
+                "Parameter": {
+                    "rover_0": {
+                        "s0": "0.0",
+                        "s1": {"type": "uniform", "var": "rock_0"},
+                        "s2": "0.0",
+                    }
+                },
+            }
+        ]
         self.maxDiff = None
-        self.assertEqual(self.reader_string.get_initial_beliefs(),
-                         expected_belief_dd)
-        self.assertEqual(self.reader_file.get_initial_beliefs(),
-                         expected_belief_dd)
+        self.assertEqual(self.reader_string.get_initial_beliefs(), expected_belief_dd)
+        self.assertEqual(self.reader_file.get_initial_beliefs(), expected_belief_dd)
 
     def test_reward_function(self):
         string = """
         <pomdpx version="1.0" id="rockSample"
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
       xsi:noNamespaceSchemaLocation="pomdpx.xsd">
          <Description>RockSample problem for map size 1 x 3.
@@ -500,34 +527,42 @@
             </Parameter>
         </Func>
     </RewardFunction>
     </pomdpx>
         """
         self.reader_string = PomdpXReader(string=string)
         self.reader_file = PomdpXReader(path=six.StringIO(string))
-        expected_reward_function_dd =\
-            [{'Var': 'reward_rover',
-              'Parent': ['action_rover', 'rover_0', 'rock_0'],
-              'Type': 'DD',
-              'Parameter': {'action_rover': {'amw': {'rover_0': {'s0': '-100.0',
-                                                                 's1': '0.0',
-                                                                 's2': '0.0'}},
-                                             'ame': {'rover_0': {'s0': '0.0',
-                                                                 's1': '10.0',
-                                                                 's2': '0.0'}},
-                                             'ac': '0.0',
-                                             'as': {'rover_0': {'s0': {'rock_0': {'good': '10',
-                                                                                  'bad': '-10'}},
-                                                                's1': '-100',
-                                                                's2': '-100'}}}}}]
+        expected_reward_function_dd = [
+            {
+                "Var": "reward_rover",
+                "Parent": ["action_rover", "rover_0", "rock_0"],
+                "Type": "DD",
+                "Parameter": {
+                    "action_rover": {
+                        "amw": {"rover_0": {"s0": "-100.0", "s1": "0.0", "s2": "0.0"}},
+                        "ame": {"rover_0": {"s0": "0.0", "s1": "10.0", "s2": "0.0"}},
+                        "ac": "0.0",
+                        "as": {
+                            "rover_0": {
+                                "s0": {"rock_0": {"good": "10", "bad": "-10"}},
+                                "s1": "-100",
+                                "s2": "-100",
+                            }
+                        },
+                    }
+                },
+            }
+        ]
         self.maxDiff = None
-        self.assertEqual(self.reader_string.get_reward_function(),
-                         expected_reward_function_dd)
-        self.assertEqual(self.reader_file.get_reward_function(),
-                         expected_reward_function_dd)
+        self.assertEqual(
+            self.reader_string.get_reward_function(), expected_reward_function_dd
+        )
+        self.assertEqual(
+            self.reader_file.get_reward_function(), expected_reward_function_dd
+        )
 
     def test_state_transition_function(self):
         string = """
          <pomdpx version="1.0" id="rockSample"
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
       xsi:noNamespaceSchemaLocation="pomdpx.xsd">
          <Description>RockSample problem for map size 1 x 3.
@@ -621,69 +656,115 @@
             </Parameter>
         </CondProb>
     </StateTransitionFunction>
 </pomdpx>
         """
         self.reader_string = PomdpXReader(string=string)
         self.reader_file = PomdpXReader(path=six.StringIO(string))
-        expected_state_transition_function = \
-            [{'Var': 'rover_1',
-              'Parent': ['action_rover', 'rover_0'],
-              'Type': 'DD',
-              'Parameter': {'action_rover': {'amw': {'rover_0': {'s0': {'type': 'deterministic',
-                                                                        'var': 'rover_1',
-                                                                        'val': 's2'},
-                                                                 's1': {'type': 'deterministic',
-                                                                        'var': 'rover_1',
-                                                                        'val': 's0'},
-                                                                 's2': {'type': 'deterministic',
-                                                                        'var': 'rover_1',
-                                                                        'val': 's2'}}},
-                                             'ame': {'rover_0': {'s0': {'type': 'deterministic',
-                                                                        'var': 'rover_1',
-                                                                        'val': 's1'},
-                                                                 's1': {'type': 'deterministic',
-                                                                        'var': 'rover_1',
-                                                                        'val': 's2'},
-                                                                 's2': {'type': 'deterministic',
-                                                                        'var': 'rover_1',
-                                                                        'val': 's2'},
-                                                                 }},
-                                             'ac': {'type': 'persistent',
-                                                    'var': 'rover_1'},
-                                             'as': {'rover_0': {'s0': {'type': 'deterministic',
-                                                                       'var': 'rover_1',
-                                                                       'val': 's0'},
-                                                                's1': {'type': 'deterministic',
-                                                                       'var': 'rover_1',
-                                                                       'val': 's2'},
-                                                                's2': {'type': 'deterministic',
-                                                                       'var': 'rover_1',
-                                                                       'val': 's2'}}}}}},
-             {'Var': 'rock_1',
-              'Parent': ['action_rover', 'rover_0', 'rock_0'],
-              'Type': 'DD',
-              'Parameter': {'action_rover': {'amw': {'type': 'persistent',
-                                                     'var': 'rock_1'},
-                                             'ame': {'type': 'persistent',
-                                                     'var': 'rock_1'},
-                                             'ac': {'type': 'persistent',
-                                                    'var': 'rock_1'},
-                                             'as': {'rover_0': {'s0': {'type': 'deterministic',
-                                                                       'var': 'rock_1',
-                                                                       'val': 'bad'},
-                                                                's1': {'type': 'persistent',
-                                                                       'var': 'rock_1'},
-                                                                's2': {'type': 'persistent',
-                                                                       'var': 'rock_1'}}}}}}]
+        expected_state_transition_function = [
+            {
+                "Var": "rover_1",
+                "Parent": ["action_rover", "rover_0"],
+                "Type": "DD",
+                "Parameter": {
+                    "action_rover": {
+                        "amw": {
+                            "rover_0": {
+                                "s0": {
+                                    "type": "deterministic",
+                                    "var": "rover_1",
+                                    "val": "s2",
+                                },
+                                "s1": {
+                                    "type": "deterministic",
+                                    "var": "rover_1",
+                                    "val": "s0",
+                                },
+                                "s2": {
+                                    "type": "deterministic",
+                                    "var": "rover_1",
+                                    "val": "s2",
+                                },
+                            }
+                        },
+                        "ame": {
+                            "rover_0": {
+                                "s0": {
+                                    "type": "deterministic",
+                                    "var": "rover_1",
+                                    "val": "s1",
+                                },
+                                "s1": {
+                                    "type": "deterministic",
+                                    "var": "rover_1",
+                                    "val": "s2",
+                                },
+                                "s2": {
+                                    "type": "deterministic",
+                                    "var": "rover_1",
+                                    "val": "s2",
+                                },
+                            }
+                        },
+                        "ac": {"type": "persistent", "var": "rover_1"},
+                        "as": {
+                            "rover_0": {
+                                "s0": {
+                                    "type": "deterministic",
+                                    "var": "rover_1",
+                                    "val": "s0",
+                                },
+                                "s1": {
+                                    "type": "deterministic",
+                                    "var": "rover_1",
+                                    "val": "s2",
+                                },
+                                "s2": {
+                                    "type": "deterministic",
+                                    "var": "rover_1",
+                                    "val": "s2",
+                                },
+                            }
+                        },
+                    }
+                },
+            },
+            {
+                "Var": "rock_1",
+                "Parent": ["action_rover", "rover_0", "rock_0"],
+                "Type": "DD",
+                "Parameter": {
+                    "action_rover": {
+                        "amw": {"type": "persistent", "var": "rock_1"},
+                        "ame": {"type": "persistent", "var": "rock_1"},
+                        "ac": {"type": "persistent", "var": "rock_1"},
+                        "as": {
+                            "rover_0": {
+                                "s0": {
+                                    "type": "deterministic",
+                                    "var": "rock_1",
+                                    "val": "bad",
+                                },
+                                "s1": {"type": "persistent", "var": "rock_1"},
+                                "s2": {"type": "persistent", "var": "rock_1"},
+                            }
+                        },
+                    }
+                },
+            },
+        ]
         self.maxDiff = None
-        self.assertEqual(self.reader_string.get_state_transition_function(),
-                         expected_state_transition_function)
-        self.assertEqual(self.reader_file.get_state_transition_function(),
-                         expected_state_transition_function)
+        self.assertEqual(
+            self.reader_string.get_state_transition_function(),
+            expected_state_transition_function,
+        )
+        self.assertEqual(
+            self.reader_file.get_state_transition_function(),
+            expected_state_transition_function,
+        )
 
     def test_obs_function_dd(self):
         string = """
         <pomdpx version="1.0" id="rockSample"
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
       xsi:noNamespaceSchemaLocation="pomdpx.xsd">
          <Description>RockSample problem for map size 1 x 3.
@@ -756,169 +837,232 @@
             </Parameter>
         </CondProb>
     </ObsFunction>
     </pomdpx>
         """
         self.reader_string = PomdpXReader(string=string)
         self.reader_file = PomdpXReader(path=six.StringIO(string))
-        expected_obs_function = \
-            [{'Var': 'obs_sensor',
-              'Parent': ['action_rover', 'rover_1', 'rock_1'],
-              'Type': 'DD',
-              'Parameter': {'action_rover': {'amw': {'type': 'deterministic',
-                                                     'var': 'obs_sensor',
-                                                     'val': 'ogood'},
-                                             'ame': {'type': 'deterministic',
-                                                     'var': 'obs_sensor',
-                                                     'val': 'ogood'},
-                                             'ac': {'rover_1': {'s0': {'rock_1': {'good': {'type': 'deterministic',
-                                                                                           'var': 'obs_sensor',
-                                                                                           'val': 'ogood'},
-                                                                                  'bad': {'type': 'deterministic',
-                                                                                          'var': 'obs_sensor',
-                                                                                          'val': 'obad'}}},
-                                                                's1': {'type': 'template',
-                                                                       'idref': 'obs_rock'},
-                                                                's2': {'type': 'template',
-                                                                       'idref': 'obs_rock'}}},
-                                             'as': {'type': 'deterministic',
-                                                    'var': 'obs_sensor',
-                                                    'val': 'ogood'}},
-                            'SubDAGTemplate': {'rock_1': {'good': {'obs_sensor': {'ogood': '0.8',
-                                                                                  'obad': '0.2'}},
-                                                          'bad': {'obs_sensor': {'ogood': '0.2',
-                                                                                 'obad': '0.8'}}}},
-                            'id': 'obs_rock'}}]
+        expected_obs_function = [
+            {
+                "Var": "obs_sensor",
+                "Parent": ["action_rover", "rover_1", "rock_1"],
+                "Type": "DD",
+                "Parameter": {
+                    "action_rover": {
+                        "amw": {
+                            "type": "deterministic",
+                            "var": "obs_sensor",
+                            "val": "ogood",
+                        },
+                        "ame": {
+                            "type": "deterministic",
+                            "var": "obs_sensor",
+                            "val": "ogood",
+                        },
+                        "ac": {
+                            "rover_1": {
+                                "s0": {
+                                    "rock_1": {
+                                        "good": {
+                                            "type": "deterministic",
+                                            "var": "obs_sensor",
+                                            "val": "ogood",
+                                        },
+                                        "bad": {
+                                            "type": "deterministic",
+                                            "var": "obs_sensor",
+                                            "val": "obad",
+                                        },
+                                    }
+                                },
+                                "s1": {"type": "template", "idref": "obs_rock"},
+                                "s2": {"type": "template", "idref": "obs_rock"},
+                            }
+                        },
+                        "as": {
+                            "type": "deterministic",
+                            "var": "obs_sensor",
+                            "val": "ogood",
+                        },
+                    },
+                    "SubDAGTemplate": {
+                        "rock_1": {
+                            "good": {"obs_sensor": {"ogood": "0.8", "obad": "0.2"}},
+                            "bad": {"obs_sensor": {"ogood": "0.2", "obad": "0.8"}},
+                        }
+                    },
+                    "id": "obs_rock",
+                },
+            }
+        ]
         self.maxDiff = None
         self.assertEqual(self.reader_string.get_obs_function(), expected_obs_function)
         self.assertEqual(self.reader_file.get_obs_function(), expected_obs_function)
 
     def tearDown(self):
         del self.reader_file
         del self.reader_string
 
 
 class TestPomdpXWriter(unittest.TestCase):
     def setUp(self):
-        self.model_data = {'discription': '',
-                           'discount': '0.95',
-                           'variables': {
-                               'StateVar': [{'vnamePrev': 'rover_0',
-                                             'vnameCurr': 'rover_1',
-                                             'ValueEnum': ['s0', 's1', 's2'],
-                                             'fullyObs': True},
-                                            {'vnamePrev': 'rock_0',
-                                             'vnameCurr': 'rock_1',
-                                             'fullyObs': False,
-                                             'ValueEnum': ['good', 'bad']}],
-                               'ObsVar': [{'vname': 'obs_sensor',
-                                           'ValueEnum': ['ogood', 'obad']}],
-                               'RewardVar': [{'vname': 'reward_rover'}],
-                               'ActionVar': [{'vname': 'action_rover',
-                                              'ValueEnum': ['amw', 'ame',
-                                                            'ac', 'as']}]},
-                           'initial_state_belief': [{'Var': 'rover_0',
-                                                     'Parent': ['null'],
-                                                     'Type': 'TBL',
-                                                     'Parameter': [{'Instance': ['-'],
-                                                                    'ProbTable': ['0.0', '1.0', '0.0']}]},
-                                                    {'Var': 'rock_0',
-                                                     'Parent': ['null'],
-                                                     'Type': 'TBL',
-                                                     'Parameter': [{'Instance': ['-'],
-                                                                    'ProbTable': ['uniform']}]}],
-                           'state_transition_function': [{'Var': 'rover_1',
-                                                          'Parent': ['action_rover', 'rover_0'],
-                                                          'Type': 'TBL',
-                                                          'Parameter': [{'Instance': ['amw', 's0', 's2'],
-                                                                         'ProbTable': ['1.0']},
-                                                                        {'Instance': ['amw', 's1', 's0'],
-                                                                         'ProbTable': ['1.0']},
-                                                                        {'Instance': ['ame', 's0', 's1'],
-                                                                         'ProbTable': ['1.0']},
-                                                                        {'Instance': ['ame', 's1', 's2'],
-                                                                         'ProbTable': ['1.0']},
-                                                                        {'Instance': ['ac', 's0', 's0'],
-                                                                         'ProbTable': ['1.0']},
-                                                                        {'Instance': ['ac', 's1', 's1'],
-                                                                         'ProbTable': ['1.0']},
-                                                                        {'Instance': ['as', 's0', 's0'],
-                                                                         'ProbTable': ['1.0']},
-                                                                        {'Instance': ['as', 's1', 's2'],
-                                                                         'ProbTable': ['1.0']},
-                                                                        {'Instance': ['*', 's2', 's2'],
-                                                                         'ProbTable': ['1.0']}]},
-                                                         {'Var': 'rock_1',
-                                                          'Parent': ['action_rover', 'rover_0', 'rock_0'],
-                                                          'Type': 'TBL',
-                                                          'Parameter': [{'Instance': ['amw', '*', '-', '-'],
-                                                                         'ProbTable': ['1.0', '0.0', '0.0', '1.0']},
-                                                                        {'Instance': ['ame', '*', '-', '-'],
-                                                                         'ProbTable': ['identity']},
-                                                                        {'Instance': ['ac', '*', '-', '-'],
-                                                                         'ProbTable': ['identity']},
-                                                                        {'Instance': ['as', '*', '-', '-'],
-                                                                         'ProbTable': ['identity']},
-                                                                        {'Instance': ['as', 's0', '*', '-'],
-                                                                         'ProbTable': ['0.0', '1.0']},
-                                                                        ]}],
-                           'obs_function': [{'Var': 'obs_sensor',
-                                             'Parent': ['action_rover', 'rover_1', 'rock_1'],
-                                             'Type': 'TBL',
-                                             'Parameter': [{'Instance': ['amw', '*', '*', '-'],
-                                                            'ProbTable': ['1.0', '0.0']},
-                                                           {'Instance': ['ame', '*', '*', '-'],
-                                                            'ProbTable': ['1.0', '0.0']},
-                                                           {'Instance': ['as', '*', '*', '-'],
-                                                            'ProbTable': ['1.0', '0.0']},
-                                                           {'Instance': ['ac', 's0', '-', '-'],
-                                                            'ProbTable': ['1.0', '0.0', '0.0', '1.0']},
-                                                           {'Instance': ['ac', 's1', '-', '-'],
-                                                            'ProbTable': ['0.8', '0.2', '0.2', '0.8']},
-                                                           {'Instance': ['ac', 's2', '*', '-'],
-                                                            'ProbTable': ['1.0', '0.0']}]}],
-                           'reward_function': [{'Var': 'reward_rover',
-                                                'Parent': ['action_rover', 'rover_0', 'rock_0'],
-                                                'Type': 'TBL',
-                                                'Parameter': [{'Instance': ['ame', 's1', '*'],
-                                                               'ValueTable': ['10']},
-                                                              {'Instance': ['amw', 's0', '*'],
-                                                               'ValueTable': ['-100']},
-                                                              {'Instance': ['as', 's1', '*'],
-                                                               'ValueTable': ['-100']},
-                                                              {'Instance': ['as', 's0', 'good'],
-                                                               'ValueTable': ['10']},
-                                                              {'Instance': ['as', 's0', 'bad'],
-                                                               'ValueTable': ['-10']}]}]}
+        self.model_data = {
+            "discription": "",
+            "discount": "0.95",
+            "variables": {
+                "StateVar": [
+                    {
+                        "vnamePrev": "rover_0",
+                        "vnameCurr": "rover_1",
+                        "ValueEnum": ["s0", "s1", "s2"],
+                        "fullyObs": True,
+                    },
+                    {
+                        "vnamePrev": "rock_0",
+                        "vnameCurr": "rock_1",
+                        "fullyObs": False,
+                        "ValueEnum": ["good", "bad"],
+                    },
+                ],
+                "ObsVar": [{"vname": "obs_sensor", "ValueEnum": ["ogood", "obad"]}],
+                "RewardVar": [{"vname": "reward_rover"}],
+                "ActionVar": [
+                    {"vname": "action_rover", "ValueEnum": ["amw", "ame", "ac", "as"]}
+                ],
+            },
+            "initial_state_belief": [
+                {
+                    "Var": "rover_0",
+                    "Parent": ["null"],
+                    "Type": "TBL",
+                    "Parameter": [
+                        {"Instance": ["-"], "ProbTable": ["0.0", "1.0", "0.0"]}
+                    ],
+                },
+                {
+                    "Var": "rock_0",
+                    "Parent": ["null"],
+                    "Type": "TBL",
+                    "Parameter": [{"Instance": ["-"], "ProbTable": ["uniform"]}],
+                },
+            ],
+            "state_transition_function": [
+                {
+                    "Var": "rover_1",
+                    "Parent": ["action_rover", "rover_0"],
+                    "Type": "TBL",
+                    "Parameter": [
+                        {"Instance": ["amw", "s0", "s2"], "ProbTable": ["1.0"]},
+                        {"Instance": ["amw", "s1", "s0"], "ProbTable": ["1.0"]},
+                        {"Instance": ["ame", "s0", "s1"], "ProbTable": ["1.0"]},
+                        {"Instance": ["ame", "s1", "s2"], "ProbTable": ["1.0"]},
+                        {"Instance": ["ac", "s0", "s0"], "ProbTable": ["1.0"]},
+                        {"Instance": ["ac", "s1", "s1"], "ProbTable": ["1.0"]},
+                        {"Instance": ["as", "s0", "s0"], "ProbTable": ["1.0"]},
+                        {"Instance": ["as", "s1", "s2"], "ProbTable": ["1.0"]},
+                        {"Instance": ["*", "s2", "s2"], "ProbTable": ["1.0"]},
+                    ],
+                },
+                {
+                    "Var": "rock_1",
+                    "Parent": ["action_rover", "rover_0", "rock_0"],
+                    "Type": "TBL",
+                    "Parameter": [
+                        {
+                            "Instance": ["amw", "*", "-", "-"],
+                            "ProbTable": ["1.0", "0.0", "0.0", "1.0"],
+                        },
+                        {"Instance": ["ame", "*", "-", "-"], "ProbTable": ["identity"]},
+                        {"Instance": ["ac", "*", "-", "-"], "ProbTable": ["identity"]},
+                        {"Instance": ["as", "*", "-", "-"], "ProbTable": ["identity"]},
+                        {
+                            "Instance": ["as", "s0", "*", "-"],
+                            "ProbTable": ["0.0", "1.0"],
+                        },
+                    ],
+                },
+            ],
+            "obs_function": [
+                {
+                    "Var": "obs_sensor",
+                    "Parent": ["action_rover", "rover_1", "rock_1"],
+                    "Type": "TBL",
+                    "Parameter": [
+                        {
+                            "Instance": ["amw", "*", "*", "-"],
+                            "ProbTable": ["1.0", "0.0"],
+                        },
+                        {
+                            "Instance": ["ame", "*", "*", "-"],
+                            "ProbTable": ["1.0", "0.0"],
+                        },
+                        {
+                            "Instance": ["as", "*", "*", "-"],
+                            "ProbTable": ["1.0", "0.0"],
+                        },
+                        {
+                            "Instance": ["ac", "s0", "-", "-"],
+                            "ProbTable": ["1.0", "0.0", "0.0", "1.0"],
+                        },
+                        {
+                            "Instance": ["ac", "s1", "-", "-"],
+                            "ProbTable": ["0.8", "0.2", "0.2", "0.8"],
+                        },
+                        {
+                            "Instance": ["ac", "s2", "*", "-"],
+                            "ProbTable": ["1.0", "0.0"],
+                        },
+                    ],
+                }
+            ],
+            "reward_function": [
+                {
+                    "Var": "reward_rover",
+                    "Parent": ["action_rover", "rover_0", "rock_0"],
+                    "Type": "TBL",
+                    "Parameter": [
+                        {"Instance": ["ame", "s1", "*"], "ValueTable": ["10"]},
+                        {"Instance": ["amw", "s0", "*"], "ValueTable": ["-100"]},
+                        {"Instance": ["as", "s1", "*"], "ValueTable": ["-100"]},
+                        {"Instance": ["as", "s0", "good"], "ValueTable": ["10"]},
+                        {"Instance": ["as", "s0", "bad"], "ValueTable": ["-10"]},
+                    ],
+                }
+            ],
+        }
 
         self.writer = PomdpXWriter(model_data=self.model_data)
 
     def test_variables(self):
-        expected_variables = etree.XML("""
+        expected_variables = etree.XML(
+            """
 <Variable>
   <StateVar fullyObs="true" vnameCurr="rover_1" vnamePrev="rover_0">
     <NumValues>3</NumValues>
   </StateVar>
   <StateVar fullyObs="false" vnameCurr="rock_1" vnamePrev="rock_0">
     <ValueEnum>good bad</ValueEnum>
   </StateVar>
   <ObsVar vname="obs_sensor">
     <ValueEnum>ogood obad</ValueEnum>
   </ObsVar>
   <ActionVar vname="action_rover">
     <ValueEnum>amw ame ac as</ValueEnum>
   </ActionVar>
   <RewardVar vname="reward_rover" />
-</Variable>""")
+</Variable>"""
+        )
         self.maxDiff = None
-        self.assertEqual(self.writer.get_variables(),
-                         etree.tostring(expected_variables))
+        self.assertEqual(
+            self.writer.get_variables(), etree.tostring(expected_variables)
+        )
 
     def test_add_initial_belief(self):
-        expected_belief_xml = etree.XML("""
+        expected_belief_xml = etree.XML(
+            """
 <InitialStateBelief>
   <CondProb>
     <Var>rover_0</Var>
     <Parent>null</Parent>
     <Parameter type="TBL">
       <Entry>
         <Instance> - </Instance>
@@ -932,21 +1076,25 @@
     <Parameter type="TBL">
       <Entry>
         <Instance> - </Instance>
         <ProbTable>uniform</ProbTable>
       </Entry>
     </Parameter>
   </CondProb>
-</InitialStateBelief>""")
+</InitialStateBelief>"""
+        )
         self.maxDiff = None
-        self.assertEqual(str(self.writer.add_initial_belief()),
-                         str(etree.tostring(expected_belief_xml)))
+        self.assertEqual(
+            str(self.writer.add_initial_belief()),
+            str(etree.tostring(expected_belief_xml)),
+        )
 
     def test_add_transition_function(self):
-        expected_transition_xml = etree.XML("""
+        expected_transition_xml = etree.XML(
+            """
 <StateTransitionFunction>
   <CondProb>
     <Var>rover_1</Var>
     <Parent>action_rover rover_0</Parent>
     <Parameter type="TBL">
       <Entry>
         <Instance>amw s0 s2</Instance>
@@ -1008,21 +1156,25 @@
       </Entry>
       <Entry>
         <Instance>as s0 * - </Instance>
         <ProbTable>0.0 1.0</ProbTable>
       </Entry>
     </Parameter>
   </CondProb>
-</StateTransitionFunction>""")
+</StateTransitionFunction>"""
+        )
         self.maxDiff = None
-        self.assertEqual(self.writer.add_state_transition_function(),
-                         etree.tostring(expected_transition_xml))
+        self.assertEqual(
+            self.writer.add_state_transition_function(),
+            etree.tostring(expected_transition_xml),
+        )
 
     def test_add_obs_function(self):
-        expected_obs_xml = etree.XML("""
+        expected_obs_xml = etree.XML(
+            """
 <ObsFunction>
   <CondProb>
     <Var>obs_sensor</Var>
     <Parent>action_rover rover_1 rock_1</Parent>
     <Parameter type="TBL">
       <Entry>
         <Instance>amw * * - </Instance>
@@ -1046,21 +1198,24 @@
       </Entry>
       <Entry>
         <Instance>ac s2 * - </Instance>
         <ProbTable>1.0 0.0</ProbTable>
       </Entry>
     </Parameter>
   </CondProb>
-</ObsFunction>""")
+</ObsFunction>"""
+        )
         self.maxDiff = None
-        self.assertEqual(self.writer.add_obs_function(),
-                         etree.tostring(expected_obs_xml))
+        self.assertEqual(
+            self.writer.add_obs_function(), etree.tostring(expected_obs_xml)
+        )
 
     def test_add_reward_function(self):
-        expected_reward_xml = etree.XML("""
+        expected_reward_xml = etree.XML(
+            """
 <RewardFunction>
   <Func>
     <Var>reward_rover</Var>
     <Parent>action_rover rover_0 rock_0</Parent>
     <Parameter type="TBL">
       <Entry>
         <Instance>ame s1 * </Instance>
@@ -1080,30 +1235,41 @@
       </Entry>
       <Entry>
         <Instance>as s0 bad</Instance>
         <ValueTable>-10</ValueTable>
       </Entry>
     </Parameter>
   </Func>
-</RewardFunction>""")
+</RewardFunction>"""
+        )
         self.maxDiff = None
-        self.assertEqual(self.writer.add_reward_function(),
-                         etree.tostring(expected_reward_xml))
+        self.assertEqual(
+            self.writer.add_reward_function(), etree.tostring(expected_reward_xml)
+        )
 
     def test_initial_state_belief_dd(self):
-        self.model_data = {'initial_state_belief': [{
-            'Var': 'rover_0',
-            'Parent': ['null'],
-            'Type': 'DD',
-            'Parameter': {'rover_0': {'s0': '0.0',
-                                      's1': {'type': 'uniform',
-                                             'var': 'rock_0'},
-                                      's2': '0.0'}}}]}
+        self.model_data = {
+            "initial_state_belief": [
+                {
+                    "Var": "rover_0",
+                    "Parent": ["null"],
+                    "Type": "DD",
+                    "Parameter": {
+                        "rover_0": {
+                            "s0": "0.0",
+                            "s1": {"type": "uniform", "var": "rock_0"},
+                            "s2": "0.0",
+                        }
+                    },
+                }
+            ]
+        }
         self.writer = PomdpXWriter(model_data=self.model_data)
-        expected_xml = etree.XML("""
+        expected_xml = etree.XML(
+            """
 <InitialStateBelief>
   <CondProb>
     <Var>rover_0</Var>
     <Parent>null</Parent>
     <Parameter type="DD">
       <DAG>
         <Node var="rover_0">
@@ -1116,77 +1282,118 @@
           <Edge val="s2">
             <Terminal>0.0</Terminal>
           </Edge>
         </Node>
       </DAG>
     </Parameter>
   </CondProb>
-</InitialStateBelief>""")
+</InitialStateBelief>"""
+        )
         self.maxDiff = None
-        self.assertEqual(self.writer.add_initial_belief(),
-                         etree.tostring(expected_xml))
+        self.assertEqual(self.writer.add_initial_belief(), etree.tostring(expected_xml))
 
     def test_state_transition_function_dd(self):
         self.model_data = {
-            'state_transition_function': [{
-                'Var': 'rover_1',
-                'Parent': ['action_rover', 'rover_0'],
-                'Type': 'DD',
-                'Parameter': {'action_rover': {
-                    'amw': {'rover_0': {'s0': {
-                        'type': 'deterministic',
-                        'var': 'rover_1',
-                        'val': 's2'},
-                        's1': {'type': 'deterministic',
-                               'var': 'rover_1',
-                               'val': 's0'},
-                        's2': {'type': 'deterministic',
-                               'var': 'rover_1',
-                               'val': 's2'}}},
-                    'ame': {'rover_0': {'s0': {'type': 'deterministic',
-                                               'var': 'rover_1',
-                                               'val': 's1'},
-                                        's1': {'type': 'deterministic',
-                                               'var': 'rover_1',
-                                               'val': 's2'},
-                                        's2': {'type': 'deterministic',
-                                               'var': 'rover_1',
-                                               'val': 's2'},
-                                        }},
-                    'ac': {'type': 'persistent',
-                           'var': 'rover_1'},
-                    'as': {'rover_0': {'s0': {'type': 'deterministic',
-                                              'var': 'rover_1',
-                                              'val': 's0'},
-                                       's1': {'type': 'deterministic',
-                                              'var': 'rover_1',
-                                              'val': 's2'},
-                                       's2': {'type': 'deterministic',
-                                              'var': 'rover_1',
-                                              'val': 's2'}}}}}},
-                {'Var': 'rock_1',
-                 'Parent': ['action_rover', 'rover_0', 'rock_0'],
-                 'Type': 'DD',
-                 'Parameter': {'action_rover': {
-                     'amw': {'type': 'persistent',
-                             'var': 'rock_1'},
-                     'ame': {'type': 'persistent',
-                             'var': 'rock_1'},
-                     'ac': {'type': 'persistent',
-                            'var': 'rock_1'},
-                     'as': {'rover_0': {'s0': {'type': 'deterministic',
-                                               'var': 'rock_1',
-                                               'val': 'bad'},
-                                        's1': {'type': 'persistent',
-                                               'var': 'rock_1'},
-                                        's2': {'type': 'persistent',
-                                               'var': 'rock_1'}}}}}}]}
+            "state_transition_function": [
+                {
+                    "Var": "rover_1",
+                    "Parent": ["action_rover", "rover_0"],
+                    "Type": "DD",
+                    "Parameter": {
+                        "action_rover": {
+                            "amw": {
+                                "rover_0": {
+                                    "s0": {
+                                        "type": "deterministic",
+                                        "var": "rover_1",
+                                        "val": "s2",
+                                    },
+                                    "s1": {
+                                        "type": "deterministic",
+                                        "var": "rover_1",
+                                        "val": "s0",
+                                    },
+                                    "s2": {
+                                        "type": "deterministic",
+                                        "var": "rover_1",
+                                        "val": "s2",
+                                    },
+                                }
+                            },
+                            "ame": {
+                                "rover_0": {
+                                    "s0": {
+                                        "type": "deterministic",
+                                        "var": "rover_1",
+                                        "val": "s1",
+                                    },
+                                    "s1": {
+                                        "type": "deterministic",
+                                        "var": "rover_1",
+                                        "val": "s2",
+                                    },
+                                    "s2": {
+                                        "type": "deterministic",
+                                        "var": "rover_1",
+                                        "val": "s2",
+                                    },
+                                }
+                            },
+                            "ac": {"type": "persistent", "var": "rover_1"},
+                            "as": {
+                                "rover_0": {
+                                    "s0": {
+                                        "type": "deterministic",
+                                        "var": "rover_1",
+                                        "val": "s0",
+                                    },
+                                    "s1": {
+                                        "type": "deterministic",
+                                        "var": "rover_1",
+                                        "val": "s2",
+                                    },
+                                    "s2": {
+                                        "type": "deterministic",
+                                        "var": "rover_1",
+                                        "val": "s2",
+                                    },
+                                }
+                            },
+                        }
+                    },
+                },
+                {
+                    "Var": "rock_1",
+                    "Parent": ["action_rover", "rover_0", "rock_0"],
+                    "Type": "DD",
+                    "Parameter": {
+                        "action_rover": {
+                            "amw": {"type": "persistent", "var": "rock_1"},
+                            "ame": {"type": "persistent", "var": "rock_1"},
+                            "ac": {"type": "persistent", "var": "rock_1"},
+                            "as": {
+                                "rover_0": {
+                                    "s0": {
+                                        "type": "deterministic",
+                                        "var": "rock_1",
+                                        "val": "bad",
+                                    },
+                                    "s1": {"type": "persistent", "var": "rock_1"},
+                                    "s2": {"type": "persistent", "var": "rock_1"},
+                                }
+                            },
+                        }
+                    },
+                },
+            ]
+        }
 
         self.writer = PomdpXWriter(model_data=self.model_data)
-        expected_xml = etree.XML("""
+        expected_xml = etree.XML(
+            """
 <StateTransitionFunction>
   <CondProb>
     <Var>rover_1</Var>
     <Parent>action_rover rover_0</Parent>
     <Parameter type="DD">
       <DAG>
         <Node var="action_rover">
@@ -1264,56 +1471,82 @@
               </Edge>
             </Node>
           </Edge>
         </Node>
       </DAG>
     </Parameter>
   </CondProb>
-</StateTransitionFunction>""")
+</StateTransitionFunction>"""
+        )
         self.maxDiff = None
-        self.assertEqual(str(self.writer.add_state_transition_function()),
-                         str(etree.tostring(expected_xml)))
+        self.assertEqual(
+            str(self.writer.add_state_transition_function()),
+            str(etree.tostring(expected_xml)),
+        )
 
     def test_obs_function_dd(self):
         self.model_data = {
-            'obs_function': [{
-                'Var': 'obs_sensor',
-                'Parent': ['action_rover', 'rover_1', 'rock_1'],
-                'Type': 'DD',
-                'Parameter': {'action_rover': {
-                    'amw': {'type': 'deterministic',
-                            'var': 'obs_sensor',
-                            'val': 'ogood'},
-                    'ame': {'type': 'deterministic',
-                            'var': 'obs_sensor',
-                            'val': 'ogood'},
-                    'ac': {'rover_1': {'s0': {'rock_1': {'good': {
-                        'type': 'deterministic',
-                        'var': 'obs_sensor',
-                        'val': 'ogood'},
-                        'bad': {'type': 'deterministic',
-                                'var': 'obs_sensor',
-                                'val': 'obad'}}},
-                        's1': {'type': 'template',
-                               'idref': 'obs_rock'},
-                        's2': {'type': 'template',
-                               'idref': 'obs_rock'}}},
-                    'as': {'type': 'deterministic',
-                           'var': 'obs_sensor',
-                           'val': 'ogood'}},
-                    'SubDAGTemplate': {'rock_1': {'good': {'obs_sensor': {
-                        'ogood': '0.8',
-                        'obad': '0.2'}},
-                        'bad': {'obs_sensor': {
-                            'ogood': '0.2',
-                            'obad': '0.8'}}}},
-                    'id': 'obs_rock'}}]}
+            "obs_function": [
+                {
+                    "Var": "obs_sensor",
+                    "Parent": ["action_rover", "rover_1", "rock_1"],
+                    "Type": "DD",
+                    "Parameter": {
+                        "action_rover": {
+                            "amw": {
+                                "type": "deterministic",
+                                "var": "obs_sensor",
+                                "val": "ogood",
+                            },
+                            "ame": {
+                                "type": "deterministic",
+                                "var": "obs_sensor",
+                                "val": "ogood",
+                            },
+                            "ac": {
+                                "rover_1": {
+                                    "s0": {
+                                        "rock_1": {
+                                            "good": {
+                                                "type": "deterministic",
+                                                "var": "obs_sensor",
+                                                "val": "ogood",
+                                            },
+                                            "bad": {
+                                                "type": "deterministic",
+                                                "var": "obs_sensor",
+                                                "val": "obad",
+                                            },
+                                        }
+                                    },
+                                    "s1": {"type": "template", "idref": "obs_rock"},
+                                    "s2": {"type": "template", "idref": "obs_rock"},
+                                }
+                            },
+                            "as": {
+                                "type": "deterministic",
+                                "var": "obs_sensor",
+                                "val": "ogood",
+                            },
+                        },
+                        "SubDAGTemplate": {
+                            "rock_1": {
+                                "good": {"obs_sensor": {"ogood": "0.8", "obad": "0.2"}},
+                                "bad": {"obs_sensor": {"ogood": "0.2", "obad": "0.8"}},
+                            }
+                        },
+                        "id": "obs_rock",
+                    },
+                }
+            ]
+        }
 
         self.writer = PomdpXWriter(model_data=self.model_data)
-        expected_xml = etree.XML("""
+        expected_xml = etree.XML(
+            """
 <ObsFunction>
   <CondProb>
     <Var>obs_sensor</Var>
     <Parent>action_rover rover_1 rock_1</Parent>
     <Parameter type="DD">
       <DAG>
         <Node var="action_rover">
@@ -1370,41 +1603,53 @@
               </Edge>
             </Node>
           </Edge>
         </Node>
       </SubDAGTemplate>
     </Parameter>
   </CondProb>
-</ObsFunction>""")
+</ObsFunction>"""
+        )
         self.maxDiff = None
-        self.assertEqual(str(self.writer.add_obs_function()),
-                         str(etree.tostring(expected_xml)))
+        self.assertEqual(
+            str(self.writer.add_obs_function()), str(etree.tostring(expected_xml))
+        )
 
     def test_reward_function_dd(self):
         self.model_data = {
-            'reward_function': [{
-                'Var': 'reward_rover',
-                'Parent': ['action_rover', 'rover_0', 'rock_0'],
-                'Type': 'DD',
-                'Parameter': {
-                    'action_rover': {
-                        'amw': {'rover_0': {'s0': '-100.0',
-                                            's1': '0.0',
-                                            's2': '0.0'}},
-                        'ame': {'rover_0': {'s0': '0.0',
-                                            's1': '10.0',
-                                            's2': '0.0'}},
-                        'ac': '0.0',
-                        'as': {'rover_0': {'s0': {'rock_0': {'good': '10',
-                                                             'bad': '-10'}},
-                                           's1': '-100',
-                                           's2': '-100'}}}}}]}
+            "reward_function": [
+                {
+                    "Var": "reward_rover",
+                    "Parent": ["action_rover", "rover_0", "rock_0"],
+                    "Type": "DD",
+                    "Parameter": {
+                        "action_rover": {
+                            "amw": {
+                                "rover_0": {"s0": "-100.0", "s1": "0.0", "s2": "0.0"}
+                            },
+                            "ame": {
+                                "rover_0": {"s0": "0.0", "s1": "10.0", "s2": "0.0"}
+                            },
+                            "ac": "0.0",
+                            "as": {
+                                "rover_0": {
+                                    "s0": {"rock_0": {"good": "10", "bad": "-10"}},
+                                    "s1": "-100",
+                                    "s2": "-100",
+                                }
+                            },
+                        }
+                    },
+                }
+            ]
+        }
 
         self.writer = PomdpXWriter(model_data=self.model_data)
-        expected_xml = etree.XML("""
+        expected_xml = etree.XML(
+            """
 <RewardFunction>
   <Func>
     <Var>reward_rover</Var>
     <Parent>action_rover rover_0 rock_0</Parent>
     <Parameter type="DD">
       <DAG>
         <Node var="action_rover">
@@ -1457,11 +1702,13 @@
               </Edge>
             </Node>
           </Edge>
         </Node>
       </DAG>
     </Parameter>
   </Func>
-</RewardFunction>""")
+</RewardFunction>"""
+        )
         self.maxDiff = None
-        self.assertEqual(self.writer.add_reward_function(),
-                         etree.tostring(expected_xml))
+        self.assertEqual(
+            self.writer.add_reward_function(), etree.tostring(expected_xml)
+        )
```

### Comparing `pgmpy-0.1.7/pgmpy/tests/test_readwrite/test_XMLBeliefNetwork.py` & `pgmpy-0.1.9/pgmpy/tests/test_readwrite/test_XMLBeliefNetwork.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 import unittest
 import warnings
 
 import numpy as np
 import numpy.testing as np_test
+import networkx as nx
 
 from pgmpy.readwrite import XMLBeliefNetwork
 from pgmpy.models import BayesianModel
 from pgmpy.factors.discrete import TabularCPD
 from pgmpy.extern import six
 
 try:
@@ -132,215 +133,295 @@
         self.reader_string = XMLBeliefNetwork.XBNReader(string=string)
         self.reader_file = XMLBeliefNetwork.XBNReader(path=six.StringIO(string))
 
     def test_init_exception(self):
         self.assertRaises(ValueError, XMLBeliefNetwork.XBNReader)
 
     def test_get_analysis_notebook(self):
-        self.assertEqual(self.reader_string.get_analysisnotebook_values()['NAME'],
-                         "Notebook.Cancer Example From Neapolitan")
-        self.assertEqual(self.reader_string.get_analysisnotebook_values()['ROOT'], "Cancer")
-        self.assertEqual(self.reader_file.get_analysisnotebook_values()['NAME'],
-                         "Notebook.Cancer Example From Neapolitan")
-        self.assertEqual(self.reader_file.get_analysisnotebook_values()['ROOT'], "Cancer")
+        self.assertEqual(
+            self.reader_string.get_analysisnotebook_values()["NAME"],
+            "Notebook.Cancer Example From Neapolitan",
+        )
+        self.assertEqual(
+            self.reader_string.get_analysisnotebook_values()["ROOT"], "Cancer"
+        )
+        self.assertEqual(
+            self.reader_file.get_analysisnotebook_values()["NAME"],
+            "Notebook.Cancer Example From Neapolitan",
+        )
+        self.assertEqual(
+            self.reader_file.get_analysisnotebook_values()["ROOT"], "Cancer"
+        )
 
     def test_get_bnmodel_name(self):
         self.assertEqual(self.reader_string.get_bnmodel_name(), "Cancer")
         self.assertEqual(self.reader_file.get_bnmodel_name(), "Cancer")
 
     def test_get_static_properties(self):
         properties = self.reader_string.get_static_properties()
-        self.assertEqual(properties['FORMAT'], "MSR DTAS XML")
-        self.assertEqual(properties['VERSION'], "0.2")
-        self.assertEqual(properties['CREATOR'], "Microsoft Research DTAS")
+        self.assertEqual(properties["FORMAT"], "MSR DTAS XML")
+        self.assertEqual(properties["VERSION"], "0.2")
+        self.assertEqual(properties["CREATOR"], "Microsoft Research DTAS")
         properties = self.reader_file.get_static_properties()
-        self.assertEqual(properties['FORMAT'], "MSR DTAS XML")
-        self.assertEqual(properties['VERSION'], "0.2")
-        self.assertEqual(properties['CREATOR'], "Microsoft Research DTAS")
+        self.assertEqual(properties["FORMAT"], "MSR DTAS XML")
+        self.assertEqual(properties["VERSION"], "0.2")
+        self.assertEqual(properties["CREATOR"], "Microsoft Research DTAS")
 
     def test_get_variables(self):
-        self.assertListEqual(sorted(list(self.reader_string.get_variables())), ['a', 'b', 'c', 'd', 'e', 'f'])
-        self.assertListEqual(sorted(list(self.reader_file.get_variables())), ['a', 'b', 'c', 'd', 'e', 'f'])
-        self.assertEqual(self.reader_string.get_variables()['a']['TYPE'], 'discrete')
-        self.assertEqual(self.reader_string.get_variables()['a']['XPOS'], '13495')
-        self.assertEqual(self.reader_string.get_variables()['a']['YPOS'], '10465')
-        self.assertEqual(self.reader_string.get_variables()['a']['DESCRIPTION'], '(a) Metastatic Cancer')
-        self.assertListEqual(self.reader_string.get_variables()['a']['STATES'], ['Present', 'Absent'])
-        self.assertEqual(self.reader_file.get_variables()['a']['TYPE'], 'discrete')
-        self.assertEqual(self.reader_file.get_variables()['a']['XPOS'], '13495')
-        self.assertEqual(self.reader_file.get_variables()['a']['YPOS'], '10465')
-        self.assertEqual(self.reader_file.get_variables()['a']['DESCRIPTION'], '(a) Metastatic Cancer')
-        self.assertListEqual(self.reader_file.get_variables()['a']['STATES'], ['Present', 'Absent'])
+        self.assertListEqual(
+            sorted(list(self.reader_string.get_variables())),
+            ["a", "b", "c", "d", "e", "f"],
+        )
+        self.assertListEqual(
+            sorted(list(self.reader_file.get_variables())),
+            ["a", "b", "c", "d", "e", "f"],
+        )
+        self.assertEqual(self.reader_string.get_variables()["a"]["TYPE"], "discrete")
+        self.assertEqual(self.reader_string.get_variables()["a"]["XPOS"], "13495")
+        self.assertEqual(self.reader_string.get_variables()["a"]["YPOS"], "10465")
+        self.assertEqual(
+            self.reader_string.get_variables()["a"]["DESCRIPTION"],
+            "(a) Metastatic Cancer",
+        )
+        self.assertListEqual(
+            self.reader_string.get_variables()["a"]["STATES"], ["Present", "Absent"]
+        )
+        self.assertEqual(self.reader_file.get_variables()["a"]["TYPE"], "discrete")
+        self.assertEqual(self.reader_file.get_variables()["a"]["XPOS"], "13495")
+        self.assertEqual(self.reader_file.get_variables()["a"]["YPOS"], "10465")
+        self.assertEqual(
+            self.reader_file.get_variables()["a"]["DESCRIPTION"],
+            "(a) Metastatic Cancer",
+        )
+        self.assertListEqual(
+            self.reader_file.get_variables()["a"]["STATES"], ["Present", "Absent"]
+        )
 
     def test_get_edges(self):
-        self.assertListEqual(self.reader_string.get_edges(),
-                             [('a', 'b'), ('a', 'c'), ('b', 'd'), ('c', 'd'), ('c', 'e')])
-        self.assertListEqual(self.reader_file.get_edges(), [('a', 'b'), ('a', 'c'), ('b', 'd'), ('c', 'd'), ('c', 'e')])
+        self.assertListEqual(
+            self.reader_string.get_edges(),
+            [("a", "b"), ("a", "c"), ("b", "d"), ("c", "d"), ("c", "e")],
+        )
+        self.assertListEqual(
+            self.reader_file.get_edges(),
+            [("a", "b"), ("a", "c"), ("b", "d"), ("c", "d"), ("c", "e")],
+        )
 
     def test_get_distribution(self):
         distribution = self.reader_string.get_distributions()
-        self.assertEqual(distribution['a']['TYPE'], 'discrete')
-        self.assertListEqual(distribution['b']['CONDSET'], ['a'])
-        np_test.assert_array_equal(distribution['a']['DPIS'], np.array([[0.2, 0.8]]))
-        np_test.assert_array_equal(distribution['f']['DPIS'], np.array([[0.3, 0.7]]))
-        np_test.assert_array_equal(distribution['e']['DPIS'], np.array([[0.8, 0.2], [0.6, 0.4]]))
-        np_test.assert_array_equal(distribution['e']['CARDINALITY'], np.array([2]))
-        np_test.assert_array_equal(distribution['d']['DPIS'],
-                                   np.array([[0.8, 0.2], [0.9, 0.1], [0.7, 0.3], [0.05, 0.95]]))
-        np_test.assert_array_equal(distribution['b']['DPIS'], np.array([[0.8, 0.2], [0.2, 0.8]]))
-        np_test.assert_array_equal(distribution['d']['CARDINALITY'], np.array([2, 2]))
-        np_test.assert_array_equal(distribution['c']['DPIS'], np.array([[0.2, 0.8], [0.05, 0.95]]))
-        np_test.assert_array_equal(distribution['c']['CARDINALITY'], np.array([2]))
+        self.assertEqual(distribution["a"]["TYPE"], "discrete")
+        self.assertListEqual(distribution["b"]["CONDSET"], ["a"])
+        np_test.assert_array_equal(distribution["a"]["DPIS"], np.array([[0.2, 0.8]]))
+        np_test.assert_array_equal(distribution["f"]["DPIS"], np.array([[0.3, 0.7]]))
+        np_test.assert_array_equal(
+            distribution["e"]["DPIS"], np.array([[0.8, 0.2], [0.6, 0.4]])
+        )
+        np_test.assert_array_equal(distribution["e"]["CARDINALITY"], np.array([2]))
+        np_test.assert_array_equal(
+            distribution["d"]["DPIS"],
+            np.array([[0.8, 0.2], [0.9, 0.1], [0.7, 0.3], [0.05, 0.95]]),
+        )
+        np_test.assert_array_equal(
+            distribution["b"]["DPIS"], np.array([[0.8, 0.2], [0.2, 0.8]])
+        )
+        np_test.assert_array_equal(distribution["d"]["CARDINALITY"], np.array([2, 2]))
+        np_test.assert_array_equal(
+            distribution["c"]["DPIS"], np.array([[0.2, 0.8], [0.05, 0.95]])
+        )
+        np_test.assert_array_equal(distribution["c"]["CARDINALITY"], np.array([2]))
         distribution = self.reader_file.get_distributions()
-        self.assertEqual(distribution['a']['TYPE'], 'discrete')
-        self.assertListEqual(distribution['b']['CONDSET'], ['a'])
-        np_test.assert_array_equal(distribution['a']['DPIS'], np.array([[0.2, 0.8]]))
-        np_test.assert_array_equal(distribution['f']['DPIS'], np.array([[0.3, 0.7]]))
-        np_test.assert_array_equal(distribution['e']['DPIS'], np.array([[0.8, 0.2], [0.6, 0.4]]))
-        np_test.assert_array_equal(distribution['e']['CARDINALITY'], np.array([2]))
-        np_test.assert_array_equal(distribution['d']['DPIS'],
-                                   np.array([[0.8, 0.2], [0.9, 0.1], [0.7, 0.3], [0.05, 0.95]]))
-        np_test.assert_array_equal(distribution['d']['CARDINALITY'], np.array([2, 2]))
-        np_test.assert_array_equal(distribution['b']['DPIS'], np.array([[0.8, 0.2], [0.2, 0.8]]))
-        np_test.assert_array_equal(distribution['c']['DPIS'], np.array([[0.2, 0.8], [0.05, 0.95]]))
-        np_test.assert_array_equal(distribution['c']['CARDINALITY'], np.array([2]))
+        self.assertEqual(distribution["a"]["TYPE"], "discrete")
+        self.assertListEqual(distribution["b"]["CONDSET"], ["a"])
+        np_test.assert_array_equal(distribution["a"]["DPIS"], np.array([[0.2, 0.8]]))
+        np_test.assert_array_equal(distribution["f"]["DPIS"], np.array([[0.3, 0.7]]))
+        np_test.assert_array_equal(
+            distribution["e"]["DPIS"], np.array([[0.8, 0.2], [0.6, 0.4]])
+        )
+        np_test.assert_array_equal(distribution["e"]["CARDINALITY"], np.array([2]))
+        np_test.assert_array_equal(
+            distribution["d"]["DPIS"],
+            np.array([[0.8, 0.2], [0.9, 0.1], [0.7, 0.3], [0.05, 0.95]]),
+        )
+        np_test.assert_array_equal(distribution["d"]["CARDINALITY"], np.array([2, 2]))
+        np_test.assert_array_equal(
+            distribution["b"]["DPIS"], np.array([[0.8, 0.2], [0.2, 0.8]])
+        )
+        np_test.assert_array_equal(
+            distribution["c"]["DPIS"], np.array([[0.2, 0.8], [0.05, 0.95]])
+        )
+        np_test.assert_array_equal(distribution["c"]["CARDINALITY"], np.array([2]))
 
     def test_get_model(self):
         model = self.reader_string.get_model()
-        node_expected = {'c': {'STATES': ['Present', 'Absent'],
-                               'DESCRIPTION': '(c) Brain Tumor',
-                               'YPOS': '11935',
-                               'XPOS': '15250',
-                               'TYPE': 'discrete'},
-                         'a': {'STATES': ['Present', 'Absent'],
-                               'DESCRIPTION': '(a) Metastatic Cancer',
-                               'YPOS': '10465',
-                               'XPOS': '13495',
-                               'TYPE': 'discrete'},
-                         'b': {'STATES': ['Present', 'Absent'],
-                               'DESCRIPTION': '(b) Serum Calcium Increase',
-                               'YPOS': '11965',
-                               'XPOS': '11290',
-                               'TYPE': 'discrete'},
-                         'e': {'STATES': ['Present', 'Absent'],
-                               'DESCRIPTION': '(e) Papilledema',
-                               'YPOS': '13240',
-                               'XPOS': '17305',
-                               'TYPE': 'discrete'},
-                         'f': {'STATES': ['Present', 'Absent'],
-                               'DESCRIPTION': '(f) Asthma',
-                               'YPOS': '10489',
-                               'XPOS': '13440',
-                               'TYPE': 'discrete'},      
-                         'd': {'STATES': ['Present', 'Absent'],
-                               'DESCRIPTION': '(d) Coma',
-                               'YPOS': '12985',
-                               'XPOS': '13960',
-                               'TYPE': 'discrete'}}
-        cpds_expected = {'b': np.array([[0.8, 0.2],
-                                        [0.2, 0.8]]),
-                         'e': np.array([[0.8, 0.2],
-                                        [0.6, 0.4]]),
-                         'f': np.array([[0.3],
-                                        [0.7]]),
-                         'c': np.array([[0.2, 0.8],
-                                        [0.05, 0.95]]),
-                         'a': np.array([[0.2],
-                                        [0.8]]),
-                         'd': np.array([[0.8, 0.2, 0.9, 0.1],
-                                        [0.7, 0.3, 0.05, 0.95]])}
+        node_expected = {
+            "c": {
+                "STATES": ["Present", "Absent"],
+                "DESCRIPTION": "(c) Brain Tumor",
+                "YPOS": "11935",
+                "XPOS": "15250",
+                "TYPE": "discrete",
+            },
+            "a": {
+                "STATES": ["Present", "Absent"],
+                "DESCRIPTION": "(a) Metastatic Cancer",
+                "YPOS": "10465",
+                "XPOS": "13495",
+                "TYPE": "discrete",
+            },
+            "b": {
+                "STATES": ["Present", "Absent"],
+                "DESCRIPTION": "(b) Serum Calcium Increase",
+                "YPOS": "11965",
+                "XPOS": "11290",
+                "TYPE": "discrete",
+            },
+            "e": {
+                "STATES": ["Present", "Absent"],
+                "DESCRIPTION": "(e) Papilledema",
+                "YPOS": "13240",
+                "XPOS": "17305",
+                "TYPE": "discrete",
+            },
+            "f": {
+                "STATES": ["Present", "Absent"],
+                "DESCRIPTION": "(f) Asthma",
+                "YPOS": "10489",
+                "XPOS": "13440",
+                "TYPE": "discrete",
+            },
+            "d": {
+                "STATES": ["Present", "Absent"],
+                "DESCRIPTION": "(d) Coma",
+                "YPOS": "12985",
+                "XPOS": "13960",
+                "TYPE": "discrete",
+            },
+        }
+        cpds_expected = {
+            "b": np.array([[0.8, 0.2], [0.2, 0.8]]),
+            "e": np.array([[0.8, 0.2], [0.6, 0.4]]),
+            "f": np.array([[0.3], [0.7]]),
+            "c": np.array([[0.2, 0.8], [0.05, 0.95]]),
+            "a": np.array([[0.2], [0.8]]),
+            "d": np.array([[0.8, 0.2, 0.9, 0.1], [0.7, 0.3, 0.05, 0.95]]),
+        }
         for cpd in model.get_cpds():
             np_test.assert_array_equal(cpd.get_values(), cpds_expected[cpd.variable])
-        self.assertListEqual(sorted(model.edges()), sorted([('b', 'd'), ('a', 'b'), ('a', 'c'),
-                                                            ('c', 'd'), ('c', 'e')]))
-        self.assertDictEqual(model.node, node_expected)
+        self.assertListEqual(
+            sorted(model.edges()),
+            sorted([("b", "d"), ("a", "b"), ("a", "c"), ("c", "d"), ("c", "e")]),
+        )
+        self.assertDictEqual(dict(model.nodes), node_expected)
 
 
 class TestXBNWriter(unittest.TestCase):
     def setUp(self):
-        nodes = {'c': {'STATES': ['Present', 'Absent'],
-                       'DESCRIPTION': '(c) Brain Tumor',
-                       'YPOS': '11935',
-                       'XPOS': '15250',
-                       'TYPE': 'discrete'},
-                 'a': {'STATES': ['Present', 'Absent'],
-                       'DESCRIPTION': '(a) Metastatic Cancer',
-                       'YPOS': '10465',
-                       'XPOS': '13495',
-                       'TYPE': 'discrete'},
-                 'b': {'STATES': ['Present', 'Absent'],
-                       'DESCRIPTION': '(b) Serum Calcium Increase',
-                       'YPOS': '11965',
-                       'XPOS': '11290',
-                       'TYPE': 'discrete'},
-                 'e': {'STATES': ['Present', 'Absent'],
-                       'DESCRIPTION': '(e) Papilledema',
-                       'YPOS': '13240',
-                       'XPOS': '17305',
-                       'TYPE': 'discrete'},
-                 'f': {'STATES': ['Present', 'Absent'],
-                       'DESCRIPTION': '(f) Asthma',
-                       'YPOS': '10489',
-                       'XPOS': '13440',
-                       'TYPE': 'discrete'},        
-                 'd': {'STATES': ['Present', 'Absent'],
-                       'DESCRIPTION': '(d) Coma',
-                       'YPOS': '12985',
-                       'XPOS': '13960',
-                       'TYPE': 'discrete'}}
+        nodes = {
+            "c": {
+                "STATES": ["Present", "Absent"],
+                "DESCRIPTION": "(c) Brain Tumor",
+                "YPOS": "11935",
+                "XPOS": "15250",
+                "TYPE": "discrete",
+            },
+            "a": {
+                "STATES": ["Present", "Absent"],
+                "DESCRIPTION": "(a) Metastatic Cancer",
+                "YPOS": "10465",
+                "XPOS": "13495",
+                "TYPE": "discrete",
+            },
+            "b": {
+                "STATES": ["Present", "Absent"],
+                "DESCRIPTION": "(b) Serum Calcium Increase",
+                "YPOS": "11965",
+                "XPOS": "11290",
+                "TYPE": "discrete",
+            },
+            "e": {
+                "STATES": ["Present", "Absent"],
+                "DESCRIPTION": "(e) Papilledema",
+                "YPOS": "13240",
+                "XPOS": "17305",
+                "TYPE": "discrete",
+            },
+            "f": {
+                "STATES": ["Present", "Absent"],
+                "DESCRIPTION": "(f) Asthma",
+                "YPOS": "10489",
+                "XPOS": "13440",
+                "TYPE": "discrete",
+            },
+            "d": {
+                "STATES": ["Present", "Absent"],
+                "DESCRIPTION": "(d) Coma",
+                "YPOS": "12985",
+                "XPOS": "13960",
+                "TYPE": "discrete",
+            },
+        }
         model = BayesianModel()
-        model.add_nodes_from(['a', 'b', 'c', 'd', 'e', 'f'])
-        model.add_edges_from([('b', 'd'), ('a', 'b'), ('a', 'c'), ('c', 'd'), ('c', 'e')])
-        cpd_distribution = {'a': {'TYPE': 'discrete',
-                                  'DPIS': np.array([[0.2, 0.8]])},
-                            'e': {'TYPE': 'discrete',
-                                  'DPIS': np.array([[0.8, 0.2],
-                                                    [0.6, 0.4]]),
-                                  'CONDSET': ['c'],
-                                  'CARDINALITY': [2]},
-                            'f': {'TYPE': 'discrete',
-                                  'DPIS': np.array([[0.3, 0.7]])},      
-                            'b': {'TYPE': 'discrete',
-                                  'DPIS': np.array([[0.8, 0.2],
-                                                    [0.2, 0.8]]),
-                                  'CONDSET': ['a'],
-                                  'CARDINALITY': [2]},
-                            'c': {'TYPE': 'discrete',
-                                  'DPIS': np.array([[0.2, 0.8],
-                                                    [0.05, 0.95]]),
-                                  'CONDSET': ['a'],
-                                  'CARDINALITY': [2]},
-                            'd': {'TYPE': 'discrete',
-                                  'DPIS': np.array([[0.8, 0.2],
-                                                    [0.9, 0.1],
-                                                    [0.7, 0.3],
-                                                    [0.05, 0.95]]),
-                                  'CONDSET': ['b', 'c'],
-                                  'CARDINALITY': [2, 2]}}
+        model.add_nodes_from(["a", "b", "c", "d", "e", "f"])
+        model.add_edges_from(
+            [("b", "d"), ("a", "b"), ("a", "c"), ("c", "d"), ("c", "e")]
+        )
+        cpd_distribution = {
+            "a": {"TYPE": "discrete", "DPIS": np.array([[0.2, 0.8]])},
+            "e": {
+                "TYPE": "discrete",
+                "DPIS": np.array([[0.8, 0.2], [0.6, 0.4]]),
+                "CONDSET": ["c"],
+                "CARDINALITY": [2],
+            },
+            "f": {"TYPE": "discrete", "DPIS": np.array([[0.3, 0.7]])},
+            "b": {
+                "TYPE": "discrete",
+                "DPIS": np.array([[0.8, 0.2], [0.2, 0.8]]),
+                "CONDSET": ["a"],
+                "CARDINALITY": [2],
+            },
+            "c": {
+                "TYPE": "discrete",
+                "DPIS": np.array([[0.2, 0.8], [0.05, 0.95]]),
+                "CONDSET": ["a"],
+                "CARDINALITY": [2],
+            },
+            "d": {
+                "TYPE": "discrete",
+                "DPIS": np.array([[0.8, 0.2], [0.9, 0.1], [0.7, 0.3], [0.05, 0.95]]),
+                "CONDSET": ["b", "c"],
+                "CARDINALITY": [2, 2],
+            },
+        }
 
         tabular_cpds = []
         for var, values in cpd_distribution.items():
-            evidence = values['CONDSET'] if 'CONDSET' in values else []
-            cpd = values['DPIS']
-            evidence_card = values['CARDINALITY'] if 'CARDINALITY' in values else []
-            states = nodes[var]['STATES']
-            cpd = TabularCPD(var, len(states), cpd,
-                             evidence=evidence,
-                             evidence_card=evidence_card)
+            evidence = values["CONDSET"] if "CONDSET" in values else []
+            cpd = values["DPIS"]
+            evidence_card = values["CARDINALITY"] if "CARDINALITY" in values else []
+            states = nodes[var]["STATES"]
+            cpd = TabularCPD(
+                var, len(states), cpd, evidence=evidence, evidence_card=evidence_card
+            )
             tabular_cpds.append(cpd)
         model.add_cpds(*tabular_cpds)
 
-        for var, properties in nodes.items():
-            model.node[var] = properties
+        if nx.__version__.startswith("1"):
+            for var, properties in nodes.items():
+                model.nodes[var] = properties
+        else:
+            for var, properties in nodes.items():
+                model._node[var] = properties
 
         self.maxDiff = None
         self.writer = XMLBeliefNetwork.XBNWriter(model=model)
 
     def test_file(self):
-        self.expected_xml = etree.XML("""<ANALYSISNOTEBOOK>
+        self.expected_xml = etree.XML(
+            """<ANALYSISNOTEBOOK>
   <BNMODEL>
     <VARIABLES>
       <VAR NAME="a" TYPE="discrete" XPOS="13495" YPOS="10465">
         <DESCRIPTION DESCRIPTION="(a) Metastatic Cancer"/>
         <STATENAME>Present</STATENAME>
         <STATENAME>Absent</STATENAME>
       </VAR>
@@ -431,9 +512,12 @@
         <PRIVATE NAME="f"/>
         <DPIS>
           <DPI> 0.3 0.7</DPI>
         </DPIS>
       </DIST>
     </DISTRIBUTIONS>
   </BNMODEL>
-</ANALYSISNOTEBOOK>""")
-        self.assertEqual(str(self.writer.__str__()[:-1]), str(etree.tostring(self.expected_xml)))
+</ANALYSISNOTEBOOK>"""
+        )
+        self.assertEqual(
+            str(self.writer.__str__()[:-1]), str(etree.tostring(self.expected_xml))
+        )
```

### Comparing `pgmpy-0.1.7/pgmpy/tests/test_readwrite/test_UAI.py` & `pgmpy-0.1.9/pgmpy/tests/test_readwrite/test_UAI.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 import numpy as np
+import networkx as nx
 import unittest
 
 from pgmpy.readwrite import UAIReader, UAIWriter
 from pgmpy.models import BayesianModel, MarkovModel
 from pgmpy.factors.discrete import TabularCPD, DiscreteFactor
 from pgmpy.extern.six.moves import map
 
@@ -23,122 +24,180 @@
 12
  2.2500 3.2500 3.7500
  0.0000 0.0000 10.0000
  1.8750 4.0000 3.3330
  2.0000 2.0000 3.4000"""
         self.maxDiff = None
         self.reader_string = UAIReader(string=string)
-        self.reader_file = UAIReader('pgmpy/tests/test_readwrite/testdata/grid4x4.uai')
+        self.reader_file = UAIReader("pgmpy/tests/test_readwrite/testdata/grid4x4.uai")
 
     def test_get_network_type(self):
         network_type_expected = "MARKOV"
         self.assertEqual(self.reader_string.network_type, network_type_expected)
 
     def test_get_variables(self):
-        variables_expected = ['var_0', 'var_1', 'var_2']
+        variables_expected = ["var_0", "var_1", "var_2"]
         self.assertListEqual(self.reader_string.variables, variables_expected)
 
     def test_get_domain(self):
-        domain_expected = {'var_1': '2', 'var_2': '3', 'var_0': '2'}
+        domain_expected = {"var_1": "2", "var_2": "3", "var_0": "2"}
         self.assertDictEqual(self.reader_string.domain, domain_expected)
 
     def test_get_edges(self):
-        edges_expected = {('var_0', 'var_1'), ('var_0', 'var_2'), ('var_1', 'var_2')}
+        edges_expected = {("var_0", "var_1"), ("var_0", "var_2"), ("var_1", "var_2")}
         self.assertSetEqual(self.reader_string.edges, edges_expected)
 
     def test_get_tables(self):
-        tables_expected = [(['var_0', 'var_1'],
-                            ['4.000', '2.400', '1.000', '0.000']),
-                           (['var_0', 'var_1', 'var_2'],
-                            ['2.2500', '3.2500', '3.7500', '0.0000', '0.0000', '10.0000',
-                             '1.8750', '4.0000', '3.3330', '2.0000', '2.0000', '3.4000'])]
+        tables_expected = [
+            (["var_0", "var_1"], ["4.000", "2.400", "1.000", "0.000"]),
+            (
+                ["var_0", "var_1", "var_2"],
+                [
+                    "2.2500",
+                    "3.2500",
+                    "3.7500",
+                    "0.0000",
+                    "0.0000",
+                    "10.0000",
+                    "1.8750",
+                    "4.0000",
+                    "3.3330",
+                    "2.0000",
+                    "2.0000",
+                    "3.4000",
+                ],
+            ),
+        ]
         self.assertListEqual(self.reader_string.tables, tables_expected)
 
     def test_get_model(self):
         model = self.reader_string.get_model()
         edge_expected = {
-            'var_2': {'var_0': {'weight': None},
-                      'var_1': {'weight': None}},
-            'var_0': {'var_2': {'weight': None},
-                      'var_1': {'weight': None}},
-            'var_1': {'var_2': {'weight': None},
-                      'var_0': {'weight': None}}}
-        self.assertListEqual(sorted(model.nodes()), sorted(['var_0', 'var_2', 'var_1']))
-        self.assertDictEqual(model.edge, edge_expected)
+            "var_2": {"var_0": {"weight": None}, "var_1": {"weight": None}},
+            "var_0": {"var_2": {"weight": None}, "var_1": {"weight": None}},
+            "var_1": {"var_2": {"weight": None}, "var_0": {"weight": None}},
+        }
+
+        self.assertListEqual(sorted(model.nodes()), sorted(["var_0", "var_2", "var_1"]))
+        if nx.__version__.startswith("1"):
+            self.assertDictEqual(dict(model.edge), edge_expected)
+        else:
+            self.assertDictEqual(dict(model.adj), edge_expected)
 
     def test_read_file(self):
         model = self.reader_file.get_model()
-        node_expected = {'var_3': {}, 'var_8': {}, 'var_5': {}, 'var_14': {},
-                         'var_15': {}, 'var_0': {}, 'var_9': {}, 'var_7': {},
-                         'var_6': {}, 'var_13': {}, 'var_10': {}, 'var_12': {},
-                         'var_1': {}, 'var_11': {}, 'var_2': {}, 'var_4': {}}
-        self.assertDictEqual(model.node, node_expected)
+        node_expected = {
+            "var_3": {},
+            "var_8": {},
+            "var_5": {},
+            "var_14": {},
+            "var_15": {},
+            "var_0": {},
+            "var_9": {},
+            "var_7": {},
+            "var_6": {},
+            "var_13": {},
+            "var_10": {},
+            "var_12": {},
+            "var_1": {},
+            "var_11": {},
+            "var_2": {},
+            "var_4": {},
+        }
+        self.assertDictEqual(dict(model.nodes), node_expected)
 
 
 class TestUAIWriter(unittest.TestCase):
     def setUp(self):
         self.maxDiff = None
-        variables = ['kid', 'bowel-problem', 'dog-out',
-                     'family-out', 'hear-bark', 'light-on']
-        edges = [['family-out', 'dog-out'],
-                 ['bowel-problem', 'dog-out'],
-                 ['family-out', 'light-on'],
-                 ['dog-out', 'hear-bark']]
-        cpds = {'kid': np.array([[0.3],
-                                 [0.7]]),
-                'bowel-problem': np.array([[0.01],
-                                           [0.99]]),
-                'dog-out': np.array([[0.99, 0.01, 0.97, 0.03],
-                                     [0.9, 0.1, 0.3, 0.7]]),
-                'family-out': np.array([[0.15],
-                                        [0.85]]),
-                'hear-bark': np.array([[0.7, 0.3],
-                                       [0.01, 0.99]]),
-                'light-on': np.array([[0.6, 0.4],
-                                      [0.05, 0.95]])}
-        states = {'kid': ['true', 'false'],
-                  'bowel-problem': ['true', 'false'],
-                  'dog-out': ['true', 'false'],
-                  'family-out': ['true', 'false'],
-                  'hear-bark': ['true', 'false'],
-                  'light-on': ['true', 'false']}
-        parents = {'kid': [],
-                   'bowel-problem': [],
-                   'dog-out': ['bowel-problem', 'family-out'],
-                   'family-out': [],
-                   'hear-bark': ['dog-out'],
-                   'light-on': ['family-out']}
+        variables = [
+            "kid",
+            "bowel-problem",
+            "dog-out",
+            "family-out",
+            "hear-bark",
+            "light-on",
+        ]
+        edges = [
+            ["family-out", "dog-out"],
+            ["bowel-problem", "dog-out"],
+            ["family-out", "light-on"],
+            ["dog-out", "hear-bark"],
+        ]
+        cpds = {
+            "kid": np.array([[0.3], [0.7]]),
+            "bowel-problem": np.array([[0.01], [0.99]]),
+            "dog-out": np.array([[0.99, 0.01, 0.97, 0.03], [0.9, 0.1, 0.3, 0.7]]),
+            "family-out": np.array([[0.15], [0.85]]),
+            "hear-bark": np.array([[0.7, 0.3], [0.01, 0.99]]),
+            "light-on": np.array([[0.6, 0.4], [0.05, 0.95]]),
+        }
+        states = {
+            "kid": ["true", "false"],
+            "bowel-problem": ["true", "false"],
+            "dog-out": ["true", "false"],
+            "family-out": ["true", "false"],
+            "hear-bark": ["true", "false"],
+            "light-on": ["true", "false"],
+        }
+        parents = {
+            "kid": [],
+            "bowel-problem": [],
+            "dog-out": ["bowel-problem", "family-out"],
+            "family-out": [],
+            "hear-bark": ["dog-out"],
+            "light-on": ["family-out"],
+        }
 
         self.bayesmodel = BayesianModel()
         self.bayesmodel.add_nodes_from(variables)
         self.bayesmodel.add_edges_from(edges)
 
-
         tabular_cpds = []
         for var, values in cpds.items():
-            cpd = TabularCPD(var, len(states[var]), values,
-                             evidence=parents[var],
-                             evidence_card=[len(states[evidence_var])
-                                            for evidence_var in parents[var]])
+            cpd = TabularCPD(
+                var,
+                len(states[var]),
+                values,
+                evidence=parents[var],
+                evidence_card=[
+                    len(states[evidence_var]) for evidence_var in parents[var]
+                ],
+            )
             tabular_cpds.append(cpd)
         self.bayesmodel.add_cpds(*tabular_cpds)
         self.bayeswriter = UAIWriter(self.bayesmodel)
 
-        edges = {('var_0', 'var_1'), ('var_0', 'var_2'), ('var_1', 'var_2')}
+        edges = {("var_0", "var_1"), ("var_0", "var_2"), ("var_1", "var_2")}
         self.markovmodel = MarkovModel(edges)
-        tables = [(['var_0', 'var_1'],
-                   ['4.000', '2.400', '1.000', '0.000']),
-                  (['var_0', 'var_1', 'var_2'],
-                   ['2.2500', '3.2500', '3.7500', '0.0000', '0.0000', '10.0000',
-                    '1.8750', '4.0000', '3.3330', '2.0000', '2.0000', '3.4000'])]
-        domain = {'var_1': '2', 'var_2': '3', 'var_0': '2'}
+        tables = [
+            (["var_0", "var_1"], ["4.000", "2.400", "1.000", "0.000"]),
+            (
+                ["var_0", "var_1", "var_2"],
+                [
+                    "2.2500",
+                    "3.2500",
+                    "3.7500",
+                    "0.0000",
+                    "0.0000",
+                    "10.0000",
+                    "1.8750",
+                    "4.0000",
+                    "3.3330",
+                    "2.0000",
+                    "2.0000",
+                    "3.4000",
+                ],
+            ),
+        ]
+        domain = {"var_1": "2", "var_2": "3", "var_0": "2"}
         factors = []
         for table in tables:
             variables = table[0]
-            cardinality = [int(domain[  var]) for var in variables]
+            cardinality = [int(domain[var]) for var in variables]
             values = list(map(float, table[1]))
             factor = DiscreteFactor(variables, cardinality, values)
             factors.append(factor)
         self.markovmodel.add_factors(*factors)
         self.markovwriter = UAIWriter(self.markovmodel)
 
     def test_bayes_model(self):
@@ -175,8 +234,10 @@
 2 0 1
 3 0 1 2
 
 4
 4.0 2.4 1.0 0.0
 12
 2.25 3.25 3.75 0.0 0.0 10.0 1.875 4.0 3.333 2.0 2.0 3.4"""
-        self.assertEqual(str(self.markovwriter.__str__()), str(self.expected_markov_file))
+        self.assertEqual(
+            str(self.markovwriter.__str__()), str(self.expected_markov_file)
+        )
```

### Comparing `pgmpy-0.1.7/pgmpy/tests/test_readwrite/test_BIF.py` & `pgmpy-0.1.9/pgmpy/tests/test_readwrite/test_XMLBIF.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,299 +1,388 @@
+import os
 import unittest
-
+import warnings
 import numpy as np
 import numpy.testing as np_test
 
-from pgmpy.readwrite import BIFReader, BIFWriter
+from pgmpy.readwrite import XMLBIFReader, XMLBIFWriter
 from pgmpy.models import BayesianModel
 from pgmpy.factors.discrete import TabularCPD
-from pgmpy.extern.six.moves import map, range
 
 
-class TestBIFReader(unittest.TestCase):
+try:
+    from lxml import etree
+except ImportError:
+    try:
+        import xml.etree.cElementTree as etree
+    except ImportError:
+        try:
+            import xml.etree.ElementTree as etree
+        except ImportError:
+            warnings.warn("Failed to import ElementTree from any known place")
+
+TEST_FILE = """<?xml version="1.0"?>
+
+
+<!--
+    Bayesian network in XMLBIF v0.3 (BayesNet Interchange Format)
+    Produced by JavaBayes (http://www.cs.cmu.edu/~javabayes/
+    Output created Mon Aug 01 10:33:28 AEST 2016
+-->
+
+
+
+<!-- DTD for the XMLBIF 0.3 format -->
+<!DOCTYPE BIF [
+    <!ELEMENT BIF ( NETWORK )*>
+          <!ATTLIST BIF VERSION CDATA #REQUIRED>
+    <!ELEMENT NETWORK ( NAME, ( PROPERTY | VARIABLE | DEFINITION )* )>
+    <!ELEMENT NAME (#PCDATA)>
+    <!ELEMENT VARIABLE ( NAME, ( OUTCOME |  PROPERTY )* ) >
+          <!ATTLIST VARIABLE TYPE (nature|decision|utility) "nature">
+    <!ELEMENT OUTCOME (#PCDATA)>
+    <!ELEMENT DEFINITION ( FOR | GIVEN | TABLE | PROPERTY )* >
+    <!ELEMENT FOR (#PCDATA)>
+    <!ELEMENT GIVEN (#PCDATA)>
+    <!ELEMENT TABLE (#PCDATA)>
+    <!ELEMENT PROPERTY (#PCDATA)>
+]>
+
+
+<BIF VERSION="0.3">
+<NETWORK>
+<NAME>Dog_Problem</NAME>
+
+<!-- Variables -->
+<VARIABLE TYPE="nature">
+    <NAME>kid</NAME>
+    <OUTCOME>true</OUTCOME>
+    <OUTCOME>false</OUTCOME>
+    <PROPERTY>position = (100, 165)</PROPERTY>
+</VARIABLE>
+
+<VARIABLE TYPE="nature">
+    <NAME>light_on</NAME>
+    <OUTCOME>true</OUTCOME>
+    <OUTCOME>false</OUTCOME>
+    <PROPERTY>position = (73, 165)</PROPERTY>
+</VARIABLE>
+
+<VARIABLE TYPE="nature">
+    <NAME>bowel_problem</NAME>
+    <OUTCOME>true</OUTCOME>
+    <OUTCOME>false</OUTCOME>
+    <PROPERTY>position = (190, 69)</PROPERTY>
+</VARIABLE>
+
+<VARIABLE TYPE="nature">
+    <NAME>dog_out</NAME>
+    <OUTCOME>true</OUTCOME>
+    <OUTCOME>false</OUTCOME>
+    <PROPERTY>position = (155, 165)</PROPERTY>
+</VARIABLE>
+
+<VARIABLE TYPE="nature">
+    <NAME>hear_bark</NAME>
+    <OUTCOME>true</OUTCOME>
+    <OUTCOME>false</OUTCOME>
+    <PROPERTY>position = (154, 241)</PROPERTY>
+</VARIABLE>
+
+<VARIABLE TYPE="nature">
+    <NAME>family_out</NAME>
+    <OUTCOME>true</OUTCOME>
+    <OUTCOME>false</OUTCOME>
+    <PROPERTY>position = (112, 69)</PROPERTY>
+</VARIABLE>
+
+
+<!-- Probability distributions -->
+<DEFINITION>
+    <FOR>kid</FOR>
+    <TABLE>0.3 0.7 </TABLE>
+</DEFINITION>
+
+<DEFINITION>
+    <FOR>light_on</FOR>
+    <GIVEN>family_out</GIVEN>
+    <TABLE>0.6 0.4 0.05 0.95 </TABLE>
+</DEFINITION>
+
+<DEFINITION>
+    <FOR>bowel_problem</FOR>
+    <TABLE>0.01 0.99 </TABLE>
+</DEFINITION>
+
+<DEFINITION>
+    <FOR>dog_out</FOR>
+    <GIVEN>bowel_problem</GIVEN>
+    <GIVEN>family_out</GIVEN>
+    <TABLE>0.99 0.01 0.97 0.03 0.9 0.1 0.3 0.7 </TABLE>
+</DEFINITION>
+
+<DEFINITION>
+    <FOR>hear_bark</FOR>
+    <GIVEN>dog_out</GIVEN>
+    <TABLE>0.7 0.3 0.01 0.99 </TABLE>
+</DEFINITION>
+
+<DEFINITION>
+    <FOR>family_out</FOR>
+    <TABLE>0.15 0.85 </TABLE>
+</DEFINITION>
 
-    def setUp(self):
 
-        self.reader = BIFReader(string="""
-// Bayesian Network in the Interchange Format
-// Produced by BayesianNetworks package in JavaBayes
-// Output created Sun Nov 02 17:49:49 GMT+00:00 1997
-// Bayesian network
-network "Dog-Problem" { //5 variables and 5 probability distributions
-        property "credal-set constant-density-bounded 1.1" ;
-}
-variable  "light-on" { //2 values
-        type discrete[2] {  "true"  "false" };
-        property "position = (218, 195)" ;
-}
-variable  "bowel-problem" { //2 values
-        type discrete[2] {  "true"  "false" };
-        property "position = (335, 99)" ;
-}
-variable  "dog-out" { //2 values
-        type discrete[2] {  "true"  "false" };
-        property "position = (300, 195)" ;
-}
-variable  "hear-bark" { //2 values
-        type discrete[2] {  "true"  "false" };
-        property "position = (296, 268)" ;
-}
-variable  "family-out" { //2 values
-        type discrete[2] {  "true"  "false" };
-        property "position = (257, 99)" ;
-}
-probability (  "light-on"  "family-out" ) { //2 variable(s) and 4 values
-        (true) 0.6 0.4 ;
-        (false) 0.05 0.95 ;
-}
-probability (  "bowel-problem" ) { //1 variable(s) and 2 values
-        table 0.01 0.99 ;
-}
-probability (  "dog-out"  "bowel-problem"  "family-out" ) { //3 variable(s) and 8 values
-        table 0.99 0.97 0.9 0.3 0.01 0.03 0.1 0.7 ;
-}
-probability (  "hear-bark"  "dog-out" ) { //2 variable(s) and 4 values
-        table 0.7 0.01 0.3 0.99 ;
-}
-probability (  "family-out" ) { //1 variable(s) and 2 values
-        table 0.15 0.85 ;
-}
-""")
+</NETWORK>
+</BIF>"""
 
-    def test_network_name(self):
 
-        name_expected = 'Dog-Problem'
-        self.assertEqual(self.reader.network_name, name_expected)
+class TestXMLBIFReaderMethods(unittest.TestCase):
+    def setUp(self):
+        self.reader = XMLBIFReader(string=TEST_FILE)
 
     def test_get_variables(self):
-
-        var_expected = ['light-on', 'bowel-problem', 'dog-out',
-                        'hear-bark', 'family-out']
-        self.assertListEqual(self.reader.get_variables(), var_expected)
-
-    def test_states(self):
-
-        states_expected = {'bowel-problem': ['true', 'false'],
-                           'dog-out': ['true', 'false'],
-                           'family-out': ['true', 'false'],
-                           'hear-bark': ['true', 'false'],
-                           'light-on': ['true', 'false']}
-        states = self.reader.get_states()
+        var_expected = [
+            "kid",
+            "light_on",
+            "bowel_problem",
+            "dog_out",
+            "hear_bark",
+            "family_out",
+        ]
+        self.assertListEqual(self.reader.variables, var_expected)
+
+    def test_get_states(self):
+        states_expected = {
+            "bowel_problem": ["true", "false"],
+            "dog_out": ["true", "false"],
+            "family_out": ["true", "false"],
+            "hear_bark": ["true", "false"],
+            "kid": ["true", "false"],
+            "light_on": ["true", "false"],
+        }
+        states = self.reader.variable_states
         for variable in states_expected:
             self.assertListEqual(states_expected[variable], states[variable])
 
-    def test_get_property(self):
+    def test_get_parents(self):
+        parents_expected = {
+            "bowel_problem": [],
+            "dog_out": ["bowel_problem", "family_out"],
+            "family_out": [],
+            "hear_bark": ["dog_out"],
+            "kid": [],
+            "light_on": ["family_out"],
+        }
+        parents = self.reader.variable_parents
+        for variable in parents_expected:
+            self.assertListEqual(parents_expected[variable], parents[variable])
 
-        property_expected = {'bowel-problem': ['position = (335, 99)'],
-                             'dog-out': ['position = (300, 195)'],
-                             'family-out': ['position = (257, 99)'],
-                             'hear-bark': ['position = (296, 268)'],
-                             'light-on': ['position = (218, 195)']}
-        prop = self.reader.get_property()
-        for variable in property_expected:
-            self.assertListEqual(property_expected[variable],
-                                 prop[variable])
+    def test_get_edges(self):
+        edges_expected = [
+            ["family_out", "dog_out"],
+            ["bowel_problem", "dog_out"],
+            ["family_out", "light_on"],
+            ["dog_out", "hear_bark"],
+        ]
+        self.assertListEqual(sorted(self.reader.edge_list), sorted(edges_expected))
 
     def test_get_values(self):
-
-        cpd_expected = {'bowel-problem': np.array([[0.01],
-                                                   [0.99]]),
-                        'dog-out': np.array([[0.99, 0.97, 0.9, 0.3],
-                                             [0.01, 0.03, 0.1, 0.7]]),
-                        'family-out': np.array([[0.15],
-                                                [0.85]]),
-                        'hear-bark': np.array([[0.7, 0.01],
-                                               [0.3, 0.99]]),
-                        'light-on': np.array([[0.6, 0.05],
-                                              [0.4, 0.95]])}
-        cpd = self.reader.variable_cpds
+        cpd_expected = {
+            "bowel_problem": np.array([[0.01], [0.99]]),
+            "dog_out": np.array([[0.99, 0.97, 0.9, 0.3], [0.01, 0.03, 0.1, 0.7]]),
+            "family_out": np.array([[0.15], [0.85]]),
+            "hear_bark": np.array([[0.7, 0.01], [0.3, 0.99]]),
+            "kid": np.array([[0.3], [0.7]]),
+            "light_on": np.array([[0.6, 0.05], [0.4, 0.95]]),
+        }
+        cpd = self.reader.variable_CPD
         for variable in cpd_expected:
-            np_test.assert_array_equal(cpd_expected[variable],
-                                       cpd[variable])
+            np_test.assert_array_equal(cpd_expected[variable], cpd[variable])
 
-    def test_get_parents(self):
+    def test_get_property(self):
+        property_expected = {
+            "bowel_problem": ["position = (190, 69)"],
+            "dog_out": ["position = (155, 165)"],
+            "family_out": ["position = (112, 69)"],
+            "hear_bark": ["position = (154, 241)"],
+            "kid": ["position = (100, 165)"],
+            "light_on": ["position = (73, 165)"],
+        }
+        prop = self.reader.variable_property
+        for variable in property_expected:
+            self.assertListEqual(property_expected[variable], prop[variable])
+
+    def test_model(self):
+        self.reader.get_model().check_model()
+
+    def tearDown(self):
+        del self.reader
 
-        parents_expected = {'bowel-problem': [],
-                            'dog-out': ['bowel-problem', 'family-out'],
-                            'family-out': [],
-                            'hear-bark': ['dog-out'],
-                            'light-on': ['family-out']}
-        parents = self.reader.get_parents()
+
+class TestXMLBIFReaderMethodsFile(unittest.TestCase):
+    def setUp(self):
+        with open("dog_problem.xml", "w") as fout:
+            fout.write(TEST_FILE)
+        self.reader = XMLBIFReader("dog_problem.xml")
+
+    def test_get_variables(self):
+        var_expected = [
+            "kid",
+            "light_on",
+            "bowel_problem",
+            "dog_out",
+            "hear_bark",
+            "family_out",
+        ]
+        self.assertListEqual(self.reader.variables, var_expected)
+
+    def test_get_states(self):
+        states_expected = {
+            "bowel_problem": ["true", "false"],
+            "dog_out": ["true", "false"],
+            "family_out": ["true", "false"],
+            "hear_bark": ["true", "false"],
+            "kid": ["true", "false"],
+            "light_on": ["true", "false"],
+        }
+        states = self.reader.variable_states
+        for variable in states_expected:
+            self.assertListEqual(states_expected[variable], states[variable])
+
+    def test_get_parents(self):
+        parents_expected = {
+            "bowel_problem": [],
+            "dog_out": ["bowel_problem", "family_out"],
+            "family_out": [],
+            "hear_bark": ["dog_out"],
+            "kid": [],
+            "light_on": ["family_out"],
+        }
+        parents = self.reader.variable_parents
         for variable in parents_expected:
-            self.assertListEqual(parents_expected[variable],
-                                 parents[variable])
+            self.assertListEqual(parents_expected[variable], parents[variable])
 
     def test_get_edges(self):
+        edges_expected = [
+            ["family_out", "dog_out"],
+            ["bowel_problem", "dog_out"],
+            ["family_out", "light_on"],
+            ["dog_out", "hear_bark"],
+        ]
+        self.assertListEqual(sorted(self.reader.edge_list), sorted(edges_expected))
 
-        edges_expected = [['family-out', 'dog-out'],
-                          ['bowel-problem', 'dog-out'],
-                          ['family-out', 'light-on'],
-                          ['dog-out', 'hear-bark']]
-        self.assertListEqual(sorted(self.reader.variable_edges),
-                             sorted(edges_expected))
-
-    def test_get_model(self):
-        edges_expected = [('family-out', 'dog-out'),
-                          ('bowel-problem', 'dog-out'),
-                          ('family-out', 'light-on'),
-                          ('dog-out', 'hear-bark')]
-        nodes_expected = ['bowel-problem', 'hear-bark', 'light-on',
-                          'dog-out', 'family-out']
-        edge_expected = {'bowel-problem': {'dog-out': {'weight': None}},
-                         'dog-out': {'hear-bark': {'weight': None}},
-                         'family-out': {'dog-out': {'weight': None},
-                                        'light-on': {'weight': None}},
-                         'hear-bark': {},
-                         'light-on': {}}
-        node_expected = {'bowel-problem': {'weight': None,
-                                           'position': '(335, 99)'},
-                         'dog-out': {'weight': None,
-                                     'position': '(300, 195)'},
-                         'family-out': {'weight': None,
-                                        'position': '(257, 99)'},
-                         'hear-bark': {'weight': None,
-                                       'position': '(296, 268)'},
-                         'light-on': {'weight': None,
-                                      'position': '(218, 195)'}}
-        cpds_expected = [np.array([[0.01],
-                                   [0.99]]),
-                         np.array([[0.99, 0.97, 0.9, 0.3],
-                                   [0.01, 0.03, 0.1, 0.7]]),
-                         np.array([[0.15],
-                                   [0.85]]),
-                         np.array([[0.7, 0.01],
-                                   [0.3, 0.99]]),
-                         np.array([[0.6, 0.05],
-                                   [0.4, 0.95]])]
-        model = self.reader.get_model()
-        for cpd_index in range(0, len(cpds_expected)):
-            np_test.assert_array_equal(model.get_cpds()[cpd_index].get_values(),
-                                       cpds_expected[cpd_index])
-        self.assertDictEqual(model.node, node_expected)
-        self.assertDictEqual(model.edge, edge_expected)
-        self.assertListEqual(sorted(model.nodes()), sorted(nodes_expected))
-        self.assertListEqual(sorted(model.edges()), sorted(edges_expected))
+    def test_get_values(self):
+        cpd_expected = {
+            "bowel_problem": np.array([[0.01], [0.99]]),
+            "dog_out": np.array([[0.99, 0.97, 0.9, 0.3], [0.01, 0.03, 0.1, 0.7]]),
+            "family_out": np.array([[0.15], [0.85]]),
+            "hear_bark": np.array([[0.7, 0.01], [0.3, 0.99]]),
+            "kid": np.array([[0.3], [0.7]]),
+            "light_on": np.array([[0.6, 0.05], [0.4, 0.95]]),
+        }
+        cpd = self.reader.variable_CPD
+        for variable in cpd_expected:
+            np_test.assert_array_equal(cpd_expected[variable], cpd[variable])
+
+    def test_get_property(self):
+        property_expected = {
+            "bowel_problem": ["position = (190, 69)"],
+            "dog_out": ["position = (155, 165)"],
+            "family_out": ["position = (112, 69)"],
+            "hear_bark": ["position = (154, 241)"],
+            "kid": ["position = (100, 165)"],
+            "light_on": ["position = (73, 165)"],
+        }
+        prop = self.reader.variable_property
+        for variable in property_expected:
+            self.assertListEqual(property_expected[variable], prop[variable])
+
+    def test_model(self):
+        self.reader.get_model().check_model()
 
     def tearDown(self):
         del self.reader
+        os.remove("dog_problem.xml")
 
 
-class TestBIFWriter(unittest.TestCase):
-
+class TestXMLBIFWriterMethodsString(unittest.TestCase):
     def setUp(self):
-        variables = ['kid', 'bowel-problem', 'dog-out',
-                     'family-out', 'hear-bark', 'light-on']
-
-        edges = [['family-out', 'dog-out'],
-                 ['bowel-problem', 'dog-out'],
-                 ['family-out', 'light-on'],
-                 ['dog-out', 'hear-bark']]
-
-        cpds = {'kid': np.array([[0.3],
-                                 [0.7]]),
-                'bowel-problem': np.array([[0.01],
-                                           [0.99]]),
-                'dog-out': np.array([[0.99, 0.01, 0.97, 0.03],
-                                     [0.9, 0.1, 0.3, 0.7]]),
-                'family-out': np.array([[0.15],
-                                        [0.85]]),
-                'hear-bark': np.array([[0.7, 0.3],
-                                       [0.01, 0.99]]),
-                'light-on': np.array([[0.6, 0.4],
-                                      [0.05, 0.95]])}
-
-        states = {'kid': ['true', 'false'],
-                  'bowel-problem': ['true', 'false'],
-                  'dog-out': ['true', 'false'],
-                  'family-out': ['true', 'false'],
-                  'hear-bark': ['true', 'false'],
-                  'light-on': ['true', 'false']}
-
-        parents = {'kid': [],
-                   'bowel-problem': [],
-                   'dog-out': ['family-out', 'bowel-problem'],
-                   'family-out': [],
-                   'hear-bark': ['dog-out'],
-                   'light-on': ['family-out']}
-
-        properties = {'kid': ['position = (100, 165)'],
-                      'bowel-problem': ['position = (335, 99)'],
-                      'dog-out': ['position = (300, 195)'],
-                      'family-out': ['position = (257, 99)'],
-                      'hear-bark': ['position = (296, 268)'],
-                      'light-on': ['position = (218, 195)']}
-
-        self.model = BayesianModel()
-        self.model.add_nodes_from(variables)
-        self.model.add_edges_from(edges)
-
-        tabular_cpds = []
-        for var in sorted(cpds.keys()):
-            values = cpds[var]
-            cpd = TabularCPD(var, len(states[var]), values,
-                             evidence=parents[var],
-                             evidence_card=[len(states[evidence_var])
-                                            for evidence_var in parents[var]])
-            tabular_cpds.append(cpd)
-        self.model.add_cpds(*tabular_cpds)
-
-        for node, properties in properties.items():
-            for prop in properties:
-                prop_name, prop_value = map(lambda t: t.strip(), prop.split('='))
-                self.model.node[node][prop_name] = prop_value
-
-        self.writer = BIFWriter(model=self.model)
-
-    def test_str(self):
-        self.expected_string = """network unknown {
-}
-variable bowel-problem {
-    type discrete [ 2 ] { bowel-problem_0, bowel-problem_1 };
-    property position = (335, 99) ;
-    property weight = None ;
-}
-variable dog-out {
-    type discrete [ 2 ] { dog-out_0, dog-out_1 };
-    property position = (300, 195) ;
-    property weight = None ;
-}
-variable family-out {
-    type discrete [ 2 ] { family-out_0, family-out_1 };
-    property position = (257, 99) ;
-    property weight = None ;
-}
-variable hear-bark {
-    type discrete [ 2 ] { hear-bark_0, hear-bark_1 };
-    property position = (296, 268) ;
-    property weight = None ;
-}
-variable kid {
-    type discrete [ 2 ] { kid_0, kid_1 };
-    property position = (100, 165) ;
-    property weight = None ;
-}
-variable light-on {
-    type discrete [ 2 ] { light-on_0, light-on_1 };
-    property position = (218, 195) ;
-    property weight = None ;
-}
-probability ( bowel-problem ) {
-    table 0.01, 0.99 ;
-}
-probability ( dog-out | bowel-problem, family-out ) {
-    table 0.99, 0.01, 0.97, 0.03, 0.9, 0.1, 0.3, 0.7 ;
-}
-probability ( family-out ) {
-    table 0.15, 0.85 ;
-}
-probability ( hear-bark | dog-out ) {
-    table 0.7, 0.3, 0.01, 0.99 ;
-}
-probability ( kid ) {
-    table 0.3, 0.7 ;
-}
-probability ( light-on | family-out ) {
-    table 0.6, 0.4, 0.05, 0.95 ;
-}
-"""
-        self.maxDiff = None
-        self.assertEqual(self.writer.__str__(), self.expected_string)
+        reader = XMLBIFReader(string=TEST_FILE)
+        self.expected_model = reader.get_model()
+        self.writer = XMLBIFWriter(self.expected_model)
+
+        self.model_stateless = BayesianModel(
+            [("D", "G"), ("I", "G"), ("G", "L"), ("I", "S")]
+        )
+        self.cpd_d = TabularCPD(variable="D", variable_card=2, values=[[0.6, 0.4]])
+        self.cpd_i = TabularCPD(variable="I", variable_card=2, values=[[0.7, 0.3]])
+
+        self.cpd_g = TabularCPD(
+            variable="G",
+            variable_card=3,
+            values=[
+                [0.3, 0.05, 0.9, 0.5],
+                [0.4, 0.25, 0.08, 0.3],
+                [0.3, 0.7, 0.02, 0.2],
+            ],
+            evidence=["I", "D"],
+            evidence_card=[2, 2],
+        )
+
+        self.cpd_l = TabularCPD(
+            variable="L",
+            variable_card=2,
+            values=[[0.1, 0.4, 0.99], [0.9, 0.6, 0.01]],
+            evidence=["G"],
+            evidence_card=[3],
+        )
+
+        self.cpd_s = TabularCPD(
+            variable="S",
+            variable_card=2,
+            values=[[0.95, 0.2], [0.05, 0.8]],
+            evidence=["I"],
+            evidence_card=[2],
+        )
+
+        self.model_stateless.add_cpds(
+            self.cpd_d, self.cpd_i, self.cpd_g, self.cpd_l, self.cpd_s
+        )
+        self.writer_stateless = XMLBIFWriter(self.model_stateless)
+
+    def test_write_xmlbif_statefull(self):
+        self.writer.write_xmlbif("dog_problem_output.xbif")
+        with open("dog_problem_output.xbif", "r") as f:
+            file_text = f.read()
+        reader = XMLBIFReader(string=file_text)
+        model = reader.get_model()
+        self.assert_models_equivelent(self.expected_model, model)
+        os.remove("dog_problem_output.xbif")
+
+    def test_write_xmlbif_stateless(self):
+        self.writer_stateless.write_xmlbif("grade_problem_output.xbif")
+        with open("grade_problem_output.xbif", "r") as f:
+            reader = XMLBIFReader(f)
+        model = reader.get_model()
+        self.assert_models_equivelent(self.model_stateless, model)
+        self.assertDictEqual(
+            {
+                "G": ["state0", "state1", "state2"],
+                "I": ["state0", "state1"],
+                "D": ["state0", "state1"],
+                "S": ["state0", "state1"],
+                "L": ["state0", "state1"],
+            },
+            model.get_cpds("D").state_names,
+        )
+        os.remove("grade_problem_output.xbif")
+
+    def assert_models_equivelent(self, expected, got):
+        self.assertSetEqual(set(expected.nodes()), set(got.nodes()))
+        for node in expected.nodes():
+            self.assertListEqual(
+                list(expected.get_parents(node)), list(got.get_parents(node))
+            )
+            cpds_expected = expected.get_cpds(node=node)
+            cpds_got = got.get_cpds(node=node)
+            np_test.assert_array_equal(cpds_expected.values, cpds_got.values)
```

### Comparing `pgmpy-0.1.7/pgmpy/tests/test_readwrite/test_ProbModelXML.py` & `pgmpy-0.1.9/pgmpy/tests/test_readwrite/test_ProbModelXML.py`

 * *Files 12% similar despite different names*

```diff
@@ -4,14 +4,15 @@
 import os
 import unittest
 import warnings
 import json
 
 import numpy as np
 import numpy.testing as np_test
+import networkx as nx
 
 from pgmpy.readwrite import ProbModelXMLReader, ProbModelXMLWriter, get_probmodel_data
 from pgmpy.models import BayesianModel
 from pgmpy.factors.discrete import TabularCPD
 from pgmpy.extern.six.moves import range
 from pgmpy.extern import six
 
@@ -19,14 +20,15 @@
     from lxml import etree
 except ImportError:
     try:
         import xml.etree.cElementTree as etree
     except ImportError:
         try:
             import xml.etree.ElementTree as etree
+
             print("running with ElementTree on Python 2.5+")
         except ImportError:
             warnings.warn("Failed to import ElementTree from any known place")
 
 
 class TestProbModelXMLReaderString(unittest.TestCase):
     def setUp(self):
@@ -209,128 +211,193 @@
 </ProbModelXML>
 """
         self.maxDiff = None
         self.reader_string = ProbModelXMLReader(string=string)
         self.reader_file = ProbModelXMLReader(path=six.StringIO(string))
 
     def test_comment(self):
-        comment_expected = ("Student example model from Probabilistic Graphical Models: "
-                            "Principles and Techniques by Daphne Koller")
-        self.assertEqual(self.reader_string.probnet['Comment'], comment_expected)
-        self.assertEqual(self.reader_file.probnet['Comment'], comment_expected)
+        comment_expected = (
+            "Student example model from Probabilistic Graphical Models: "
+            "Principles and Techniques by Daphne Koller"
+        )
+        self.assertEqual(self.reader_string.probnet["Comment"], comment_expected)
+        self.assertEqual(self.reader_file.probnet["Comment"], comment_expected)
 
     def test_variables(self):
-        variables = {'difficulty':
-                     {'Comment': None,
-                      'Coordinates': {},
-                      'role': 'Chance',
-                      'type': 'FiniteState',
-                      'States': {'difficult': {}, 'easy': {}}},
-                     'intelligence':
-                     {'Comment': None,
-                      'Coordinates': {},
-                      'role': 'Chance',
-                      'type': 'FiniteState',
-                      'States': {'smart': {}, 'dumb': {}}}}
-        self.assertDictEqual(self.reader_string.probnet['Variables'], variables)
-        self.assertDictEqual(self.reader_file.probnet['Variables'], variables)
+        variables = {
+            "difficulty": {
+                "Comment": None,
+                "Coordinates": {},
+                "role": "Chance",
+                "type": "FiniteState",
+                "States": {"difficult": {}, "easy": {}},
+            },
+            "intelligence": {
+                "Comment": None,
+                "Coordinates": {},
+                "role": "Chance",
+                "type": "FiniteState",
+                "States": {"smart": {}, "dumb": {}},
+            },
+        }
+        self.assertDictEqual(self.reader_string.probnet["Variables"], variables)
+        self.assertDictEqual(self.reader_file.probnet["Variables"], variables)
 
     def test_edges(self):
-        edge = {('grade', 'recommendation_letter'):
-                {'directed': '1',
-                 'Comment': 'Directed Edge from grade to recommendation_letter',
-                 'Label': 'grad_to_reco'},
-                ('intelligence', 'grade'):
-                {'directed': '1',
-                 'Comment': 'Directed Edge from intelligence to grade',
-                 'Label': 'intel_to_grad'},
-                ('difficulty', 'grade'):
-                {'directed': '1',
-                 'Comment': 'Directed Edge from difficulty to grade',
-                 'Label': 'diff_to_grad'},
-                ('intelligence', 'SAT'):
-                {'directed': '1',
-                 'Comment': 'Directed Edge from intelligence to SAT',
-                 'Label': 'intel_to_sat'}}
-        self.assertDictEqual(self.reader_string.probnet['edges'], edge)
-        self.assertDictEqual(self.reader_file.probnet['edges'], edge)
+        edge = {
+            ("grade", "recommendation_letter"): {
+                "directed": "1",
+                "Comment": "Directed Edge from grade to recommendation_letter",
+                "Label": "grad_to_reco",
+            },
+            ("intelligence", "grade"): {
+                "directed": "1",
+                "Comment": "Directed Edge from intelligence to grade",
+                "Label": "intel_to_grad",
+            },
+            ("difficulty", "grade"): {
+                "directed": "1",
+                "Comment": "Directed Edge from difficulty to grade",
+                "Label": "diff_to_grad",
+            },
+            ("intelligence", "SAT"): {
+                "directed": "1",
+                "Comment": "Directed Edge from intelligence to SAT",
+                "Label": "intel_to_sat",
+            },
+        }
+        self.assertDictEqual(self.reader_string.probnet["edges"], edge)
+        self.assertDictEqual(self.reader_file.probnet["edges"], edge)
 
     def test_additionalconstraints(self):
-        additionalconstraints_expected = {'MaxNumParents':
-                                          {'numParents': '5'}}
-        self.assertDictEqual(self.reader_string.probnet['AdditionalConstraints'],
-                             additionalconstraints_expected)
-        self.assertDictEqual(self.reader_file.probnet['AdditionalConstraints'],
-                             additionalconstraints_expected)
+        additionalconstraints_expected = {"MaxNumParents": {"numParents": "5"}}
+        self.assertDictEqual(
+            self.reader_string.probnet["AdditionalConstraints"],
+            additionalconstraints_expected,
+        )
+        self.assertDictEqual(
+            self.reader_file.probnet["AdditionalConstraints"],
+            additionalconstraints_expected,
+        )
 
     def test_additionalproperties(self):
-        additionalproperties_expected = {'elvira.title': 'X ray result'}
-        self.assertDictEqual(self.reader_string.probnet['AdditionalProperties'],
-                             additionalproperties_expected)
-        self.assertDictEqual(self.reader_file.probnet['AdditionalProperties'],
-                             additionalproperties_expected)
+        additionalproperties_expected = {"elvira.title": "X ray result"}
+        self.assertDictEqual(
+            self.reader_string.probnet["AdditionalProperties"],
+            additionalproperties_expected,
+        )
+        self.assertDictEqual(
+            self.reader_file.probnet["AdditionalProperties"],
+            additionalproperties_expected,
+        )
 
     def test_decisioncriteria(self):
-        decisioncriteria_expected = {'effectiveness': {},
-                                     'cost': {}}
-        self.assertDictEqual(self.reader_string.probnet['DecisionCriteria'],
-                             decisioncriteria_expected)
-        self.assertDictEqual(self.reader_file.probnet['DecisionCriteria'],
-                             decisioncriteria_expected)
+        decisioncriteria_expected = {"effectiveness": {}, "cost": {}}
+        self.assertDictEqual(
+            self.reader_string.probnet["DecisionCriteria"], decisioncriteria_expected
+        )
+        self.assertDictEqual(
+            self.reader_file.probnet["DecisionCriteria"], decisioncriteria_expected
+        )
 
     def test_potential(self):
-        potential_expected = [{
-            'role': 'Utility',
-            'Variables': {'D0': ['D1', 'C0', 'C1']},
-            'type': 'Tree/ADD',
-            'UtilityVaribale': 'U1',
-            'Branches': [{
-                'Potential': {
-                    'type': 'Tree/ADD',
-                    'Branches': [{'Thresholds': [{'value': u'–Infinity'},
-                                                 {'value': '0', 'belongsTo': 'Left'}],
-                                  'Potential': {'Subpotentials': [{'Potential': {'type': 'Table',
-                                                                                 'Values': '3'},
-                                                                   'type': 'Exponential'},
-                                                                  {'NumericVariables': ['C0', 'C1'],
-                                                                   'Potential': {'type': 'Table',
-                                                                                 'Values': u'–1'},
-                                                                   'Coefficients': u'4 –1',
-                                                                   'type': 'Exponential'}],
-                                                'Variables': {'C0': ['C1']},
-                                                'type': 'MixtureOfExponentials'}},
-                                 {'Thresholds': [{'value': '0', 'belongsTo': 'Left'},
-                                                 {'value': '+Infinity'}],
-                                  'Potential': {'Subpotentials': [{'NumericVariables': ['C1'],
-                                                                   'Potential': {'Variables': {'D1': []},
-                                                                                 'type': 'Table',
-                                                                                 'Values': '10  5'},
-                                                                   'Coefficients': '0.25',
-                                                                   'type': 'Exponential'}],
-                                                'Variables': {'C1': ['D1']},
-                                                'type': 'MixtureOfExponentials'}}],
-                    'TopVariable': 'C1'},
-                'States': [{'name': 'no'}]},
-                {'Potential': {'Subpotentials': [{'NumericVariables': ['C0'],
-                                                  'Potential': {'type': 'Table',
-                                                                'Values': '0.3'},
-                                                  'Coefficients': '1',
-                                                  'type': 'Exponential'},
-                                                 {'Potential': {'type': 'Table',
-                                                                'Values': '0.7'},
-                                                  'type': 'Exponential'}],
-                               'Variables': {'C0': []},
-                               'type': 'MixtureOfExponentials'},
-                 'States': [{'name': 'yes'}]}],
-            'TopVariable': 'D0'}]
-
-        self.assertListEqual(self.reader_string.probnet['Potentials'],
-                             potential_expected)
-        self.assertListEqual(self.reader_file.probnet['Potentials'],
-                             potential_expected)
+        potential_expected = [
+            {
+                "role": "Utility",
+                "Variables": {"D0": ["D1", "C0", "C1"]},
+                "type": "Tree/ADD",
+                "UtilityVaribale": "U1",
+                "Branches": [
+                    {
+                        "Potential": {
+                            "type": "Tree/ADD",
+                            "Branches": [
+                                {
+                                    "Thresholds": [
+                                        {"value": u"–Infinity"},
+                                        {"value": "0", "belongsTo": "Left"},
+                                    ],
+                                    "Potential": {
+                                        "Subpotentials": [
+                                            {
+                                                "Potential": {
+                                                    "type": "Table",
+                                                    "Values": "3",
+                                                },
+                                                "type": "Exponential",
+                                            },
+                                            {
+                                                "NumericVariables": ["C0", "C1"],
+                                                "Potential": {
+                                                    "type": "Table",
+                                                    "Values": u"–1",
+                                                },
+                                                "Coefficients": u"4 –1",
+                                                "type": "Exponential",
+                                            },
+                                        ],
+                                        "Variables": {"C0": ["C1"]},
+                                        "type": "MixtureOfExponentials",
+                                    },
+                                },
+                                {
+                                    "Thresholds": [
+                                        {"value": "0", "belongsTo": "Left"},
+                                        {"value": "+Infinity"},
+                                    ],
+                                    "Potential": {
+                                        "Subpotentials": [
+                                            {
+                                                "NumericVariables": ["C1"],
+                                                "Potential": {
+                                                    "Variables": {"D1": []},
+                                                    "type": "Table",
+                                                    "Values": "10  5",
+                                                },
+                                                "Coefficients": "0.25",
+                                                "type": "Exponential",
+                                            }
+                                        ],
+                                        "Variables": {"C1": ["D1"]},
+                                        "type": "MixtureOfExponentials",
+                                    },
+                                },
+                            ],
+                            "TopVariable": "C1",
+                        },
+                        "States": [{"name": "no"}],
+                    },
+                    {
+                        "Potential": {
+                            "Subpotentials": [
+                                {
+                                    "NumericVariables": ["C0"],
+                                    "Potential": {"type": "Table", "Values": "0.3"},
+                                    "Coefficients": "1",
+                                    "type": "Exponential",
+                                },
+                                {
+                                    "Potential": {"type": "Table", "Values": "0.7"},
+                                    "type": "Exponential",
+                                },
+                            ],
+                            "Variables": {"C0": []},
+                            "type": "MixtureOfExponentials",
+                        },
+                        "States": [{"name": "yes"}],
+                    },
+                ],
+                "TopVariable": "D0",
+            }
+        ]
+
+        self.assertListEqual(
+            self.reader_string.probnet["Potentials"], potential_expected
+        )
+        self.assertListEqual(self.reader_file.probnet["Potentials"], potential_expected)
 
     def test_get_model(self):
         string = """<ProbModelXML formatVersion="0.2.0">
   <ProbNet type="BayesianNetwork">
     <Comment>Student example model from Probabilistic Graphical Models: Principles and Techniques by Daphne Koller</Comment>
     <Variables>
       <Variable name="X-ray" type="finiteStates" role="chance" isInput="false">
@@ -531,199 +598,290 @@
       <Property name="WhoChanged" value="Jose A. Gamez" />
     </AdditionalProperties>
   </ProbNet>
 </ProbModelXML>"""
         self.maxDiff = None
         self.reader = ProbModelXMLReader(string=string)
         model = self.reader.get_model()
-        edges_expected = [('VisitToAsia', 'Tuberculosis'),
-                          ('LungCancer', 'TuberculosisOrCancer'),
-                          ('Smoker', 'LungCancer'),
-                          ('Smoker', 'Bronchitis'),
-                          ('Tuberculosis', 'TuberculosisOrCancer'),
-                          ('Bronchitis', 'Dyspnea'),
-                          ('TuberculosisOrCancer', 'Dyspnea'),
-                          ('TuberculosisOrCancer', 'X-ray')]
-        node_expected = {'Smoker': {'States': {'no': {}, 'yes': {}},
-                                    'role': 'chance',
-                                    'type': 'finiteStates',
-                                    'Coordinates': {'y': '52', 'x': '568'},
-                                    'AdditionalProperties': {'Title': 'S', 'Relevance': '7.0'},
-                                    'weight': None},
-                         'Bronchitis': {'States': {'no': {}, 'yes': {}},
-                                        'role': 'chance',
-                                        'type': 'finiteStates',
-                                        'Coordinates': {'y': '181', 'x': '698'},
-                                        'AdditionalProperties': {'Title': 'B', 'Relevance': '7.0'},
-                                        'weight': None},
-                         'VisitToAsia': {'States': {'no': {}, 'yes': {}},
-                                         'role': 'chance',
-                                         'type': 'finiteStates',
-                                         'Coordinates': {'y': '58', 'x': '290'},
-                                         'AdditionalProperties': {'Title': 'A', 'Relevance': '7.0'},
-                                         'weight': None},
-                         'Tuberculosis': {'States': {'no': {}, 'yes': {}},
-                                          'role': 'chance',
-                                          'type': 'finiteStates',
-                                          'Coordinates': {'y': '150', 'x': '201'},
-                                          'AdditionalProperties': {'Title': 'T', 'Relevance': '7.0'},
-                                          'weight': None},
-                         'X-ray': {'States': {'no': {}, 'yes': {}},
-                                   'role': 'chance',
-                                   'AdditionalProperties': {'Title': 'X', 'Relevance': '7.0'},
-                                   'Coordinates': {'y': '322', 'x': '252'},
-                                   'Comment': 'Indica si el test de rayos X ha sido positivo',
-                                   'type': 'finiteStates',
-                                   'weight': None},
-                         'Dyspnea': {'States': {'no': {}, 'yes': {}},
-                                     'role': 'chance',
-                                     'type': 'finiteStates',
-                                     'Coordinates': {'y': '321', 'x': '533'},
-                                     'AdditionalProperties': {'Title': 'D', 'Relevance': '7.0'},
-                                     'weight': None},
-                         'TuberculosisOrCancer': {'States': {'no': {}, 'yes': {}},
-                                                  'role': 'chance',
-                                                  'type': 'finiteStates',
-                                                  'Coordinates': {'y': '238', 'x': '336'},
-                                                  'AdditionalProperties': {'Title': 'E', 'Relevance': '7.0'},
-                                                  'weight': None},
-                         'LungCancer': {'States': {'no': {}, 'yes': {}},
-                                        'role': 'chance',
-                                        'type': 'finiteStates',
-                                        'Coordinates': {'y': '152', 'x': '421'},
-                                        'AdditionalProperties': {'Title': 'L', 'Relevance': '7.0'},
-                                        'weight': None}}
-        edge_expected = {'LungCancer': {'TuberculosisOrCancer': {'weight': None,
-                                                                 'directed': 'true'}},
-                         'Smoker': {'LungCancer': {'weight': None,
-                                                   'directed': 'true'},
-                                    'Bronchitis': {'weight': None,
-                                                   'directed': 'true'}},
-                         'Dyspnea': {},
-                         'X-ray': {},
-                         'VisitToAsia': {'Tuberculosis': {'weight': None,
-                                                          'directed': 'true'}},
-                         'TuberculosisOrCancer': {'X-ray': {'weight': None,
-                                                            'directed': 'true'},
-                                                  'Dyspnea': {'weight': None,
-                                                              'directed': 'true'}},
-                         'Bronchitis': {'Dyspnea': {'weight': None,
-                                                    'directed': 'true'}},
-                         'Tuberculosis': {'TuberculosisOrCancer': {'weight': None,
-                                                                   'directed': 'true'}}}
-
-        cpds_expected = [np.array([[0.95, 0.05], [0.02, 0.98]]),
-                         np.array([[0.7, 0.3], [0.4,  0.6]]),
-                         np.array([[0.9, 0.1,  0.3,  0.7], [0.2,  0.8,  0.1,  0.9]]),
-                         np.array([[0.99], [0.01]]),
-                         np.array([[0.5], [0.5]]),
-                         np.array([[0.99, 0.01], [0.9, 0.1]]),
-                         np.array([[0.99, 0.01], [0.95, 0.05]]),
-                         np.array([[1, 0, 0, 1], [0, 1, 0, 1]])]
+        edges_expected = [
+            ("VisitToAsia", "Tuberculosis"),
+            ("LungCancer", "TuberculosisOrCancer"),
+            ("Smoker", "LungCancer"),
+            ("Smoker", "Bronchitis"),
+            ("Tuberculosis", "TuberculosisOrCancer"),
+            ("Bronchitis", "Dyspnea"),
+            ("TuberculosisOrCancer", "Dyspnea"),
+            ("TuberculosisOrCancer", "X-ray"),
+        ]
+        node_expected = {
+            "Smoker": {
+                "States": {"no": {}, "yes": {}},
+                "role": "chance",
+                "type": "finiteStates",
+                "Coordinates": {"y": "52", "x": "568"},
+                "AdditionalProperties": {"Title": "S", "Relevance": "7.0"},
+                "weight": None,
+            },
+            "Bronchitis": {
+                "States": {"no": {}, "yes": {}},
+                "role": "chance",
+                "type": "finiteStates",
+                "Coordinates": {"y": "181", "x": "698"},
+                "AdditionalProperties": {"Title": "B", "Relevance": "7.0"},
+                "weight": None,
+            },
+            "VisitToAsia": {
+                "States": {"no": {}, "yes": {}},
+                "role": "chance",
+                "type": "finiteStates",
+                "Coordinates": {"y": "58", "x": "290"},
+                "AdditionalProperties": {"Title": "A", "Relevance": "7.0"},
+                "weight": None,
+            },
+            "Tuberculosis": {
+                "States": {"no": {}, "yes": {}},
+                "role": "chance",
+                "type": "finiteStates",
+                "Coordinates": {"y": "150", "x": "201"},
+                "AdditionalProperties": {"Title": "T", "Relevance": "7.0"},
+                "weight": None,
+            },
+            "X-ray": {
+                "States": {"no": {}, "yes": {}},
+                "role": "chance",
+                "AdditionalProperties": {"Title": "X", "Relevance": "7.0"},
+                "Coordinates": {"y": "322", "x": "252"},
+                "Comment": "Indica si el test de rayos X ha sido positivo",
+                "type": "finiteStates",
+                "weight": None,
+            },
+            "Dyspnea": {
+                "States": {"no": {}, "yes": {}},
+                "role": "chance",
+                "type": "finiteStates",
+                "Coordinates": {"y": "321", "x": "533"},
+                "AdditionalProperties": {"Title": "D", "Relevance": "7.0"},
+                "weight": None,
+            },
+            "TuberculosisOrCancer": {
+                "States": {"no": {}, "yes": {}},
+                "role": "chance",
+                "type": "finiteStates",
+                "Coordinates": {"y": "238", "x": "336"},
+                "AdditionalProperties": {"Title": "E", "Relevance": "7.0"},
+                "weight": None,
+            },
+            "LungCancer": {
+                "States": {"no": {}, "yes": {}},
+                "role": "chance",
+                "type": "finiteStates",
+                "Coordinates": {"y": "152", "x": "421"},
+                "AdditionalProperties": {"Title": "L", "Relevance": "7.0"},
+                "weight": None,
+            },
+        }
+        edge_expected = {
+            "LungCancer": {
+                "TuberculosisOrCancer": {"weight": None, "directed": "true"}
+            },
+            "Smoker": {
+                "LungCancer": {"weight": None, "directed": "true"},
+                "Bronchitis": {"weight": None, "directed": "true"},
+            },
+            "Dyspnea": {},
+            "X-ray": {},
+            "VisitToAsia": {"Tuberculosis": {"weight": None, "directed": "true"}},
+            "TuberculosisOrCancer": {
+                "X-ray": {"weight": None, "directed": "true"},
+                "Dyspnea": {"weight": None, "directed": "true"},
+            },
+            "Bronchitis": {"Dyspnea": {"weight": None, "directed": "true"}},
+            "Tuberculosis": {
+                "TuberculosisOrCancer": {"weight": None, "directed": "true"}
+            },
+        }
+
+        cpds_expected = [
+            np.array([[0.95, 0.05], [0.02, 0.98]]),
+            np.array([[0.7, 0.3], [0.4, 0.6]]),
+            np.array([[0.9, 0.1, 0.3, 0.7], [0.2, 0.8, 0.1, 0.9]]),
+            np.array([[0.99], [0.01]]),
+            np.array([[0.5], [0.5]]),
+            np.array([[0.99, 0.01], [0.9, 0.1]]),
+            np.array([[0.99, 0.01], [0.95, 0.05]]),
+            np.array([[1, 0, 0, 1], [0, 1, 0, 1]]),
+        ]
         for cpd_index in range(0, len(cpds_expected)):
-            np_test.assert_array_equal(model.get_cpds()[cpd_index].get_values(),
-                                       cpds_expected[cpd_index])
-        self.assertDictEqual(model.node, node_expected)
-        self.assertDictEqual(model.edge, edge_expected)
+            np_test.assert_array_equal(
+                model.get_cpds()[cpd_index].get_values(), cpds_expected[cpd_index]
+            )
+        self.assertDictEqual(dict(model.nodes), node_expected)
+        if nx.__version__.startswith("1"):
+            self.assertDictEqual(model.edge, edge_expected)
+        else:
+            self.assertDictEqual(dict(model.adj), edge_expected)
         self.assertListEqual(sorted(model.edges()), sorted(edges_expected))
 
 
 class TestProbModelXMLWriter(unittest.TestCase):
     def setUp(self):
-        self.model_data = {'probnet':
-                           {'type': 'BayesianNetwork',
-                            'Language': 'English',
-                            'AdditionalConstraints': {'MaxNumParents':
-                                                      {'numParents': '5'}},
-                            'AdditionalProperties': {'elvira.title': 'X ray result'},
-                            'DecisionCriteria': {'effectiveness': {},
-                                                 'cost': {}},
-                            'Variables': {'difficulty':
-                                          {'type': 'FiniteState',
-                                           'role': 'Chance',
-                                           'States': {'difficult': {}, 'easy': {}},
-                                           'Comment': None,
-                                           'Coordinates': {}},
-                                          'intelligence':
-                                          {'type': 'FiniteState',
-                                           'role': 'Chance',
-                                           'States': {'smart': {}, 'dumb': {}},
-                                           'Comment': None,
-                                           'Coordinates': {}}},
-                            'Comment': 'Student example model from Probabilistic Graphical Models: '
-                                       'Principles and Techniques by Daphne Koller',
-                            'edges': {('difficulty', 'grade'):
-                                      {'directed': '1',
-                                       'Label': 'diff_to_grad',
-                                       'Comment': 'Directed Edge from difficulty to grade'},
-                                      ('intelligence', 'grade'):
-                                      {'directed': '1',
-                                       'Label': 'intel_to_grad',
-                                       'Comment': 'Directed Edge from intelligence to grade'},
-                                      ('intelligence', 'SAT'):
-                                      {'directed': '1',
-                                       'Label': 'intel_to_sat',
-                                       'Comment': 'Directed Edge from intelligence to SAT'},
-                                      ('grade', 'recommendation_letter'):
-                                      {'directed': '1',
-                                       'Label': 'grad_to_reco',
-                                       'Comment': 'Directed Edge from grade to recommendation_letter'}},
-                            'Potentials': [{'role': 'Utility',
-                                            'Variables': {'D0': ['D1', 'C0', 'C1']},
-                                            'type': 'Tree/ADD',
-                                            'UtilityVaribale': 'U1',
-                                            'Branches': [{
-                                                'Potential': {
-                                                    'type': 'Tree/ADD',
-                                                    'Branches': [{
-                                                        'Thresholds': [{'value': '-Infinity'},
-                                                                       {'value': '0', 'belongsTo': 'Left'}],
-                                                        'Potential': {'Subpotentials': [
-                                                            {'Potential': {'type': 'Table',
-                                                                           'Values': '3'},
-                                                             'type': 'Exponential'},
-                                                            {'NumericVariables': ['C0', 'C1'],
-                                                             'Potential': {'type': 'Table',
-                                                                           'Values': '-1'},
-                                                             'Coefficients': '4 -1',
-                                                             'type': 'Exponential'}],
-                                                            'Variables': {'C0': ['C1']},
-                                                            'type': 'MixtureOfExponentials'}},
-                                                        {'Thresholds': [{'value': '0', 'belongsTo': 'Left'},
-                                                                        {'value': '+Infinity'}],
-                                                         'Potential': {'Subpotentials': [
-                                                             {'NumericVariables': ['C1'],
-                                                              'Potential': {'Variables': {'D1': []},
-                                                                            'type': 'Table',
-                                                                            'Values': '10  5'},
-                                                              'Coefficients': '0.25',
-                                                              'type': 'Exponential'}],
-                                                             'Variables': {'C1': ['D1']},
-                                                             'type': 'MixtureOfExponentials'}}],
-                                                    'TopVariable': 'C1'},
-                                                'States': [{'name': 'no'}]},
-                                                {'Potential': {'Subpotentials': [
-                                                    {'NumericVariables': ['C0'],
-                                                     'Potential': {'type': 'Table',
-                                                                   'Values': '0.3'},
-                                                     'Coefficients': '1',
-                                                     'type': 'Exponential'},
-                                                    {'Potential': {'type': 'Table',
-                                                                   'Values': '0.7'},
-                                                     'type': 'Exponential'}],
-                                                    'Variables': {'C0': []},
-                                                    'type': 'MixtureOfExponentials'},
-                                                    'States': [{'name': 'yes'}]}],
-                                            'TopVariable': 'D0'}]}}
+        self.model_data = {
+            "probnet": {
+                "type": "BayesianNetwork",
+                "Language": "English",
+                "AdditionalConstraints": {"MaxNumParents": {"numParents": "5"}},
+                "AdditionalProperties": {"elvira.title": "X ray result"},
+                "DecisionCriteria": {"effectiveness": {}, "cost": {}},
+                "Variables": {
+                    "difficulty": {
+                        "type": "FiniteState",
+                        "role": "Chance",
+                        "States": {"difficult": {}, "easy": {}},
+                        "Comment": None,
+                        "Coordinates": {},
+                    },
+                    "intelligence": {
+                        "type": "FiniteState",
+                        "role": "Chance",
+                        "States": {"smart": {}, "dumb": {}},
+                        "Comment": None,
+                        "Coordinates": {},
+                    },
+                },
+                "Comment": "Student example model from Probabilistic Graphical Models: "
+                "Principles and Techniques by Daphne Koller",
+                "edges": {
+                    ("difficulty", "grade"): {
+                        "directed": "1",
+                        "Label": "diff_to_grad",
+                        "Comment": "Directed Edge from difficulty to grade",
+                    },
+                    ("intelligence", "grade"): {
+                        "directed": "1",
+                        "Label": "intel_to_grad",
+                        "Comment": "Directed Edge from intelligence to grade",
+                    },
+                    ("intelligence", "SAT"): {
+                        "directed": "1",
+                        "Label": "intel_to_sat",
+                        "Comment": "Directed Edge from intelligence to SAT",
+                    },
+                    ("grade", "recommendation_letter"): {
+                        "directed": "1",
+                        "Label": "grad_to_reco",
+                        "Comment": "Directed Edge from grade to recommendation_letter",
+                    },
+                },
+                "Potentials": [
+                    {
+                        "role": "Utility",
+                        "Variables": {"D0": ["D1", "C0", "C1"]},
+                        "type": "Tree/ADD",
+                        "UtilityVaribale": "U1",
+                        "Branches": [
+                            {
+                                "Potential": {
+                                    "type": "Tree/ADD",
+                                    "Branches": [
+                                        {
+                                            "Thresholds": [
+                                                {"value": "-Infinity"},
+                                                {"value": "0", "belongsTo": "Left"},
+                                            ],
+                                            "Potential": {
+                                                "Subpotentials": [
+                                                    {
+                                                        "Potential": {
+                                                            "type": "Table",
+                                                            "Values": "3",
+                                                        },
+                                                        "type": "Exponential",
+                                                    },
+                                                    {
+                                                        "NumericVariables": [
+                                                            "C0",
+                                                            "C1",
+                                                        ],
+                                                        "Potential": {
+                                                            "type": "Table",
+                                                            "Values": "-1",
+                                                        },
+                                                        "Coefficients": "4 -1",
+                                                        "type": "Exponential",
+                                                    },
+                                                ],
+                                                "Variables": {"C0": ["C1"]},
+                                                "type": "MixtureOfExponentials",
+                                            },
+                                        },
+                                        {
+                                            "Thresholds": [
+                                                {"value": "0", "belongsTo": "Left"},
+                                                {"value": "+Infinity"},
+                                            ],
+                                            "Potential": {
+                                                "Subpotentials": [
+                                                    {
+                                                        "NumericVariables": ["C1"],
+                                                        "Potential": {
+                                                            "Variables": {"D1": []},
+                                                            "type": "Table",
+                                                            "Values": "10  5",
+                                                        },
+                                                        "Coefficients": "0.25",
+                                                        "type": "Exponential",
+                                                    }
+                                                ],
+                                                "Variables": {"C1": ["D1"]},
+                                                "type": "MixtureOfExponentials",
+                                            },
+                                        },
+                                    ],
+                                    "TopVariable": "C1",
+                                },
+                                "States": [{"name": "no"}],
+                            },
+                            {
+                                "Potential": {
+                                    "Subpotentials": [
+                                        {
+                                            "NumericVariables": ["C0"],
+                                            "Potential": {
+                                                "type": "Table",
+                                                "Values": "0.3",
+                                            },
+                                            "Coefficients": "1",
+                                            "type": "Exponential",
+                                        },
+                                        {
+                                            "Potential": {
+                                                "type": "Table",
+                                                "Values": "0.7",
+                                            },
+                                            "type": "Exponential",
+                                        },
+                                    ],
+                                    "Variables": {"C0": []},
+                                    "type": "MixtureOfExponentials",
+                                },
+                                "States": [{"name": "yes"}],
+                            },
+                        ],
+                        "TopVariable": "D0",
+                    }
+                ],
+            }
+        }
 
         self.maxDiff = None
         self.writer = ProbModelXMLWriter(model_data=self.model_data)
 
     def test_file(self):
-        self.expected_xml = etree.XML("""<ProbModelXML formatVersion="1.0">
+        self.expected_xml = etree.XML(
+            """<ProbModelXML formatVersion="1.0">
   <ProbNet type="BayesianNetwork">
     <Variables>
       <Variable name="difficulty" role="Chance" type="FiniteState">
         <Comment/>
         <Coordinates/>
         <AdditionalProperties/>
         <States>
@@ -890,19 +1048,23 @@
     <Criterion name="cost">
       <AdditionalProperties/>
     </Criterion>
     <Criterion name="effectiveness">
       <AdditionalProperties/>
     </Criterion>
   </DecisionCriteria>
-</ProbModelXML>""")
-        self.assertEqual(str(self.writer.__str__()[:-1]), str(etree.tostring(self.expected_xml)))
+</ProbModelXML>"""
+        )
+        self.assertEqual(
+            str(self.writer.__str__()[:-1]), str(etree.tostring(self.expected_xml))
+        )
 
     def test_write_file(self):
-        self.expected_xml = etree.XML("""<ProbModelXML formatVersion="1.0">
+        self.expected_xml = etree.XML(
+            """<ProbModelXML formatVersion="1.0">
   <ProbNet type="BayesianNetwork">
     <Variables>
       <Variable name="difficulty" role="Chance" type="FiniteState">
         <Comment/>
         <Coordinates/>
         <AdditionalProperties/>
         <States>
@@ -1069,143 +1231,208 @@
     <Criterion name="cost">
       <AdditionalProperties/>
     </Criterion>
     <Criterion name="effectiveness">
       <AdditionalProperties/>
     </Criterion>
   </DecisionCriteria>
-</ProbModelXML>""")
+</ProbModelXML>"""
+        )
         self.writer.write_file("test_xml.pgmx")
         with open("test_xml.pgmx", "r") as myfile:
             data = myfile.read()
-        self.assertEqual(str(self.writer.__str__()[:-1]), str(etree.tostring(self.expected_xml)))
-        self.assertEqual(str(data), str(etree.tostring(self.expected_xml).decode('utf-8')))
+        self.assertEqual(
+            str(self.writer.__str__()[:-1]), str(etree.tostring(self.expected_xml))
+        )
+        self.assertEqual(
+            str(data), str(etree.tostring(self.expected_xml).decode("utf-8"))
+        )
 
     def tearDown(self):
         try:
-            os.remove('test_xml.pgmx')
+            os.remove("test_xml.pgmx")
         except OSError:
             pass
 
 
 class TestProbModelXMLmethods(unittest.TestCase):
     def setUp(self):
-        variables = ['VisitToAsia', 'Tuberculosis', 'Smoker', 'LungCancer',
-                     'Bronchitis', 'Dyspnea', 'TuberculosisOrCancer', 'X-ray', 'Sinus']
-
-        edges_list = [('VisitToAsia', 'Tuberculosis'),
-                      ('LungCancer', 'TuberculosisOrCancer'),
-                      ('Smoker', 'LungCancer'),
-                      ('Smoker', 'Bronchitis'),
-                      ('Tuberculosis', 'TuberculosisOrCancer'),
-                      ('Bronchitis', 'Dyspnea'),
-                      ('TuberculosisOrCancer', 'Dyspnea'),
-                      ('TuberculosisOrCancer', 'X-ray')]
-        nodes = {'Smoker': {'States': {'no': {}, 'yes': {}},
-                            'role': 'chance',
-                            'type': 'finiteStates',
-                            'Coordinates': {'y': '52', 'x': '568'},
-                            'AdditionalProperties': {'Title': 'S', 'Relevance': '7.0'}},
-                 'Bronchitis': {'States': {'no': {}, 'yes': {}},
-                                'role': 'chance',
-                                'type': 'finiteStates',
-                                'Coordinates': {'y': '181', 'x': '698'},
-                                'AdditionalProperties': {'Title': 'B', 'Relevance': '7.0'}},
-                 'VisitToAsia': {'States': {'no': {}, 'yes': {}},
-                                 'role': 'chance',
-                                 'type': 'finiteStates',
-                                 'Coordinates': {'y': '58', 'x': '290'},
-                                 'AdditionalProperties': {'Title': 'A', 'Relevance': '7.0'}},
-                 'Tuberculosis': {'States': {'no': {}, 'yes': {}},
-                                  'role': 'chance',
-                                  'type': 'finiteStates',
-                                  'Coordinates': {'y': '150', 'x': '201'},
-                                  'AdditionalProperties': {'Title': 'T', 'Relevance': '7.0'}},
-                 'X-ray': {'States': {'no': {}, 'yes': {}},
-                           'role': 'chance',
-                           'AdditionalProperties': {'Title': 'X', 'Relevance': '7.0'},
-                           'Coordinates': {'y': '322', 'x': '252'},
-                           'Comment': 'Indica si el test de rayos X ha sido positivo',
-                           'type': 'finiteStates'},
-                 'Dyspnea': {'States': {'no': {}, 'yes': {}},
-                             'role': 'chance',
-                             'type': 'finiteStates',
-                             'Coordinates': {'y': '321', 'x': '533'},
-                             'AdditionalProperties': {'Title': 'D', 'Relevance': '7.0'}},
-                 'TuberculosisOrCancer': {'States': {'no': {}, 'yes': {}},
-                                          'role': 'chance',
-                                          'type': 'finiteStates',
-                                          'Coordinates': {'y': '238', 'x': '336'},
-                                          'AdditionalProperties': {'Title': 'E', 'Relevance': '7.0'}},
-                 'LungCancer': {'States': {'no': {}, 'yes': {}},
-                                'role': 'chance',
-                                'type': 'finiteStates',
-                                'Coordinates': {'y': '152', 'x': '421'},
-                                'AdditionalProperties': {'Title': 'L', 'Relevance': '7.0'}},
-                 'Sinus': {'States': {'no': {}, 'yes': {}},
-                           'role': 'chance',
-                           'type': 'finiteStates',
-                           'Coordinates': {'y': '200', 'x': '100'},
-                           'AdditionalProperties': {'Title': 'S', 'Relevance': '7.0'}}}
-        edges = {'LungCancer': {'TuberculosisOrCancer': {'directed': 'true'}},
-                 'Smoker': {'LungCancer': {'directed': 'true'},
-                            'Bronchitis': {'directed': 'true'}},
-                 'Dyspnea': {},
-                 'X-ray': {},
-                 'VisitToAsia': {'Tuberculosis': {'directed': 'true'}},
-                 'TuberculosisOrCancer': {'X-ray': {'directed': 'true'},
-                                          'Dyspnea': {'directed': 'true'}},
-                 'Bronchitis': {'Dyspnea': {'directed': 'true'}},
-                 'Tuberculosis': {'TuberculosisOrCancer': {'directed': 'true'}}}
-
-        cpds = [{'Values': np.array([[0.95, 0.05], [0.02, 0.98]]),
-                 'Variables': {'X-ray': ['TuberculosisOrCancer']}},
-                {'Values': np.array([[0.7, 0.3], [0.4,  0.6]]),
-                 'Variables': {'Bronchitis': ['Smoker']}},
-                {'Values':  np.array([[0.9, 0.1,  0.3,  0.7], [0.2,  0.8,  0.1,  0.9]]),
-                 'Variables': {'Dyspnea': ['TuberculosisOrCancer', 'Bronchitis']}},
-                {'Values': np.array([[0.99], [0.01]]),
-                 'Variables': {'VisitToAsia': []}},
-                {'Values': np.array([[0.5], [0.5]]),
-                 'Variables': {'Smoker': []}},
-                {'Values': np.array([[0.99, 0.01], [0.9, 0.1]]),
-                 'Variables': {'LungCancer': ['Smoker']}},
-                {'Values': np.array([[0.99, 0.01], [0.95, 0.05]]),
-                 'Variables': {'Tuberculosis': ['VisitToAsia']}},
-                {'Values': np.array([[1, 0, 0, 1], [0, 1, 0, 1]]),
-                 'Variables': {'TuberculosisOrCancer': ['LungCancer', 'Tuberculosis']}},
-                {'Values': np.array([[0.3], [0.7]]),
-                 'Variables': {'Sinus': []}}]
+        variables = [
+            "VisitToAsia",
+            "Tuberculosis",
+            "Smoker",
+            "LungCancer",
+            "Bronchitis",
+            "Dyspnea",
+            "TuberculosisOrCancer",
+            "X-ray",
+            "Sinus",
+        ]
+
+        edges_list = [
+            ("VisitToAsia", "Tuberculosis"),
+            ("LungCancer", "TuberculosisOrCancer"),
+            ("Smoker", "LungCancer"),
+            ("Smoker", "Bronchitis"),
+            ("Tuberculosis", "TuberculosisOrCancer"),
+            ("Bronchitis", "Dyspnea"),
+            ("TuberculosisOrCancer", "Dyspnea"),
+            ("TuberculosisOrCancer", "X-ray"),
+        ]
+        nodes = {
+            "Smoker": {
+                "States": {"no": {}, "yes": {}},
+                "role": "chance",
+                "type": "finiteStates",
+                "Coordinates": {"y": "52", "x": "568"},
+                "AdditionalProperties": {"Title": "S", "Relevance": "7.0"},
+            },
+            "Bronchitis": {
+                "States": {"no": {}, "yes": {}},
+                "role": "chance",
+                "type": "finiteStates",
+                "Coordinates": {"y": "181", "x": "698"},
+                "AdditionalProperties": {"Title": "B", "Relevance": "7.0"},
+            },
+            "VisitToAsia": {
+                "States": {"no": {}, "yes": {}},
+                "role": "chance",
+                "type": "finiteStates",
+                "Coordinates": {"y": "58", "x": "290"},
+                "AdditionalProperties": {"Title": "A", "Relevance": "7.0"},
+            },
+            "Tuberculosis": {
+                "States": {"no": {}, "yes": {}},
+                "role": "chance",
+                "type": "finiteStates",
+                "Coordinates": {"y": "150", "x": "201"},
+                "AdditionalProperties": {"Title": "T", "Relevance": "7.0"},
+            },
+            "X-ray": {
+                "States": {"no": {}, "yes": {}},
+                "role": "chance",
+                "AdditionalProperties": {"Title": "X", "Relevance": "7.0"},
+                "Coordinates": {"y": "322", "x": "252"},
+                "Comment": "Indica si el test de rayos X ha sido positivo",
+                "type": "finiteStates",
+            },
+            "Dyspnea": {
+                "States": {"no": {}, "yes": {}},
+                "role": "chance",
+                "type": "finiteStates",
+                "Coordinates": {"y": "321", "x": "533"},
+                "AdditionalProperties": {"Title": "D", "Relevance": "7.0"},
+            },
+            "TuberculosisOrCancer": {
+                "States": {"no": {}, "yes": {}},
+                "role": "chance",
+                "type": "finiteStates",
+                "Coordinates": {"y": "238", "x": "336"},
+                "AdditionalProperties": {"Title": "E", "Relevance": "7.0"},
+            },
+            "LungCancer": {
+                "States": {"no": {}, "yes": {}},
+                "role": "chance",
+                "type": "finiteStates",
+                "Coordinates": {"y": "152", "x": "421"},
+                "AdditionalProperties": {"Title": "L", "Relevance": "7.0"},
+            },
+            "Sinus": {
+                "States": {"no": {}, "yes": {}},
+                "role": "chance",
+                "type": "finiteStates",
+                "Coordinates": {"y": "200", "x": "100"},
+                "AdditionalProperties": {"Title": "S", "Relevance": "7.0"},
+            },
+        }
+        edges = {
+            "LungCancer": {"TuberculosisOrCancer": {"directed": "true"}},
+            "Smoker": {
+                "LungCancer": {"directed": "true"},
+                "Bronchitis": {"directed": "true"},
+            },
+            "Dyspnea": {},
+            "X-ray": {},
+            "VisitToAsia": {"Tuberculosis": {"directed": "true"}},
+            "TuberculosisOrCancer": {
+                "X-ray": {"directed": "true"},
+                "Dyspnea": {"directed": "true"},
+            },
+            "Bronchitis": {"Dyspnea": {"directed": "true"}},
+            "Tuberculosis": {"TuberculosisOrCancer": {"directed": "true"}},
+        }
+
+        cpds = [
+            {
+                "Values": np.array([[0.95, 0.05], [0.02, 0.98]]),
+                "Variables": {"X-ray": ["TuberculosisOrCancer"]},
+            },
+            {
+                "Values": np.array([[0.7, 0.3], [0.4, 0.6]]),
+                "Variables": {"Bronchitis": ["Smoker"]},
+            },
+            {
+                "Values": np.array([[0.9, 0.1, 0.3, 0.7], [0.2, 0.8, 0.1, 0.9]]),
+                "Variables": {"Dyspnea": ["TuberculosisOrCancer", "Bronchitis"]},
+            },
+            {"Values": np.array([[0.99], [0.01]]), "Variables": {"VisitToAsia": []}},
+            {"Values": np.array([[0.5], [0.5]]), "Variables": {"Smoker": []}},
+            {
+                "Values": np.array([[0.99, 0.01], [0.9, 0.1]]),
+                "Variables": {"LungCancer": ["Smoker"]},
+            },
+            {
+                "Values": np.array([[0.99, 0.01], [0.95, 0.05]]),
+                "Variables": {"Tuberculosis": ["VisitToAsia"]},
+            },
+            {
+                "Values": np.array([[1, 0, 0, 1], [0, 1, 0, 1]]),
+                "Variables": {"TuberculosisOrCancer": ["LungCancer", "Tuberculosis"]},
+            },
+            {"Values": np.array([[0.3], [0.7]]), "Variables": {"Sinus": []}},
+        ]
         self.model = BayesianModel()
         self.model.add_nodes_from(variables)
         self.model.add_edges_from(edges_list)
-        for node in nodes:
-            self.model.node[node] = nodes[node]
-        for edge in edges:
-            self.model.edge[edge] = edges[edge]
+
+        if nx.__version__.startswith("1"):
+            for node in nodes:
+                self.model.nodes[node] = nodes[node]
+            for edge in edges:
+                self.model.edge[edge] = edges[edge]
+        else:
+            for node in nodes:
+                self.model._node[node] = nodes[node]
+            for edge in edges:
+                self.model._adj[edge] = edges[edge]
 
         tabular_cpds = []
         for cpd in cpds:
-            var = list(cpd['Variables'].keys())[0]
-            evidence = cpd['Variables'][var]
-            values = cpd['Values']
-            states = len(nodes[var]['States'])
-            evidence_card = [len(nodes[evidence_var]['States'])
-                             for evidence_var in evidence]
+            var = list(cpd["Variables"].keys())[0]
+            evidence = cpd["Variables"][var]
+            values = cpd["Values"]
+            states = len(nodes[var]["States"])
+            evidence_card = [
+                len(nodes[evidence_var]["States"]) for evidence_var in evidence
+            ]
             tabular_cpds.append(
-                TabularCPD(var, states, values, evidence, evidence_card))
+                TabularCPD(var, states, values, evidence, evidence_card)
+            )
         self.maxDiff = None
         self.model.add_cpds(*tabular_cpds)
 
     def test_get_probmodel_data(self):
         model_data = get_probmodel_data(self.model)
         xmlfile = ProbModelXMLWriter(model_data)
-        with open('pgmpy/tests/test_readwrite/testdata/test_probmodelxml_data.json') as data_file:
+        with open(
+            "pgmpy/tests/test_readwrite/testdata/test_probmodelxml_data.json"
+        ) as data_file:
             model_data_expected = json.load(data_file)
-        xmlfile_expected = etree.XML("""<ProbModelXML formatVersion="1.0">
+        xmlfile_expected = etree.XML(
+            """<ProbModelXML formatVersion="1.0">
   <ProbNet type="BayesianNetwork">
     <Variables>
       <Variable name="Bronchitis" role="chance" type="finiteStates">
         <Coordinates x="698" y="181"/>
         <Property name="Relevance" value="7.0"/>
         <Property name="Title" value="B"/>
         <States>
@@ -1413,10 +1640,13 @@
         <Values>0.3 0.7 </Values>
       </Potential>
     </Potentials>
     <AdditionalConstraints/>
     <AdditionalProperties/>
     <DecisionCriteria/>
   </ProbNet>
-</ProbModelXML>""")
+</ProbModelXML>"""
+        )
         self.assertDictEqual(model_data, model_data_expected)
-        self.assertEqual(str(xmlfile.__str__()[:-1]), str(etree.tostring(xmlfile_expected)))
+        self.assertEqual(
+            str(xmlfile.__str__()[:-1]), str(etree.tostring(xmlfile_expected))
+        )
```

### Comparing `pgmpy-0.1.7/pgmpy/tests/test_sampling/test_base_continuous.py` & `pgmpy-0.1.9/pgmpy/tests/test_sampling/test_base_continuous.py`

 * *Files 16% similar despite different names*

```diff
@@ -3,107 +3,193 @@
 import numpy as np
 
 from pgmpy.factors.distributions import GaussianDistribution as JGD
 from pgmpy.sampling import LeapFrog, ModifiedEuler, GradLogPDFGaussian
 
 
 class TestGradLogPDFGaussian(unittest.TestCase):
-
     def setUp(self):
         mean = np.array([1, 2, 3, 4])
-        covariance = np.array([[1, 0.2, 0.4, 0.7], [0.2, 2, 0.5, 0.8], [0.4, 0.5, 3, 0.6], [0.7, 0.8, 0.6, 4]])
-        self.test_model = JGD(['x', 'y', 'z', 't'], mean, covariance)
+        covariance = np.array(
+            [
+                [1, 0.2, 0.4, 0.7],
+                [0.2, 2, 0.5, 0.8],
+                [0.4, 0.5, 3, 0.6],
+                [0.7, 0.8, 0.6, 4],
+            ]
+        )
+        self.test_model = JGD(["x", "y", "z", "t"], mean, covariance)
         self.test_gradient = GradLogPDFGaussian([0, 0, 0, 0], self.test_model)
 
     def test_error(self):
         with self.assertRaises(TypeError):
             GradLogPDFGaussian(1, self.test_model)
         with self.assertRaises(ValueError):
             GradLogPDFGaussian([1, 1], self.test_model)
 
     def test_gradient(self):
         grad, log = self.test_gradient.get_gradient_log_pdf()
-        np.testing.assert_almost_equal(grad, np.array([0.05436475, 0.49454937, 0.75465073, 0.77837868]))
+        np.testing.assert_almost_equal(
+            grad, np.array([0.05436475, 0.49454937, 0.75465073, 0.77837868])
+        )
         np.testing.assert_almost_equal(log, -3.21046521505)
 
 
 class TestLeapFrog(unittest.TestCase):
-
     def setUp(self):
         mean = np.array([-1, 1, -1])
         covariance = np.array([[1, 0.6, 0.5], [0.6, 2, 0.3], [0.5, 0.3, 1]])
-        self.test_model = JGD(['x', 'y', 'z'], mean, covariance)
+        self.test_model = JGD(["x", "y", "z"], mean, covariance)
         position = [0, 0, 0]
         momentum = [-1, -1, -1]
-        self.test_with_grad_log = LeapFrog(model=self.test_model, position=position, momentum=momentum,
-                                           stepsize=0.3, grad_log_pdf=GradLogPDFGaussian, grad_log_position=None)
-        grad_log_position, _ = GradLogPDFGaussian(position, self.test_model).get_gradient_log_pdf()
-        self.test_without_grad_log = LeapFrog(model=self.test_model, position=position, momentum=momentum,
-                                              stepsize=0.4, grad_log_pdf=GradLogPDFGaussian,
-                                              grad_log_position=grad_log_position)
+        self.test_with_grad_log = LeapFrog(
+            model=self.test_model,
+            position=position,
+            momentum=momentum,
+            stepsize=0.3,
+            grad_log_pdf=GradLogPDFGaussian,
+            grad_log_position=None,
+        )
+        grad_log_position, _ = GradLogPDFGaussian(
+            position, self.test_model
+        ).get_gradient_log_pdf()
+        self.test_without_grad_log = LeapFrog(
+            model=self.test_model,
+            position=position,
+            momentum=momentum,
+            stepsize=0.4,
+            grad_log_pdf=GradLogPDFGaussian,
+            grad_log_position=grad_log_position,
+        )
 
     def test_errors(self):
         with self.assertRaises(TypeError):
-            LeapFrog(model=self.test_model, position=1, momentum=[1, 1], stepsize=0.1,
-                     grad_log_pdf=GradLogPDFGaussian)
+            LeapFrog(
+                model=self.test_model,
+                position=1,
+                momentum=[1, 1],
+                stepsize=0.1,
+                grad_log_pdf=GradLogPDFGaussian,
+            )
         with self.assertRaises(TypeError):
-            LeapFrog(model=self.test_model, position=[1, 1], momentum=1, stepsize=0.1,
-                     grad_log_pdf=GradLogPDFGaussian)
+            LeapFrog(
+                model=self.test_model,
+                position=[1, 1],
+                momentum=1,
+                stepsize=0.1,
+                grad_log_pdf=GradLogPDFGaussian,
+            )
         with self.assertRaises(ValueError):
-            LeapFrog(model=self.test_model, position=[1, 1], momentum=[1], stepsize=0.1,
-                     grad_log_pdf=GradLogPDFGaussian)
+            LeapFrog(
+                model=self.test_model,
+                position=[1, 1],
+                momentum=[1],
+                stepsize=0.1,
+                grad_log_pdf=GradLogPDFGaussian,
+            )
         with self.assertRaises(TypeError):
-            LeapFrog(model=self.test_model, position=[1], momentum=[1], stepsize=0.1, grad_log_pdf=1)
+            LeapFrog(
+                model=self.test_model,
+                position=[1],
+                momentum=[1],
+                stepsize=0.1,
+                grad_log_pdf=1,
+            )
         with self.assertRaises(ValueError):
-            LeapFrog(model=self.test_model, position=[1, 1], momentum=[1, 1], stepsize=0.1,
-                     grad_log_pdf=GradLogPDFGaussian)
+            LeapFrog(
+                model=self.test_model,
+                position=[1, 1],
+                momentum=[1, 1],
+                stepsize=0.1,
+                grad_log_pdf=GradLogPDFGaussian,
+            )
         with self.assertRaises(TypeError):
-            LeapFrog(model=self.test_model, position=[1, 1, 1], momentum=[1, 1, 1], stepsize=0.1,
-                     grad_log_pdf=GradLogPDFGaussian, grad_log_position=1)
+            LeapFrog(
+                model=self.test_model,
+                position=[1, 1, 1],
+                momentum=[1, 1, 1],
+                stepsize=0.1,
+                grad_log_pdf=GradLogPDFGaussian,
+                grad_log_position=1,
+            )
         with self.assertRaises(ValueError):
-            LeapFrog(model=self.test_model, position=[1, 1, 1], momentum=[1, 1, 1], stepsize=0.1,
-                     grad_log_pdf=GradLogPDFGaussian, grad_log_position=[1, 1])
+            LeapFrog(
+                model=self.test_model,
+                position=[1, 1, 1],
+                momentum=[1, 1, 1],
+                stepsize=0.1,
+                grad_log_pdf=GradLogPDFGaussian,
+                grad_log_position=[1, 1],
+            )
 
     def test_leapfrog_methods(self):
         new_pos, new_momentum, new_grad = self.test_with_grad_log.get_proposed_values()
-        np.testing.assert_almost_equal(new_pos, np.array([-0.35634146, -0.25609756, -0.33]))
-        np.testing.assert_almost_equal(new_momentum, np.array([-1.3396624, -0.70344884, -1.16963415]))
-        np.testing.assert_almost_equal(new_grad, np.array([-1.0123835, 1.00139798, -0.46422764]))
-        new_pos, new_momentum, new_grad = self.test_without_grad_log.get_proposed_values()
-        np.testing.assert_almost_equal(new_pos, np.array([-0.5001626, -0.32195122, -0.45333333]))
-        np.testing.assert_almost_equal(new_momentum, np.array([-1.42947981, -0.60709102, -1.21246612]))
-        np.testing.assert_almost_equal(new_grad, np.array([-0.89536651, 0.98893516, -0.39566396]))
+        np.testing.assert_almost_equal(
+            new_pos, np.array([-0.35634146, -0.25609756, -0.33])
+        )
+        np.testing.assert_almost_equal(
+            new_momentum, np.array([-1.3396624, -0.70344884, -1.16963415])
+        )
+        np.testing.assert_almost_equal(
+            new_grad, np.array([-1.0123835, 1.00139798, -0.46422764])
+        )
+        new_pos, new_momentum, new_grad = (
+            self.test_without_grad_log.get_proposed_values()
+        )
+        np.testing.assert_almost_equal(
+            new_pos, np.array([-0.5001626, -0.32195122, -0.45333333])
+        )
+        np.testing.assert_almost_equal(
+            new_momentum, np.array([-1.42947981, -0.60709102, -1.21246612])
+        )
+        np.testing.assert_almost_equal(
+            new_grad, np.array([-0.89536651, 0.98893516, -0.39566396])
+        )
 
     def tearDown(self):
         del self.test_model
         del self.test_with_grad_log
         del self.test_without_grad_log
 
 
 class TestModifiedEuler(unittest.TestCase):
-
     def setUp(self):
         mean = np.array([0, 0])
         covariance = np.array([[-1, 0.8], [0.8, 3]])
-        self.test_model = JGD(['x', 'y'], mean, covariance)
+        self.test_model = JGD(["x", "y"], mean, covariance)
         position = [0, 0]
         momentum = [-2, 1]
-        self.test_with_grad_log = ModifiedEuler(model=self.test_model, position=position, momentum=momentum,
-                                                stepsize=0.5, grad_log_pdf=GradLogPDFGaussian, grad_log_position=None)
-        grad_log_position, _ = GradLogPDFGaussian(position, self.test_model).get_gradient_log_pdf()
-        self.test_without_grad_log = ModifiedEuler(model=self.test_model, position=position, momentum=momentum,
-                                                   stepsize=0.3, grad_log_pdf=GradLogPDFGaussian,
-                                                   grad_log_position=grad_log_position)
+        self.test_with_grad_log = ModifiedEuler(
+            model=self.test_model,
+            position=position,
+            momentum=momentum,
+            stepsize=0.5,
+            grad_log_pdf=GradLogPDFGaussian,
+            grad_log_position=None,
+        )
+        grad_log_position, _ = GradLogPDFGaussian(
+            position, self.test_model
+        ).get_gradient_log_pdf()
+        self.test_without_grad_log = ModifiedEuler(
+            model=self.test_model,
+            position=position,
+            momentum=momentum,
+            stepsize=0.3,
+            grad_log_pdf=GradLogPDFGaussian,
+            grad_log_position=grad_log_position,
+        )
 
     def test_modified_euler_methods(self):
         new_pos, new_momentum, new_grad = self.test_with_grad_log.get_proposed_values()
         np.testing.assert_almost_equal(new_pos, np.array([-1.0, 0.5]))
         np.testing.assert_almost_equal(new_momentum, np.array([-2.0, 1.0]))
         np.testing.assert_almost_equal(new_grad, np.array([-0.93406593, 0.08241758]))
-        new_pos, new_momentum, new_grad = self.test_without_grad_log.get_proposed_values()
+        new_pos, new_momentum, new_grad = (
+            self.test_without_grad_log.get_proposed_values()
+        )
         np.testing.assert_almost_equal(new_pos, np.array([-0.6, 0.3]))
         np.testing.assert_almost_equal(new_momentum, np.array([-2.0, 1.0]))
         np.testing.assert_almost_equal(new_grad, np.array([-0.56043956, 0.04945055]))
 
     def tearDown(self):
         del self.test_model
         del self.test_with_grad_log
```

### Comparing `pgmpy-0.1.7/pgmpy/tests/test_sampling/test_Sampling.py` & `pgmpy-0.1.9/pgmpy/tests/test_sampling/test_Sampling.py`

 * *Files 7% similar despite different names*

```diff
@@ -5,90 +5,92 @@
 from pgmpy.factors.discrete import DiscreteFactor, TabularCPD, State
 from pgmpy.models import BayesianModel, MarkovModel
 from pgmpy.sampling import BayesianModelSampling, GibbsSampling
 
 
 class TestBayesianModelSampling(unittest.TestCase):
     def setUp(self):
-        self.bayesian_model = BayesianModel([('A', 'J'), ('R', 'J'), ('J', 'Q'),
-                                             ('J', 'L'), ('G', 'L')])
-        cpd_a = TabularCPD('A', 2, [[0.2], [0.8]])
-        cpd_r = TabularCPD('R', 2, [[0.4], [0.6]])
-        cpd_j = TabularCPD('J', 2,
-                           [[0.9, 0.6, 0.7, 0.1],
-                            [0.1, 0.4, 0.3, 0.9]],
-                           ['R', 'A'], [2, 2])
-        cpd_q = TabularCPD('Q', 2,
-                           [[0.9, 0.2],
-                            [0.1, 0.8]],
-                           ['J'], [2])
-        cpd_l = TabularCPD('L', 2,
-                           [[0.9, 0.45, 0.8, 0.1],
-                            [0.1, 0.55, 0.2, 0.9]],
-                           ['G', 'J'], [2, 2])
-        cpd_g = TabularCPD('G', 2, [[0.6], [0.4]])
+        self.bayesian_model = BayesianModel(
+            [("A", "J"), ("R", "J"), ("J", "Q"), ("J", "L"), ("G", "L")]
+        )
+        cpd_a = TabularCPD("A", 2, [[0.2], [0.8]])
+        cpd_r = TabularCPD("R", 2, [[0.4], [0.6]])
+        cpd_j = TabularCPD(
+            "J", 2, [[0.9, 0.6, 0.7, 0.1], [0.1, 0.4, 0.3, 0.9]], ["R", "A"], [2, 2]
+        )
+        cpd_q = TabularCPD("Q", 2, [[0.9, 0.2], [0.1, 0.8]], ["J"], [2])
+        cpd_l = TabularCPD(
+            "L", 2, [[0.9, 0.45, 0.8, 0.1], [0.1, 0.55, 0.2, 0.9]], ["G", "J"], [2, 2]
+        )
+        cpd_g = TabularCPD("G", 2, [[0.6], [0.4]])
         self.bayesian_model.add_cpds(cpd_a, cpd_g, cpd_j, cpd_l, cpd_q, cpd_r)
         self.sampling_inference = BayesianModelSampling(self.bayesian_model)
         self.markov_model = MarkovModel()
 
     def test_init(self):
         with self.assertRaises(TypeError):
             BayesianModelSampling(self.markov_model)
 
     def test_forward_sample(self):
         sample = self.sampling_inference.forward_sample(25)
         self.assertEquals(len(sample), 25)
         self.assertEquals(len(sample.columns), 6)
-        self.assertIn('A', sample.columns)
-        self.assertIn('J', sample.columns)
-        self.assertIn('R', sample.columns)
-        self.assertIn('Q', sample.columns)
-        self.assertIn('G', sample.columns)
-        self.assertIn('L', sample.columns)
+        self.assertIn("A", sample.columns)
+        self.assertIn("J", sample.columns)
+        self.assertIn("R", sample.columns)
+        self.assertIn("Q", sample.columns)
+        self.assertIn("G", sample.columns)
+        self.assertIn("L", sample.columns)
         self.assertTrue(set(sample.A).issubset({0, 1}))
         self.assertTrue(set(sample.J).issubset({0, 1}))
         self.assertTrue(set(sample.R).issubset({0, 1}))
         self.assertTrue(set(sample.Q).issubset({0, 1}))
         self.assertTrue(set(sample.G).issubset({0, 1}))
         self.assertTrue(set(sample.L).issubset({0, 1}))
 
     def test_rejection_sample_basic(self):
-        sample = self.sampling_inference.rejection_sample([State('A', 1), State('J', 1), State('R', 1)], 25)
+        sample = self.sampling_inference.rejection_sample()
+        sample = self.sampling_inference.rejection_sample(
+            [State("A", 1), State("J", 1), State("R", 1)], 25
+        )
         self.assertEquals(len(sample), 25)
         self.assertEquals(len(sample.columns), 6)
-        self.assertIn('A', sample.columns)
-        self.assertIn('J', sample.columns)
-        self.assertIn('R', sample.columns)
-        self.assertIn('Q', sample.columns)
-        self.assertIn('G', sample.columns)
-        self.assertIn('L', sample.columns)
+        self.assertIn("A", sample.columns)
+        self.assertIn("J", sample.columns)
+        self.assertIn("R", sample.columns)
+        self.assertIn("Q", sample.columns)
+        self.assertIn("G", sample.columns)
+        self.assertIn("L", sample.columns)
         self.assertTrue(set(sample.A).issubset({1}))
         self.assertTrue(set(sample.J).issubset({1}))
         self.assertTrue(set(sample.R).issubset({1}))
         self.assertTrue(set(sample.Q).issubset({0, 1}))
         self.assertTrue(set(sample.G).issubset({0, 1}))
         self.assertTrue(set(sample.L).issubset({0, 1}))
 
     @patch("pgmpy.sampling.BayesianModelSampling.forward_sample", autospec=True)
     def test_rejection_sample_less_arg(self, forward_sample):
         sample = self.sampling_inference.rejection_sample(size=5)
         forward_sample.assert_called_once_with(self.sampling_inference, 5)
         self.assertEqual(sample, forward_sample.return_value)
 
     def test_likelihood_weighted_sample(self):
-        sample = self.sampling_inference.likelihood_weighted_sample([State('A', 0), State('J', 1), State('R', 0)], 25)
+        sample = self.sampling_inference.likelihood_weighted_sample()
+        sample = self.sampling_inference.likelihood_weighted_sample(
+            [State("A", 0), State("J", 1), State("R", 0)], 25
+        )
         self.assertEquals(len(sample), 25)
         self.assertEquals(len(sample.columns), 7)
-        self.assertIn('A', sample.columns)
-        self.assertIn('J', sample.columns)
-        self.assertIn('R', sample.columns)
-        self.assertIn('Q', sample.columns)
-        self.assertIn('G', sample.columns)
-        self.assertIn('L', sample.columns)
-        self.assertIn('_weight', sample.columns)
+        self.assertIn("A", sample.columns)
+        self.assertIn("J", sample.columns)
+        self.assertIn("R", sample.columns)
+        self.assertIn("Q", sample.columns)
+        self.assertIn("G", sample.columns)
+        self.assertIn("L", sample.columns)
+        self.assertIn("_weight", sample.columns)
         self.assertTrue(set(sample.A).issubset({0, 1}))
         self.assertTrue(set(sample.J).issubset({0, 1}))
         self.assertTrue(set(sample.R).issubset({0, 1}))
         self.assertTrue(set(sample.Q).issubset({0, 1}))
         self.assertTrue(set(sample.G).issubset({0, 1}))
         self.assertTrue(set(sample.L).issubset({0, 1}))
 
@@ -97,89 +99,108 @@
         del self.bayesian_model
         del self.markov_model
 
 
 class TestGibbsSampling(unittest.TestCase):
     def setUp(self):
         # A test Bayesian model
-        diff_cpd = TabularCPD('diff', 2, [[0.6], [0.4]])
-        intel_cpd = TabularCPD('intel', 2, [[0.7], [0.3]])
-        grade_cpd = TabularCPD('grade', 3, [[0.3, 0.05, 0.9, 0.5], [0.4, 0.25, 0.08, 0.3], [0.3, 0.7, 0.02, 0.2]],
-                               evidence=['diff', 'intel'], evidence_card=[2, 2])
+        diff_cpd = TabularCPD("diff", 2, [[0.6], [0.4]])
+        intel_cpd = TabularCPD("intel", 2, [[0.7], [0.3]])
+        grade_cpd = TabularCPD(
+            "grade",
+            3,
+            [[0.3, 0.05, 0.9, 0.5], [0.4, 0.25, 0.08, 0.3], [0.3, 0.7, 0.02, 0.2]],
+            evidence=["diff", "intel"],
+            evidence_card=[2, 2],
+        )
         self.bayesian_model = BayesianModel()
-        self.bayesian_model.add_nodes_from(['diff', 'intel', 'grade'])
-        self.bayesian_model.add_edges_from([('diff', 'grade'), ('intel', 'grade')])
+        self.bayesian_model.add_nodes_from(["diff", "intel", "grade"])
+        self.bayesian_model.add_edges_from([("diff", "grade"), ("intel", "grade")])
         self.bayesian_model.add_cpds(diff_cpd, intel_cpd, grade_cpd)
 
         # A test Markov model
-        self.markov_model = MarkovModel([('A', 'B'), ('C', 'B'), ('B', 'D')])
-        factor_ab = DiscreteFactor(['A', 'B'], [2, 3], [1, 2, 3, 4, 5, 6])
-        factor_cb = DiscreteFactor(['C', 'B'], [4, 3], [3, 1, 4, 5, 7, 8, 1, 3, 10, 4, 5, 6])
-        factor_bd = DiscreteFactor(['B', 'D'], [3, 2], [5, 7, 2, 1, 9, 3])
+        self.markov_model = MarkovModel([("A", "B"), ("C", "B"), ("B", "D")])
+        factor_ab = DiscreteFactor(["A", "B"], [2, 3], [1, 2, 3, 4, 5, 6])
+        factor_cb = DiscreteFactor(
+            ["C", "B"], [4, 3], [3, 1, 4, 5, 7, 8, 1, 3, 10, 4, 5, 6]
+        )
+        factor_bd = DiscreteFactor(["B", "D"], [3, 2], [5, 7, 2, 1, 9, 3])
         self.markov_model.add_factors(factor_ab, factor_cb, factor_bd)
 
         self.gibbs = GibbsSampling(self.bayesian_model)
 
     def tearDown(self):
         del self.bayesian_model
         del self.markov_model
 
-    @patch('pgmpy.sampling.GibbsSampling._get_kernel_from_bayesian_model', autospec=True)
-    @patch('pgmpy.models.MarkovChain.__init__', autospec=True)
+    @patch(
+        "pgmpy.sampling.GibbsSampling._get_kernel_from_bayesian_model", autospec=True
+    )
+    @patch("pgmpy.models.MarkovChain.__init__", autospec=True)
     def test_init_bayesian_model(self, init, get_kernel):
         model = MagicMock(spec_set=BayesianModel)
         gibbs = GibbsSampling(model)
         init.assert_called_once_with(gibbs)
         get_kernel.assert_called_once_with(gibbs, model)
 
-    @patch('pgmpy.sampling.GibbsSampling._get_kernel_from_markov_model', autospec=True)
+    @patch("pgmpy.sampling.GibbsSampling._get_kernel_from_markov_model", autospec=True)
     def test_init_markov_model(self, get_kernel):
         model = MagicMock(spec_set=MarkovModel)
         gibbs = GibbsSampling(model)
         get_kernel.assert_called_once_with(gibbs, model)
 
     def test_get_kernel_from_bayesian_model(self):
         gibbs = GibbsSampling()
         gibbs._get_kernel_from_bayesian_model(self.bayesian_model)
-        self.assertListEqual(list(gibbs.variables), self.bayesian_model.nodes())
-        self.assertDictEqual(gibbs.cardinalities, {'diff': 2, 'intel': 2, 'grade': 3})
+        self.assertListEqual(list(gibbs.variables), list(self.bayesian_model.nodes()))
+        self.assertDictEqual(gibbs.cardinalities, {"diff": 2, "intel": 2, "grade": 3})
 
     def test_get_kernel_from_markov_model(self):
         gibbs = GibbsSampling()
         gibbs._get_kernel_from_markov_model(self.markov_model)
-        self.assertListEqual(list(gibbs.variables), self.markov_model.nodes())
-        self.assertDictEqual(gibbs.cardinalities, {'A': 2, 'B': 3, 'C': 4, 'D': 2})
+        self.assertListEqual(list(gibbs.variables), list(self.markov_model.nodes()))
+        self.assertDictEqual(gibbs.cardinalities, {"A": 2, "B": 3, "C": 4, "D": 2})
 
     def test_sample(self):
-        start_state = [State('diff', 0), State('intel', 0), State('grade', 0)]
+        start_state = [State("diff", 0), State("intel", 0), State("grade", 0)]
         sample = self.gibbs.sample(start_state, 2)
         self.assertEquals(len(sample), 2)
         self.assertEquals(len(sample.columns), 3)
-        self.assertIn('diff', sample.columns)
-        self.assertIn('intel', sample.columns)
-        self.assertIn('grade', sample.columns)
-        self.assertTrue(set(sample['diff']).issubset({0, 1}))
-        self.assertTrue(set(sample['intel']).issubset({0, 1}))
-        self.assertTrue(set(sample['grade']).issubset({0, 1, 2}))
+        self.assertIn("diff", sample.columns)
+        self.assertIn("intel", sample.columns)
+        self.assertIn("grade", sample.columns)
+        self.assertTrue(set(sample["diff"]).issubset({0, 1}))
+        self.assertTrue(set(sample["intel"]).issubset({0, 1}))
+        self.assertTrue(set(sample["grade"]).issubset({0, 1, 2}))
 
     @patch("pgmpy.sampling.GibbsSampling.random_state", autospec=True)
     def test_sample_less_arg(self, random_state):
         self.gibbs.state = None
-        random_state.return_value = [State('diff', 0), State('intel', 0), State('grade', 0)]
+        random_state.return_value = [
+            State("diff", 0),
+            State("intel", 0),
+            State("grade", 0),
+        ]
         sample = self.gibbs.sample(size=2)
         random_state.assert_called_once_with(self.gibbs)
         self.assertEqual(len(sample), 2)
 
     def test_generate_sample(self):
-        start_state = [State('diff', 0), State('intel', 0), State('grade', 0)]
+        start_state = [State("diff", 0), State("intel", 0), State("grade", 0)]
         gen = self.gibbs.generate_sample(start_state, 2)
         samples = [sample for sample in gen]
         self.assertEqual(len(samples), 2)
-        self.assertEqual({samples[0][0].var, samples[0][1].var, samples[0][2].var}, {'diff', 'intel', 'grade'})
-        self.assertEqual({samples[1][0].var, samples[1][1].var, samples[1][2].var}, {'diff', 'intel', 'grade'})
+        self.assertEqual(
+            {samples[0][0].var, samples[0][1].var, samples[0][2].var},
+            {"diff", "intel", "grade"},
+        )
+        self.assertEqual(
+            {samples[1][0].var, samples[1][1].var, samples[1][2].var},
+            {"diff", "intel", "grade"},
+        )
 
     @patch("pgmpy.sampling.GibbsSampling.random_state", autospec=True)
     def test_generate_sample_less_arg(self, random_state):
         self.gibbs.state = None
         gen = self.gibbs.generate_sample(size=2)
         samples = [sample for sample in gen]
         random_state.assert_called_once_with(self.gibbs)
```

### Comparing `pgmpy-0.1.7/pgmpy/tests/test_sampling/test_continuous_sampling.py` & `pgmpy-0.1.9/pgmpy/tests/test_sampling/test_continuous_sampling.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,151 +1,223 @@
 import unittest
 
 import numpy as np
 
 from pgmpy.factors.distributions import GaussianDistribution as JGD
-from pgmpy.sampling import (HamiltonianMC as HMC, HamiltonianMCDA as HMCda, GradLogPDFGaussian, NoUTurnSampler as NUTS,
-                            NoUTurnSamplerDA as NUTSda)
+from pgmpy.sampling import (
+    HamiltonianMC as HMC,
+    HamiltonianMCDA as HMCda,
+    GradLogPDFGaussian,
+    NoUTurnSampler as NUTS,
+    NoUTurnSamplerDA as NUTSda,
+)
 
 
 class TestHMCInference(unittest.TestCase):
-
     def setUp(self):
         mean = [-1, 1, -1]
         covariance = np.array([[3, 0.8, 0.2], [0.8, 2, 0.3], [0.2, 0.3, 1]])
-        self.test_model = JGD(['x', 'y', 'z'], mean, covariance)
+        self.test_model = JGD(["x", "y", "z"], mean, covariance)
         self.hmc_sampler = HMCda(model=self.test_model, grad_log_pdf=GradLogPDFGaussian)
 
     def test_errors(self):
         with self.assertRaises(TypeError):
             HMCda(model=self.test_model, grad_log_pdf=1)
         with self.assertRaises(TypeError):
-            HMCda(model=self.test_model, grad_log_pdf=GradLogPDFGaussian, simulate_dynamics=1)
+            HMCda(
+                model=self.test_model,
+                grad_log_pdf=GradLogPDFGaussian,
+                simulate_dynamics=1,
+            )
         with self.assertRaises(ValueError):
             HMCda(model=self.test_model, delta=-1)
         with self.assertRaises(TypeError):
-            self.hmc_sampler.sample(initial_pos=1, num_adapt=1, num_samples=1, trajectory_length=1)
+            self.hmc_sampler.sample(
+                initial_pos=1, num_adapt=1, num_samples=1, trajectory_length=1
+            )
         with self.assertRaises(TypeError):
             self.hmc_sampler.generate_sample(1, 1, 1, 1).send(None)
         with self.assertRaises(TypeError):
-            HMC(model=self.test_model).sample(initial_pos=1, num_samples=1, trajectory_length=1)
+            HMC(model=self.test_model).sample(
+                initial_pos=1, num_samples=1, trajectory_length=1
+            )
         with self.assertRaises(TypeError):
             HMC(model=self.test_model).generate_sample(1, 1, 1).send(None)
 
     def test_acceptance_prob(self):
-        acceptance_probability = self.hmc_sampler._acceptance_prob(np.array([1, 2, 3]), np.array([2, 3, 4]),
-                                                                   np.array([1, -1, 1]), np.array([0, 0, 0]))
+        acceptance_probability = self.hmc_sampler._acceptance_prob(
+            np.array([1, 2, 3]),
+            np.array([2, 3, 4]),
+            np.array([1, -1, 1]),
+            np.array([0, 0, 0]),
+        )
         np.testing.assert_almost_equal(acceptance_probability, 0.0347363)
 
     def test_find_resonable_stepsize(self):
         np.random.seed(987654321)
         stepsize = self.hmc_sampler._find_reasonable_stepsize(np.array([-1, 1, -1]))
         np.testing.assert_almost_equal(stepsize, 2.0)
 
     def test_adapt_params(self):
-        stepsize, stepsize_bar, h_bar = self.hmc_sampler._adapt_params(0.0025, 1, 1, np.log(0.025), 2, 1)
+        stepsize, stepsize_bar, h_bar = self.hmc_sampler._adapt_params(
+            0.0025, 1, 1, np.log(0.025), 2, 1
+        )
         np.testing.assert_almost_equal(stepsize, 3.13439452e-13)
         np.testing.assert_almost_equal(stepsize_bar, 3.6742481e-08)
         np.testing.assert_almost_equal(h_bar, 0.8875)
 
     def test_sample(self):
         # Seeding is done for _find_reasonable_stepsize method
         # Testing sample method simple HMC
         np.random.seed(3124141)
-        samples = self.hmc_sampler.sample(initial_pos=[0.3, 0.4, 0.2], num_adapt=0,
-                                          num_samples=10000, trajectory_length=4)
+        samples = self.hmc_sampler.sample(
+            initial_pos=[0.3, 0.4, 0.2],
+            num_adapt=0,
+            num_samples=10000,
+            trajectory_length=4,
+        )
         covariance = np.cov(samples.values.T)
         self.assertTrue(np.linalg.norm(covariance - self.test_model.covariance) < 3)
 
         # Testing sample of method of HMCda
         np.random.seed(3124141)
-        samples = self.hmc_sampler.sample(initial_pos=[0.6, 0.2, 0.8], num_adapt=10000,
-                                          num_samples=10000, trajectory_length=4)
+        samples = self.hmc_sampler.sample(
+            initial_pos=[0.6, 0.2, 0.8],
+            num_adapt=10000,
+            num_samples=10000,
+            trajectory_length=4,
+        )
         covariance = np.cov(samples.values.T)
         self.assertTrue(np.linalg.norm(covariance - self.test_model.covariance) < 0.3)
 
         # Testing generate_sample method of simple HMC
         np.random.seed(3124141)
-        gen_samples = self.hmc_sampler.generate_sample(initial_pos=[0.3, 0.4, 0.2], num_adapt=0,
-                                                       num_samples=10000, trajectory_length=4)
+        gen_samples = self.hmc_sampler.generate_sample(
+            initial_pos=[0.3, 0.4, 0.2],
+            num_adapt=0,
+            num_samples=10000,
+            trajectory_length=4,
+        )
         samples = np.array([sample for sample in gen_samples])
         covariance = np.cov(samples.T)
         self.assertTrue(np.linalg.norm(covariance - self.test_model.covariance) < 3)
 
         # Testing sample of method of HMCda
         np.random.seed(3124141)
-        gen_samples = self.hmc_sampler.generate_sample(initial_pos=[0.6, 0.2, 0.8], num_adapt=10000,
-                                                       num_samples=10000, trajectory_length=4)
+        gen_samples = self.hmc_sampler.generate_sample(
+            initial_pos=[0.6, 0.2, 0.8],
+            num_adapt=10000,
+            num_samples=10000,
+            trajectory_length=4,
+        )
         samples = np.array([sample for sample in gen_samples])
         covariance = np.cov(samples.T)
         self.assertTrue(np.linalg.norm(covariance - self.test_model.covariance) < 0.3)
 
     def tearDown(self):
         del self.hmc_sampler
         del self.test_model
 
 
 class TestNUTSInference(unittest.TestCase):
-
     def setUp(self):
         mean = np.array([-1, 1, 0])
         covariance = np.array([[6, 0.7, 0.2], [0.7, 3, 0.9], [0.2, 0.9, 1]])
-        self.test_model = JGD(['x', 'y', 'z'], mean, covariance)
-        self.nuts_sampler = NUTSda(model=self.test_model, grad_log_pdf=GradLogPDFGaussian)
+        self.test_model = JGD(["x", "y", "z"], mean, covariance)
+        self.nuts_sampler = NUTSda(
+            model=self.test_model, grad_log_pdf=GradLogPDFGaussian
+        )
 
     def test_errors(self):
         with self.assertRaises(TypeError):
             NUTS(model=self.test_model, grad_log_pdf=JGD)
         with self.assertRaises(TypeError):
-            NUTS(model=self.test_model, grad_log_pdf=None, simulate_dynamics=GradLogPDFGaussian)
+            NUTS(
+                model=self.test_model,
+                grad_log_pdf=None,
+                simulate_dynamics=GradLogPDFGaussian,
+            )
         with self.assertRaises(ValueError):
             NUTSda(model=self.test_model, delta=-0.2, grad_log_pdf=None)
         with self.assertRaises(ValueError):
             NUTSda(model=self.test_model, delta=1.1, grad_log_pdf=GradLogPDFGaussian)
         with self.assertRaises(TypeError):
-            NUTS(self.test_model, GradLogPDFGaussian).sample(initial_pos={1, 1, 1}, num_samples=1)
-        with self.assertRaises(ValueError):
-            NUTS(self.test_model, GradLogPDFGaussian).sample(initial_pos=[1, 1], num_samples=1)
-        with self.assertRaises(TypeError):
-            NUTSda(self.test_model, GradLogPDFGaussian).sample(initial_pos=1, num_samples=1, num_adapt=1)
-        with self.assertRaises(ValueError):
-            NUTSda(self.test_model, GradLogPDFGaussian).sample(initial_pos=[1, 1, 1, 1], num_samples=1, num_adapt=1)
-        with self.assertRaises(TypeError):
-            NUTS(self.test_model, GradLogPDFGaussian).generate_sample(initial_pos=0.1, num_samples=1).send(None)
-        with self.assertRaises(ValueError):
-            NUTS(self.test_model, GradLogPDFGaussian).generate_sample(initial_pos=(0, 1, 1, 1),
-                                                                      num_samples=1).send(None)
-        with self.assertRaises(TypeError):
-            NUTSda(self.test_model, GradLogPDFGaussian).generate_sample(initial_pos=[[1, 2, 3]], num_samples=1,
-                                                                        num_adapt=1).send(None)
-        with self.assertRaises(ValueError):
-            NUTSda(self.test_model, GradLogPDFGaussian).generate_sample(initial_pos=[1], num_samples=1,
-                                                                        num_adapt=1).send(None)
+            NUTS(self.test_model, GradLogPDFGaussian).sample(
+                initial_pos={1, 1, 1}, num_samples=1
+            )
+        with self.assertRaises(ValueError):
+            NUTS(self.test_model, GradLogPDFGaussian).sample(
+                initial_pos=[1, 1], num_samples=1
+            )
+        with self.assertRaises(TypeError):
+            NUTSda(self.test_model, GradLogPDFGaussian).sample(
+                initial_pos=1, num_samples=1, num_adapt=1
+            )
+        with self.assertRaises(ValueError):
+            NUTSda(self.test_model, GradLogPDFGaussian).sample(
+                initial_pos=[1, 1, 1, 1], num_samples=1, num_adapt=1
+            )
+        with self.assertRaises(TypeError):
+            NUTS(self.test_model, GradLogPDFGaussian).generate_sample(
+                initial_pos=0.1, num_samples=1
+            ).send(None)
+        with self.assertRaises(ValueError):
+            NUTS(self.test_model, GradLogPDFGaussian).generate_sample(
+                initial_pos=(0, 1, 1, 1), num_samples=1
+            ).send(None)
+        with self.assertRaises(TypeError):
+            NUTSda(self.test_model, GradLogPDFGaussian).generate_sample(
+                initial_pos=[[1, 2, 3]], num_samples=1, num_adapt=1
+            ).send(None)
+        with self.assertRaises(ValueError):
+            NUTSda(self.test_model, GradLogPDFGaussian).generate_sample(
+                initial_pos=[1], num_samples=1, num_adapt=1
+            ).send(None)
 
     def test_sampling(self):
         np.random.seed(1010101)
-        samples = self.nuts_sampler.sample(initial_pos=[-0.4, 1, 3.6], num_adapt=0, num_samples=10000,
-                                           return_type='recarray')
-        sample_array = np.array([samples[var_name] for var_name in self.test_model.variables])
+        samples = self.nuts_sampler.sample(
+            initial_pos=[-0.4, 1, 3.6],
+            num_adapt=0,
+            num_samples=10000,
+            return_type="recarray",
+        )
+        sample_array = np.array(
+            [samples[var_name] for var_name in self.test_model.variables]
+        )
         sample_covariance = np.cov(sample_array)
-        self.assertTrue(np.linalg.norm(sample_covariance - self.test_model.covariance) < 3)
+        self.assertTrue(
+            np.linalg.norm(sample_covariance - self.test_model.covariance) < 3
+        )
 
         np.random.seed(1210161)
-        samples = self.nuts_sampler.generate_sample(initial_pos=[-0.4, 1, 3.6], num_adapt=0, num_samples=10000)
+        samples = self.nuts_sampler.generate_sample(
+            initial_pos=[-0.4, 1, 3.6], num_adapt=0, num_samples=10000
+        )
         samples_array = np.array([sample for sample in samples])
         sample_covariance = np.cov(samples_array.T)
-        self.assertTrue(np.linalg.norm(sample_covariance - self.test_model.covariance) < 3)
+        self.assertTrue(
+            np.linalg.norm(sample_covariance - self.test_model.covariance) < 3
+        )
 
         np.random.seed(12313131)
-        samples = self.nuts_sampler.sample(initial_pos=[0.2, 0.4, 2.2], num_adapt=10000, num_samples=10000)
+        samples = self.nuts_sampler.sample(
+            initial_pos=[0.2, 0.4, 2.2], num_adapt=10000, num_samples=10000
+        )
         sample_covariance = np.cov(samples.values.T)
-        self.assertTrue(np.linalg.norm(sample_covariance - self.test_model.covariance) < 0.4)
+        self.assertTrue(
+            np.linalg.norm(sample_covariance - self.test_model.covariance) < 0.4
+        )
 
         np.random.seed(921312312)
-        samples = self.nuts_sampler.generate_sample(initial_pos=[0.2, 0.4, 2.2], num_adapt=10000, num_samples=10000)
+        samples = self.nuts_sampler.generate_sample(
+            initial_pos=[0.2, 0.4, 2.2], num_adapt=10000, num_samples=10000
+        )
         samples_array = np.array([sample for sample in samples])
         sample_covariance = np.cov(samples_array.T)
-        self.assertTrue(np.linalg.norm(sample_covariance - self.test_model.covariance) < 0.4)
+        self.assertTrue(
+            np.linalg.norm(sample_covariance - self.test_model.covariance) < 0.4
+        )
 
     def tearDown(self):
         del self.test_model
         del self.nuts_sampler
```

### Comparing `pgmpy-0.1.7/pgmpy/base/DirectedGraph.py` & `pgmpy-0.1.9/pgmpy/base/UndirectedGraph.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,130 +1,135 @@
 #!/usr/bin/env python3
 
 import itertools
 
 import networkx as nx
 
-from pgmpy.base import UndirectedGraph
 
-
-class DirectedGraph(nx.DiGraph):
+class UndirectedGraph(nx.Graph):
     """
-    Base class for all Directed Graphical Models.
+    Base class for all the Undirected Graphical models.
 
     Each node in the graph can represent either a random variable, `Factor`,
-    or a cluster of random variables. Edges in the graph represent the
-    dependencies between these.
+    or a cluster of random variables. Edges in the graph are interactions
+    between the nodes.
 
     Parameters
     ----------
     data: input graph
         Data to initialize graph. If data=None (default) an empty graph is
         created. The data can be an edge list or any Networkx graph object.
 
     Examples
     --------
-    Create an empty DirectedGraph with no nodes and no edges
+    Create an empty UndirectedGraph with no nodes and no edges
 
-    >>> from pgmpy.base import DirectedGraph
-    >>> G = DirectedGraph()
+    >>> from pgmpy.base import UndirectedGraph
+    >>> G = UndirectedGraph()
 
-    G can be grown in several ways:
+    G can be grown in several ways
 
     **Nodes:**
 
     Add one node at a time:
 
-    >>> G.add_node(node='a')
+    >>> G.add_node('a')
 
     Add the nodes from any container (a list, set or tuple or the nodes
     from another graph).
 
-    >>> G.add_nodes_from(nodes=['a', 'b'])
+    >>> G.add_nodes_from(['a', 'b'])
 
     **Edges:**
 
     G can also be grown by adding edges.
 
     Add one edge,
 
-    >>> G.add_edge(u='a', v='b')
+    >>> G.add_edge('a', 'b')
 
     a list of edges,
 
-    >>> G.add_edges_from(ebunch=[('a', 'b'), ('b', 'c')])
+    >>> G.add_edges_from([('a', 'b'), ('b', 'c')])
 
     If some edges connect nodes not yet in the model, the nodes
-    are added automatically. There are no errors when adding
+    are added automatically.  There are no errors when adding
     nodes or edges that already exist.
 
     **Shortcuts:**
 
     Many common graph features allow python syntax for speed reporting.
 
     >>> 'a' in G     # check if node in graph
     True
     >>> len(G)  # number of nodes in graph
     3
     """
 
     def __init__(self, ebunch=None):
-        super(DirectedGraph, self).__init__(ebunch)
+        super(UndirectedGraph, self).__init__(ebunch)
 
     def add_node(self, node, weight=None):
         """
-        Adds a single node to the Graph.
+        Add a single node to the Graph.
 
         Parameters
         ----------
         node: str, int, or any hashable python object.
             The node to add to the graph.
 
         weight: int, float
             The weight of the node.
 
         Examples
         --------
-        >>> from pgmpy.base import DirectedGraph
-        >>> G = DirectedGraph()
+        >>> from pgmpy.base import UndirectedGraph
+        >>> G = UndirectedGraph()
         >>> G.add_node(node='A')
         >>> G.nodes()
         ['A']
 
         Adding a node with some weight.
         >>> G.add_node(node='B', weight=0.3)
 
         The weight of these nodes can be accessed as:
         >>> G.node['B']
         {'weight': 0.3}
         >>> G.node['A']
         {'weight': None}
         """
-        super(DirectedGraph, self).add_node(node, weight=weight)
+        # Check for networkx 2.0 syntax
+        if isinstance(node, tuple) and len(node) == 2 and isinstance(node[1], dict):
+            node, attrs = node
+            if attrs.get("weight", None) is not None:
+                attrs["weight"] = weight
+        else:
+            attrs = {"weight": weight}
+        super(UndirectedGraph, self).add_node(node, weight=weight)
 
     def add_nodes_from(self, nodes, weights=None):
         """
         Add multiple nodes to the Graph.
 
-        **The behviour of adding weights is different than in networkx.
+        **The behaviour of adding weights is different than in networkx.
 
         Parameters
         ----------
         nodes: iterable container
             A container of nodes (list, dict, set, or any hashable python
             object).
 
         weights: list, tuple (default=None)
             A container of weights (int, float). The weight value at index i
             is associated with the variable at index i.
 
         Examples
         --------
-        >>> from pgmpy.base import DirectedGraph
-        >>> G = DirectedGraph()
+        >>> from pgmpy.base import UndirectedGraph
+        >>> G = UndirectedGraph()
         >>> G.add_nodes_from(nodes=['A', 'B', 'C'])
         >>> G.nodes()
         ['A', 'B', 'C']
 
         Adding nodes with weights:
         >>> G.add_nodes_from(nodes=['D', 'E'], weights=[0.3, 0.6])
         >>> G.node['D']
@@ -134,16 +139,17 @@
         >>> G.node['A']
         {'weight': None}
         """
         nodes = list(nodes)
 
         if weights:
             if len(nodes) != len(weights):
-                raise ValueError("The number of elements in nodes and weights"
-                                 "should be equal.")
+                raise ValueError(
+                    "The number of elements in nodes and weights" "should be equal."
+                )
             for index in range(len(nodes)):
                 self.add_node(node=nodes[index], weight=weights[index])
         else:
             for node in nodes:
                 self.add_node(node=node)
 
     def add_edge(self, u, v, weight=None):
@@ -155,20 +161,20 @@
 
         Parameters
         ----------
         u, v : nodes
             Nodes can be any hashable Python object.
 
         weight: int, float (default=None)
-            The weight of the edge
+            The weight of the edge.
 
         Examples
         --------
-        >>> from pgmpy.base import DirectedGraph
-        >>> G = DirectedGraph()
+        >>> from pgmpy.base import UndirectedGraph
+        >>> G = UndirectedGraph()
         >>> G.add_nodes_from(nodes=['Alice', 'Bob', 'Charles'])
         >>> G.add_edge(u='Alice', v='Bob')
         >>> G.nodes()
         ['Alice', 'Bob', 'Charles']
         >>> G.edges()
         [('Alice', 'Bob')]
 
@@ -180,15 +186,15 @@
         [('Alice', 'Bob'), ('Alice', 'Ankur')]
 
         Adding edges with weight:
         >>> G.add_edge('Ankur', 'Maria', weight=0.1)
         >>> G.edge['Ankur']['Maria']
         {'weight': 0.1}
         """
-        super(DirectedGraph, self).add_edge(u, v, weight=weight)
+        super(UndirectedGraph, self).add_edge(u, v, weight=weight)
 
     def add_edges_from(self, ebunch, weights=None):
         """
         Add all the edges in ebunch.
 
         If nodes referred in the ebunch are not already present, they
         will be automatically added. Node names can be any hashable python
@@ -204,27 +210,27 @@
 
         weights: list, tuple (default=None)
             A container of weights (int, float). The weight value at index i
             is associated with the edge at index i.
 
         Examples
         --------
-        >>> from pgmpy.base import DirectedGraph
-        >>> G = DirectedGraph()
+        >>> from pgmpy.base import UndirectedGraph
+        >>> G = UndirectedGraph()
         >>> G.add_nodes_from(nodes=['Alice', 'Bob', 'Charles'])
         >>> G.add_edges_from(ebunch=[('Alice', 'Bob'), ('Bob', 'Charles')])
         >>> G.nodes()
         ['Alice', 'Bob', 'Charles']
         >>> G.edges()
         [('Alice', 'Bob'), ('Bob', 'Charles')]
 
         When the node is not already in the model:
         >>> G.add_edges_from(ebunch=[('Alice', 'Ankur')])
         >>> G.nodes()
-        ['Alice', 'Bob', 'Charles', 'Ankur']
+        ['Alice', 'Ankur', 'Charles', 'Bob']
         >>> G.edges()
         [('Alice', 'Bob'), ('Bob', 'Charles'), ('Alice', 'Ankur')]
 
         Adding edges with weights:
         >>> G.add_edges_from([('Ankur', 'Maria'), ('Maria', 'Mason')],
         ...                  weights=[0.3, 0.5])
         >>> G.edge['Ankur']['Maria']
@@ -232,107 +238,67 @@
         >>> G.edge['Maria']['Mason']
         {'weight': 0.5}
         """
         ebunch = list(ebunch)
 
         if weights:
             if len(ebunch) != len(weights):
-                raise ValueError("The number of elements in ebunch and weights"
-                                 "should be equal")
+                raise ValueError(
+                    "The number of elements in ebunch and weights" "should be equal"
+                )
             for index in range(len(ebunch)):
-                self.add_edge(ebunch[index][0], ebunch[index][1],
-                              weight=weights[index])
+                self.add_edge(ebunch[index][0], ebunch[index][1], weight=weights[index])
         else:
             for edge in ebunch:
                 self.add_edge(edge[0], edge[1])
 
-    def get_parents(self, node):
+    def is_clique(self, nodes):
         """
-        Returns a list of parents of node.
-
-        Throws an error if the node is not present in the graph.
+        Check if the given nodes form a clique.
 
         Parameters
         ----------
-        node: string, int or any hashable python object.
-            The node whose parents would be returned.
+        nodes: list, array-like
+            List of nodes to check if they are a part of any clique.
 
         Examples
         --------
-        >>> from pgmpy.base import DirectedGraph
-        >>> G = DirectedGraph(ebunch=[('diff', 'grade'), ('intel', 'grade')])
-        >>> G.parents(node='grade')
-        ['diff', 'intel']
-        """
-        return self.predecessors(node)
-
-    def moralize(self):
-        """
-        Removes all the immoralities in the DirectedGraph and creates a moral
-        graph (UndirectedGraph).
+        >>> from pgmpy.base import UndirectedGraph
+        >>> G = UndirectedGraph(ebunch=[('A', 'B'), ('C', 'B'), ('B', 'D'),
+                                        ('B', 'E'), ('D', 'E'), ('E', 'F'),
+                                        ('D', 'F'), ('B', 'F')])
+        >>> G.is_clique(nodes=['A', 'B', 'C', 'D'])
+        False
+        >>> G.is_clique(nodes=['B', 'D', 'E', 'F'])
+        True
+
+        Since B, D, E and F are clique, any subset of these should also
+        be clique.
+        >>> G.is_clique(nodes=['D', 'E', 'B'])
+        True
+        """
+        for node1, node2 in itertools.combinations(nodes, 2):
+            if not self.has_edge(node1, node2):
+                return False
+        return True
 
-        A v-structure X->Z<-Y is an immorality if there is no directed edge
-        between X and Y.
-
-        Examples
-        --------
-        >>> from pgmpy.base import DirectedGraph
-        >>> G = DirectedGraph(ebunch=[('diff', 'grade'), ('intel', 'grade')])
-        >>> moral_graph = G.moralize()
-        >>> moral_graph.edges()
-        [('intel', 'grade'), ('intel', 'diff'), ('grade', 'diff')]
+    def is_triangulated(self):
         """
-        moral_graph = UndirectedGraph(self.to_undirected().edges())
+        Checks whether the undirected graph is triangulated (also known
+        as chordal) or not.
 
-        for node in self.nodes():
-            moral_graph.add_edges_from(
-                itertools.combinations(self.get_parents(node), 2))
-
-        return moral_graph
-
-    def get_leaves(self):
-        """
-        Returns a list of leaves of the graph.
-
-        Examples
-        --------
-        >>> from pgmpy.base import DirectedGraph
-        >>> graph = DirectedGraph([('A', 'B'), ('B', 'C'), ('B', 'D')])
-        >>> graph.get_leaves()
-        ['C', 'D']
-        """
-        return [node for node, out_degree in self.out_degree_iter() if
-                out_degree == 0]
-
-    def get_roots(self):
-        """
-        Returns a list of roots of the graph.
-
-        Examples
-        --------
-        >>> from pgmpy.base import DirectedGraph
-        >>> graph = DirectedGraph([('A', 'B'), ('B', 'C'), ('B', 'D'), ('E', 'B')])
-        >>> graph.get_roots()
-        ['A', 'E']
-        """
-        return [node for node, in_degree in self.in_degree().items() if in_degree == 0]
-    
-    def get_children(self, node):
-        """
-        Returns a list of children of node.
-        Throws an error if the node is not present in the graph.
-        
-        Parameters
-        ----------
-        node: string, int or any hashable python object.
-            The node whose children would be returned.
+        Chordal Graph: A chordal graph is one in which all cycles of four
+                       or more vertices have a chord.
 
         Examples
         --------
-        >>> from pgmpy.base import DirectedGraph
-        >>> g = DirectedGraph(ebunch=[('A', 'B'), ('C', 'B'), ('B', 'D'), 
-                                      ('B', 'E'), ('B', 'F'), ('E', 'G')])
-        >>> g.children(node='B')
-        ['D', 'E', 'F']
+        >>> from pgmpy.base import UndirectedGraph
+        >>> G = UndirectedGraph()
+        >>> G.add_edges_from(ebunch=[('x1', 'x2'), ('x1', 'x3'),
+        ...                          ('x2', 'x4'), ('x3', 'x4')])
+        >>> G.is_triangulated()
+        False
+        >>> G.add_edge(u='x1', v='x4')
+        >>> G.is_triangulated()
+        True
         """
-        return self.successors(node)
-    
+        return nx.is_chordal(self)
```

### Comparing `pgmpy-0.1.7/pgmpy/estimators/ConstraintBasedEstimator.py` & `pgmpy-0.1.9/pgmpy/estimators/ConstraintBasedEstimator.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,26 +1,26 @@
 #!/usr/bin/env python
 
 from warnings import warn
 from itertools import combinations
 
 from pgmpy.base import UndirectedGraph
-from pgmpy.models import BayesianModel
+from pgmpy.base import DAG
 from pgmpy.estimators import StructureEstimator
 from pgmpy.independencies import Independencies, IndependenceAssertion
 
 
 class ConstraintBasedEstimator(StructureEstimator):
     def __init__(self, data, **kwargs):
         """
-        Class for constraint-based estimation of BayesianModels from a given
+        Class for constraint-based estimation of DAGs from a given
         data set. Identifies (conditional) dependencies in data set using
         chi_square dependency test and uses the PC algorithm to estimate a DAG
         pattern that satisfies the identified dependencies. The DAG pattern can
-        then be completed to a faithful BayesianModel, if possible.
+        then be completed to a faithful DAG, if possible.
 
         Parameters
         ----------
         data: pandas DataFrame object
             datafame object where each column represents one variable.
             (If some values in the data are missing the data cells should be set to `numpy.NaN`.
             Note that pandas converts each column containing `numpy.NaN`s to dtype `float`.)
@@ -43,35 +43,35 @@
         [2] Neapolitan, Learning Bayesian Networks, Section 10.1.2 for the PC algorithm (page 550),
         http://www.cs.technion.ac.il/~dang/books/Learning%20Bayesian%20Networks(Neapolitan,%20Richard).pdf
         """
         super(ConstraintBasedEstimator, self).__init__(data, **kwargs)
 
     def estimate(self, significance_level=0.01):
         """
-        Estimates a BayesianModel for the data set, using the PC constraint-based
+        Estimates a DAG for the data set, using the PC constraint-based
         structure learning algorithm. Independencies are identified from the
         data set using a chi-squared statistic with the acceptance threshold of
         `significance_level`. PC identifies a partially directed acyclic graph (PDAG), given
         that the tested independencies admit a faithful Bayesian network representation.
-        This method returns a BayesianModel that is a completion of this PDAG.
+        This method returns a DAG that is a completion of this PDAG.
 
         Parameters
         ----------
         significance_level: float, default: 0.01
             The significance level to use for conditional independence tests in the data set.
 
             `significance_level` is the desired Type 1 error probability of
             falsely rejecting the null hypothesis that variables are independent,
             given that they are. The lower `significance_level`, the less likely
             we are to accept dependencies, resulting in a sparser graph.
 
         Returns
         -------
-        model: BayesianModel()-instance
-            An estimate for the BayesianModel for the data set (not yet parametrized).
+        model: DAG()-instance
+            An estimate for the DAG for the data set (not yet parametrized).
 
         Reference
         ---------
         Neapolitan, Learning Bayesian Networks, Section 10.1.2, Algorithm 10.2 (page 550)
         http://www.cs.technion.ac.il/~dang/books/Learning%20Bayesian%20Networks(Neapolitan,%20Richard).pdf
 
         Examples
@@ -171,22 +171,21 @@
 
         nodes = self.state_names.keys()
 
         def is_independent(X, Y, Zs):
             """Returns result of hypothesis test for the null hypothesis that
             X _|_ Y | Zs, using a chi2 statistic and threshold `significance_level`.
             """
-            chi2, p_value, sufficient_data = self.test_conditional_independence(X, Y, Zs)
-            return p_value >= significance_level
+            return self.test_conditional_independence(X, Y, Zs, method="chi_square")
 
         return self.build_skeleton(nodes, is_independent)
 
     @staticmethod
     def estimate_from_independencies(nodes, independencies):
-        """Estimates a BayesianModel from an Independencies()-object or a
+        """Estimates a DAG from an Independencies()-object or a
         decision function for conditional independencies. This requires that
         the set of independencies admits a faithful representation (e.g. is a
         set of d-seperation for some BN or is closed under the semi-graphoid
         axioms). See `build_skeleton`, `skeleton_to_pdag`, `pdag_to_dag` for
         details.
 
         Parameters
@@ -199,59 +198,61 @@
             The provided Independencies should admit a faithful representation.
             Can either be provided as an Independencies()-instance or by passing a
             function `f(X, Y, Zs)` that returns `True` when X _|_ Y | Zs,
             otherwise `False`. (X, Y being individual nodes and Zs a list of nodes).
 
         Returns
         -------
-        model: BayesianModel instance
+        model: DAG instance
 
         Examples
         --------
         >>> from pgmpy.estimators import ConstraintBasedEstimator
-        >>> from pgmpy.models import BayesianModel
+        >>> from pgmpy.models import DAG
         >>> from pgmpy.independencies import Independencies
 
         >>> ind = Independencies(['B', 'C'], ['A', ['B', 'C'], 'D'])
         >>> ind = ind.closure()
         >>> skel = ConstraintBasedEstimator.estimate_from_independencies("ABCD", ind)
         >>> print(skel.edges())
         [('B', 'D'), ('A', 'D'), ('C', 'D')]
 
-        >>> model = BayesianModel([('A', 'C'), ('B', 'C'), ('B', 'D'), ('C', 'E')])
+        >>> model = DAG([('A', 'C'), ('B', 'C'), ('B', 'D'), ('C', 'E')])
         >>> skel = ConstraintBasedEstimator.estimate_from_independencies(model.nodes(), model.get_independencies())
         >>> print(skel.edges())
         [('B', 'C'), ('A', 'C'), ('C', 'E'), ('D', 'B')]
         >>> # note that ('D', 'B') is flipped compared to the original network;
         >>> # Both networks belong to the same PDAG/are I-equivalent
         """
 
-        skel, separating_sets = ConstraintBasedEstimator.build_skeleton(nodes, independencies)
+        skel, separating_sets = ConstraintBasedEstimator.build_skeleton(
+            nodes, independencies
+        )
         pdag = ConstraintBasedEstimator.skeleton_to_pdag(skel, separating_sets)
         dag = ConstraintBasedEstimator.pdag_to_dag(pdag)
 
         return dag
 
     @staticmethod
     def pdag_to_dag(pdag):
         """Completes a PDAG to a DAG, without adding v-structures, if such a
         completion exists. If no faithful extension is possible, some fully
         oriented DAG that corresponds to the PDAG is returned and a warning is
         generated. This is a static method.
 
         Parameters
         ----------
-        pdag: DirectedGraph
+        pdag: DAG
             A directed acyclic graph pattern, consisting in (acyclic) directed edges
             as well as "undirected" edges, represented as both-way edges between
             nodes.
 
         Returns
         -------
-        dag: BayesianModel
+        dag: DAG
             A faithful orientation of pdag, if one exists. Otherwise any
             fully orientated DAG/BayesianModel with the structure of pdag.
 
         References
         ----------
         [1] Chickering, Learning Equivalence Classes of Bayesian-Network Structures,
             2002; See page 454 (last paragraph) for the algorithm pdag_to_dag
@@ -260,120 +261,134 @@
             of a partially oriented graph, 1992,
             http://ftp.cs.ucla.edu/pub/stat_ser/r185-dor-tarsi.pdf
 
         Examples
         --------
         >>> import pandas as pd
         >>> import numpy as np
-        >>> from pgmpy.base import DirectedGraph
+        >>> from pgmpy.base import DAG
         >>> from pgmpy.estimators import ConstraintBasedEstimator
         >>> data = pd.DataFrame(np.random.randint(0, 4, size=(5000, 3)), columns=list('ABD'))
         >>> data['C'] = data['A'] - data['B']
         >>> data['D'] += data['A']
         >>> c = ConstraintBasedEstimator(data)
         >>> pdag = c.skeleton_to_pdag(*c.estimate_skeleton())
         >>> pdag.edges()
         [('B', 'C'), ('D', 'A'), ('A', 'D'), ('A', 'C')]
         >>> c.pdag_to_dag(pdag).edges()
         [('B', 'C'), ('A', 'D'), ('A', 'C')]
 
         >>> # pdag_to_dag is static:
-        ... pdag1 = DirectedGraph([('A', 'B'), ('C', 'B'), ('C', 'D'), ('D', 'C'), ('D', 'A'), ('A', 'D')])
+        ... pdag1 = DAG([('A', 'B'), ('C', 'B'), ('C', 'D'), ('D', 'C'), ('D', 'A'), ('A', 'D')])
         >>> ConstraintBasedEstimator.pdag_to_dag(pdag1).edges()
         [('D', 'C'), ('C', 'B'), ('A', 'B'), ('A', 'D')]
 
         >>> # example of a pdag with no faithful extension:
-        ... pdag2 = DirectedGraph([('A', 'B'), ('A', 'C'), ('B', 'C'), ('C', 'B')])
+        ... pdag2 = DAG([('A', 'B'), ('A', 'C'), ('B', 'C'), ('C', 'B')])
         >>> ConstraintBasedEstimator.pdag_to_dag(pdag2).edges()
         UserWarning: PDAG has no faithful extension (= no oriented DAG with the same v-structures as PDAG).
         Remaining undirected PDAG edges oriented arbitrarily.
         [('B', 'C'), ('A', 'B'), ('A', 'C')]
         """
 
         pdag = pdag.copy()
-        dag = BayesianModel()
+        dag = DAG()
         dag.add_nodes_from(pdag.nodes())
 
         # add already directed edges of pdag to dag
         for X, Y in pdag.edges():
             if not pdag.has_edge(Y, X):
                 dag.add_edge(X, Y)
 
         while pdag.number_of_nodes() > 0:
             # find node with (1) no directed outgoing edges and
             #                (2) the set of undirected neighbors is either empty or
             #                    undirected neighbors + parents of X are a clique
             found = False
             for X in pdag.nodes():
-                directed_outgoing_edges = set(pdag.successors(X)) - set(pdag.predecessors(X))
-                undirected_neighbors = set(pdag.successors(X)) & set(pdag.predecessors(X))
-                neighbors_are_clique = all((pdag.has_edge(Y, Z)
-                                            for Z in pdag.predecessors(X)
-                                            for Y in undirected_neighbors if not Y == Z))
-
-                if not directed_outgoing_edges and \
-                        (not undirected_neighbors or neighbors_are_clique):
+                directed_outgoing_edges = set(pdag.successors(X)) - set(
+                    pdag.predecessors(X)
+                )
+                undirected_neighbors = set(pdag.successors(X)) & set(
+                    pdag.predecessors(X)
+                )
+                neighbors_are_clique = all(
+                    (
+                        pdag.has_edge(Y, Z)
+                        for Z in pdag.predecessors(X)
+                        for Y in undirected_neighbors
+                        if not Y == Z
+                    )
+                )
+
+                if not directed_outgoing_edges and (
+                    not undirected_neighbors or neighbors_are_clique
+                ):
                     found = True
                     # add all edges of X as outgoing edges to dag
                     for Y in pdag.predecessors(X):
                         dag.add_edge(Y, X)
 
                     pdag.remove_node(X)
                     break
 
             if not found:
-                warn("PDAG has no faithful extension (= no oriented DAG with the " +
-                     "same v-structures as PDAG). Remaining undirected PDAG edges " +
-                     "oriented arbitrarily.")
+                warn(
+                    "PDAG has no faithful extension (= no oriented DAG with the "
+                    + "same v-structures as PDAG). Remaining undirected PDAG edges "
+                    + "oriented arbitrarily."
+                )
                 for X, Y in pdag.edges():
                     if not dag.has_edge(Y, X):
                         try:
                             dag.add_edge(X, Y)
                         except ValueError:
                             pass
                 break
 
         return dag
 
     @staticmethod
     def model_to_pdag(model):
         """Construct the DAG pattern (representing the I-equivalence class) for
-        a given BayesianModel. This is the "inverse" to pdag_to_dag.
+        a given DAG. This is the "inverse" to pdag_to_dag.
         """
 
-        if not isinstance(model, BayesianModel):
-            raise TypeError("model: Expected BayesianModel instance, " +
-                            "got type {model_type}".format(model_type=type(model)))
+        if not isinstance(model, DAG):
+            raise TypeError(
+                "model: Expected DAG instance, "
+                + "got type {model_type}".format(model_type=type(model))
+            )
 
         skel, separating_sets = ConstraintBasedEstimator.build_skeleton(
-                                    model.nodes(),
-                                    model.get_independencies())
+            model.nodes(), model.get_independencies()
+        )
         pdag = ConstraintBasedEstimator.skeleton_to_pdag(skel, separating_sets)
 
         return pdag
 
     @staticmethod
     def skeleton_to_pdag(skel, separating_sets):
         """Orients the edges of a graph skeleton based on information from
-        `separating_sets` to form a DAG pattern (DirectedGraph).
+        `separating_sets` to form a DAG pattern (DAG).
 
         Parameters
         ----------
         skel: UndirectedGraph
             An undirected graph skeleton as e.g. produced by the
             estimate_skeleton method.
 
         separating_sets: dict
             A dict containing for each pair of not directly connected nodes a
             separating set ("witnessing set") of variables that makes then
             conditionally independent. (needed for edge orientation)
 
         Returns
         -------
-        pdag: DirectedGraph
+        pdag: DAG
             An estimate for the DAG pattern of the BN underlying the data. The
             graph might contain some nodes with both-way edges (X->Y and Y->X).
             Any completion by (removing one of the both-way edges for each such
             pair) results in a I-equivalent Bayesian network DAG.
 
         Reference
         ---------
@@ -408,16 +423,17 @@
 
         progress = True
         while progress:  # as long as edges can be oriented (removed)
             num_edges = pdag.number_of_edges()
 
             # 2) for each X->Z-Y, orient edges to Z->Y
             for X, Y in node_pairs:
-                for Z in ((set(pdag.successors(X)) - set(pdag.predecessors(X))) &
-                          (set(pdag.successors(Y)) & set(pdag.predecessors(Y)))):
+                for Z in (set(pdag.successors(X)) - set(pdag.predecessors(X))) & (
+                    set(pdag.successors(Y)) & set(pdag.predecessors(Y))
+                ):
                     pdag.remove(Y, Z)
 
             # 3) for each X-Y with a directed path from X to Y, orient edges to X->Y
             for X, Y in node_pairs:
                 for path in nx.all_simple_paths(pdag, X, Y):
                     is_directed = True
                     for src, dst in path:
@@ -425,19 +441,25 @@
                             is_directed = False
                     if is_directed:
                         pdag.remove(Y, X)
                         break
 
             # 4) for each X-Z-Y with X->W, Y->W, and Z-W, orient edges to Z->W
             for X, Y in node_pairs:
-                for Z in (set(pdag.successors(X)) & set(pdag.predecessors(X)) &
-                          set(pdag.successors(Y)) & set(pdag.predecessors(Y))):
-                    for W in ((set(pdag.successors(X)) - set(pdag.predecessors(X))) &
-                              (set(pdag.successors(Y)) - set(pdag.predecessors(Y))) &
-                              (set(pdag.successors(Z)) & set(pdag.predecessors(Z)))):
+                for Z in (
+                    set(pdag.successors(X))
+                    & set(pdag.predecessors(X))
+                    & set(pdag.successors(Y))
+                    & set(pdag.predecessors(Y))
+                ):
+                    for W in (
+                        (set(pdag.successors(X)) - set(pdag.predecessors(X)))
+                        & (set(pdag.successors(Y)) - set(pdag.predecessors(Y)))
+                        & (set(pdag.successors(Z)) & set(pdag.predecessors(Z)))
+                    ):
                         pdag.remove(W, Z)
 
             progress = num_edges > pdag.number_of_edges()
 
         return pdag
 
     @staticmethod
@@ -482,53 +504,63 @@
             http://www.cs.technion.ac.il/~dang/books/Learning%20Bayesian%20Networks(Neapolitan,%20Richard).pdf
         [2] Koller & Friedman, Probabilistic Graphical Models - Principles and Techniques, 2009
             Section 3.4.2.1 (page 85), Algorithm 3.3
 
         Examples
         --------
         >>> from pgmpy.estimators import ConstraintBasedEstimator
-        >>> from pgmpy.models import BayesianModel
+        >>> from pgmpy.models import DAG
         >>> from pgmpy.independencies import Independencies
 
         >>> # build skeleton from list of independencies:
         ... ind = Independencies(['B', 'C'], ['A', ['B', 'C'], 'D'])
         >>> # we need to compute closure, otherwise this set of independencies doesn't
         ... # admit a faithful representation:
         ... ind = ind.closure()
         >>> skel, sep_sets = ConstraintBasedEstimator.build_skeleton("ABCD", ind)
         >>> print(skel.edges())
         [('A', 'D'), ('B', 'D'), ('C', 'D')]
 
-        >>> # build skeleton from d-seperations of BayesianModel:
-        ... model = BayesianModel([('A', 'C'), ('B', 'C'), ('B', 'D'), ('C', 'E')])
+        >>> # build skeleton from d-seperations of DAG:
+        ... model = DAG([('A', 'C'), ('B', 'C'), ('B', 'D'), ('C', 'E')])
         >>> skel, sep_sets = ConstraintBasedEstimator.build_skeleton(model.nodes(), model.get_independencies())
         >>> print(skel.edges())
         [('A', 'C'), ('B', 'C'), ('B', 'D'), ('C', 'E')]
         """
 
         nodes = list(nodes)
 
         if isinstance(independencies, Independencies):
+
             def is_independent(X, Y, Zs):
                 return IndependenceAssertion(X, Y, Zs) in independencies
+
         elif callable(independencies):
             is_independent = independencies
         else:
-            raise ValueError("'independencies' must be either Independencies-instance " +
-                             "or a ternary function that decides independencies.")
+            raise ValueError(
+                "'independencies' must be either Independencies-instance "
+                + "or a ternary function that decides independencies."
+            )
 
         graph = UndirectedGraph(combinations(nodes, 2))
         lim_neighbors = 0
         separating_sets = dict()
-        while not all([len(graph.neighbors(node)) < lim_neighbors for node in nodes]):
+        while not all(
+            [len(list(graph.neighbors(node))) < lim_neighbors for node in nodes]
+        ):
             for node in nodes:
-                for neighbor in graph.neighbors(node):
+                for neighbor in list(graph.neighbors(node)):
                     # search if there is a set of neighbors (of size lim_neighbors)
                     # that makes X and Y independent:
-                    for separating_set in combinations(set(graph.neighbors(node)) - set([neighbor]), lim_neighbors):
+                    for separating_set in combinations(
+                        set(graph.neighbors(node)) - set([neighbor]), lim_neighbors
+                    ):
                         if is_independent(node, neighbor, separating_set):
-                            separating_sets[frozenset((node, neighbor))] = separating_set
+                            separating_sets[
+                                frozenset((node, neighbor))
+                            ] = separating_set
                             graph.remove_edge(node, neighbor)
                             break
             lim_neighbors += 1
 
         return graph, separating_sets
```

### Comparing `pgmpy-0.1.7/pgmpy/estimators/StructureScore.py` & `pgmpy-0.1.9/pgmpy/estimators/StructureScore.py`

 * *Files identical despite different names*

### Comparing `pgmpy-0.1.7/pgmpy/estimators/BdeuScore.py` & `pgmpy-0.1.9/pgmpy/estimators/BdeuScore.py`

 * *Files 5% similar despite different names*

```diff
@@ -41,28 +41,37 @@
         [2] AM Carvalho, Scoring functions for learning Bayesian networks,
         http://www.lx.it.pt/~asmc/pub/talks/09-TA/ta_pres.pdf
         """
         self.equivalent_sample_size = equivalent_sample_size
         super(BdeuScore, self).__init__(data, **kwargs)
 
     def local_score(self, variable, parents):
-        "Computes a score that measures how much a \
-        given variable is \"influenced\" by a given list of potential parents."
+        'Computes a score that measures how much a \
+        given variable is "influenced" by a given list of potential parents.'
 
         var_states = self.state_names[variable]
         var_cardinality = len(var_states)
         state_counts = self.state_counts(variable, parents)
         num_parents_states = float(len(state_counts.columns))
 
         score = 0
-        for parents_state in state_counts:  # iterate over df columns (only 1 if no parents)
+        for (
+            parents_state
+        ) in state_counts:  # iterate over df columns (only 1 if no parents)
             conditional_sample_size = sum(state_counts[parents_state])
 
-            score += (lgamma(self.equivalent_sample_size / num_parents_states) -
-                      lgamma(conditional_sample_size + self.equivalent_sample_size / num_parents_states))
+            score += lgamma(self.equivalent_sample_size / num_parents_states) - lgamma(
+                conditional_sample_size
+                + self.equivalent_sample_size / num_parents_states
+            )
 
             for state in var_states:
                 if state_counts[parents_state][state] > 0:
-                    score += (lgamma(state_counts[parents_state][state] +
-                                     self.equivalent_sample_size / (num_parents_states * var_cardinality)) -
-                              lgamma(self.equivalent_sample_size / (num_parents_states * var_cardinality)))
+                    score += lgamma(
+                        state_counts[parents_state][state]
+                        + self.equivalent_sample_size
+                        / (num_parents_states * var_cardinality)
+                    ) - lgamma(
+                        self.equivalent_sample_size
+                        / (num_parents_states * var_cardinality)
+                    )
         return score
```

### Comparing `pgmpy-0.1.7/pgmpy/estimators/BayesianEstimator.py` & `pgmpy-0.1.9/pgmpy/estimators/BayesianEstimator.py`

 * *Files 6% similar despite different names*

```diff
@@ -10,19 +10,23 @@
 class BayesianEstimator(ParameterEstimator):
     def __init__(self, model, data, **kwargs):
         """
         Class used to compute parameters for a model using Bayesian Parameter Estimation.
         See `MaximumLikelihoodEstimator` for constructor parameters.
         """
         if not isinstance(model, BayesianModel):
-            raise NotImplementedError("Bayesian Parameter Estimation is only implemented for BayesianModel")
+            raise NotImplementedError(
+                "Bayesian Parameter Estimation is only implemented for BayesianModel"
+            )
 
         super(BayesianEstimator, self).__init__(model, data, **kwargs)
 
-    def get_parameters(self, prior_type='BDeu', equivalent_sample_size=5, pseudo_counts=None):
+    def get_parameters(
+        self, prior_type="BDeu", equivalent_sample_size=5, pseudo_counts=None
+    ):
         """
         Method to estimate the model parameters (CPDs).
 
         Parameters
         ----------
         prior_type: 'dirichlet', 'BDeu', or 'K2'
             string indicting which type of prior to use for the model parameters.
@@ -60,27 +64,34 @@
         <TabularCPD representing P(B:2 | C:2, A:2) at 0x7f7b4dfd4da0>,
         <TabularCPD representing P(A:2) at 0x7f7b4dfd4fd0>,
         <TabularCPD representing P(D:2 | C:2) at 0x7f7b4df822b0>]
         """
         parameters = []
 
         for node in self.model.nodes():
-            _equivalent_sample_size = equivalent_sample_size[node] if isinstance(equivalent_sample_size, dict) else \
-                                      equivalent_sample_size
+            _equivalent_sample_size = (
+                equivalent_sample_size[node]
+                if isinstance(equivalent_sample_size, dict)
+                else equivalent_sample_size
+            )
             _pseudo_counts = pseudo_counts[node] if pseudo_counts else None
 
-            cpd = self.estimate_cpd(node,
-                                    prior_type=prior_type,
-                                    equivalent_sample_size=_equivalent_sample_size,
-                                    pseudo_counts=_pseudo_counts)
+            cpd = self.estimate_cpd(
+                node,
+                prior_type=prior_type,
+                equivalent_sample_size=_equivalent_sample_size,
+                pseudo_counts=_pseudo_counts,
+            )
             parameters.append(cpd)
 
         return parameters
 
-    def estimate_cpd(self, node, prior_type='BDeu', pseudo_counts=[], equivalent_sample_size=5):
+    def estimate_cpd(
+        self, node, prior_type="BDeu", pseudo_counts=[], equivalent_sample_size=5
+    ):
         """
         Method to estimate the CPD for a given variable.
 
         Parameters
         ----------
         node: int, string (any hashable python object)
             The name of the variable for which the CPD is to be estimated.
@@ -126,29 +137,38 @@
         """
 
         node_cardinality = len(self.state_names[node])
         parents = sorted(self.model.get_parents(node))
         parents_cardinalities = [len(self.state_names[parent]) for parent in parents]
         cpd_shape = (node_cardinality, np.prod(parents_cardinalities, dtype=int))
 
-        if prior_type == 'K2':
+        if prior_type == "K2":
             pseudo_counts = np.ones(cpd_shape, dtype=int)
-        elif prior_type == 'BDeu':
-            alpha = float(equivalent_sample_size) / (node_cardinality * np.prod(parents_cardinalities))
+        elif prior_type == "BDeu":
+            alpha = float(equivalent_sample_size) / (
+                node_cardinality * np.prod(parents_cardinalities)
+            )
             pseudo_counts = np.ones(cpd_shape, dtype=float) * alpha
-        elif prior_type == 'dirichlet':
+        elif prior_type == "dirichlet":
             pseudo_counts = np.array(pseudo_counts)
             if pseudo_counts.shape != cpd_shape:
-                raise ValueError("The shape of pseudo_counts must be: {shape}".format(
-                   shape=str(cpd_shape)))
+                raise ValueError(
+                    "The shape of pseudo_counts for the node: {node} must be of shape: {shape}".format(
+                        node=node, shape=str(cpd_shape)
+                    )
+                )
         else:
             raise ValueError("'prior_type' not specified")
 
         state_counts = self.state_counts(node)
         bayesian_counts = state_counts + pseudo_counts
 
-        cpd = TabularCPD(node, node_cardinality, np.array(bayesian_counts),
-                         evidence=parents,
-                         evidence_card=parents_cardinalities,
-                         state_names=self.state_names)
+        cpd = TabularCPD(
+            node,
+            node_cardinality,
+            np.array(bayesian_counts),
+            evidence=parents,
+            evidence_card=parents_cardinalities,
+            state_names=self.state_names,
+        )
         cpd.normalize()
         return cpd
```

### Comparing `pgmpy-0.1.7/pgmpy/estimators/K2Score.py` & `pgmpy-0.1.9/pgmpy/estimators/K2Score.py`

 * *Files 4% similar despite different names*

```diff
@@ -35,24 +35,28 @@
         Section 18.3.4-18.3.6 (esp. page 806)
         [2] AM Carvalho, Scoring functions for learning Bayesian networks,
         http://www.lx.it.pt/~asmc/pub/talks/09-TA/ta_pres.pdf
         """
         super(K2Score, self).__init__(data, **kwargs)
 
     def local_score(self, variable, parents):
-        "Computes a score that measures how much a \
-        given variable is \"influenced\" by a given list of potential parents."
+        'Computes a score that measures how much a \
+        given variable is "influenced" by a given list of potential parents.'
 
         var_states = self.state_names[variable]
         var_cardinality = len(var_states)
         state_counts = self.state_counts(variable, parents)
 
         score = 0
-        for parents_state in state_counts:  # iterate over df columns (only 1 if no parents)
+        for (
+            parents_state
+        ) in state_counts:  # iterate over df columns (only 1 if no parents)
             conditional_sample_size = sum(state_counts[parents_state])
 
-            score += lgamma(var_cardinality) - lgamma(conditional_sample_size + var_cardinality)
+            score += lgamma(var_cardinality) - lgamma(
+                conditional_sample_size + var_cardinality
+            )
 
             for state in var_states:
                 if state_counts[parents_state][state] > 0:
                     score += lgamma(state_counts[parents_state][state] + 1)
         return score
```

### Comparing `pgmpy-0.1.7/pgmpy/estimators/BicScore.py` & `pgmpy-0.1.9/pgmpy/estimators/BicScore.py`

 * *Files 4% similar despite different names*

```diff
@@ -37,28 +37,32 @@
         Section 18.3.4-18.3.6 (esp. page 802)
         [2] AM Carvalho, Scoring functions for learning Bayesian networks,
         http://www.lx.it.pt/~asmc/pub/talks/09-TA/ta_pres.pdf
         """
         super(BicScore, self).__init__(data, **kwargs)
 
     def local_score(self, variable, parents):
-        "Computes a score that measures how much a \
-        given variable is \"influenced\" by a given list of potential parents."
+        'Computes a score that measures how much a \
+        given variable is "influenced" by a given list of potential parents.'
 
         var_states = self.state_names[variable]
         var_cardinality = len(var_states)
         state_counts = self.state_counts(variable, parents)
         sample_size = len(self.data)
         num_parents_states = float(len(state_counts.columns))
 
         score = 0
-        for parents_state in state_counts:  # iterate over df columns (only 1 if no parents)
+        for (
+            parents_state
+        ) in state_counts:  # iterate over df columns (only 1 if no parents)
             conditional_sample_size = sum(state_counts[parents_state])
 
             for state in var_states:
                 if state_counts[parents_state][state] > 0:
-                    score += state_counts[parents_state][state] * (log(state_counts[parents_state][state]) -
-                                                                   log(conditional_sample_size))
+                    score += state_counts[parents_state][state] * (
+                        log(state_counts[parents_state][state])
+                        - log(conditional_sample_size)
+                    )
 
         score -= 0.5 * log(sample_size) * num_parents_states * (var_cardinality - 1)
 
         return score
```

### Comparing `pgmpy-0.1.7/pgmpy/estimators/HillClimbSearch.py` & `pgmpy-0.1.9/pgmpy/estimators/HillClimbSearch.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,20 +1,20 @@
 #!/usr/bin/env python
 from itertools import permutations
 
 import networkx as nx
 
 from pgmpy.estimators import StructureEstimator, K2Score
-from pgmpy.models import BayesianModel
+from pgmpy.base import DAG
 
 
 class HillClimbSearch(StructureEstimator):
     def __init__(self, data, scoring_method=None, **kwargs):
         """
-        Class for heuristic hill climb searches for BayesianModels, to learn
+        Class for heuristic hill climb searches for DAGs, to learn
         network structure from data. `estimate` attempts to find a model with optimal score.
 
         Parameters
         ----------
         data: pandas DataFrame object
             datafame object where each column represents one variable.
             (If some values in the data are missing the data cells should be set to `numpy.NaN`.
@@ -48,78 +48,96 @@
         (1) add, (2) remove, or (3) flip a single edge. For details on scoring
         see Koller & Fridman, Probabilistic Graphical Models, Section 18.4.3.3 (page 818).
         If a number `max_indegree` is provided, only modifications that keep the number
         of parents for each node below `max_indegree` are considered."""
 
         local_score = self.scoring_method.local_score
         nodes = self.state_names.keys()
-        potential_new_edges = (set(permutations(nodes, 2)) -
-                               set(model.edges()) -
-                               set([(Y, X) for (X, Y) in model.edges()]))
+        potential_new_edges = (
+            set(permutations(nodes, 2))
+            - set(model.edges())
+            - set([(Y, X) for (X, Y) in model.edges()])
+        )
 
         for (X, Y) in potential_new_edges:  # (1) add single edge
-            if nx.is_directed_acyclic_graph(nx.DiGraph(model.edges() + [(X, Y)])):
-                operation = ('+', (X, Y))
+            if nx.is_directed_acyclic_graph(nx.DiGraph(list(model.edges()) + [(X, Y)])):
+                operation = ("+", (X, Y))
                 if operation not in tabu_list:
                     old_parents = model.get_parents(Y)
                     new_parents = old_parents + [X]
                     if max_indegree is None or len(new_parents) <= max_indegree:
-                        score_delta = local_score(Y, new_parents) - local_score(Y, old_parents)
-                        yield(operation, score_delta)
+                        score_delta = local_score(Y, new_parents) - local_score(
+                            Y, old_parents
+                        )
+                        yield (operation, score_delta)
 
         for (X, Y) in model.edges():  # (2) remove single edge
-            operation = ('-', (X, Y))
+            operation = ("-", (X, Y))
             if operation not in tabu_list:
                 old_parents = model.get_parents(Y)
                 new_parents = old_parents[:]
                 new_parents.remove(X)
                 score_delta = local_score(Y, new_parents) - local_score(Y, old_parents)
-                yield(operation, score_delta)
+                yield (operation, score_delta)
 
         for (X, Y) in model.edges():  # (3) flip single edge
-            new_edges = model.edges() + [(Y, X)]
+            new_edges = list(model.edges()) + [(Y, X)]
             new_edges.remove((X, Y))
             if nx.is_directed_acyclic_graph(nx.DiGraph(new_edges)):
-                operation = ('flip', (X, Y))
-                if operation not in tabu_list and ('flip', (Y, X)) not in tabu_list:
+                operation = ("flip", (X, Y))
+                if operation not in tabu_list and ("flip", (Y, X)) not in tabu_list:
                     old_X_parents = model.get_parents(X)
                     old_Y_parents = model.get_parents(Y)
                     new_X_parents = old_X_parents + [Y]
                     new_Y_parents = old_Y_parents[:]
                     new_Y_parents.remove(X)
                     if max_indegree is None or len(new_X_parents) <= max_indegree:
-                        score_delta = (local_score(X, new_X_parents) +
-                                       local_score(Y, new_Y_parents) -
-                                       local_score(X, old_X_parents) -
-                                       local_score(Y, old_Y_parents))
-                        yield(operation, score_delta)
-
-    def estimate(self, start=None, tabu_length=0, max_indegree=None):
+                        score_delta = (
+                            local_score(X, new_X_parents)
+                            + local_score(Y, new_Y_parents)
+                            - local_score(X, old_X_parents)
+                            - local_score(Y, old_Y_parents)
+                        )
+                        yield (operation, score_delta)
+
+    def estimate(
+        self, start=None, tabu_length=0, max_indegree=None, epsilon=1e-4, max_iter=1e6
+    ):
         """
-        Performs local hill climb search to estimates the `BayesianModel` structure
+        Performs local hill climb search to estimates the `DAG` structure
         that has optimal score, according to the scoring method supplied in the constructor.
         Starts at model `start` and proceeds by step-by-step network modifications
         until a local maximum is reached. Only estimates network structure, no parametrization.
 
         Parameters
         ----------
-        start: BayesianModel instance
+        start: DAG instance
             The starting point for the local search. By default a completely disconnected network is used.
+
         tabu_length: int
             If provided, the last `tabu_length` graph modifications cannot be reversed
             during the search procedure. This serves to enforce a wider exploration
             of the search space. Default value: 100.
+
         max_indegree: int or None
             If provided and unequal None, the procedure only searches among models
             where all nodes have at most `max_indegree` parents. Defaults to None.
 
+        epsilon: float (default: 1e-4)
+            Defines the exit condition. If the improvement in score is less than `epsilon`,
+            the learned model is returned.
+
+        max_iter: int (default: 1e6)
+            The maximum number of iterations allowed. Returns the learned model when the
+            number of iterations is greater than `max_iter`.
+
         Returns
         -------
-        model: `BayesianModel` instance
-            A `BayesianModel` at a (local) score maximum.
+        model: `DAG` instance
+            A `DAG` at a (local) score maximum.
 
         Examples
         --------
         >>> import pandas as pd
         >>> import numpy as np
         >>> from pgmpy.estimators import HillClimbSearch, BicScore
         >>> # create data sample with 9 random variables:
@@ -132,42 +150,48 @@
         ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']
         >>> best_model.edges()
         [('B', 'J'), ('A', 'J')]
         >>> # search a model with restriction on the number of parents:
         >>> est.estimate(max_indegree=1).edges()
         [('J', 'A'), ('B', 'J')]
         """
-        epsilon = 1e-8
         nodes = self.state_names.keys()
         if start is None:
-            start = BayesianModel()
+            start = DAG()
             start.add_nodes_from(nodes)
-        elif not isinstance(start, BayesianModel) or not set(start.nodes()) == set(nodes):
-            raise ValueError("'start' should be a BayesianModel with the same variables as the data set, or 'None'.")
+        elif not isinstance(start, DAG) or not set(start.nodes()) == set(nodes):
+            raise ValueError(
+                "'start' should be a DAG with the same variables as the data set, or 'None'."
+            )
 
         tabu_list = []
         current_model = start
 
-        while True:
+        iter_no = 0
+        while iter_no <= max_iter:
+            iter_no += 1
+
             best_score_delta = 0
             best_operation = None
 
-            for operation, score_delta in self._legal_operations(current_model, tabu_list, max_indegree):
+            for operation, score_delta in self._legal_operations(
+                current_model, tabu_list, max_indegree
+            ):
                 if score_delta > best_score_delta:
                     best_operation = operation
                     best_score_delta = score_delta
 
             if best_operation is None or best_score_delta < epsilon:
                 break
-            elif best_operation[0] == '+':
+            elif best_operation[0] == "+":
                 current_model.add_edge(*best_operation[1])
-                tabu_list = ([('-', best_operation[1])] + tabu_list)[:tabu_length]
-            elif best_operation[0] == '-':
+                tabu_list = ([("-", best_operation[1])] + tabu_list)[:tabu_length]
+            elif best_operation[0] == "-":
                 current_model.remove_edge(*best_operation[1])
-                tabu_list = ([('+', best_operation[1])] + tabu_list)[:tabu_length]
-            elif best_operation[0] == 'flip':
+                tabu_list = ([("+", best_operation[1])] + tabu_list)[:tabu_length]
+            elif best_operation[0] == "flip":
                 X, Y = best_operation[1]
                 current_model.remove_edge(X, Y)
                 current_model.add_edge(Y, X)
                 tabu_list = ([best_operation] + tabu_list)[:tabu_length]
 
         return current_model
```

### Comparing `pgmpy-0.1.7/pgmpy/estimators/ExhaustiveSearch.py` & `pgmpy-0.1.9/pgmpy/estimators/ExhaustiveSearch.py`

 * *Files 4% similar despite different names*

```diff
@@ -4,21 +4,21 @@
 from itertools import combinations
 
 import networkx as nx
 
 from pgmpy.estimators import StructureEstimator
 from pgmpy.estimators import K2Score
 from pgmpy.utils.mathext import powerset
-from pgmpy.models import BayesianModel
+from pgmpy.base import DAG
 
 
 class ExhaustiveSearch(StructureEstimator):
     def __init__(self, data, scoring_method=None, **kwargs):
         """
-        Search class for exhaustive searches over all BayesianModels with a given set of variables.
+        Search class for exhaustive searches over all DAGs with a given set of variables.
         Takes a `StructureScore`-Instance as parameter; `estimate` finds the model with maximal score.
 
         Parameters
         ----------
         data: pandas DataFrame object
             datafame object where each column represents one variable.
             (If some values in the data are missing the data cells should be set to `numpy.NaN`.
@@ -81,15 +81,19 @@
         [('Weather', 'Humidity'), ('Weather', 'Temperature'), ('Temperature', 'Humidity')]]
 
         """
         if nodes is None:
             nodes = sorted(self.state_names.keys())
         if len(nodes) > 6:
             warn("Generating all DAGs of n nodes likely not feasible for n>6!")
-            warn("Attempting to search through {0} graphs".format(2**(len(nodes)*(len(nodes)-1))))
+            warn(
+                "Attempting to search through {0} graphs".format(
+                    2 ** (len(nodes) * (len(nodes) - 1))
+                )
+            )
 
         edges = list(combinations(nodes, 2))  # n*(n-1) possible directed edges
         edges.extend([(y, x) for x, y in edges])
         all_graphs = powerset(edges)  # 2^(n*(n-1)) graphs
 
         for graph_edges in all_graphs:
             graph = nx.DiGraph()
@@ -142,44 +146,46 @@
         -16268.324530291524     [('C', 'A'), ('C', 'B')]
         -16268.324530291524     [('B', 'A'), ('B', 'C'), ('C', 'A')]
         -16268.324530291524     [('B', 'A'), ('C', 'A'), ('C', 'B')]
         -16237.575725538434     [('B', 'C')]
         -16237.575725538434     [('C', 'B')]
         """
 
-        scored_dags = sorted([(self.scoring_method.score(dag), dag) for dag in self.all_dags()],
-                             key=lambda x: x[0])
+        scored_dags = sorted(
+            [(self.scoring_method.score(dag), dag) for dag in self.all_dags()],
+            key=lambda x: x[0],
+        )
         return scored_dags
 
     def estimate(self):
         """
-        Estimates the `BayesianModel` structure that fits best to the given data set,
+        Estimates the `DAG` structure that fits best to the given data set,
         according to the scoring method supplied in the constructor.
         Exhaustively searches through all models. Only estimates network structure, no parametrization.
 
         Returns
         -------
-        model: `BayesianModel` instance
-            A `BayesianModel` with maximal score.
+        model: `DAG` instance
+            A `DAG` with maximal score.
 
         Examples
         --------
         >>> import pandas as pd
         >>> import numpy as np
         >>> from pgmpy.estimators import ExhaustiveSearch
         >>> # create random data sample with 3 variables, where B and C are identical:
         >>> data = pd.DataFrame(np.random.randint(0, 5, size=(5000, 2)), columns=list('AB'))
         >>> data['C'] = data['B']
         >>> est = ExhaustiveSearch(data)
         >>> best_model = est.estimate()
         >>> best_model
-        <pgmpy.models.BayesianModel.BayesianModel object at 0x7f695c535470>
+        <pgmpy.base.DAG.DAG object at 0x7f695c535470>
         >>> best_model.edges()
         [('B', 'C')]
         """
 
         best_dag = max(self.all_dags(), key=self.scoring_method.score)
 
-        best_model = BayesianModel()
+        best_model = DAG()
         best_model.add_nodes_from(sorted(best_dag.nodes()))
         best_model.add_edges_from(sorted(best_dag.edges()))
         return best_model
```

### Comparing `pgmpy-0.1.7/pgmpy/estimators/ScoreCache.py` & `pgmpy-0.1.9/pgmpy/estimators/ScoreCache.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,13 +1,12 @@
 #!/usr/bin/env python
 from pgmpy.estimators import StructureScore
 
 
 class ScoreCache(StructureScore):
-
     def __init__(self, base_scorer, data, max_size=10000, **kwargs):
         """
         A wrapper class for StructureScore instances, which implement a decomposable score,
         that caches local scores.
         Based on the global decomposition property of Bayesian networks for decomposable scores.
 
         Parameters
@@ -25,18 +24,22 @@
             Additional arguments that will be handed to the super constructor.
 
         Reference
         ---------
         Koller & Friedman, Probabilistic Graphical Models - Principles and Techniques, 2009
         Section 18.3
         """
-        assert isinstance(base_scorer, StructureScore), "Base scorer has to be of type StructureScore."
+        assert isinstance(
+            base_scorer, StructureScore
+        ), "Base scorer has to be of type StructureScore."
 
         self.base_scorer = base_scorer
-        self.cache = LRUCache(original_function=self._wrapped_original, max_size=int(max_size))
+        self.cache = LRUCache(
+            original_function=self._wrapped_original, max_size=int(max_size)
+        )
         super(ScoreCache, self).__init__(data, **kwargs)
 
     def local_score(self, variable, parents):
         hashable = tuple(parents)
         return self.cache(variable, hashable)
 
     def _wrapped_original(self, variable, parents):
@@ -45,15 +48,14 @@
 
 
 # link fields
 _PREV, _NEXT, _KEY, _VALUE = 0, 1, 2, 3
 
 
 class LRUCache:
-
     def __init__(self, original_function, max_size=10000):
         """
         Least-Recently-Used cache.
         Acts as a wrapper around a arbitrary function and caches the return values.
 
         Based on the implementation of Raymond Hettinger
         (https://stackoverflow.com/questions/2437617/limiting-the-size-of-a-python-dictionary)
```

### Comparing `pgmpy-0.1.7/pgmpy/estimators/MLE.py` & `pgmpy-0.1.9/pgmpy/estimators/MLE.py`

 * *Files 7% similar despite different names*

```diff
@@ -40,15 +40,17 @@
         >>> data = pd.DataFrame(np.random.randint(low=0, high=2, size=(1000, 5)),
         ...                       columns=['A', 'B', 'C', 'D', 'E'])
         >>> model = BayesianModel([('A', 'B'), ('C', 'B'), ('C', 'D'), ('B', 'E')])
         >>> estimator = MaximumLikelihoodEstimator(model, data)
         """
 
         if not isinstance(model, BayesianModel):
-            raise NotImplementedError("Maximum Likelihood Estimate is only implemented for BayesianModel")
+            raise NotImplementedError(
+                "Maximum Likelihood Estimate is only implemented for BayesianModel"
+            )
 
         super(MaximumLikelihoodEstimator, self).__init__(model, data, **kwargs)
 
     def get_parameters(self):
         """
         Method to estimate the model parameters (CPDs) using Maximum Likelihood Estimation.
 
@@ -121,19 +123,33 @@
         ╘══════╧══════╧══════╧══════╧══════╛
         """
 
         state_counts = self.state_counts(node)
 
         # if a column contains only `0`s (no states observed for some configuration
         # of parents' states) fill that column uniformly instead
-        state_counts.ix[:, (state_counts == 0).all()] = 1
+        state_counts.loc[:, (state_counts == 0).all()] = 1
 
         parents = sorted(self.model.get_parents(node))
         parents_cardinalities = [len(self.state_names[parent]) for parent in parents]
         node_cardinality = len(self.state_names[node])
 
-        cpd = TabularCPD(node, node_cardinality, np.array(state_counts),
-                         evidence=parents,
-                         evidence_card=parents_cardinalities,
-                         state_names=self.state_names)
+        # Get the state names for the CPD
+        state_names = {node: list(state_counts.index)}
+        if parents:
+            state_names.update(
+                {
+                    state_counts.columns.names[i]: list(state_counts.columns.levels[i])
+                    for i in range(len(parents))
+                }
+            )
+
+        cpd = TabularCPD(
+            node,
+            node_cardinality,
+            np.array(state_counts),
+            evidence=parents,
+            evidence_card=parents_cardinalities,
+            state_names=state_names,
+        )
         cpd.normalize()
         return cpd
```

### Comparing `pgmpy-0.1.7/pgmpy/independencies/Independencies.py` & `pgmpy-0.1.9/pgmpy/independencies/Independencies.py`

 * *Files 6% similar despite different names*

```diff
@@ -42,29 +42,35 @@
     add_assertions
     get_assertions
     get_factorized_product
     closure
     entails
     is_equivalent
     """
+
     def __init__(self, *assertions):
         self.independencies = []
         self.add_assertions(*assertions)
 
     def __str__(self):
-        string = '\n'.join([str(assertion) for assertion in self.independencies])
+        string = "\n".join([str(assertion) for assertion in self.independencies])
         return string
 
     __repr__ = __str__
 
     def __eq__(self, other):
         if not isinstance(other, Independencies):
             return False
-        return (all(independency in other.get_assertions() for independency in self.get_assertions()) and
-                all(independency in self.get_assertions() for independency in other.get_assertions()))
+        return all(
+            independency in other.get_assertions()
+            for independency in self.get_assertions()
+        ) and all(
+            independency in self.get_assertions()
+            for independency in other.get_assertions()
+        )
 
     def __ne__(self, other):
         return not self.__eq__(other)
 
     def contains(self, assertion):
         """
         Returns `True` if `assertion` is contained in this `Independencies`-object,
@@ -84,16 +90,18 @@
         >>> IndependenceAssertion('B', 'A', ['D', 'C']) in ind
         True
         >>> # but does not check entailment:
         >>> IndependenceAssertion('X', 'Y', 'Z') in Independencies(['X', 'Y'])
         False
         """
         if not isinstance(assertion, IndependenceAssertion):
-            raise TypeError("' in <Independencies()>' requires IndependenceAssertion" +
-                            " as left operand, not {0}".format(type(assertion)))
+            raise TypeError(
+                "' in <Independencies()>' requires IndependenceAssertion"
+                + " as left operand, not {0}".format(type(assertion))
+            )
 
         return assertion in self.get_assertions()
 
     __contains__ = contains
 
     def get_assertions(self):
         """
@@ -124,17 +132,21 @@
         >>> independencies.add_assertions(['a', ['b', 'c'], 'd'])
         """
         for assertion in assertions:
             if isinstance(assertion, IndependenceAssertion):
                 self.independencies.append(assertion)
             else:
                 try:
-                    self.independencies.append(IndependenceAssertion(assertion[0], assertion[1], assertion[2]))
+                    self.independencies.append(
+                        IndependenceAssertion(assertion[0], assertion[1], assertion[2])
+                    )
                 except IndexError:
-                    self.independencies.append(IndependenceAssertion(assertion[0], assertion[1]))
+                    self.independencies.append(
+                        IndependenceAssertion(assertion[0], assertion[1])
+                    )
 
     def closure(self):
         """
         Returns a new `Independencies()`-object that additionally contains those `IndependenceAssertions`
         that are implied by the the current independencies (using with the `semi-graphoid axioms
         <https://en.wikipedia.org/w/index.php?title=Conditional_independence&oldid=708760689#Rules_of_conditional_independence>`_;
         see (Pearl, 1989, `Conditional Independence and its representations
@@ -165,15 +177,15 @@
         (W _|_ Z | X)
         (W _|_ Z, Y | X)
         [..]
         """
 
         def single_var(var):
             "Checks if var represents a single variable"
-            if not hasattr(var, '__iter__'):
+            if not hasattr(var, "__iter__"):
                 return True
             else:
                 return len(var) == 1
 
         def sg0(ind):
             "Symmetry rule: 'X ⟂ Y | Z' -> 'Y ⟂ X | Z'"
             return IndependenceAssertion(ind.event2, ind.event1, ind.event3)
@@ -181,35 +193,46 @@
         # since X⟂Y|Z == Y⟂X|Z in pgmpy, sg0 (symmetry) is not used as an axiom/rule.
         # instead we use a decorator for the other axioms to apply them on both sides
         def apply_left_and_right(func):
             def symmetric_func(*args):
                 if len(args) == 1:
                     return func(args[0]) + func(sg0(args[0]))
                 if len(args) == 2:
-                    return (func(*args) + func(args[0], sg0(args[1])) +
-                            func(sg0(args[0]), args[1]) + func(sg0(args[0]), sg0(args[1])))
+                    return (
+                        func(*args)
+                        + func(args[0], sg0(args[1]))
+                        + func(sg0(args[0]), args[1])
+                        + func(sg0(args[0]), sg0(args[1]))
+                    )
+
             return symmetric_func
 
         @apply_left_and_right
         def sg1(ind):
             "Decomposition rule: 'X ⟂ Y,W | Z' -> 'X ⟂ Y | Z', 'X ⟂ W | Z'"
             if single_var(ind.event2):
                 return []
             else:
-                return [IndependenceAssertion(ind.event1, ind.event2 - {elem}, ind.event3)
-                        for elem in ind.event2]
+                return [
+                    IndependenceAssertion(ind.event1, ind.event2 - {elem}, ind.event3)
+                    for elem in ind.event2
+                ]
 
         @apply_left_and_right
         def sg2(ind):
             "Weak Union rule: 'X ⟂ Y,W | Z' -> 'X ⟂ Y | W,Z', 'X ⟂ W | Y,Z' "
             if single_var(ind.event2):
                 return []
             else:
-                return [IndependenceAssertion(ind.event1, ind.event2 - {elem}, {elem} | ind.event3)
-                        for elem in ind.event2]
+                return [
+                    IndependenceAssertion(
+                        ind.event1, ind.event2 - {elem}, {elem} | ind.event3
+                    )
+                    for elem in ind.event2
+                ]
 
         @apply_left_and_right
         def sg3(ind1, ind2):
             "Contraction rule: 'X ⟂ W | Y,Z' & 'X ⟂ Y | Z' -> 'X ⟂ W,Y | Z'"
             if ind1.event1 != ind2.event1:
                 return []
 
@@ -222,22 +245,29 @@
                 return []
 
         # apply semi-graphoid axioms as long as new independencies are found.
         all_independencies = set()
         new_inds = set(self.independencies)
 
         while new_inds:
-            new_pairs = (set(itertools.permutations(new_inds, 2)) |
-                         set(itertools.product(new_inds, all_independencies)) |
-                         set(itertools.product(all_independencies, new_inds)))
+            new_pairs = (
+                set(itertools.permutations(new_inds, 2))
+                | set(itertools.product(new_inds, all_independencies))
+                | set(itertools.product(all_independencies, new_inds))
+            )
 
             all_independencies |= new_inds
-            new_inds = set(sum([sg1(ind) for ind in new_inds] +
-                               [sg2(ind) for ind in new_inds] +
-                               [sg3(*inds) for inds in new_pairs], []))
+            new_inds = set(
+                sum(
+                    [sg1(ind) for ind in new_inds]
+                    + [sg2(ind) for ind in new_inds]
+                    + [sg3(*inds) for inds in new_pairs],
+                    [],
+                )
+            )
             new_inds -= all_independencies
 
         return Independencies(*list(all_independencies))
 
     def entails(self, entailed_independencies):
         """
         Returns `True` if the `entailed_independencies` are implied by this `Independencies`-object, otherwise `False`.
@@ -259,15 +289,17 @@
         >>> ind2.entails(ind1)
         False
         """
         if not isinstance(entailed_independencies, Independencies):
             return False
 
         implications = self.closure().get_assertions()
-        return all(ind in implications for ind in entailed_independencies.get_assertions())
+        return all(
+            ind in implications for ind in entailed_independencies.get_assertions()
+        )
 
     def is_equivalent(self, other):
         """
         Returns True if the two Independencies-objects are equivalent, otherwise False.
         (i.e. any Bayesian Network that satisfies the one set
         of conditional independencies also satisfies the other).
 
@@ -287,14 +319,15 @@
         False
         >>> ind1.is_equivalent(ind3)
         True
         """
         return self.entails(other) and other.entails(self)
 
         # TODO: write reduce function.
+
     def reduce(self):
         """
         Add function to remove duplicate Independence Assertions
         """
         pass
 
     def latex_string(self):
@@ -353,53 +386,62 @@
     >>> assertion = IndependenceAssertion(['U', 'V'], ['X', 'Y'], ['Z', 'A'])
 
 
     Public Methods
     --------------
     get_assertion
     """
+
     def __init__(self, event1=[], event2=[], event3=[]):
         """
         Initialize an IndependenceAssertion object with event1, event2 and event3 attributes.
 
                   event2
                   ^
       event1     /   event3
          ^      /     ^
          |     /      |
         (U || X, Y | Z) read as Random variable U is independent of X and Y given Z.
           ---
         """
         if event1 and not event2:
-            raise ValueError('event2 needs to be specified')
+            raise ValueError("event2 needs to be specified")
         if any([event2, event3]) and not event1:
-            raise ValueError('event1 needs to be specified')
+            raise ValueError("event1 needs to be specified")
         if event3 and not all([event1, event2]):
-            raise ValueError('event1' if not event1 else 'event2' + ' needs to be specified')
+            raise ValueError(
+                "event1" if not event1 else "event2" + " needs to be specified"
+            )
 
         self.event1 = frozenset(self._return_list_if_str(event1))
         self.event2 = frozenset(self._return_list_if_str(event2))
         self.event3 = frozenset(self._return_list_if_str(event3))
 
     def __str__(self):
         if self.event3:
-            return('({event1} _|_ {event2} | {event3})'.format(event1=', '.join(self.event1),
-                                                               event2=', '.join(self.event2),
-                                                               event3=', '.join(self.event3)))
+            return "({event1} _|_ {event2} | {event3})".format(
+                event1=", ".join(self.event1),
+                event2=", ".join(self.event2),
+                event3=", ".join(self.event3),
+            )
         else:
-            return('({event1} _|_ {event2})'.format(event1=', '.join(self.event1),
-                                                    event2=', '.join(self.event2)))
+            return "({event1} _|_ {event2})".format(
+                event1=", ".join(self.event1), event2=", ".join(self.event2)
+            )
 
     __repr__ = __str__
 
     def __eq__(self, other):
         if not isinstance(other, IndependenceAssertion):
             return False
-        return ((self.event1, self.event2, self.event3) == other.get_assertion() or
-                (self.event2, self.event1, self.event3) == other.get_assertion())
+        return (self.event1, self.event2, self.event3) == other.get_assertion() or (
+            self.event2,
+            self.event1,
+            self.event3,
+        ) == other.get_assertion()
 
     def __ne__(self, other):
         return not self.__eq__(other)
 
     def __hash__(self):
         return hash((frozenset((self.event1, self.event2)), self.event3))
 
@@ -423,9 +465,12 @@
         >>> from pgmpy.independencies import IndependenceAssertion
         >>> asser = IndependenceAssertion('X', 'Y', 'Z')
         >>> asser.get_assertion()
         """
         return self.event1, self.event2, self.event3
 
     def latex_string(self):
-        return ('%s \perp %s \mid %s' % (', '.join(self.event1), ', '.join(self.event2),
-                                         ', '.join(self.event3)))
+        return "%s \perp %s \mid %s" % (
+            ", ".join(self.event1),
+            ", ".join(self.event2),
+            ", ".join(self.event3),
+        )
```

### Comparing `pgmpy-0.1.7/pgmpy/extern/tabulate.py` & `pgmpy-0.1.9/pgmpy/extern/tabulate.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,24 +1,24 @@
 # -*- coding: utf-8 -*-
 
 # LICENSE INFORMATION
 #
 # Copyright (c) 2011-2013 Sergey Astanin
-# 
+#
 # Permission is hereby granted, free of charge, to any person obtaining
 # a copy of this software and associated documentation files (the
 #  "Software"), to deal in the Software without restriction, including
 # without limitation the rights to use, copy, modify, merge, publish,
 # distribute, sublicense, and/or sell copies of the Software, and to
 # permit persons to whom the Software is furnished to do so, subject to
 # the following conditions:
-# 
+#
 #     The above copyright notice and this permission notice shall be
 #     included in all copies or substantial portions of the Software.
-# 
+#
 #     THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
 #     EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 #     MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
 #     NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
 #     LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
 #     OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
 #     WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
@@ -31,22 +31,24 @@
 from platform import python_version_tuple
 import re
 
 
 if python_version_tuple()[0] < "3":
     from itertools import izip_longest
     from functools import partial
+
     _none_type = type(None)
     _int_type = int
     _float_type = float
     _text_type = unicode
     _binary_type = str
 else:
     from itertools import zip_longest as izip_longest
     from functools import reduce, partial
+
     _none_type = type(None)
     _int_type = int
     _float_type = float
     _text_type = str
     _binary_type = bytes
 
 
@@ -87,131 +89,169 @@
 # padding (an integer) is the amount of white space around data values.
 #
 # with_header_hide:
 #
 #   - either None, to display all table elements unconditionally,
 #   - or a list of elements not to be displayed if the table has column headers.
 #
-TableFormat = namedtuple("TableFormat", ["lineabove", "linebelowheader",
-                                         "linebetweenrows", "linebelow",
-                                         "headerrow", "datarow",
-                                         "padding", "with_header_hide"])
+TableFormat = namedtuple(
+    "TableFormat",
+    [
+        "lineabove",
+        "linebelowheader",
+        "linebetweenrows",
+        "linebelow",
+        "headerrow",
+        "datarow",
+        "padding",
+        "with_header_hide",
+    ],
+)
 
 
 def _pipe_segment_with_colons(align, colwidth):
     """Return a segment of a horizontal line with optional colons which
     indicate column's alignment (as in `pipe` output format)."""
     w = colwidth
     if align in ["right", "decimal"]:
-        return ('-' * (w - 1)) + ":"
+        return ("-" * (w - 1)) + ":"
     elif align == "center":
-        return ":" + ('-' * (w - 2)) + ":"
+        return ":" + ("-" * (w - 2)) + ":"
     elif align == "left":
-        return ":" + ('-' * (w - 1))
+        return ":" + ("-" * (w - 1))
     else:
-        return '-' * w
+        return "-" * w
 
 
 def _pipe_line_with_colons(colwidths, colaligns):
     """Return a horizontal line with optional colons to indicate column's
     alignment (as in `pipe` output format)."""
     segments = [_pipe_segment_with_colons(a, w) for a, w in zip(colaligns, colwidths)]
     return "|" + "|".join(segments) + "|"
 
 
 def _mediawiki_row_with_attrs(separator, cell_values, colwidths, colaligns):
-    alignment = { "left":    '',
-                  "right":   'align="right"| ',
-                  "center":  'align="center"| ',
-                  "decimal": 'align="right"| ' }
+    alignment = {
+        "left": "",
+        "right": 'align="right"| ',
+        "center": 'align="center"| ',
+        "decimal": 'align="right"| ',
+    }
     # hard-coded padding _around_ align attribute and value together
     # rather than padding parameter which affects only the value
-    values_with_attrs = [' ' + alignment.get(a, '') + c + ' '
-                         for c, a in zip(cell_values, colaligns)]
-    colsep = separator*2
+    values_with_attrs = [
+        " " + alignment.get(a, "") + c + " " for c, a in zip(cell_values, colaligns)
+    ]
+    colsep = separator * 2
     return (separator + colsep.join(values_with_attrs)).rstrip()
 
 
 def _latex_line_begin_tabular(colwidths, colaligns):
-    alignment = { "left": "l", "right": "r", "center": "c", "decimal": "r" }
+    alignment = {"left": "l", "right": "r", "center": "c", "decimal": "r"}
     tabular_columns_fmt = "".join([alignment.get(a, "l") for a in colaligns])
     return "\\begin{tabular}{" + tabular_columns_fmt + "}\n\hline"
 
 
-_table_formats = {"simple":
-                  TableFormat(lineabove=Line("", "-", "  ", ""),
-                              linebelowheader=Line("", "-", "  ", ""),
-                              linebetweenrows=None,
-                              linebelow=Line("", "-", "  ", ""),
-                              headerrow=DataRow("", "  ", ""),
-                              datarow=DataRow("", "  ", ""),
-                              padding=0,
-                              with_header_hide=["lineabove", "linebelow"]),
-                  "plain":
-                  TableFormat(lineabove=None, linebelowheader=None,
-                              linebetweenrows=None, linebelow=None,
-                              headerrow=DataRow("", "  ", ""),
-                              datarow=DataRow("", "  ", ""),
-                              padding=0, with_header_hide=None),
-                  "grid":
-                  TableFormat(lineabove=Line("+", "-", "+", "+"),
-                              linebelowheader=Line("+", "=", "+", "+"),
-                              linebetweenrows=Line("+", "-", "+", "+"),
-                              linebelow=Line("+", "-", "+", "+"),
-                              headerrow=DataRow("|", "|", "|"),
-                              datarow=DataRow("|", "|", "|"),
-                              padding=1, with_header_hide=None),
-                  "pipe":
-                  TableFormat(lineabove=_pipe_line_with_colons,
-                              linebelowheader=_pipe_line_with_colons,
-                              linebetweenrows=None,
-                              linebelow=None,
-                              headerrow=DataRow("|", "|", "|"),
-                              datarow=DataRow("|", "|", "|"),
-                              padding=1,
-                              with_header_hide=["lineabove"]),
-                  "orgtbl":
-                  TableFormat(lineabove=None,
-                              linebelowheader=Line("|", "-", "+", "|"),
-                              linebetweenrows=None,
-                              linebelow=None,
-                              headerrow=DataRow("|", "|", "|"),
-                              datarow=DataRow("|", "|", "|"),
-                              padding=1, with_header_hide=None),
-                  "rst":
-                  TableFormat(lineabove=Line("", "=", "  ", ""),
-                              linebelowheader=Line("", "=", "  ", ""),
-                              linebetweenrows=None,
-                              linebelow=Line("", "=", "  ", ""),
-                              headerrow=DataRow("", "  ", ""),
-                              datarow=DataRow("", "  ", ""),
-                              padding=0, with_header_hide=None),
-                  "mediawiki":
-                  TableFormat(lineabove=Line("{| class=\"wikitable\" style=\"text-align: left;\"",
-                                             "", "", "\n|+ <!-- caption -->\n|-"),
-                              linebelowheader=Line("|-", "", "", ""),
-                              linebetweenrows=Line("|-", "", "", ""),
-                              linebelow=Line("|}", "", "", ""),
-                              headerrow=partial(_mediawiki_row_with_attrs, "!"),
-                              datarow=partial(_mediawiki_row_with_attrs, "|"),
-                              padding=0, with_header_hide=None),
-                  "latex":
-                  TableFormat(lineabove=_latex_line_begin_tabular,
-                              linebelowheader=Line("\\hline", "", "", ""),
-                              linebetweenrows=None,
-                              linebelow=Line("\\hline\n\\end{tabular}", "", "", ""),
-                              headerrow=DataRow("", "&", "\\\\"),
-                              datarow=DataRow("", "&", "\\\\"),
-                              padding=1, with_header_hide=None),
-                  "tsv":
-                  TableFormat(lineabove=None, linebelowheader=None,
-                              linebetweenrows=None, linebelow=None,
-                              headerrow=DataRow("", "\t", ""),
-                              datarow=DataRow("", "\t", ""),
-                              padding=0, with_header_hide=None)}
+_table_formats = {
+    "simple": TableFormat(
+        lineabove=Line("", "-", "  ", ""),
+        linebelowheader=Line("", "-", "  ", ""),
+        linebetweenrows=None,
+        linebelow=Line("", "-", "  ", ""),
+        headerrow=DataRow("", "  ", ""),
+        datarow=DataRow("", "  ", ""),
+        padding=0,
+        with_header_hide=["lineabove", "linebelow"],
+    ),
+    "plain": TableFormat(
+        lineabove=None,
+        linebelowheader=None,
+        linebetweenrows=None,
+        linebelow=None,
+        headerrow=DataRow("", "  ", ""),
+        datarow=DataRow("", "  ", ""),
+        padding=0,
+        with_header_hide=None,
+    ),
+    "grid": TableFormat(
+        lineabove=Line("+", "-", "+", "+"),
+        linebelowheader=Line("+", "=", "+", "+"),
+        linebetweenrows=Line("+", "-", "+", "+"),
+        linebelow=Line("+", "-", "+", "+"),
+        headerrow=DataRow("|", "|", "|"),
+        datarow=DataRow("|", "|", "|"),
+        padding=1,
+        with_header_hide=None,
+    ),
+    "pipe": TableFormat(
+        lineabove=_pipe_line_with_colons,
+        linebelowheader=_pipe_line_with_colons,
+        linebetweenrows=None,
+        linebelow=None,
+        headerrow=DataRow("|", "|", "|"),
+        datarow=DataRow("|", "|", "|"),
+        padding=1,
+        with_header_hide=["lineabove"],
+    ),
+    "orgtbl": TableFormat(
+        lineabove=None,
+        linebelowheader=Line("|", "-", "+", "|"),
+        linebetweenrows=None,
+        linebelow=None,
+        headerrow=DataRow("|", "|", "|"),
+        datarow=DataRow("|", "|", "|"),
+        padding=1,
+        with_header_hide=None,
+    ),
+    "rst": TableFormat(
+        lineabove=Line("", "=", "  ", ""),
+        linebelowheader=Line("", "=", "  ", ""),
+        linebetweenrows=None,
+        linebelow=Line("", "=", "  ", ""),
+        headerrow=DataRow("", "  ", ""),
+        datarow=DataRow("", "  ", ""),
+        padding=0,
+        with_header_hide=None,
+    ),
+    "mediawiki": TableFormat(
+        lineabove=Line(
+            '{| class="wikitable" style="text-align: left;"',
+            "",
+            "",
+            "\n|+ <!-- caption -->\n|-",
+        ),
+        linebelowheader=Line("|-", "", "", ""),
+        linebetweenrows=Line("|-", "", "", ""),
+        linebelow=Line("|}", "", "", ""),
+        headerrow=partial(_mediawiki_row_with_attrs, "!"),
+        datarow=partial(_mediawiki_row_with_attrs, "|"),
+        padding=0,
+        with_header_hide=None,
+    ),
+    "latex": TableFormat(
+        lineabove=_latex_line_begin_tabular,
+        linebelowheader=Line("\\hline", "", "", ""),
+        linebetweenrows=None,
+        linebelow=Line("\\hline\n\\end{tabular}", "", "", ""),
+        headerrow=DataRow("", "&", "\\\\"),
+        datarow=DataRow("", "&", "\\\\"),
+        padding=1,
+        with_header_hide=None,
+    ),
+    "tsv": TableFormat(
+        lineabove=None,
+        linebelowheader=None,
+        linebetweenrows=None,
+        linebelow=None,
+        headerrow=DataRow("", "\t", ""),
+        datarow=DataRow("", "\t", ""),
+        padding=0,
+        with_header_hide=None,
+    ),
+}
 
 
 tabulate_formats = list(sorted(_table_formats.keys()))
 
 
 _invisible_codes = re.compile("\x1b\[\d*m")  # ANSI color codes
 _invisible_codes_bytes = re.compile(b"\x1b\[\d*m")  # ANSI color codes
@@ -221,18 +261,24 @@
     """Construct a simple TableFormat with columns separated by a separator.
 
     >>> tsv = simple_separated_format("\\t") ; \
         tabulate([["foo", 1], ["spam", 23]], tablefmt=tsv) == 'foo \\t 1\\nspam\\t23'
     True
 
     """
-    return TableFormat(None, None, None, None,
-                       headerrow=DataRow('', separator, ''),
-                       datarow=DataRow('', separator, ''),
-                       padding=0, with_header_hide=None)
+    return TableFormat(
+        None,
+        None,
+        None,
+        None,
+        headerrow=DataRow("", separator, ""),
+        datarow=DataRow("", separator, ""),
+        padding=0,
+        with_header_hide=None,
+    )
 
 
 def _isconvertible(conv, string):
     try:
         n = conv(string)
         return True
     except ValueError:
@@ -254,17 +300,19 @@
 def _isint(string):
     """
     >>> _isint("123")
     True
     >>> _isint("123.45")
     False
     """
-    return type(string) is int or \
-           (isinstance(string, _binary_type) or isinstance(string, _text_type)) and \
-           _isconvertible(int, string)
+    return (
+        type(string) is int
+        or (isinstance(string, _binary_type) or isinstance(string, _text_type))
+        and _isconvertible(int, string)
+    )
 
 
 def _type(string, has_invisible=True):
     """The least generic type (type(None), int, float, str, unicode).
 
     >>> _type(None) is type(None)
     True
@@ -275,16 +323,17 @@
     >>> _type('\x1b[31m42\x1b[0m') is type(42)
     True
     >>> _type('\x1b[31m42\x1b[0m') is type(42)
     True
 
     """
 
-    if has_invisible and \
-       (isinstance(string, _text_type) or isinstance(string, _binary_type)):
+    if has_invisible and (
+        isinstance(string, _text_type) or isinstance(string, _binary_type)
+    ):
         string = _strip_invisible(string)
 
     if string is None:
         return _none_type
     elif hasattr(string, "isoformat"):  # datetime.datetime, date, and time
         return _text_type
     elif _isint(string):
@@ -396,16 +445,15 @@
         padfn = _padleft
     elif alignment == "center":
         strings = [s.strip() for s in strings]
         padfn = _padboth
     elif alignment == "decimal":
         decimals = [_afterpoint(s) for s in strings]
         maxdecimals = max(decimals)
-        strings = [s + (maxdecimals - decs) * " "
-                   for s, decs in zip(strings, decimals)]
+        strings = [s + (maxdecimals - decs) * " " for s, decs in zip(strings, decimals)]
         padfn = _padleft
     elif not alignment:
         return strings
     else:
         strings = [s.strip() for s in strings]
         padfn = _padright
 
@@ -416,16 +464,16 @@
 
     maxwidth = max(max(map(width_fn, strings)), minwidth)
     padded_strings = [padfn(maxwidth, s, has_invisible) for s in strings]
     return padded_strings
 
 
 def _more_generic(type1, type2):
-    types = { _none_type: 0, int: 1, float: 2, _binary_type: 3, _text_type: 4 }
-    invtypes = { 4: _text_type, 3: _binary_type, 2: float, 1: int, 0: _none_type }
+    types = {_none_type: 0, int: 1, float: 2, _binary_type: 3, _text_type: 4}
+    invtypes = {4: _text_type, 3: _binary_type, 2: float, 1: int, 0: _none_type}
     moregeneric = max(types.get(type1, 4), types.get(type2, 4))
     return invtypes[moregeneric]
 
 
 def _column_type(strings, has_invisible=True):
     """The least generic type all column values are convertible to.
 
@@ -442,15 +490,15 @@
     >>> _column_type([1, 2, None]) is _int_type
     True
     >>> import datetime as dt
     >>> _column_type([dt.datetime(1991,2,19), dt.time(17,35)]) is _text_type
     True
 
     """
-    types = [_type(s, has_invisible) for s in strings ]
+    types = [_type(s, has_invisible) for s in strings]
     return reduce(_more_generic, types, int)
 
 
 def _format(val, valtype, floatfmt, missingval=""):
     """Format a value accoding to its type.
 
     Unicode is supported:
@@ -516,88 +564,99 @@
     """
 
     if hasattr(tabular_data, "keys") and hasattr(tabular_data, "values"):
         # dict-like and pandas.DataFrame?
         if hasattr(tabular_data.values, "__call__"):
             # likely a conventional dict
             keys = tabular_data.keys()
-            rows = list(izip_longest(*tabular_data.values()))  # columns have to be transposed
+            rows = list(
+                izip_longest(*tabular_data.values())
+            )  # columns have to be transposed
         elif hasattr(tabular_data, "index"):
             # values is a property, has .index => it's likely a pandas.DataFrame (pandas 0.11.0)
             keys = tabular_data.keys()
             vals = tabular_data.values  # values matrix doesn't need to be transposed
             names = tabular_data.index
-            rows = [[v]+list(row) for v,row in zip(names, vals)]
+            rows = [[v] + list(row) for v, row in zip(names, vals)]
         else:
             raise ValueError("tabular data doesn't appear to be a dict or a DataFrame")
 
         if headers == "keys":
-            headers = list(map(_text_type,keys))  # headers should be strings
+            headers = list(map(_text_type, keys))  # headers should be strings
 
     else:  # it's a usual an iterable of iterables, or a NumPy array
         rows = list(tabular_data)
 
-        if (headers == "keys" and
-            hasattr(tabular_data, "dtype") and
-            getattr(tabular_data.dtype, "names")):
+        if (
+            headers == "keys"
+            and hasattr(tabular_data, "dtype")
+            and getattr(tabular_data.dtype, "names")
+        ):
             # numpy record array
             headers = tabular_data.dtype.names
-        elif (headers == "keys"
-              and len(rows) > 0
-              and isinstance(rows[0], tuple)
-              and hasattr(rows[0], "_fields")):
+        elif (
+            headers == "keys"
+            and len(rows) > 0
+            and isinstance(rows[0], tuple)
+            and hasattr(rows[0], "_fields")
+        ):
             # namedtuple
             headers = list(map(_text_type, rows[0]._fields))
-        elif (len(rows) > 0
-              and isinstance(rows[0], dict)):
+        elif len(rows) > 0 and isinstance(rows[0], dict):
             # dict or OrderedDict
-            uniq_keys = set() # implements hashed lookup
-            keys = [] # storage for set
+            uniq_keys = set()  # implements hashed lookup
+            keys = []  # storage for set
             if headers == "firstrow":
                 firstdict = rows[0] if len(rows) > 0 else {}
                 keys.extend(firstdict.keys())
                 uniq_keys.update(keys)
                 rows = rows[1:]
             for row in rows:
                 for k in row.keys():
-                    #Save unique items in input order
+                    # Save unique items in input order
                     if k not in uniq_keys:
                         keys.append(k)
                         uniq_keys.add(k)
-            if headers == 'keys':
+            if headers == "keys":
                 headers = keys
             elif headers == "firstrow" and len(rows) > 0:
                 headers = [firstdict.get(k, k) for k in keys]
                 headers = list(map(_text_type, headers))
             rows = [[row.get(k) for k in keys] for row in rows]
         elif headers == "keys" and len(rows) > 0:
             # keys are column indices
             headers = list(map(_text_type, range(len(rows[0]))))
 
     # take headers from the first row if necessary
     if headers == "firstrow" and len(rows) > 0:
-        headers = list(map(_text_type, rows[0])) # headers should be strings
+        headers = list(map(_text_type, rows[0]))  # headers should be strings
         rows = rows[1:]
 
-    headers = list(map(_text_type,headers))
-    rows = list(map(list,rows))
+    headers = list(map(_text_type, headers))
+    rows = list(map(list, rows))
 
     # pad with empty headers for initial columns if necessary
     if headers and len(rows) > 0:
-       nhs = len(headers)
-       ncols = len(rows[0])
-       if nhs < ncols:
-           headers = [""]*(ncols - nhs) + headers
+        nhs = len(headers)
+        ncols = len(rows[0])
+        if nhs < ncols:
+            headers = [""] * (ncols - nhs) + headers
 
     return rows, headers
 
 
-def tabulate(tabular_data, headers=[], tablefmt="simple",
-             floatfmt="g", numalign="decimal", stralign="left",
-             missingval=""):
+def tabulate(
+    tabular_data,
+    headers=[],
+    tablefmt="simple",
+    floatfmt="g",
+    numalign="decimal",
+    stralign="left",
+    missingval="",
+):
     """Format a fixed width table for pretty printing.
 
     >>> print(tabulate([[1, 2.34], [-56, "8.999"], ["2", "10001"]]))
     ---  ---------
       1      2.34
     -56      8.999
       2  10001
@@ -791,39 +850,46 @@
 
     """
 
     list_of_lists, headers = _normalize_tabular_data(tabular_data, headers)
 
     # optimization: look for ANSI control codes once,
     # enable smart width functions only if a control code is found
-    plain_text = '\n'.join(['\t'.join(map(_text_type, headers))] + \
-                            ['\t'.join(map(_text_type, row)) for row in list_of_lists])
+    plain_text = "\n".join(
+        ["\t".join(map(_text_type, headers))]
+        + ["\t".join(map(_text_type, row)) for row in list_of_lists]
+    )
     has_invisible = re.search(_invisible_codes, plain_text)
     if has_invisible:
         width_fn = _visible_width
     else:
         width_fn = len
 
     # format rows and columns, convert numeric values to strings
     cols = list(zip(*list_of_lists))
     coltypes = list(map(_column_type, cols))
-    cols = [[_format(v, ct, floatfmt, missingval) for v in c]
-             for c,ct in zip(cols, coltypes)]
+    cols = [
+        [_format(v, ct, floatfmt, missingval) for v in c]
+        for c, ct in zip(cols, coltypes)
+    ]
 
     # align columns
-    aligns = [numalign if ct in [int,float] else stralign for ct in coltypes]
-    minwidths = [width_fn(h)+2 for h in headers] if headers else [0]*len(cols)
-    cols = [_align_column(c, a, minw, has_invisible)
-            for c, a, minw in zip(cols, aligns, minwidths)]
+    aligns = [numalign if ct in [int, float] else stralign for ct in coltypes]
+    minwidths = [width_fn(h) + 2 for h in headers] if headers else [0] * len(cols)
+    cols = [
+        _align_column(c, a, minw, has_invisible)
+        for c, a, minw in zip(cols, aligns, minwidths)
+    ]
 
     if headers:
         # align headers and add headers
         minwidths = [max(minw, width_fn(c[0])) for minw, c in zip(minwidths, cols)]
-        headers = [_align_header(h, a, minw)
-                   for h, a, minw in zip(headers, aligns, minwidths)]
+        headers = [
+            _align_header(h, a, minw) for h, a, minw in zip(headers, aligns, minwidths)
+        ]
         rows = list(zip(*cols))
     else:
         minwidths = [width_fn(c[0]) for c in cols]
         rows = list(zip(*cols))
 
     if not isinstance(tablefmt, TableFormat):
         tablefmt = _table_formats.get(tablefmt, _table_formats["simple"])
@@ -850,36 +916,36 @@
 def _build_line(colwidths, colaligns, linefmt):
     "Return a string which represents a horizontal line."
     if not linefmt:
         return None
     if hasattr(linefmt, "__call__"):
         return linefmt(colwidths, colaligns)
     else:
-        begin, fill, sep,  end = linefmt
-        cells = [fill*w for w in colwidths]
+        begin, fill, sep, end = linefmt
+        cells = [fill * w for w in colwidths]
         return _build_simple_row(cells, (begin, sep, end))
 
 
 def _pad_row(cells, padding):
     if cells:
-        pad = " "*padding
+        pad = " " * padding
         padded_cells = [pad + cell + pad for cell in cells]
         return padded_cells
     else:
         return cells
 
 
 def _format_table(fmt, headers, rows, colwidths, colaligns):
     """Produce a plain-text representation of the table."""
     lines = []
     hidden = fmt.with_header_hide if (headers and fmt.with_header_hide) else []
     pad = fmt.padding
     headerrow = fmt.headerrow
 
-    padded_widths = [(w + 2*pad) for w in colwidths]
+    padded_widths = [(w + 2 * pad) for w in colwidths]
     padded_headers = _pad_row(headers, pad)
     padded_rows = [_pad_row(row, pad) for row in rows]
 
     if fmt.lineabove and "lineabove" not in hidden:
         lines.append(_build_line(padded_widths, colaligns, fmt.lineabove))
 
     if padded_headers:
```

### Comparing `pgmpy-0.1.7/pgmpy/extern/six.py` & `pgmpy-0.1.9/pgmpy/extern/six.py`

 * *Files 1% similar despite different names*

```diff
@@ -34,37 +34,37 @@
 
 # Useful for very coarse version differentiation.
 PY2 = sys.version_info[0] == 2
 PY3 = sys.version_info[0] == 3
 PY34 = sys.version_info[0:2] >= (3, 4)
 
 if PY3:
-    string_types = str,
-    integer_types = int,
-    class_types = type,
+    string_types = (str,)
+    integer_types = (int,)
+    class_types = (type,)
     text_type = str
     binary_type = bytes
 
     MAXSIZE = sys.maxsize
 else:
-    string_types = basestring,
+    string_types = (basestring,)
     integer_types = (int, long)
     class_types = (type, types.ClassType)
     text_type = unicode
     binary_type = str
 
     if sys.platform.startswith("java"):
         # Jython always uses 32 bits.
         MAXSIZE = int((1 << 31) - 1)
     else:
         # It's possible to have sizeof(long) != sizeof(Py_ssize_t).
         class X(object):
-
             def __len__(self):
                 return 1 << 31
+
         try:
             len(X())
         except OverflowError:
             # 32-bit
             MAXSIZE = int((1 << 31) - 1)
         else:
             # 64-bit
@@ -80,15 +80,14 @@
 def _import_module(name):
     """Import module, returning the module after the last dot."""
     __import__(name)
     return sys.modules[name]
 
 
 class _LazyDescr(object):
-
     def __init__(self, name):
         self.name = name
 
     def __get__(self, obj, tp):
         result = self._resolve()
         setattr(obj, self.name, result)  # Invokes __set__.
         try:
@@ -97,15 +96,14 @@
             delattr(obj.__class__, self.name)
         except AttributeError:
             pass
         return result
 
 
 class MovedModule(_LazyDescr):
-
     def __init__(self, name, old, new=None):
         super(MovedModule, self).__init__(name)
         if PY3:
             if new is None:
                 new = name
             self.mod = new
         else:
@@ -118,30 +116,28 @@
         _module = self._resolve()
         value = getattr(_module, attr)
         setattr(self, attr, value)
         return value
 
 
 class _LazyModule(types.ModuleType):
-
     def __init__(self, name):
         super(_LazyModule, self).__init__(name)
         self.__doc__ = self.__class__.__doc__
 
     def __dir__(self):
         attrs = ["__doc__", "__name__"]
         attrs += [attr.name for attr in self._moved_attributes]
         return attrs
 
     # Subclasses should override this
     _moved_attributes = []
 
 
 class MovedAttribute(_LazyDescr):
-
     def __init__(self, name, old_mod, new_mod, old_attr=None, new_attr=None):
         super(MovedAttribute, self).__init__(name)
         if PY3:
             if new_mod is None:
                 new_mod = name
             self.mod = new_mod
             if new_attr is None:
@@ -217,60 +213,71 @@
 
     def get_code(self, fullname):
         """Return None
 
         Required, if is_package is implemented"""
         self.__get_module(fullname)  # eventually raises ImportError
         return None
+
     get_source = get_code  # same as get_code
 
+
 _importer = _SixMetaPathImporter(__name__)
 
 
 class _MovedItems(_LazyModule):
 
     """Lazy loading of moved objects"""
+
     __path__ = []  # mark as package
 
 
 _moved_attributes = [
     MovedAttribute("cStringIO", "cStringIO", "io", "StringIO"),
     MovedAttribute("filter", "itertools", "builtins", "ifilter", "filter"),
-    MovedAttribute("filterfalse", "itertools", "itertools", "ifilterfalse", "filterfalse"),
+    MovedAttribute(
+        "filterfalse", "itertools", "itertools", "ifilterfalse", "filterfalse"
+    ),
     MovedAttribute("input", "__builtin__", "builtins", "raw_input", "input"),
     MovedAttribute("intern", "__builtin__", "sys"),
     MovedAttribute("map", "itertools", "builtins", "imap", "map"),
     MovedAttribute("getcwd", "os", "os", "getcwdu", "getcwd"),
     MovedAttribute("getcwdb", "os", "os", "getcwd", "getcwdb"),
     MovedAttribute("getoutput", "commands", "subprocess"),
     MovedAttribute("range", "__builtin__", "builtins", "xrange", "range"),
-    MovedAttribute("reload_module", "__builtin__", "importlib" if PY34 else "imp", "reload"),
+    MovedAttribute(
+        "reload_module", "__builtin__", "importlib" if PY34 else "imp", "reload"
+    ),
     MovedAttribute("reduce", "__builtin__", "functools"),
     MovedAttribute("shlex_quote", "pipes", "shlex", "quote"),
     MovedAttribute("StringIO", "StringIO", "io"),
     MovedAttribute("UserDict", "UserDict", "collections"),
     MovedAttribute("UserList", "UserList", "collections"),
     MovedAttribute("UserString", "UserString", "collections"),
     MovedAttribute("xrange", "__builtin__", "builtins", "xrange", "range"),
     MovedAttribute("zip", "itertools", "builtins", "izip", "zip"),
-    MovedAttribute("zip_longest", "itertools", "itertools", "izip_longest", "zip_longest"),
+    MovedAttribute(
+        "zip_longest", "itertools", "itertools", "izip_longest", "zip_longest"
+    ),
     MovedModule("builtins", "__builtin__"),
     MovedModule("configparser", "ConfigParser"),
     MovedModule("copyreg", "copy_reg"),
     MovedModule("dbm_gnu", "gdbm", "dbm.gnu"),
     MovedModule("_dummy_thread", "dummy_thread", "_dummy_thread"),
     MovedModule("http_cookiejar", "cookielib", "http.cookiejar"),
     MovedModule("http_cookies", "Cookie", "http.cookies"),
     MovedModule("html_entities", "htmlentitydefs", "html.entities"),
     MovedModule("html_parser", "HTMLParser", "html.parser"),
     MovedModule("http_client", "httplib", "http.client"),
     MovedModule("email_mime_base", "email.MIMEBase", "email.mime.base"),
     MovedModule("email_mime_image", "email.MIMEImage", "email.mime.image"),
     MovedModule("email_mime_multipart", "email.MIMEMultipart", "email.mime.multipart"),
-    MovedModule("email_mime_nonmultipart", "email.MIMENonMultipart", "email.mime.nonmultipart"),
+    MovedModule(
+        "email_mime_nonmultipart", "email.MIMENonMultipart", "email.mime.nonmultipart"
+    ),
     MovedModule("email_mime_text", "email.MIMEText", "email.mime.text"),
     MovedModule("BaseHTTPServer", "BaseHTTPServer", "http.server"),
     MovedModule("CGIHTTPServer", "CGIHTTPServer", "http.server"),
     MovedModule("SimpleHTTPServer", "SimpleHTTPServer", "http.server"),
     MovedModule("cPickle", "cPickle", "pickle"),
     MovedModule("queue", "Queue"),
     MovedModule("reprlib", "repr"),
@@ -281,35 +288,30 @@
     MovedModule("tkinter_filedialog", "FileDialog", "tkinter.filedialog"),
     MovedModule("tkinter_scrolledtext", "ScrolledText", "tkinter.scrolledtext"),
     MovedModule("tkinter_simpledialog", "SimpleDialog", "tkinter.simpledialog"),
     MovedModule("tkinter_tix", "Tix", "tkinter.tix"),
     MovedModule("tkinter_ttk", "ttk", "tkinter.ttk"),
     MovedModule("tkinter_constants", "Tkconstants", "tkinter.constants"),
     MovedModule("tkinter_dnd", "Tkdnd", "tkinter.dnd"),
-    MovedModule("tkinter_colorchooser", "tkColorChooser",
-                "tkinter.colorchooser"),
-    MovedModule("tkinter_commondialog", "tkCommonDialog",
-                "tkinter.commondialog"),
+    MovedModule("tkinter_colorchooser", "tkColorChooser", "tkinter.colorchooser"),
+    MovedModule("tkinter_commondialog", "tkCommonDialog", "tkinter.commondialog"),
     MovedModule("tkinter_tkfiledialog", "tkFileDialog", "tkinter.filedialog"),
     MovedModule("tkinter_font", "tkFont", "tkinter.font"),
     MovedModule("tkinter_messagebox", "tkMessageBox", "tkinter.messagebox"),
-    MovedModule("tkinter_tksimpledialog", "tkSimpleDialog",
-                "tkinter.simpledialog"),
+    MovedModule("tkinter_tksimpledialog", "tkSimpleDialog", "tkinter.simpledialog"),
     MovedModule("urllib_parse", __name__ + ".moves.urllib_parse", "urllib.parse"),
     MovedModule("urllib_error", __name__ + ".moves.urllib_error", "urllib.error"),
     MovedModule("urllib", __name__ + ".moves.urllib", __name__ + ".moves.urllib"),
     MovedModule("urllib_robotparser", "robotparser", "urllib.robotparser"),
     MovedModule("xmlrpc_client", "xmlrpclib", "xmlrpc.client"),
     MovedModule("xmlrpc_server", "SimpleXMLRPCServer", "xmlrpc.server"),
 ]
 # Add windows specific modules.
 if sys.platform == "win32":
-    _moved_attributes += [
-        MovedModule("winreg", "_winreg"),
-    ]
+    _moved_attributes += [MovedModule("winreg", "_winreg")]
 
 for attr in _moved_attributes:
     setattr(_MovedItems, attr.name, attr)
     if isinstance(attr, MovedModule):
         _importer._add_module(attr, "moves." + attr.name)
 del attr
 
@@ -335,15 +337,17 @@
     MovedAttribute("urlsplit", "urlparse", "urllib.parse"),
     MovedAttribute("urlunparse", "urlparse", "urllib.parse"),
     MovedAttribute("urlunsplit", "urlparse", "urllib.parse"),
     MovedAttribute("quote", "urllib", "urllib.parse"),
     MovedAttribute("quote_plus", "urllib", "urllib.parse"),
     MovedAttribute("unquote", "urllib", "urllib.parse"),
     MovedAttribute("unquote_plus", "urllib", "urllib.parse"),
-    MovedAttribute("unquote_to_bytes", "urllib", "urllib.parse", "unquote", "unquote_to_bytes"),
+    MovedAttribute(
+        "unquote_to_bytes", "urllib", "urllib.parse", "unquote", "unquote_to_bytes"
+    ),
     MovedAttribute("urlencode", "urllib", "urllib.parse"),
     MovedAttribute("splitquery", "urllib", "urllib.parse"),
     MovedAttribute("splittag", "urllib", "urllib.parse"),
     MovedAttribute("splituser", "urllib", "urllib.parse"),
     MovedAttribute("splitvalue", "urllib", "urllib.parse"),
     MovedAttribute("uses_fragment", "urlparse", "urllib.parse"),
     MovedAttribute("uses_netloc", "urlparse", "urllib.parse"),
@@ -353,16 +357,19 @@
 ]
 for attr in _urllib_parse_moved_attributes:
     setattr(Module_six_moves_urllib_parse, attr.name, attr)
 del attr
 
 Module_six_moves_urllib_parse._moved_attributes = _urllib_parse_moved_attributes
 
-_importer._add_module(Module_six_moves_urllib_parse(__name__ + ".moves.urllib_parse"),
-                      "moves.urllib_parse", "moves.urllib.parse")
+_importer._add_module(
+    Module_six_moves_urllib_parse(__name__ + ".moves.urllib_parse"),
+    "moves.urllib_parse",
+    "moves.urllib.parse",
+)
 
 
 class Module_six_moves_urllib_error(_LazyModule):
 
     """Lazy loading of moved objects in six.moves.urllib_error"""
 
 
@@ -373,16 +380,19 @@
 ]
 for attr in _urllib_error_moved_attributes:
     setattr(Module_six_moves_urllib_error, attr.name, attr)
 del attr
 
 Module_six_moves_urllib_error._moved_attributes = _urllib_error_moved_attributes
 
-_importer._add_module(Module_six_moves_urllib_error(__name__ + ".moves.urllib.error"),
-                      "moves.urllib_error", "moves.urllib.error")
+_importer._add_module(
+    Module_six_moves_urllib_error(__name__ + ".moves.urllib.error"),
+    "moves.urllib_error",
+    "moves.urllib.error",
+)
 
 
 class Module_six_moves_urllib_request(_LazyModule):
 
     """Lazy loading of moved objects in six.moves.urllib_request"""
 
 
@@ -425,16 +435,19 @@
 ]
 for attr in _urllib_request_moved_attributes:
     setattr(Module_six_moves_urllib_request, attr.name, attr)
 del attr
 
 Module_six_moves_urllib_request._moved_attributes = _urllib_request_moved_attributes
 
-_importer._add_module(Module_six_moves_urllib_request(__name__ + ".moves.urllib.request"),
-                      "moves.urllib_request", "moves.urllib.request")
+_importer._add_module(
+    Module_six_moves_urllib_request(__name__ + ".moves.urllib.request"),
+    "moves.urllib_request",
+    "moves.urllib.request",
+)
 
 
 class Module_six_moves_urllib_response(_LazyModule):
 
     """Lazy loading of moved objects in six.moves.urllib_response"""
 
 
@@ -446,51 +459,62 @@
 ]
 for attr in _urllib_response_moved_attributes:
     setattr(Module_six_moves_urllib_response, attr.name, attr)
 del attr
 
 Module_six_moves_urllib_response._moved_attributes = _urllib_response_moved_attributes
 
-_importer._add_module(Module_six_moves_urllib_response(__name__ + ".moves.urllib.response"),
-                      "moves.urllib_response", "moves.urllib.response")
+_importer._add_module(
+    Module_six_moves_urllib_response(__name__ + ".moves.urllib.response"),
+    "moves.urllib_response",
+    "moves.urllib.response",
+)
 
 
 class Module_six_moves_urllib_robotparser(_LazyModule):
 
     """Lazy loading of moved objects in six.moves.urllib_robotparser"""
 
 
 _urllib_robotparser_moved_attributes = [
-    MovedAttribute("RobotFileParser", "robotparser", "urllib.robotparser"),
+    MovedAttribute("RobotFileParser", "robotparser", "urllib.robotparser")
 ]
 for attr in _urllib_robotparser_moved_attributes:
     setattr(Module_six_moves_urllib_robotparser, attr.name, attr)
 del attr
 
-Module_six_moves_urllib_robotparser._moved_attributes = _urllib_robotparser_moved_attributes
-
-_importer._add_module(Module_six_moves_urllib_robotparser(__name__ + ".moves.urllib.robotparser"),
-                      "moves.urllib_robotparser", "moves.urllib.robotparser")
+Module_six_moves_urllib_robotparser._moved_attributes = (
+    _urllib_robotparser_moved_attributes
+)
+
+_importer._add_module(
+    Module_six_moves_urllib_robotparser(__name__ + ".moves.urllib.robotparser"),
+    "moves.urllib_robotparser",
+    "moves.urllib.robotparser",
+)
 
 
 class Module_six_moves_urllib(types.ModuleType):
 
     """Create a six.moves.urllib namespace that resembles the Python 3 namespace"""
+
     __path__ = []  # mark as package
     parse = _importer._get_module("moves.urllib_parse")
     error = _importer._get_module("moves.urllib_error")
     request = _importer._get_module("moves.urllib_request")
     response = _importer._get_module("moves.urllib_response")
     robotparser = _importer._get_module("moves.urllib_robotparser")
 
     def __dir__(self):
-        return ['parse', 'error', 'request', 'response', 'robotparser']
+        return ["parse", "error", "request", "response", "robotparser"]
+
 
-_importer._add_module(Module_six_moves_urllib(__name__ + ".moves.urllib"),
-                      "moves.urllib")
+_importer._add_module(
+    Module_six_moves_urllib(__name__ + ".moves.urllib"), "moves.urllib"
+)
 
 
 def add_move(move):
     """Add an item to six.moves."""
     setattr(_MovedItems, move.name, move)
 
 
@@ -522,65 +546,72 @@
     _func_defaults = "func_defaults"
     _func_globals = "func_globals"
 
 
 try:
     advance_iterator = next
 except NameError:
+
     def advance_iterator(it):
         return it.next()
+
+
 next = advance_iterator
 
 
 try:
     callable = callable
 except NameError:
+
     def callable(obj):
         return any("__call__" in klass.__dict__ for klass in type(obj).__mro__)
 
 
 if PY3:
+
     def get_unbound_function(unbound):
         return unbound
 
     create_bound_method = types.MethodType
 
     def create_unbound_method(func, cls):
         return func
 
     Iterator = object
 else:
+
     def get_unbound_function(unbound):
         return unbound.im_func
 
     def create_bound_method(func, obj):
         return types.MethodType(func, obj, obj.__class__)
 
     def create_unbound_method(func, cls):
         return types.MethodType(func, None, cls)
 
     class Iterator(object):
-
         def next(self):
             return type(self).__next__(self)
 
     callable = callable
-_add_doc(get_unbound_function,
-         """Get the function out of a possibly unbound function""")
+_add_doc(
+    get_unbound_function, """Get the function out of a possibly unbound function"""
+)
 
 
 get_method_function = operator.attrgetter(_meth_func)
 get_method_self = operator.attrgetter(_meth_self)
 get_function_closure = operator.attrgetter(_func_closure)
 get_function_code = operator.attrgetter(_func_code)
 get_function_defaults = operator.attrgetter(_func_defaults)
 get_function_globals = operator.attrgetter(_func_globals)
 
 
 if PY3:
+
     def iterkeys(d, **kw):
         return iter(d.keys(**kw))
 
     def itervalues(d, **kw):
         return iter(d.values(**kw))
 
     def iteritems(d, **kw):
@@ -591,14 +622,15 @@
 
     viewkeys = operator.methodcaller("keys")
 
     viewvalues = operator.methodcaller("values")
 
     viewitems = operator.methodcaller("items")
 else:
+
     def iterkeys(d, **kw):
         return d.iterkeys(**kw)
 
     def itervalues(d, **kw):
         return d.itervalues(**kw)
 
     def iteritems(d, **kw):
@@ -611,60 +643,69 @@
 
     viewvalues = operator.methodcaller("viewvalues")
 
     viewitems = operator.methodcaller("viewitems")
 
 _add_doc(iterkeys, "Return an iterator over the keys of a dictionary.")
 _add_doc(itervalues, "Return an iterator over the values of a dictionary.")
-_add_doc(iteritems,
-         "Return an iterator over the (key, value) pairs of a dictionary.")
-_add_doc(iterlists,
-         "Return an iterator over the (key, [values]) pairs of a dictionary.")
+_add_doc(iteritems, "Return an iterator over the (key, value) pairs of a dictionary.")
+_add_doc(
+    iterlists, "Return an iterator over the (key, [values]) pairs of a dictionary."
+)
 
 
 if PY3:
+
     def b(s):
         return s.encode("latin-1")
 
     def u(s):
         return s
+
     unichr = chr
     import struct
+
     int2byte = struct.Struct(">B").pack
     del struct
     byte2int = operator.itemgetter(0)
     indexbytes = operator.getitem
     iterbytes = iter
     import io
+
     StringIO = io.StringIO
     BytesIO = io.BytesIO
     _assertCountEqual = "assertCountEqual"
     if sys.version_info[1] <= 1:
         _assertRaisesRegex = "assertRaisesRegexp"
         _assertRegex = "assertRegexpMatches"
     else:
         _assertRaisesRegex = "assertRaisesRegex"
         _assertRegex = "assertRegex"
 else:
+
     def b(s):
         return s
+
     # Workaround for standalone backslash
 
     def u(s):
-        return unicode(s.replace(r'\\', r'\\\\'), "unicode_escape")
+        return unicode(s.replace(r"\\", r"\\\\"), "unicode_escape")
+
     unichr = unichr
     int2byte = chr
 
     def byte2int(bs):
         return ord(bs[0])
 
     def indexbytes(buf, i):
         return ord(buf[i])
+
     iterbytes = functools.partial(itertools.imap, ord)
     import StringIO
+
     StringIO = BytesIO = StringIO.StringIO
     _assertCountEqual = "assertItemsEqual"
     _assertRaisesRegex = "assertRaisesRegexp"
     _assertRegex = "assertRegexpMatches"
 _add_doc(b, """Byte literal""")
 _add_doc(u, """Text literal""")
 
@@ -691,76 +732,89 @@
             if value.__traceback__ is not tb:
                 raise value.with_traceback(tb)
             raise value
         finally:
             value = None
             tb = None
 
+
 else:
+
     def exec_(_code_, _globs_=None, _locs_=None):
         """Execute code in a namespace."""
         if _globs_ is None:
             frame = sys._getframe(1)
             _globs_ = frame.f_globals
             if _locs_ is None:
                 _locs_ = frame.f_locals
             del frame
         elif _locs_ is None:
             _locs_ = _globs_
         exec("""exec _code_ in _globs_, _locs_""")
 
-    exec_("""def reraise(tp, value, tb=None):
+    exec_(
+        """def reraise(tp, value, tb=None):
     try:
         raise tp, value, tb
     finally:
         tb = None
-""")
+"""
+    )
 
 
 if sys.version_info[:2] == (3, 2):
-    exec_("""def raise_from(value, from_value):
+    exec_(
+        """def raise_from(value, from_value):
     try:
         if from_value is None:
             raise value
         raise value from from_value
     finally:
         value = None
-""")
+"""
+    )
 elif sys.version_info[:2] > (3, 2):
-    exec_("""def raise_from(value, from_value):
+    exec_(
+        """def raise_from(value, from_value):
     try:
         raise value from from_value
     finally:
         value = None
-""")
+"""
+    )
 else:
+
     def raise_from(value, from_value):
         raise value
 
 
 print_ = getattr(moves.builtins, "print", None)
 if print_ is None:
+
     def print_(*args, **kwargs):
         """The new-style print function for Python 2.4 and 2.5."""
         fp = kwargs.pop("file", sys.stdout)
         if fp is None:
             return
 
         def write(data):
             if not isinstance(data, basestring):
                 data = str(data)
             # If the file has an encoding, encode unicode with it.
-            if (isinstance(fp, file) and
-                    isinstance(data, unicode) and
-                    fp.encoding is not None):
+            if (
+                isinstance(fp, file)
+                and isinstance(data, unicode)
+                and fp.encoding is not None
+            ):
                 errors = getattr(fp, "errors", None)
                 if errors is None:
                     errors = "strict"
                 data = data.encode(fp.encoding, errors)
             fp.write(data)
+
         want_unicode = False
         sep = kwargs.pop("sep", None)
         if sep is not None:
             if isinstance(sep, unicode):
                 want_unicode = True
             elif not isinstance(sep, str):
                 raise TypeError("sep must be None or a string")
@@ -788,85 +842,98 @@
         if end is None:
             end = newline
         for i, arg in enumerate(args):
             if i:
                 write(sep)
             write(arg)
         write(end)
+
+
 if sys.version_info[:2] < (3, 3):
     _print = print_
 
     def print_(*args, **kwargs):
         fp = kwargs.get("file", sys.stdout)
         flush = kwargs.pop("flush", False)
         _print(*args, **kwargs)
         if flush and fp is not None:
             fp.flush()
 
+
 _add_doc(reraise, """Reraise an exception.""")
 
 if sys.version_info[0:2] < (3, 4):
-    def wraps(wrapped, assigned=functools.WRAPPER_ASSIGNMENTS,
-              updated=functools.WRAPPER_UPDATES):
+
+    def wraps(
+        wrapped,
+        assigned=functools.WRAPPER_ASSIGNMENTS,
+        updated=functools.WRAPPER_UPDATES,
+    ):
         def wrapper(f):
             f = functools.wraps(wrapped, assigned, updated)(f)
             f.__wrapped__ = wrapped
             return f
+
         return wrapper
+
+
 else:
     wraps = functools.wraps
 
 
 def with_metaclass(meta, *bases):
     """Create a base class with a metaclass."""
     # This requires a bit of explanation: the basic idea is to make a dummy
     # metaclass for one level of class instantiation that replaces itself with
     # the actual metaclass.
     class metaclass(type):
-
         def __new__(cls, name, this_bases, d):
             return meta(name, bases, d)
 
         @classmethod
         def __prepare__(cls, name, this_bases):
             return meta.__prepare__(name, bases)
-    return type.__new__(metaclass, 'temporary_class', (), {})
+
+    return type.__new__(metaclass, "temporary_class", (), {})
 
 
 def add_metaclass(metaclass):
     """Class decorator for creating a class with a metaclass."""
+
     def wrapper(cls):
         orig_vars = cls.__dict__.copy()
-        slots = orig_vars.get('__slots__')
+        slots = orig_vars.get("__slots__")
         if slots is not None:
             if isinstance(slots, str):
                 slots = [slots]
             for slots_var in slots:
                 orig_vars.pop(slots_var)
-        orig_vars.pop('__dict__', None)
-        orig_vars.pop('__weakref__', None)
+        orig_vars.pop("__dict__", None)
+        orig_vars.pop("__weakref__", None)
         return metaclass(cls.__name__, cls.__bases__, orig_vars)
+
     return wrapper
 
 
 def python_2_unicode_compatible(klass):
     """
     A decorator that defines __unicode__ and __str__ methods under Python 2.
     Under Python 3 it does nothing.
 
     To support Python 2 and 3 with a single code base, define a __str__ method
     returning text and apply this decorator to the class.
     """
     if PY2:
-        if '__str__' not in klass.__dict__:
-            raise ValueError("@python_2_unicode_compatible cannot be applied "
-                             "to %s because it doesn't define __str__()." %
-                             klass.__name__)
+        if "__str__" not in klass.__dict__:
+            raise ValueError(
+                "@python_2_unicode_compatible cannot be applied "
+                "to %s because it doesn't define __str__()." % klass.__name__
+            )
         klass.__unicode__ = klass.__str__
-        klass.__str__ = lambda self: self.__unicode__().encode('utf-8')
+        klass.__str__ = lambda self: self.__unicode__().encode("utf-8")
     return klass
 
 
 # Complete the moves implementation.
 # This code is at the end of this module to speed up module loading.
 # Turn this module into a package.
 __path__ = []  # required for PEP 302 and PEP 451
@@ -878,14 +945,16 @@
 # this for some reason.)
 if sys.meta_path:
     for i, importer in enumerate(sys.meta_path):
         # Here's some real nastiness: Another "instance" of the six module might
         # be floating around. Therefore, we can't use isinstance() to check for
         # the six meta path importer, since the other six instance will have
         # inserted an importer with different class.
-        if (type(importer).__name__ == "_SixMetaPathImporter" and
-                importer.name == __name__):
+        if (
+            type(importer).__name__ == "_SixMetaPathImporter"
+            and importer.name == __name__
+        ):
             del sys.meta_path[i]
             break
     del i, importer
 # Finally, add the importer to the meta path import hook.
 sys.meta_path.append(_importer)
```

### Comparing `pgmpy-0.1.7/setup.py` & `pgmpy-0.1.9/setup.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,31 +1,29 @@
 #!/usr/bin/env python3
 
 from setuptools import setup, find_packages
 
+import pgmpy
+
 setup(
     name="pgmpy",
-    version="0.1.7",
+    version=pgmpy.__version__,
     description="A library for Probabilistic Graphical Models",
-    packages=find_packages(exclude=['tests']),
+    packages=find_packages(exclude=["tests"]),
     author="Ankur Ankan",
     author_email="ankurankan@gmail.com",
     url="https://github.com/pgmpy/pgmpy",
     license="MIT",
     classifiers=[
         "Programming Language :: Python :: 3.5",
-        "Programming Language :: Python :: 3.4",
-        "Programming Language :: Python :: 2.7",
+        "Programming Language :: Python :: 3.6",
+        "Programming Language :: Python :: 3.7",
         "Intended Audience :: Developers",
         "Operating System :: Unix",
         "Operating System :: POSIX",
         "Operating System :: Microsoft :: Windows",
         "Operating System :: MacOS",
-        "Topic :: Scientific/Engineering"
+        "Topic :: Scientific/Engineering",
     ],
     long_description="https://github.com/pgmpy/pgmpy/blob/dev/README.md",
-    install_requires=[
-        "networkx >= 1.11, <1.12",
-        "scipy >= 1.0.0",
-        "numpy >= 1.14.0",
-    ],
+    install_requires=[],
 )
```

### Comparing `pgmpy-0.1.7/README.md` & `pgmpy-0.1.9/README.md`

 * *Files 11% similar despite different names*

```diff
@@ -1,9 +1,7 @@
-## Please make sure that the you are using the exact versions of dependencies as mentioned in requirements.txt. 
-
 pgmpy
 =====
 [![Build Status](https://travis-ci.org/pgmpy/pgmpy.svg?style=flat)](https://travis-ci.org/pgmpy/pgmpy)
 [![Appveyor](https://ci.appveyor.com/api/projects/status/github/pgmpy/pgmpy?branch=dev)](https://www.appveyor.com/)
 [![codecov](https://codecov.io/gh/pgmpy/pgmpy/branch/dev/graph/badge.svg)](https://codecov.io/gh/pgmpy/pgmpy)
 [![Code Health](https://landscape.io/github/pgmpy/pgmpy/dev/landscape.svg?style=flat)](https://landscape.io/github/pgmpy/pgmpy/dev)
 [![Join the chat at https://gitter.im/pgmpy/pgmpy](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/pgmpy/pgmpy?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)
@@ -17,41 +15,42 @@
 Our mailing list is at https://groups.google.com/forum/#!forum/pgmpy .
 
 We have our community chat at [gitter](https://gitter.im/pgmpy/pgmpy).
 
 Dependencies
 =============
 pgmpy has following non optional dependencies:
-- Python 2.7 or Python 3
-- NetworkX 1.11 
-- Scipy 0.18.0 
-- Numpy 1.11.1 
-- Pandas 0.18.1 
+- Python 3.4 or higher
+- NetworkX 2.x
+- Scipy 
+- Numpy
+- Pandas
 
 Installation
 =============
+The preferred way to install pgmpy is through the `dev` brach:
+```
+$ git clone https://github.com/pgmpy/pgmpy 
+$ cd pgmpy/
+$ pip install -r requirements.txt
+$ python setup.py install
+```
+
+pgmpy can also be installed using conda or pip. 
 Using conda:
 ```
 $ conda install -c ankurankan pgmpy
 ```
 
 Using pip:
 ```
 $ pip install -r requirements.txt  # or requirements-dev.txt if you want to run unittests
 $ pip install pgmpy
 ```
 
-Or for installing the latest codebase:
-```
-$ git clone https://github.com/pgmpy/pgmpy 
-$ cd pgmpy/
-$ pip install -r requirements.txt
-$ python setup.py install
-```
-
 If you face any problems during installation let us know, via issues, mail or at our gitter channel.
 
 Development
 ============
 
 Code
 ----
@@ -102,11 +101,24 @@
 Then the docs will be in _build/html
 
 Examples:
 =========
 We have a few example jupyter notebooks here: https://github.com/pgmpy/pgmpy/tree/dev/examples
 For more detailed jupyter notebooks and basic tutorials on Graphical Models check: https://github.com/pgmpy/pgmpy_notebook/
 
+Citing:
+=======
+Please use the following bibtex for citing `pgmpy` in your research:
+```
+@inproceedings{ankan2015pgmpy,
+  title={pgmpy: Probabilistic graphical models using python},
+  author={Ankan, Ankur and Panda, Abinash},
+  booktitle={Proceedings of the 14th Python in Science Conference (SCIPY 2015)},
+  year={2015},
+  organization={Citeseer}
+}
+```
+
 License
 =======
 pgmpy is released under MIT License. You can read about our license at [here](https://github.com/pgmpy/pgmpy/blob/dev/LICENSE)
```

### Comparing `pgmpy-0.1.7/pgmpy.egg-info/SOURCES.txt` & `pgmpy-0.1.9/pgmpy.egg-info/SOURCES.txt`

 * *Files 3% similar despite different names*

```diff
@@ -1,27 +1,31 @@
 README.md
 setup.py
-pgmpy/Dependencies.py
 pgmpy/__init__.py
+pgmpy/global_vars.py
 pgmpy.egg-info/PKG-INFO
 pgmpy.egg-info/SOURCES.txt
 pgmpy.egg-info/dependency_links.txt
-pgmpy.egg-info/requires.txt
 pgmpy.egg-info/top_level.txt
-pgmpy/base/DirectedGraph.py
+pgmpy/base/DAG.py
 pgmpy/base/UndirectedGraph.py
 pgmpy/base/__init__.py
+pgmpy/data/Data.py
+pgmpy/data/__init__.py
 pgmpy/estimators/BayesianEstimator.py
 pgmpy/estimators/BdeuScore.py
 pgmpy/estimators/BicScore.py
+pgmpy/estimators/CITests.py
 pgmpy/estimators/ConstraintBasedEstimator.py
 pgmpy/estimators/ExhaustiveSearch.py
 pgmpy/estimators/HillClimbSearch.py
 pgmpy/estimators/K2Score.py
+pgmpy/estimators/LinearModel.py
 pgmpy/estimators/MLE.py
+pgmpy/estimators/SEMEstimator.py
 pgmpy/estimators/ScoreCache.py
 pgmpy/estimators/StructureScore.py
 pgmpy/estimators/__init__.py
 pgmpy/estimators/base.py
 pgmpy/extern/__init__.py
 pgmpy/extern/six.py
 pgmpy/extern/tabulate.py
@@ -39,14 +43,15 @@
 pgmpy/factors/distributions/CanonicalDistribution.py
 pgmpy/factors/distributions/CustomDistribution.py
 pgmpy/factors/distributions/GaussianDistribution.py
 pgmpy/factors/distributions/__init__.py
 pgmpy/factors/distributions/base.py
 pgmpy/independencies/Independencies.py
 pgmpy/independencies/__init__.py
+pgmpy/inference/CausalInference.py
 pgmpy/inference/EliminationOrder.py
 pgmpy/inference/ExactInference.py
 pgmpy/inference/__init__.py
 pgmpy/inference/base.py
 pgmpy/inference/dbn_inference.py
 pgmpy/inference/mplp.py
 pgmpy/models/BayesianModel.py
@@ -55,14 +60,15 @@
 pgmpy/models/FactorGraph.py
 pgmpy/models/JunctionTree.py
 pgmpy/models/LinearGaussianBayesianNetwork.py
 pgmpy/models/MarkovChain.py
 pgmpy/models/MarkovModel.py
 pgmpy/models/NaiveBayes.py
 pgmpy/models/NoisyOrModel.py
+pgmpy/models/SEM.py
 pgmpy/models/__init__.py
 pgmpy/readwrite/BIF.py
 pgmpy/readwrite/PomdpX.py
 pgmpy/readwrite/ProbModelXML.py
 pgmpy/readwrite/UAI.py
 pgmpy/readwrite/XMLBIF.py
 pgmpy/readwrite/XMLBeliefNetwork.py
@@ -71,41 +77,44 @@
 pgmpy/sampling/NUTS.py
 pgmpy/sampling/Sampling.py
 pgmpy/sampling/__init__.py
 pgmpy/sampling/base.py
 pgmpy/tests/__init__.py
 pgmpy/tests/help_functions.py
 pgmpy/tests/test_base/__init__.py
-pgmpy/tests/test_base/test_DirectedGraph.py
+pgmpy/tests/test_base/test_DAG.py
 pgmpy/tests/test_base/test_UndirectedGraph.py
 pgmpy/tests/test_estimators/__init__.py
 pgmpy/tests/test_estimators/test_BaseEstimator.py
 pgmpy/tests/test_estimators/test_BayesianEstimator.py
 pgmpy/tests/test_estimators/test_BdeuScore.py
 pgmpy/tests/test_estimators/test_BicScore.py
+pgmpy/tests/test_estimators/test_CITests.py
 pgmpy/tests/test_estimators/test_ConstraintBasedEstimator.py
 pgmpy/tests/test_estimators/test_ExhaustiveSearch.py
 pgmpy/tests/test_estimators/test_HillClimbSearch.py
 pgmpy/tests/test_estimators/test_K2Score.py
 pgmpy/tests/test_estimators/test_MaximumLikelihoodEstimator.py
 pgmpy/tests/test_estimators/test_ParameterEstimator.py
+pgmpy/tests/test_estimators/test_SEMEstimator.py
 pgmpy/tests/test_estimators/test_ScoreCache.py
 pgmpy/tests/test_factors/__init__.py
 pgmpy/tests/test_factors/test_FactorSet.py
 pgmpy/tests/test_factors/test_continuous/__init__.py
 pgmpy/tests/test_factors/test_continuous/test_Canonical_Factor.py
 pgmpy/tests/test_factors/test_continuous/test_ContinuousFactor.py
 pgmpy/tests/test_factors/test_continuous/test_CustomDistribution.py
 pgmpy/tests/test_factors/test_continuous/test_JointGaussianDistribution.py
 pgmpy/tests/test_factors/test_continuous/test_Linear_Gaussain_CPD.py
 pgmpy/tests/test_factors/test_discrete/__init__.py
 pgmpy/tests/test_factors/test_discrete/test_Factor.py
 pgmpy/tests/test_independencies/__init__.py
 pgmpy/tests/test_independencies/test_Independencies.py
 pgmpy/tests/test_inference/__init__.py
+pgmpy/tests/test_inference/test_CausalInference.py
 pgmpy/tests/test_inference/test_ExactInference.py
 pgmpy/tests/test_inference/test_Inference.py
 pgmpy/tests/test_inference/test_Mplp.py
 pgmpy/tests/test_inference/test_dbn_inference.py
 pgmpy/tests/test_inference/test_elimination_order.py
 pgmpy/tests/test_models/__init__.py
 pgmpy/tests/test_models/test_BayesianModel.py
@@ -114,22 +123,26 @@
 pgmpy/tests/test_models/test_FactorGraph.py
 pgmpy/tests/test_models/test_JunctionTree.py
 pgmpy/tests/test_models/test_LinearGaussianBayesianNetwork.py
 pgmpy/tests/test_models/test_MarkovChain.py
 pgmpy/tests/test_models/test_MarkovModel.py
 pgmpy/tests/test_models/test_NaiveBayes.py
 pgmpy/tests/test_models/test_NoisyOrModels.py
+pgmpy/tests/test_models/test_SEM.py
 pgmpy/tests/test_readwrite/__init__.py
 pgmpy/tests/test_readwrite/test_BIF.py
 pgmpy/tests/test_readwrite/test_PomdpX.py
 pgmpy/tests/test_readwrite/test_ProbModelXML.py
 pgmpy/tests/test_readwrite/test_UAI.py
 pgmpy/tests/test_readwrite/test_XMLBIF.py
 pgmpy/tests/test_readwrite/test_XMLBeliefNetwork.py
 pgmpy/tests/test_sampling/__init__.py
 pgmpy/tests/test_sampling/test_Sampling.py
 pgmpy/tests/test_sampling/test_base_continuous.py
 pgmpy/tests/test_sampling/test_continuous_sampling.py
 pgmpy/utils/__init__.py
 pgmpy/utils/check_functions.py
+pgmpy/utils/decorators.py
 pgmpy/utils/mathext.py
+pgmpy/utils/optimizer.py
+pgmpy/utils/sets.py
 pgmpy/utils/state_name.py
```

### Comparing `pgmpy-0.1.7/pgmpy.egg-info/PKG-INFO` & `pgmpy-0.1.9/pgmpy.egg-info/PKG-INFO`

 * *Files 18% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 Metadata-Version: 1.1
 Name: pgmpy
-Version: 0.1.7
+Version: 0.1.9
 Summary: A library for Probabilistic Graphical Models
 Home-page: https://github.com/pgmpy/pgmpy
 Author: Ankur Ankan
 Author-email: ankurankan@gmail.com
 License: MIT
 Description: https://github.com/pgmpy/pgmpy/blob/dev/README.md
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3.5
-Classifier: Programming Language :: Python :: 3.4
-Classifier: Programming Language :: Python :: 2.7
+Classifier: Programming Language :: Python :: 3.6
+Classifier: Programming Language :: Python :: 3.7
 Classifier: Intended Audience :: Developers
 Classifier: Operating System :: Unix
 Classifier: Operating System :: POSIX
 Classifier: Operating System :: Microsoft :: Windows
 Classifier: Operating System :: MacOS
 Classifier: Topic :: Scientific/Engineering
```

### Comparing `pgmpy-0.1.7/PKG-INFO` & `pgmpy-0.1.9/PKG-INFO`

 * *Files 18% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 Metadata-Version: 1.1
 Name: pgmpy
-Version: 0.1.7
+Version: 0.1.9
 Summary: A library for Probabilistic Graphical Models
 Home-page: https://github.com/pgmpy/pgmpy
 Author: Ankur Ankan
 Author-email: ankurankan@gmail.com
 License: MIT
 Description: https://github.com/pgmpy/pgmpy/blob/dev/README.md
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3.5
-Classifier: Programming Language :: Python :: 3.4
-Classifier: Programming Language :: Python :: 2.7
+Classifier: Programming Language :: Python :: 3.6
+Classifier: Programming Language :: Python :: 3.7
 Classifier: Intended Audience :: Developers
 Classifier: Operating System :: Unix
 Classifier: Operating System :: POSIX
 Classifier: Operating System :: Microsoft :: Windows
 Classifier: Operating System :: MacOS
 Classifier: Topic :: Scientific/Engineering
```

